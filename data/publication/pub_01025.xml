<?xml version="1.0" encoding='UTF-8'?>
<results algorithmName="GS" algorithmVersion="101001" time="9:00" date="2013">
<query content="author:&quot;Wang Ye&quot; NUS" label="" count="78" totalresults="78">
  <result id="0">
    <url>http://dl.acm.org/citation.cfm?id=1027576</url>
    <title status="complete" source="scholar.google.com">LyricAlly: automatic synchronization of acoustic musical signals and textual lyrics</title>
    <pdf>http://www.comp.nus.edu/~kanmy/dossier/papers/p1568934817-wang.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Wang, MY Kan, TL Nwe, A Shenoy, J Yin</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th &#8230;</proceeding>
    <year>2004</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Signals and Textual Lyrics Ye Wang Min-Yen Kan Tin Lay Nwe Arun Shenoy Jun Yin Departmentof Computer Science, School of Computing National University of Singapore, Singapore 177543 ++(65) 6874-2980 {wangye, kanmy, nwetl, arunshen, yinjun}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:gj3dVP-I65UJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>36</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=10802878761400089986&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>61</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=10802878761400089986&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="61">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4156205</url>
        <title status="complete" source="scholar.google.com">Separation of singing voice from music accompaniment for monaural recordings</title>
        <pdf>ftp://ftp.cse.ohio-state.edu/pub/tech-report/2005/TR61.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Li, DL Wang</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language Processing, &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Separating singing voice from music accompaniment is very useful in many applications, such as lyrics recognition and alignment, singer identification, and music information retrieval. Although speech separation has been extensively studied for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PJ4gQrBmOuoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16877915460734066236&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>74</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16877915460734066236&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4061176</url>
        <title status="complete" source="scholar.google.com">Automatic synchronization between lyrics and music CD recordings based on Viterbi alignment of segregated vocal signals</title>
        <pdf>http://staff.aist.go.jp/h.fujihara/pdf/ism2006_fujihara.pdf</pdf>
        <author status="partial" source="scholar.google.com">H Fujihara, M Goto, J Ogata, K Komatani&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; , 2006. ISM&#39;06. &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper describes a system that can automatically synchronize between polyphonic musical audio signals and corresponding lyrics. Although there were methods that can synchronize between monophonic speech signals and corresponding text ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0y_jQcZGvsAJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13888616118180065235&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>50</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13888616118180065235&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432643</url>
        <title status="complete" source="scholar.google.com">LyricAlly: Automatic synchronization of textual lyrics to acoustic music signals</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.119.3200&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="partial" source="scholar.google.com">MY Kan, Y Wang, D Iskandar, TL New&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present LyricAlly, a prototype that automatically aligns acoustic musical signals with their corresponding textual lyrics, in a manner similar to manually-aligned karaoke. We tackle this problem based on a multimodal approach, using an appropriate pairing of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:UKV6kRlYEmEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6994750038097962320&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>27</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6994750038097962320&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ismir2007.ismir.net/proceedings/ismir2007_p369_gruhne.pdf</url>
        <title status="complete" source="scholar.google.com">Phoneme recognition in popular music</title>
        <pdf>http://ismir2007.ismir.net/proceedings/ismir2007_p369_gruhne.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Gruhne, K Schmidt, C Dittmar</author>
        <proceeding status="complete" source="scholar.google.com">Proc. ISMIR 2007</proceeding>
        <year>2007</year>
        <source>ismir2007.ismir.net</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Automatic lyrics synchronization for karaoke applications is a major challenge in the field of music information retrieval. An important pre-requisite in order to precisely synchronize the music and corresponding text is the detection of single phonemes in the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gQzTOKQnWMYJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:gQzTOKQnWMYJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14292217003741744257&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>22</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14292217003741744257&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://dl.acm.org/citation.cfm?id=1112857</url>
        <title status="complete" source="scholar.google.com">Key, chord, and rhythm tracking of popular music recordings</title>
        <author status="complete" source="scholar.google.com">A Shenoy, Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">Computer Music Journal</proceeding>
        <year>2005</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">In this article, we propose a framework to analyze a musical audio signal (sampled from a popular music CD) and determine its key, provide usable chord transcriptions, and obtain the hierarchical rhythm structure representation comprising the quarternote, half-note, and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:33p2o7nmEuIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16290336487138294495&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>22</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16290336487138294495&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://dl.acm.org/citation.cfm?id=1180777</url>
        <title status="complete" source="scholar.google.com">Syllabic level automatic synchronization of music signals and text lyrics</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2006_Syllabic_Level_Automatic_Synchronization_of_Music_Signals_and_Text_Lyrics.pdf</pdf>
        <author status="complete" source="scholar.google.com">D Iskandar, Y Wang, MY Kan, H Li</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 14th annual ACM &#8230;</proceeding>
        <year>2006</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present a framework to synchronize pop music to corresponding text lyric. We refine line level alignment achievable by existing work to syllabic level by using a dynamic programming process. Our main contribution is using music knowledge to constrain the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bHstnjZhhukJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>23</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16827243944926346092&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>21</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16827243944926346092&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://www.springerlink.com/index/0PK181480J557877.pdf</url>
        <title status="complete" source="scholar.google.com">Lyrics-based audio retrieval and multimodal navigation in music collections</title>
        <pdf>https://mpi-inf.mpg.de/~mmueller/publications/2007_MuellerKuDaFrCl_LyricsBasedAudioRetrieval_ECDL.pdf</pdf>
        <author status="partial" source="scholar.google.com">M MÃ¼ller, F Kurth, D Damm, C Fremerey&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Research and Advanced &#8230;</proceeding>
        <year>2007</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Modern digital music libraries contain textual, visual, and audio data describing music on various semantic levels. Exploiting the availability of different semantically interrelated representations for a piece of music, this paper presents a query-by-lyrics retrieval system ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:MB441jj8QTIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3621452896424959536&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>23</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3621452896424959536&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4517548</url>
        <title status="partial" source="scholar.google.com">Three techniques for improving automatic synchronization between music and lyrics: Fricative detection, filler model, and novel feature vectors for vocal activity &#8230;</title>
        <pdf>http://staff.aist.go.jp/h.fujihara/pdf/icassp2008_fujihara.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Fujihara, M Goto</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and Signal Processing, &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Three techniques are described that improve a previously developed system for automatically synchronizing lyrics with musical audio signals. Although this system achieves state-of-the-art accuracy by extracting vocal vowels from polyphonic sound mixtures and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:hb3gWhwIylkJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6469992732547923333&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>19</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6469992732547923333&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://www.springerlink.com/index/j1h044n4v1767603.pdf</url>
        <title status="complete" source="scholar.google.com">Automatic lyrics alignment for Cantonese popular music</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.1526&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">CH Wong, WM Szeto, KH Wong</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Systems</proceeding>
        <year>2007</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract From lyrics-display on electronic music players and Karaoke videos to surtitles for live Chinese opera performance, one feature is common to all these everyday functionalities temporal: synchronization of the written text and its corresponding musical phrase. Our ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8ee-SWoY3QoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=782808755015182321&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>16</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=782808755015182321&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://dl.acm.org/citation.cfm?id=1255192</url>
        <title status="complete" source="scholar.google.com">SlideSeer: A digital library of aligned document and presentation pairs</title>
        <pdf>http://www.comp.nus.edu/~kanmy/dossier/papers/jcdl2007.pdf</pdf>
        <author status="complete" source="scholar.google.com">MY Kan</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 7th ACM/IEEE-CS joint conference &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Research findings are often transmitted both as written documents and narrated slide presentations. As these two forms of media contain both unique and replicated information, it is useful to combine and align these two views to create a single ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:A8QPYt41ABIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>14</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1297095921926915075&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>14</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1297095921926915075&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://dl.acm.org/citation.cfm?id=1255085</url>
        <title status="complete" source="scholar.google.com">A voice-to-MIDI system for singing melodies with lyrics</title>
        <author status="complete" source="scholar.google.com">N Itou, K Nishimoto</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international conference on &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we propose a robust Voice-to-MIDI (V to M) system with which a user can input MIDI sequence data by naturally singing melodies with lyrics. A Voice-to-MIDI system translates singing voices into digital musical data, ie, MIDI sequence data. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:U9mpmMHQR3kJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8739183131844270419&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>16</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8739183131844270419&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>https://mpi-sb.mpg.de/~mmueller/publications/2005_KurthMuDaFrRiCl_SyncPlayer_ISMIR.pdf</url>
        <title status="complete" source="scholar.google.com">Syncplayer-an advanced system for multimodal music access</title>
        <pdf>https://mpi-sb.mpg.de/~mmueller/publications/2005_KurthMuDaFrRiCl_SyncPlayer_ISMIR.pdf</pdf>
        <author status="partial" source="scholar.google.com">F Kurth, M MÃ¼ller, D Damm, C Fremerey&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">ISMIR, London, &#8230;</proceeding>
        <year>2005</year>
        <source>mpi-sb.mpg.de</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In this paper, we present the SyncPlayer system for multimodal presentation of high quality audio and associated music-related data. Using the SyncPlayer client interface, a user may play back an audio recording that is locally available on his computer. The ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:TRv6q6FdZOQJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:TRv6q6FdZOQJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>15</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16457381887275047757&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>13</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16457381887275047757&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://books.google.com/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA395&amp;ots=oDOSrGfCd1&amp;sig=kPsNwxZsTSsxgP0kEeqzbIr60so</url>
        <title status="complete" source="scholar.google.com">Segmentation-based lyrics-audio alignment using dynamic programming</title>
        <pdf>http://ismir2008.ismir.net/papers/ISMIR2008_126.pdf..</pdf>
        <author status="complete" source="scholar.google.com">K Lee, M Cremer</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the 9th International Conference on Music &#8230;</proceeding>
        <year>2008</year>
        <source>books.google.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In this paper, we present a system for automatic alignment of textual lyrics with musical audio. Given an input audio signal, structural segmentation is ï¬rst performed and similar segments are assigned a label by computing the distance between the segment ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WHVqdJo9770J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13686225526189618520&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>14</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13686225526189618520&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255</url>
        <title status="complete" source="scholar.google.com">Singing voice detection for karaoke application</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Shenoy, Y Wu, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Visual &#8230;</proceeding>
        <year>2005</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">abstract We present a framework to detect the regions of singing voice in musical audio signals. This work is oriented towards the development of a robust transcriber of lyrics for karaoke applications. The technique leverages on a combination of low-level audio ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://dl.acm.org/citation.cfm?id=1101293</url>
        <title status="complete" source="scholar.google.com">Multimodal content-based structure analysis of karaoke music</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.8423&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Zhu, K Chen, Q Sun</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 13th annual ACM &#8230;</proceeding>
        <year>2005</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a novel approach for content-based analysis of karaoke music, which utilizes multimodal contents including synchronized lyrics text from the video channel and original singing audio as well as accompaniment audio in the two audio channels. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:lppaUu_9FH4J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>14</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9085165552633813654&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>12</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9085165552633813654&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://books.google.com/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA281&amp;ots=oDOSrGfCd1&amp;sig=IaRH4uny3di1f5dRCw4wP1Ual-k</url>
        <title status="complete" source="scholar.google.com">Hyperlinking lyrics: a method for creating hyperlinks between phrases in song lyrics</title>
        <pdf>http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202008/papers/ISMIR2008_111.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Fujihara, M Goto, J Ogata</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the International &#8230;</proceeding>
        <year>2008</year>
        <source>books.google.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We describe a novel method for creating a hyperlink from a phrase in the lyrics of a song to the same phrase in the lyrics of another song. This method can be applied to various applications, such as song clustering based on the meaning of the lyrics and a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:syQ-h36txwIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=200319467387757747&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=200319467387757747&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.212.6683&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Automatic alignment of music audio and lyrics</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.212.6683&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">A Mesaros, T Virtanen</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 11th Int. Conference on Digital &#8230;</proceeding>
        <year>2008</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper proposes an algorithm for aligning singing in polyphonic music audio with textual lyrics. As preprocessing, the system uses a voice separation algorithm based on melody transcription and sinusoidal modeling. The alignment is based on a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:DAm0vJTculUJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:DAm0vJTculUJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6177492370271242508&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>10</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6177492370271242508&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://www.springerlink.com/index/54X3MW16X062VK68.pdf</url>
        <title status="complete" source="scholar.google.com">A digital library framework for heterogeneous music collections: from document acquisition to cross-modal interaction</title>
        <author status="partial" source="scholar.google.com">D Damm, C Fremerey, V Thomas, M Clausen&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">International Journal on &#8230;</proceeding>
        <year>2012</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we present a digital library system for managing heterogeneous music collections. The heterogeneity refers to various document types and formats as well as to different modalities, eg, CD-audio recordings, scanned sheet music, and lyrics. The ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:eIiyjClYerEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12788631027349358712&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>10</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12788631027349358712&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://www.springerlink.com/index/018G51633707W350.pdf</url>
        <title status="complete" source="scholar.google.com">Towards reliable partial music alignments using multiple synchronization strategies</title>
        <pdf>https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/fff4f54111fe8e56c12567530068624e/91006d6f98299d09c125767b002b9920/$FILE/2009_EwertMuellerDannenberg_ReliableAlignments_AMR.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Ewert, M MÃ¼ller, R Dannenberg</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Media and Adapting to the User</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">The general goal of music synchronization is to align multiple information sources related to a given piece of music. This becomes a hard problem when the various representations to be aligned reveal significant differences not only in tempo, instrumentation, or dynamics ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:u-KJypk7CZAJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10378892347966087867&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>9</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10378892347966087867&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5876296</url>
        <title status="complete" source="scholar.google.com">Lyricsynchronizer: Automatic synchronization system between musical audio signals and lyrics</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/IEEEJSTSP201110fujihara.pdf</pdf>
        <author status="partial" source="scholar.google.com">H Fujihara, M Goto, J Ogata&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Selected Topics in Signal &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper describes a system that can automatically synchronize polyphonic musical audio signals with their corresponding lyrics. Although methods for synchronizing monophonic speech signals and corresponding text transcriptions by using Viterbi ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:1VQohGZUJiAJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2316631857609331925&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2316631857609331925&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.2653&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Relationships between lyrics and melody in popular music</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.2653&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">E Nichols, D Morris, S Basu, C Raphael</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 10&#39;th &#8230;</proceeding>
        <year>2009</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Composers of popular music weave lyrics, melody, and instrumentation together to create a consistent and compelling emotional scene. The relationships among these elements are critical to musical communication, and understanding the statistics ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:F1D2-7Fe3twJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:F1D2-7Fe3twJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>22</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15915262251703357463&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15915262251703357463&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>http://www.springerlink.com/index/Q8205NH13L1RH912.pdf</url>
        <title status="complete" source="scholar.google.com">Effectiveness of signal segmentation for music content representation</title>
        <author status="complete" source="scholar.google.com">N Maddage, M Kankanhalli, H Li</author>
        <proceeding status="complete" source="scholar.google.com">Advances in Multimedia Modeling</proceeding>
        <year>2008</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">In this paper we compare the effectiveness of rhythm based signal segmentation technique with the traditional fixed length segmentation for music contents representation. We consider vocal regions, instrumental regions and chords which represent the harmony as different ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:u3oNKau615IJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10581131093821192891&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10581131093821192891&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://www.matthiasmauch.net/_pdf/mauch_laa_2010.pdf</url>
        <title status="complete" source="scholar.google.com">Lyrics-to-audio alignment and phrase-level segmentation using incomplete internet-style chord annotations</title>
        <pdf>http://www.matthiasmauch.net/_pdf/mauch_laa_2010.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Mauch, H Fujihara, M Goto</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the 7th Sound and Music &#8230;</proceeding>
        <year>2010</year>
        <source>matthiasmauch.net</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We propose two novel lyrics-to-audio alignment methods which make use of additional chord information. In the first method we extend an existing hidden Markov model (HMM) for lyrics alignment [1] by adding a chord model based on the chroma features ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2CXce0GZO6YJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:2CXce0GZO6YJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11978336140451915224&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11978336140451915224&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="23">
        <url>http://www.springerlink.com/index/d014803786723k32.pdf</url>
        <title status="complete" source="scholar.google.com">Refinement strategies for music synchronization</title>
        <pdf>http://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/b9ef55817af82095c125675300686247/2c78ab21924a46ddc125753f0065baa3/$FILE/2008_EwertMueller_RefinementStrategies_CMMR.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Ewert, M MÃ¼ller</author>
        <proceeding status="partial" source="scholar.google.com">Computer Music Modeling and Retrieval. Genesis of &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">For a single musical work, there often exists a large number of relevant digital documents including various audio recordings, MIDI files, or digitized sheet music. The general goal of music synchronization is to automatically align the multiple information sources related to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mEUvqyHQ_scJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>15</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14411184700656666008&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14411184700656666008&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="24">
        <url>http://proceedings.spiedigitallibrary.org/data/Conferences/SPIEP/17645/682004_1.pdf</url>
        <title status="complete" source="scholar.google.com">Enriching text with images and colored light</title>
        <pdf>http://144.206.159.178/FT/CONF/16408942/16408945.pdf</pdf>
        <author status="partial" source="scholar.google.com">D Sekulovski, G Geleijnse&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the is&amp;t/spie &#8230;</proceeding>
        <year>2008</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We present an unsupervised method to enrich textual applications with relevant images and colors. The images are collected by querying large image repositories and subsequently the colors are computed using image processing. A prototype system based ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:cCokTHsdtWsJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7761141948238539376&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7761141948238539376&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="25">
        <url>http://www.ingentaconnect.com/content/maney/isr/2010/00000035/00000002/art00004</url>
        <title status="complete" source="scholar.google.com">A multimodal way of experiencing and exploring music</title>
        <pdf>https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/93832a04987390a3c12567530068622d/b991405b573fcbb5c125775400566182/$FILE/2010_MuellerClausenKonzEwertFremerey_MusicSynchronization_ISR.pdf</pdf>
        <author status="partial" source="scholar.google.com">M MÃ¼LLER, V KOnZ, M CLAusEn&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Interdisciplinary &#8230;</proceeding>
        <year>2010</year>
        <source>ingentaconnect.com</source>
        <snippet status="partial" source="scholar.google.com">Abstract: Significant digitization efforts have resulted in large multimodal music collections, which comprise music-related documents of various types and formats including text, symbolic data, audio, image, and video. The challenge is to organize, understand, and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Cw5NYbZnEWgJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7498888887562735115&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7498888887562735115&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="26">
        <url>http://www.springerlink.com/index/611548724U535241.pdf</url>
        <title status="complete" source="scholar.google.com">Online music search by tapping</title>
        <author status="partial" source="scholar.google.com">G Peters, D Cukierman, C Anthony&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Ambient Intelligence in &#8230;</proceeding>
        <year>2006</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Query by Tapping is an emerging paradigm for content-based music retrieval, which we have explored through our web-based music search system. Based on the results obtained from our system we argue that searching for music by tapping the rhythm of a song&#39;s ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pfU9bZokpH8J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9197516584673736101&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9197516584673736101&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="27">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5876304</url>
        <title status="complete" source="scholar.google.com">Integrating Additional Chord Information into HMM-Based Lyrics-to-Audio Alignment</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/IEEETASLP201201mauch.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Mauch, H Fujihara, M Goto</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Aligning lyrics to audio has a wide range of applications such as the automatic generation of karaoke scores, song-browsing by lyrics, and the generation of audio thumbnails. Existing methods are restricted to using only lyrics and match them to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:4wt8-SvvXy0J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3269594826642557923&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3269594826642557923&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="28">
        <url>http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8005666&amp;id=RDDsAQAAEBAJ&amp;oi=fnd&amp;printsec=abstract</url>
        <title status="complete" source="scholar.google.com">Automatic system for temporal alignment of music audio signal with lyrics</title>
        <author status="complete" source="scholar.google.com">M Goto, H Fujihara, H Okuno</author>
        <proceeding status="complete" source="scholar.google.com">US Patent 8,005,666</proceeding>
        <year>2011</year>
        <source>Google Patents</source>
        <snippet status="partial" source="scholar.google.com">An automatic system for temporal alignment between a music audio signal and lyrics is provided. The automatic system can prevent accuracy for temporal alignment from being lowered due to the influence of non-vocal sections. Alignment means of the system is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BxEhLzg7I-QJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16439048177327345927&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16439048177327345927&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="29">
        <url>http://dl.acm.org/citation.cfm?id=1363164</url>
        <title status="complete" source="scholar.google.com">Enriching music with synchronized lyrics, images and colored lights</title>
        <pdf>http://www.dse.nl/~gijsg/AmbiSys-GeleijnseEtAl.pdf</pdf>
        <author status="partial" source="scholar.google.com">G Geleijnse, D Sekulovski, J Korst, S Pauws&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 1st &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present a method to synchronize popular music with its lyrics at the stanza level. First we apply an algorithm to segment audio content into harmonically similar and/or contrasting progressions, ie the stanzas. We map the stanzas found to a sequence of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:GGSBRK85bhQJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1472177553128121368&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1472177553128121368&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="30">
        <url>http://marcs.uws.edu.au/links/ICoMusic/ArchiveCD/Full_Paper_PDF/Gruhne_Schmidt_Dittmar.pdf</url>
        <title status="complete" source="scholar.google.com">Detecting Phonemes within the Singing of Polyphonic Music</title>
        <pdf>http://marcs.uws.edu.au/links/ICoMusic/ArchiveCD/Full_Paper_PDF/Gruhne_Schmidt_Dittmar.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Gruhne, K Schmidt, C Dittmar</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of ICoMCS &#8230;</proceeding>
        <year>2007</year>
        <source>marcs.uws.edu.au</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Automated detection of phonemes in polyphonic music is an important prerequisite for synchronizing music and corresponding lyrics. This paper describes a novel approach of automatic phoneme estimation within digitized music pieces. Since there are ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:5kK4lX97Uc8J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:5kK4lX97Uc8J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14938857226867589862&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="31">
        <url>http://www.cse.bgu.ac.il/hadar/Pdf/27%20-%20A%20musical%20source%20separation.pdf</url>
        <title status="complete" source="scholar.google.com">A musical source separation system with lyrics alignment.</title>
        <pdf>http://www.cse.bgu.ac.il/hadar/Pdf/27%20-%20A%20musical%20source%20separation.pdf</pdf>
        <author status="partial" source="scholar.google.com">O Hadar, D Bykhovsky, G Goldwasser&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">WSEAS Transactions on &#8230;</proceeding>
        <year>2006</year>
        <source>cse.bgu.ac.il</source>
        <snippet status="partial" source="scholar.google.com">Abstract: This paper examines the use of the generalized likelihood ratio test (GLRT) for the purposes of audio extraction and manipulation. The GLRT, which was designed originally for distinguishing between harmonic and non-harmonic audio frames, is extended for two ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:X4iVvKZRxIkJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:X4iVvKZRxIkJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9927149255201753183&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9927149255201753183&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="32">
        <url>http://dl.acm.org/citation.cfm?id=1823753</url>
        <title status="complete" source="scholar.google.com">Word level automatic alignment of music and lyrics using vocal synthesis</title>
        <author status="complete" source="scholar.google.com">NC Maddage, KC Sim, H Li</author>
        <proceeding status="partial" source="scholar.google.com">ACM Transactions on Multimedia &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a signal-based approach instead of the commonly used model-based approach, to automatically align vocal music with text lyrics at the word level. In this approach, we use a text-to-speech system to synthesize the singing voice according to the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:HJ9bzctkTegJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16739146216492474140&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16739146216492474140&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="33">
        <url>http://www.springerlink.com/index/pn3430677t4406l4.pdf</url>
        <title status="complete" source="scholar.google.com">A cross-modal approach for karaoke artifacts correction</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.1122&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">WQ Yan, MS Kankanhalli</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Tools and Applications</proceeding>
        <year>2008</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Karaoke singing is a popular form of entertainment in several parts of the world. Since this genre of performance attracts amateurs, the singing often has artifacts related to scale, tempo, and synchrony. We have developed an approach to correct these artifacts ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KkcF2WnpeasJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12356163693489506090&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12356163693489506090&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="34">
        <url>http://www-mmdb.iai.uni-bonn.de/download/Diplomarbeiten/Diplomarbeit_Damm.pdf</url>
        <title status="complete" source="scholar.google.com">Textbasierte Musiksuche im Rahmen des SyncPlayer-Frameworks</title>
        <pdf>http://www-mmdb.iai.uni-bonn.de/download/Diplomarbeiten/Diplomarbeit_Damm.pdf</pdf>
        <author status="complete" source="scholar.google.com">D Damm</author>
        <year>2007</year>
        <source>www-mmdb.iai.uni-bonn.de</source>
        <snippet status="partial" source="scholar.google.com">Wer kennt nicht die Situation: Man hÃ¶rt ein MusikstÃ¼ck, das einem gefÃ¤llt, weiÃ aber nicht, wie es heiÃt oder von wem es ist. Man kann sich nur an die Melodie und einige einschlÃ¤gige Textphrasen des Gesangs erinnern. Zwar existieren mit der zunehmenden Verbreitung ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:moDcGp8ZOv4J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:moDcGp8ZOv4J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=18318982605424066714&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="35">
        <url>http://staff.aist.go.jp/m.goto/PAPER/SIGMUS200608fujihara.pdf</url>
        <title status="complete" source="scholar.google.com">é³æ¥½é³é¿ä¿¡å·ã¨æ­è©ã®æéçå¯¾å¿ä»ãææ³: æ­å£°ã®åé¢ã¨æ¯é³ã® Viterbi ã¢ã©ã¤ã³ã¡ã³ã</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/SIGMUS200608fujihara.pdf</pdf>
        <author status="partial" source="scholar.google.com">è¤åå¼å°ï¼ å¾è¤çå­ï¼ ç·æ¹æ·³ï¼ é§è°·åç¯&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">æå ±å¦çå­¦ä¼ç ç©¶å ±å. &#8230;</proceeding>
        <year>2006</year>
        <source>staff.aist.go.jp</source>
        <snippet status="partial" source="scholar.google.com">æ¬ç¨¿ã§ã¯, ä¼´å¥é³ãå«ãé³æ¥½é³é¿ä¿¡å·ã¨å¯¾å¿ããæ­è©ã®æéçãªå¯¾å¿ä»ãææ³ã«ã¤ãã¦è¿°ã¹ã. ã¯ãªã¼ã³ãªé³å£°ä¿¡å·ã¨ãã®çºè©±åå®¹ã®æéçå¯¾å¿ä»ããæ¨å®ããã Viterbi ã¢ã©ã¤ã³ã¡ã³ãææ³ã¯ããã¾ã§ãå­å¨ããã, æ­å£°ã¨åæã«æ¼å¥ãããä¼´å¥é³ã®æªå½±é¿ã§å¸è²© CD ä¸­ã®æ­å£°ã«ã¯é©ç¨ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:MO-f7APWq5sJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11217294609239502640&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11217294609239502640&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="36">
        <url>http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf</url>
        <title status="complete" source="scholar.google.com">Melody Separation from Polyphonic Audio Recordings</title>
        <pdf>http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Vembu</author>
        <year>2005</year>
        <source>www-kd.iai.uni-bonn.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract In the field of music information retrieval, query-by-humming is an interesting application area in which humming sequences or melodies are matched with a database of songs to come up with a list of indexed songs that are similar in melodic content to the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16884184643397726041&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="37">
        <url>http://cdn.intechweb.org/pdfs/9261.pdf</url>
        <title status="complete" source="scholar.google.com">Music Structure Analysis Statistics for Popular Songs</title>
        <pdf>http://cdn.intechweb.org/pdfs/9261.pdf</pdf>
        <author status="complete" source="scholar.google.com">NC Maddage, L Haizhou, MS Kankanhalli</author>
        <proceeding status="complete" source="scholar.google.com">cdn.intechweb.org</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract In this chapter, we have proposed a better procedure for manual annotation of music information. The proposed annotation procedure involves carrying out listening tests and then incorporating music knowledge to iteratively refine the detected music ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7LRoGihTg1UJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:7LRoGihTg1UJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6161860146879837420&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="38">
        <title status="complete" source="scholar.google.com">Phonetic Segmentation of Singing Voice Using MIDI and Parallel Speech</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:JTF8G2KpMasJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12335827093177512229&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="39">
        <url>http://www.springerlink.com/index/lh7413485m406rv4.pdf</url>
        <title status="complete" source="scholar.google.com">Cross-Modal Approach for Karaoke Artifacts Correction</title>
        <author status="complete" source="scholar.google.com">WQ Yan, MS Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of Multimedia for Digital Entertainment and &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">In this chapter, we combine adaptive sampling in conjunction with video analogies (VA) to correct the audio stream in the karaoke environment k= k (t): k (t)=(U (t), K (t)), t Ã (ts, te) Îº=\ left {Îº (t):\ kappa (t)=\ left (U (t),\ K (t)\ right),\ t â\ left (t _ s,{t _ e\ right)\ right\} where ts and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:oYqzO5N08cYJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14335367264607636129&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="40">
        <url>http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf</url>
        <title status="complete" source="scholar.google.com">A repetition-based framework for lyric alignment in popular songs</title>
        <pdf>http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf</pdf>
        <author status="complete" source="scholar.google.com">LM Thang, KANM Yen</author>
        <proceeding status="partial" source="scholar.google.com">National University of Singapore &#8230;</proceeding>
        <year>2007</year>
        <source>w.comp.nus.edu</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We examine the problem of automatically aligning acoustic musical audio and textual lyric in popular songs. Existing works have tackled the problem using computationally-expensive audio processing techniques, resulting in solutions unsuitable ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>27</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17581284984694044119&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="41">
        <url>http://www.springerlink.com/index/FT671731J0403662.pdf</url>
        <title status="complete" source="scholar.google.com">A Survey of Music Structure Analysis Techniques for Music Applications</title>
        <author status="complete" source="scholar.google.com">N Maddage, H Li, M Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">Recent Advances in Multimedia Signal &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Music carries multilayer information which forms different structures. The information embedded in the music can be categorized into time information, harmony/melody, music regions, music similarities, song structures and music semantics. In this chapter, we first ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3vo8SvZRZuUJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16529989600559299294&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="42">
        <title status="complete" source="scholar.google.com">An Advanced System for Content-based Access to Music Information</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vwUgTwhhUfgJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="43">
        <title status="complete" source="scholar.google.com">Monaural musical sound separation</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jpa0VTJdi4EJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9334657123423131278&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="44">
        <url>http://scholarworks.sjsu.edu/etd_projects/145/</url>
        <title status="complete" source="scholar.google.com">Common Emotion Modeling in Distinct Medium Analysis and Matching</title>
        <pdf>http://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=1144&amp;context=etd_projects</pdf>
        <author status="complete" source="scholar.google.com">A Cho</author>
        <year>2009</year>
        <source>scholarworks.sjsu.edu</source>
        <snippet status="partial" source="scholar.google.com">Abstract With the ever growing amount of digital information and multimedia on the World Wide Web and the current trend towards personalizing technology, users find themselves wanting a more intuitive way of finding related information, and not just any information but ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WBKxWryTez8J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4574412283709559384&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="45">
        <url>http://www.springerlink.com/index/x31g53215j053046.pdf</url>
        <title status="complete" source="scholar.google.com">Instant customized summaries streaming: a service for immediate awareness of new video content</title>
        <author status="partial" source="scholar.google.com">Ã GarcÃ­a-MartÃ­n, J Molina, F LÃ³pez, V ValdÃ©s&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Media and Adapting to &#8230;</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">This paper presents the Instant Customized Summaries Streaming service, a multimedia service able to deliver customized video summaries with a minimum delay after the original content has been uploaded to a video repository. Uploaded videos start being analyzed, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:OauSajPSutsJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15833198558247562041&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="46">
        <title status="complete" source="scholar.google.com">Singing Voice Detection in Western Popular Music</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dx-cdoqIEO4J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="47">
        <url>http://www.audiolabs-erlangen.de/meinard/students/thesis/2012_EwertSebastian_SignalProcessingMethods_Phd-Thesis.pdf</url>
        <title status="complete" source="scholar.google.com">Signal Processing Methods for Music Synchronization, Audio Matching, and Source Separation</title>
        <pdf>http://www.audiolabs-erlangen.de/meinard/students/thesis/2012_EwertSebastian_SignalProcessingMethods_Phd-Thesis.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Ewert</author>
        <year>2012</year>
        <source>audiolabs-erlangen.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract The field of music information retrieval (MIR) aims at developing techniques and tools for organizing, understanding, and searching multimodal information in large music collections in a robust, efficient and intelligent manner. In this context, this thesis presents ...</snippet>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Li_TocU8mCEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="48">
        <url>http://www.lrec-conf.org/proceedings/lrec2012/pdf/730_Paper.pdf</url>
        <title status="complete" source="scholar.google.com">A Parallel Corpus of Music and Lyrics Annotated with Emotions</title>
        <pdf>http://www.lrec-conf.org/proceedings/lrec2012/pdf/730_Paper.pdf</pdf>
        <author status="complete" source="scholar.google.com">C Strapparava, R Mihalcea, A Battocchi</author>
        <proceeding status="complete" source="scholar.google.com">lrec-conf.org</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we introduce a novel parallel corpus of music and lyrics, annotated with emotions at line level. We first describe the corpus, consisting of 100 popular songs, each of them including a music component, provided in the MIDI format, as well as a lyrics ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:t6syon1vUekJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:t6syon1vUekJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="49">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.5609&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Efficient Lyrics Retrieval and Alignment</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.5609&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">JKG Geleijnse</author>
        <proceeding status="complete" source="scholar.google.com">Citeseer</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract We present an algorithm to efficiently retrieve from the Web multiple versions of the lyrics of a given song. First, multiple web pages are collected that potentially contain the lyrics of the given song, by querying Google with the song title and artist name. Next, from ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bj2vrbZp62UJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:bj2vrbZp62UJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7344079850676632942&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="50">
        <url>http://ismir2007.ismir.net/posters/ISMIR2007_p369_gruhne_poster.pdf</url>
        <title status="complete" source="scholar.google.com">Phoneme Detection in Popular Music</title>
        <pdf>http://ismir2007.ismir.net/posters/ISMIR2007_p369_gruhne_poster.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Gruhne, K Schmidt, C Dittmar</author>
        <proceeding status="complete" source="scholar.google.com">ismir2007.ismir.net</proceeding>
        <snippet status="complete" source="scholar.google.com">Phoneme detection in polyphonic music is an important prerequisite for lyrics synchronization for karaoke applications or browsing in music catalogues. Phoneme detection might be furthermore used for the singing recognition in the polyphonic music.</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:c-q8reHPoYIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:c-q8reHPoYIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="51">
        <url>http://140.92.88.20/ir/handle/105076416/2285</url>
        <title status="complete" source="scholar.google.com">æ­è²åï§ªä»¥åæ°£é³éå»º</title>
        <pdf>http://140.92.88.20/ir/bitstream/105076416/2285/2/IA97FA3132B0232B02.pdf</pdf>
        <author status="complete" source="scholar.google.com">è¨±èå</author>
        <year>2008</year>
        <source>140.92.88.20</source>
        <snippet status="partial" source="scholar.google.com">Separating singing voice from music accompaniment is an appealing but challenging problem, especially in the monaural case. One existing approach is based on computational audio scene analysis which uses pitch as the cue to resynthesize the singing voice. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:K-OcsGb44J8J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11520480966747153195&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="52">
        <url>http://www.pharos-audiovisual-search.eu/res/files/publications/papers/Phoneme%20Detection.pdf</url>
        <title status="complete" source="scholar.google.com">Phoneme Detection for Lyrics Synchronization</title>
        <pdf>http://www.pharos-audiovisual-search.eu/res/files/publications/papers/Phoneme%20Detection.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Gruhne, C Dittmar</author>
        <year>2008</year>
        <source>pharos-audiovisual-search.eu</source>
        <snippet status="partial" source="scholar.google.com">Automated detection of phonemes in polyphonic music is an important prerequisite for synchronizing music and corresponding lyrics. This paper describes a novel approach of automatic phoneme estimation within digitized music pieces. Since there are already a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dyvltK_NKdUJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:dyvltK_NKdUJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15360034158661675895&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="53">
        <url>http://scholarbank.nus.sg/handle/10635/13382</url>
        <title status="partial" source="scholar.google.com">REFINING MUSIC SIGNAL TO LYRIC TEXT SYNCHRONIZATION FROM LINE-LEVEL TO SYLLABLE-LEVEL BY CONSTRAINING DYNAMIC TIME WARPING &#8230;</title>
        <pdf>http://scholarbank.nus.sg/bitstream/handle/10635/13382/thesis_21_amendment_2.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">D ISKANDAR</author>
        <year>2007</year>
        <source>scholarbank.nus.sg</source>
        <snippet status="partial" source="scholar.google.com">The problem we consider in this thesis is synchronization between lyric text and the corresponding singing voice recording. We limit the singing to the pop genre to make the problem manageable. The recordings we consider in this problem are those we can ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:q87ftenzmuwJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17049207524468706987&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="54">
        <url>http://acl.eldoc.ub.rug.nl/mirror/D/D12/D12-1054.pdf</url>
        <title status="complete" source="scholar.google.com">Lyrics, Music, and Emotions</title>
        <pdf>http://acl.eldoc.ub.rug.nl/mirror/D/D12/D12-1054.pdf</pdf>
        <author status="complete" source="scholar.google.com">R Mihalcea, C Strapparava</author>
        <proceeding status="complete" source="scholar.google.com">acl.eldoc.ub.rug.nl</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we explore the classification of emotions in songs, using the music and the lyrics representation of the songs. We introduce a novel corpus of music and lyrics, consisting of 100 songs annotated for emotions. We show that textual and musical ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:umhiuyjuoLEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:umhiuyjuoLEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12799491999696840890&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="55">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6121941</url>
        <title status="complete" source="scholar.google.com">A Tandem Algorithm for Singing Pitch Extraction and Voice Separation From Music Accompaniment</title>
        <pdf>http://www.cse.ohio-state.edu/~dwang/papers/HWJH.taslp12.pdf</pdf>
        <author status="partial" source="scholar.google.com">CL Hsu, DL Wang, JSR Jang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Singing pitch estimation and singing voice separation are challenging due to the presence of music accompaniments that are often nonstationary and harmonic. Inspired by computational auditory scene analysis (CASA), this paper investigates a tandem algorithm ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:HpddwzaazwQJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=346665256327419678&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=346665256327419678&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="56">
        <url>http://computationalcreativity.net/iccc2012/wp-content/uploads/2012/05/087-Monteith.pdf</url>
        <title status="complete" source="scholar.google.com">Automatic Generation of Melodic Accompaniments for Lyrics</title>
        <pdf>http://computationalcreativity.net/iccc2012/wp-content/uploads/2012/05/087-Monteith.pdf</pdf>
        <author status="partial" source="scholar.google.com">K Monteith, T Martinez&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">International &#8230;</proceeding>
        <year>2012</year>
        <source>computationalcreativity.net</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music and speech are two realms predominately species-specific to humans, and many human creative endeavors involve these two modalities. The pairing of music and spoken text can heighten the emotional and cognitive impact of both-the complete song ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KN5B1YxAHFYJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:KN5B1YxAHFYJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="57">
        <url>http://ismir2012.ismir.net/event/papers/361-ismir-2012.pdf</url>
        <title status="complete" source="scholar.google.com">RANKING LYRICS FOR ONLINE SEARCH</title>
        <pdf>http://ismir2012.ismir.net/event/papers/361-ismir-2012.pdf</pdf>
        <author status="complete" source="scholar.google.com">R Macrae, S Dixon</author>
        <proceeding status="complete" source="scholar.google.com">ismir2012.ismir.net</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT When someone wishes to find the lyrics for a song they typically go online and use a search engine. There are a large number of lyrics available on the internet as the effort required to transcribe and post lyrics is minimal. These lyrics are promptly returned to the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:882m1oY-pkoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:882m1oY-pkoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="58">
        <url>https://ohm.fing.edu.uy/publicaciones/2008/SPD08/SPD08.pdf</url>
        <title status="complete" source="scholar.google.com">SeparaciÃ³n de Voz Cantada (Singing Voice Separation)</title>
        <pdf>https://ohm.fing.edu.uy/publicaciones/2008/SPD08/SPD08.pdf</pdf>
        <author status="partial" source="scholar.google.com">A Samas, A Palermo, A Decarlini, M Rocamora&#8230;</author>
        <proceeding status="complete" source="scholar.google.com">ohm.fing.edu.uy</proceeding>
        <snippet status="partial" source="scholar.google.com">El problema principal que abordÃ³ este proyecto de fin de carrera es la extracciÃ³n de la voz cantada en una grabaciÃ³n musical. El objetivo es construir un sistema que reciba como entrada un archivo de mÃºsica, y que devuelva como salida la voz cantada. Con el fin de ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dDo3aajjYasJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:dDo3aajjYasJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12349401965685848692&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="59">
        <url>https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/4e77efd5c6e2ceadc12567530068624d/d3d83c1d4ae52000c125753f005f5543/$FILE/2008_DammFrKuMuCl_SyncPlayer_LWA.pdf</url>
        <title status="complete" source="scholar.google.com">SyncPlayer-Multimodale Wiedergabe, Navigation und Suche in heterogenen digitalen Musikkollektionen</title>
        <pdf>https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/4e77efd5c6e2ceadc12567530068624d/d3d83c1d4ae52000c125753f005f5543/$FILE/2008_DammFrKuMuCl_SyncPlayer_LWA.pdf</pdf>
        <author status="partial" source="scholar.google.com">D Damm, C Fremerey, F Kurth&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">6.-8. October 2008, &#8230;</proceeding>
        <year>2008</year>
        <source>domino.mpi-inf.mpg.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract Durch systematische Digitalisierung sind in den letzten Jahren umfangreiche BestÃ¤nde von Musikdokumenten entstanden, die digitale Inhalte unterschiedlichster AusprÃ¤gungen und Formate enthalten. Man denke hier beispielsweise an CD-Aufnahmen ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9A6ve9rI_mEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:9A6ve9rI_mEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>14</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7061302106467012340&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7061302106467012340&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="60">
        <url>http://aless.mine.nu/paper_proyecto.pdf</url>
        <title status="complete" source="scholar.google.com">Separacion de Voz Cantada (Singing Voice Separation)</title>
        <pdf>http://aless.mine.nu/paper_proyecto.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Decarlini, A Palermo, A Samas</author>
        <proceeding status="complete" source="scholar.google.com">aless.mine.nu</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract El problema principal que abordÃ³ este proyecto de fin de carrera es la extracciÃ³n de la voz cantada en una grabaciÃ³n musical. El objetivo es construir un sistema que reciba como entrada un archivo de mÃºsica, y que devuelva como salida la voz cantada. Con el ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:o4RX933RTWkJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:o4RX933RTWkJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7587951286139978915&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="1">
    <url>http://dl.acm.org/citation.cfm?id=1101353</url>
    <title status="complete" source="scholar.google.com">Digital violin tutor: an integrated system for beginning violin learners</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2005_Digital_Violin_Tutor-An_Integrated_System_for_Beginning_Violin_Learners.pdf</pdf>
    <author status="complete" source="scholar.google.com">J Yin, Y Wang, D Hsu</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 13th annual ACM &#8230;</proceeding>
    <year>2005</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. Digital Violin Tutor: An Integrated System for Beginning Violin Learners JunYin, Ye Wang and David Hsu School of Computing National University of Singapore{yinjun, wangye, dyhsu}@comp.nus.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:keJO9X-wIy8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>16</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=3396752607590408849&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>22</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=3396752607590408849&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="22">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1290156</url>
        <title status="complete" source="scholar.google.com">Effective use of multimedia for computer-assisted musical instrument tutoring</title>
        <pdf>http://strum.googlecode.com/svn-history/r41/trunk/Research/CAMIT.pdf</pdf>
        <author status="complete" source="scholar.google.com">G Percival, Y Wang, G Tzanetakis</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a survey of recent work in computer-assisted musical instrumental tutoring and outlines several questions to consider when developing future projects. In particular, we suggest that the area ingreatest need of computer assistance is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:sSgpS4WrYA0J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>21</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=963958909237274801&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>18</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=963958909237274801&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1291361</url>
        <title status="complete" source="scholar.google.com">Visual analysis of fingering for pedagogical violin transcription</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.169.8333&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">B Zhang, J Zhu, Y Wang, WK Leow</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 15th &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic music transcription, in spite of decades of research, remains a challenging research problem. The traditional audio-only approach has yet to achieve a satisfactory performance for any computer-aided pedagogical system. Inspired by the high ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:NXVIML9pAOIJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16285132522441438517&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>15</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16285132522441438517&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://dl.acm.org/citation.cfm?id=1462005</url>
        <title status="complete" source="scholar.google.com">Temporal interaction between an artificial orchestra conductor and human musicians</title>
        <pdf>http://doc.utwente.nl/65015/1/bnaic2008-3.pdf</pdf>
        <author status="complete" source="scholar.google.com">D Reidsma, A Nijholt, P Bos</author>
        <proceeding status="complete" source="scholar.google.com">Computers in Entertainment (CIE)</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The Virtual Conductor project concerns the development of the first properly interactive virtual orchestra conductorâa Virtual Human that can conduct a piece of music through interaction with musicians, leading and following them while they are playing. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:84aFOrfh4EgJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>23</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5251445342591092467&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>16</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5251445342591092467&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://dl.acm.org/citation.cfm?id=1290154</url>
        <title status="complete" source="scholar.google.com">Educational violin transcription by fusing multimedia streams</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2007_Educational_Violin_Transcription_by_Fusing_Multimedia_Streams.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, B Zhang, O Schleusing</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Computer-assisted violin tutoring requires accurate violin transcription. For pitched non-percussive (PNP) sounds such as from the violin, note segmentation is a much more difficult task than pitch detection. This issue is accentuated when the audio is recorded ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:w23vslgkmBsJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1988379198861831619&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1988379198861831619&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.5003&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Low level descriptors for automatic violin transcription</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.5003&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">A Loscos, Y Wang, WJJ Boo</author>
        <proceeding status="complete" source="scholar.google.com">Proc. of ISMIR2006</proceeding>
        <year>2006</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">Abstract On top of previous work in automatic violin transcription we present a set of straight forward low level descriptors for assisting the transcription techniques and saving computational cost. Proposed descriptors have been tested against a database of 1500 ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2--epYjX8FMJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:2--epYjX8FMJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>25</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6048571281452756955&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>12</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6048571281452756955&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4037041</url>
        <title status="complete" source="scholar.google.com">A violin music transcriber for personalized learning</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2006_A_Violin_Music_Transcriber_for_Personalized_Learning.pdf</pdf>
        <author status="complete" source="scholar.google.com">WJJ Boo, Y Wang, A Loscos</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2006 IEEE &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a new version of our violin music transcriber to support personalized learning. The proposed method is designed to detect duo-pitch (two strings being bowed at the same time) from real-world violin audio signals recorded in a home ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mCNMB2GpzukJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16847589490238956440&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16847589490238956440&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4623948</url>
        <title status="complete" source="scholar.google.com">Application-specific music transcription for tutoring</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_Application_Specific_Music_Transcription_for_Tutoring.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, B Zhang</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic music transcription (AMT) refers to the ability of computers to write note information, such as the pitch, onset time, duration, and source of each sound, after listening to the music. Our application scenario is computer-assisted, musical-instrument tutoring, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vK-ug8uAcwIJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>18</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=176626421973561276&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=176626421973561276&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://dl.acm.org/citation.cfm?id=1979016</url>
        <title status="complete" source="scholar.google.com">MOGCLASS: evaluation of a collaborative system of mobile devices for classroom music education of young children</title>
        <pdf>https://www0.comp.nus.edu.sg/~zhaosd/paper/chi2011_mogclass.pdf</pdf>
        <author status="partial" source="scholar.google.com">Y Zhou, G Percival, X Wang, Y Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 2011 &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Composition, listening, and performance are essential activities in classroom music education, yet conventional music classes impose unnecessary limitations on students&#39; ability to develop these skills. Based on in-depth fieldwork and a user-centered design ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jQvxbCYwtW8J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8049392850589256589&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8049392850589256589&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://proceedings.spiedigitallibrary.org/data/Conferences/SPIEP/64101/82940A_1.pdf</url>
        <title status="complete" source="scholar.google.com">Visualization feedback for musical ensemble practice: A case study on phrase articulation and dynamics</title>
        <pdf>http://www.cim.mcgill.ca/sre/publications/2012-VDA.pdf</pdf>
        <author status="partial" source="scholar.google.com">T Knight, N Bouillot&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of &#8230;</proceeding>
        <year>2012</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We consider the possible advantages of visualization in supporting musical interpretation. Specifically, we investigate the use of visualizations in making a subjective judgement of a student&#39;s performance compared to reference âexpertâ performance for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:VVWYIlCLw9MJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15259193138755425621&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15259193138755425621&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://dl.acm.org/citation.cfm?id=1255135</url>
        <title status="complete" source="scholar.google.com">Interactive Digital Violin Tutor (iDVT): An edutainment system for violin learners</title>
        <author status="complete" source="scholar.google.com">Y Wang, J Zhu</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international conference on &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present in this demo our current design of an Interactive Digital Violin Tutor (iDVT), which enables students to have reflective practice when human teachers are not available. iDVT combines violin audio transcription with visualization. Our transcription ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:DshDr4sSkfYJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17767002396103526414&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17767002396103526414&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>https://dl.comp.nus.edu.sg/dspace/handle/1900.100/3056</url>
        <title status="complete" source="scholar.google.com">Automatic music transcription using audio-visual fusion for violin practice in home environment</title>
        <pdf>https://dl.comp.nus.edu.sg/dspace/bitstream/1900.100/3056/1/TRA7-09.pdf</pdf>
        <author status="complete" source="scholar.google.com">B Zhang, Y Wang</author>
        <year>2009</year>
        <source>dl.comp.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">Abstract: Violin practice in a home environment, where there is often no teacher available, can benefit from automatic music transcription to provide feedback to the student. This paper describes a high performance violin transcription system with three main contributions. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WDqOhljuXd0J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15951167519198165592&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15951167519198165592&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://www.mitpressjournals.org/doi/abs/10.1162/COMJ_a_00119</url>
        <title status="complete" source="scholar.google.com">A High-Fidelity Orchestra Simulator for Individual Musicians&#39; Practice</title>
        <pdf>http://130.102.44.246/journals/computer_music_journal/v036/36.2.olmos.pdf</pdf>
        <author status="partial" source="scholar.google.com">A Olmos, N Bouillot, T Knight, N Mabire, J Redel&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Computer Music &#8230;</proceeding>
        <year>2012</year>
        <source>MIT Press</source>
        <snippet status="partial" source="scholar.google.com">We developed the Open Orchestra system to provide individual musicians with a high-fidelity experience of ensemble rehearsal or performance, combined with the convenience and flexibility of solo study. This builds on the theme of an immersive orchestral simulator ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KputC2FCj3gJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8687235190796163882&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6266249</url>
        <title status="complete" source="scholar.google.com">Real-Time Pitch Training System for Violin Learners</title>
        <pdf>http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Workshops/data/4729a163.pdf</pdf>
        <author status="partial" source="scholar.google.com">JH Wang, SA Wang, WC Chen&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper specifically targets violin learners who are working on their pitch accuracy. We employ a pitch tracking algorithm to extract the pitch played. Through volume thresholding and region detection, only parts of frames are processed. So our system can ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gMp1DiddLkkJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5273254636025137792&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://137.132.14.55/handle/10635/20949</url>
        <title status="complete" source="scholar.google.com">Adaptive multimodal fusion based similarity measures in music information retrieval</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/20949/ZhangBJ.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">Z BINGJUN</author>
        <year>2010</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">In the field of music information retrieval (MIR), one fundamental research problem is the measuring of the similarity between music documents. Based on a viable similarity measure, MIR systems can be made more effective to help users retrieve relevant music information. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9FjpHoxTlRIJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1339068325491726580&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6125707</url>
        <title status="complete" source="scholar.google.com">E-Santur: A customizable tool for learning Santur music instrument using game-based learning approach</title>
        <author status="complete" source="scholar.google.com">KM Yazdi, SP Lee</author>
        <proceeding status="partial" source="scholar.google.com">Research and Innovation in Information &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Instruction through game-based applications is one of the most efficient methods for learning that has high potential of providing interactive and effective educational environments. For example, learning music is a popular subject in which many people are ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bAOfKG9yCKMJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://www.springerlink.com/index/M0135GN0W62120V8.pdf</url>
        <title status="complete" source="scholar.google.com">PWGL, towards an open and intelligent learning environment for higher music education</title>
        <author status="complete" source="scholar.google.com">M Kuuskankare, M Laurson</author>
        <proceeding status="partial" source="scholar.google.com">Sustaining TEL: From Innovation to Learning &#8230;</proceeding>
        <year>2010</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">We have studied the computer based applications of composition, music theory and analysis, software synthesis, and music notation for well over two decades. The most important result of these activities is PWGL, a modern visual computer program with the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:VZUPW-7UBpYJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10810562075744507221&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://130.89.10.12/~maatm/publications/Masters%20Thesis%20Mark%20ter%20Maat.pdf</url>
        <title status="complete" source="scholar.google.com">What does the conductor want?</title>
        <pdf>http://130.89.10.12/~maatm/publications/Masters%20Thesis%20Mark%20ter%20Maat.pdf</pdf>
        <author status="complete" source="scholar.google.com">Z Ruttkay, M ter Maat</author>
        <proceeding status="complete" source="scholar.google.com">130.89.10.12</proceeding>
        <snippet status="partial" source="scholar.google.com">In this chapter all research related to the virtual conductor is presented. The original virtual conductor system is already explained in Chapter 2.2 and will not be treated here again. As far as I know no other similar project is as extensive as this one and no other system can ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tAJ_bX7pU84J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:tAJ_bX7pU84J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>19</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14867483524092330676&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://search.ieice.org/bin/summary.php?id=e95-a_5_969</url>
        <title status="complete" source="scholar.google.com">Iterative Frequency Estimation for Accuracy Improvement of Three DFT Phase-Based Methods</title>
        <author status="partial" source="scholar.google.com">P Hee-Suk, LIM Jun-Seok, K Oh-Jin&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; on Fundamentals of &#8230;</proceeding>
        <year>2012</year>
        <source>search.ieice.org</source>
        <snippet status="partial" source="scholar.google.com">We propose an iterative frequency estimation method for accuracy improvement of discrete Fourier transform (DFT) phase-based methods. It iterates frequency estimation and phase calculation based on the DFT phase-based methods, which maximizes the signal-to-noise ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mBvTgarZBk8J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5694478105202334616&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://pdf.aminer.org/000/287/524/cognitively_oriented_design_of_a_multimedia_system_to_learn_guitar.pdf</url>
        <title status="complete" source="scholar.google.com">Graeme Smith</title>
        <pdf>http://pdf.aminer.org/000/287/524/cognitively_oriented_design_of_a_multimedia_system_to_learn_guitar.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Johnston</author>
        <proceeding status="complete" source="scholar.google.com">pdf.aminer.org</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper we present software designed to help address problems encountered by beginning guitarists, using interactive software to find effective solutions to enhance the learning process. Software can be utilised to improve a player&#39;s ability to hear mistakes in ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Eh0tglG-JuIJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Eh0tglG-JuIJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16295921558972341522&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://www-apps.cs.colorado.edu/department/publications/theses/docs/bs/ransom_christofferson.pdf</url>
        <title status="complete" source="scholar.google.com">Digital Drum Tutor</title>
        <pdf>http://www-apps.cs.colorado.edu/department/publications/theses/docs/bs/ransom_christofferson.pdf</pdf>
        <author status="complete" source="scholar.google.com">R Christofferson</author>
        <proceeding status="complete" source="scholar.google.com">www-apps.cs.colorado.edu</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract This thesis investigates issues surrounding the design and implementation of a digital drum tutor. People have been developing digital tutors in all fields of study in an attempt to replace or complement human instructors. This drum tutor aims to replace a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tN8inIVJijYJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:tN8inIVJijYJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3930034463033188276&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://www.editlib.org/p/25878?nl</url>
        <title status="complete" source="scholar.google.com">Practice Perfectâan Intelligent Music Tutoring System for Assisting and Improving Practice and Exam Preparation</title>
        <pdf>http://www.editlib.org/p/25878/proceeding_25878.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Dervan, B McDaniel</author>
        <proceeding status="partial" source="scholar.google.com">World Conference on Educational Multimedia, &#8230;</proceeding>
        <year>2007</year>
        <source>editlib.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract For more than 20 years eLearning technology has been applied to education but thus far has failed to meet its potential promised by its proponents. Rather than attempting to replace classroom-based education as before, the next generation of learning ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:R66Ix2sOY4YJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9683599479848611399&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>http://www.lps.usp.br/~magno/papers/SBGames10.pdf</url>
        <title status="complete" source="scholar.google.com">Rainbow Strings: Jogo para aprendizado de violino com processamento de audio</title>
        <pdf>http://www.lps.usp.br/~magno/papers/SBGames10.pdf</pdf>
        <author status="partial" source="scholar.google.com">PRR Vianna, R Nakamura, EMF Mesquita, MTM Silva&#8230;</author>
        <proceeding status="complete" source="scholar.google.com">lps.usp.br</proceeding>
        <snippet status="partial" source="scholar.google.com">Resumo Neste artigo discute-se o desenvolvimento do jogo âRainbow Stringsâ, com finalidades lÃºdica e didÃ¡tica. Trata-se de um jogo para computadores que apresenta uma partitura musical simplificada, ao mesmo tempo em que o som de um violino tocado pelo ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mHNpPmXYeEgJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mHNpPmXYeEgJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5222161697286484888&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="2">
    <url>http://dl.acm.org/citation.cfm?id=1065994</url>
    <title status="complete" source="scholar.google.com">Power-efficient streaming for mobile terminals</title>
    <pdf>https://www-new.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2005_Power-Efficient_Streaming_for_Mobile_Terminals.pdf</pdf>
    <author status="complete" source="scholar.google.com">J Korhonen, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the international workshop on &#8230;</proceeding>
    <year>2005</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Ye Wang National University of Singapore 3 Science Drive 2 Singapore 117543wangye@comp.nus.edu.sg ABSTRACT Wireless Network Interface (WNI) is one of the most criticalcomponents for power efficiency in multimedia streaming to mobile devices. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:AdDmBoq27LkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>12</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=13397283695457914881&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>23</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=13397283695457914881&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="23">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4289422</url>
        <title status="complete" source="scholar.google.com">Adaptive-buffer power save mechanism for mobile multimedia streaming</title>
        <pdf>http://doras.dcu.ie/95/1/adams_janet_2007.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Adams</author>
        <proceeding status="partial" source="scholar.google.com">Communications, 2007. ICC&#39;07. IEEE International &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Wireless networks are becoming a part of everyday life for many people. When a mobile device has wireless LAN capability, multimedia content can be streamed over a wireless network to that device. However, a major disadvantage of all mobile devices is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mpl4nmIJd-cJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>18</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16678810064160070042&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>22</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16678810064160070042&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1555845</url>
        <title status="complete" source="scholar.google.com">iScope: personalized multi-modality image search for mobile devices</title>
        <pdf>http://ziyang.eecs.umich.edu/~dickrp/publications/zhu09jun.pdf</pdf>
        <author status="complete" source="scholar.google.com">C Zhu, K Li, Q Lv, L Shang, RP Dick</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 7th &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Mobile devices are becoming a primary medium for personal information gathering, management, and sharing. Managing personal image data on mobile platforms is a difficult problem due to large data set size, content diversity, heterogeneous individual usage ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BzXbtVdrGuEJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16220395032385631495&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16220395032385631495&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4216913</url>
        <title status="complete" source="scholar.google.com">Power save adaptation algorithm for multimedia streaming to mobile devices</title>
        <pdf>http://elm.eeng.dcu.ie/~pel/papers/2007_POR_JA.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Adams, GM Muntean</author>
        <proceeding status="partial" source="scholar.google.com">Portable Information Devices, 2007. &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Batteries have not followed the exponential technological improvements of other mobile device related hardware, such as CPU, memory and wireless networking. Battery power often introduces significant limitations to the use of mobile devices and their ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:26-Y5kvBEZ4J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11390097464332169179&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11390097464332169179&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.lbd.dcc.ufmg.br:8080/colecoes/sbrc/2006/st10_4.pdf</url>
        <title status="complete" source="scholar.google.com">Otimizando o Consumo de Energia de Dispositivos MÃ³veis em Sistemas de VoD Baseados em Pontos de Acesso Co-localizados</title>
        <pdf>http://www.lbd.dcc.ufmg.br:8080/colecoes/sbrc/2006/st10_4.pdf</pdf>
        <author status="complete" source="scholar.google.com">LB Pinho, CL Amorim</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the 24th Brazilian Symposium on &#8230;</proceeding>
        <year>2006</year>
        <source>lbd.dcc.ufmg.br</source>
        <snippet status="partial" source="scholar.google.com">Resumo. Este artigo apresenta um estudo sobre reduÃ§ ao no consumo de energia de dispositivos mÃ³veis em sistemas de VoD escalÃ¡veis, baseados em pontos de acesso co-localizados. Em particular, Ã© proposto e avaliado o uso combinado de duas tÃ©cnicas de ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ZH02Q_Ezu_QJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:ZH02Q_Ezu_QJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17634745877207088484&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17634745877207088484&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://hub.hku.hk/handle/10722/51492</url>
        <title status="complete" source="scholar.google.com">Non-cooperative peer-to-peer media streaming: game theoretic analysis and algorithms</title>
        <pdf>http://hub.hku.hk/bitstream/10722/51492/1/FullText.pdf?accept=1</pdf>
        <author status="complete" source="scholar.google.com">K Yeung</author>
        <year>2007</year>
        <source>hub.hku.hk</source>
        <snippet status="partial" source="scholar.google.com">Mark Kai Ho YEUNG for the degree of Doctor of Philosophy at The University of Hong Kong September 28, 2007 Peer-to-peer (P2P) media streaming is widely considered as a promising platform for delivering high quality media content at the global scale. We ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:plK9dEllRjoJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4199155068735148710&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4199155068735148710&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5558388</url>
        <title status="complete" source="scholar.google.com">System-level power management for system-on-a-chip-based mobile devices</title>
        <author status="complete" source="scholar.google.com">J Choi, H Cha</author>
        <proceeding status="complete" source="scholar.google.com">Computers &amp; Digital Techniques, IET</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Since power management policies tend to have limits when based on individual components of a system, such as the processor, memory or LCD, it is necessary to have a system-wide approach that considers power components in an integrated way. The ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:VAbyWCcHb0QJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4931167982571357780&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4931167982571357780&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://www.springerlink.com/index/D030618684556Q72.pdf</url>
        <title status="complete" source="scholar.google.com">Energy-efficient internetworking with DTN</title>
        <pdf>http://jie-online-org.spice-center.org/index.php/jie/article/view/92/53</pdf>
        <author status="complete" source="scholar.google.com">D Vardalis, V Tsaoussidis</author>
        <proceeding status="complete" source="scholar.google.com">Wired/Wireless Internet Communications</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">We claim that Delay-Tolerant Networking has the potential to form an internetworking overlay that shapes traffic in a manner that exploits the capacity of last hop wireless channels and allows for energy-efficient internetworking. We demonstrate DTN potential ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Eonfu20ne8EJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13941780423712409874&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13941780423712409874&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5502590</url>
        <title status="complete" source="scholar.google.com">Framework for Energy-Aware Lossless Compression in Mobile Services: The Case of E-Mail</title>
        <pdf>http://www.cse.tkk.fi/~yuxiao/ICC-compression-submission-0210.pdf</pdf>
        <author status="partial" source="scholar.google.com">Y Xiao, M Siekkinen&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; (ICC), 2010 IEEE &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Energy consumption caused by wireless transmission poses a big challenge to the battery lifetime of mobile devices. While the potential of using lossless compression for saving energy has been long acknowledged, no general solution has been proposed for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tbGryshAu2YJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7402581643651428789&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7402581643651428789&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>https://aaltodoc.aalto.fi/bitstream/handle/123456789/5096/publication4.pdf?sequence=5</url>
        <title status="complete" source="scholar.google.com">Publication IV</title>
        <pdf>https://aaltodoc.aalto.fi/bitstream/handle/123456789/5096/publication4.pdf?sequence=5</pdf>
        <author status="complete" source="scholar.google.com">RS Kalyanaraman, Y Xiao, AYJN Prediction</author>
        <proceeding status="complete" source="scholar.google.com">aaltodoc.aalto.fi</proceeding>
        <snippet status="partial" source="scholar.google.com">AbstractâNetwork parameters such as signal-to-noise-ratio (SNR), throughput, and packet loss rate can be used for measuring the wireless network performance which highly depends on the wireless network conditions. Previous works on energy consumption have ...</snippet>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:QMFPCPJngqwJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://smartech.gatech.edu/handle/1853/37164</url>
        <title status="complete" source="scholar.google.com">Application acceleration for wireless and mobile data networks</title>
        <pdf>http://smartech.gatech.edu/bitstream/handle/1853/37164/zhuang_zhenyun_201008_phd.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">Z Zhuang</author>
        <year>2010</year>
        <source>smartech.gatech.edu</source>
        <snippet status="partial" source="scholar.google.com">This work studies application acceleration for wireless and mobile data networks. The problem of accelerating application can be addressed along multiple dimensions. The first dimension is advanced network protocol design, ie, optimizing underlying network ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:b7QaIaJXbo4J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10263236954675655791&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://www.springerlink.com/index/62304J77H6556JKP.pdf</url>
        <title status="complete" source="scholar.google.com">Mobile hosts participating in peer-to-peer data networks: challenges and solutions</title>
        <author status="partial" source="scholar.google.com">Z Zhuang, S Kakumanu, Y Jeong, R Sivakumar&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Wireless &#8230;</proceeding>
        <year>2010</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Peer-to-peer (P2P) data networks dominate Internet traffic, accounting for over 60% of the overall traffic in a recent study. In this work, we study the problems that arise when mobile hosts participate in P2P networks. We primarily focus on the performance issues ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:37WqJxtHTcUJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14217097780633712095&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://144.92.161.87/handle/1793/60716</url>
        <title status="complete" source="scholar.google.com">Date Mar 15, 2012</title>
        <pdf>http://144.92.161.87/bitstream/handle/1793/60716/TR1679.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">A Gember, A Anand, A Akella</author>
        <year>2012</year>
        <source>144.92.161.87</source>
        <snippet status="partial" source="scholar.google.com">Abstract Smartphones, portable music players, and other handheld devices have become a major computing platform. Wherever users go, they utilize 3G and WiFi connectivity to access a wide array of Internet services. The small, mobile nature of these devices results ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PNk1LokBm8MJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14094861147483527484&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>https://scholarbank.nus.edu.sg/handle/10635/16736</url>
        <title status="complete" source="scholar.google.com">Workload model for video decoding and its applications</title>
        <pdf>https://scholarbank.nus.edu.sg/bitstream/handle/10635/16736/Thesis_HuangYicheng.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">H Yicheng</author>
        <year>2008</year>
        <source>scholarbank.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">In this thesis, we study the relationship between decoding workload and video quality. Based on the analysis of video structure and decoder implementations, we propose a decoding workload model. Given a video clip, the model can accurately estimate the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:SWymZq2_0vUJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17713431035874012233&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://gemberdesign.com/docs/gember2010tr1679.pdf</url>
        <title status="complete" source="scholar.google.com">Department</title>
        <pdf>http://gemberdesign.com/docs/gember2010tr1679.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Gember, A Anand, A Akella</author>
        <year>2010</year>
        <source>gemberdesign.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Smartphones, portable music players, and other handheld devices have become a major computing platform. Wherever users go, they utilize 3G and WiFi connectivity to access a wide array of Internet services. The small, mobile nature of these ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BmzJgmFySsIJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:BmzJgmFySsIJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14000128154766240774&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://dl.acm.org/citation.cfm?id=1925106</url>
        <title status="complete" source="scholar.google.com">A fuzzy algorithm for dynamically adaptive multimedia streaming</title>
        <author status="complete" source="scholar.google.com">S Bagchi</author>
        <proceeding status="partial" source="scholar.google.com">ACM Transactions on Multimedia Computing, &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The QoS-aware delivery model of multimedia is an interesting research area. The wireless networking systems connecting mobile clients and media servers have created the paradigm of mobile multimedia. In mobile multimedia systems, the media delivery model ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:cPcehjgWPPAJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17310735499729500016&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17310735499729500016&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6365157</url>
        <title status="complete" source="scholar.google.com">Energy Efficient Multimedia Streaming to Mobile DevicesâA Survey</title>
        <pdf>http://users.tkk.fi/~siekkine/pub/hoque12survey.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Hoque, M Siekkinen, J Nurminen</author>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Energy conservation in battery powered mobile devices that perform wireless multimedia streaming has been a significant research problem since last decade. This is because these mobile devices consume a lot of power while receiving, decoding and ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5636333</url>
        <title status="complete" source="scholar.google.com">Power-thermal analysis of multimedia applications</title>
        <author status="complete" source="scholar.google.com">M Marcu, C Milos, D Tudor</author>
        <proceeding status="partial" source="scholar.google.com">Thermal Investigations of ICs and &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Multimedia applications become one of the most popular applications on mobile devices. An important current problem of multimedia applications running on battery powered devices is their power intensive profile, which entails an increase of device heat ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:epdTk7VhWqAJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://www.sciencedirect.com/science/article/pii/S0140366412001727</url>
        <title status="complete" source="scholar.google.com">Power consumption analysis of constant bit rate video transmission over 3G networks</title>
        <author status="partial" source="scholar.google.com">A Ukhanova, E Belyaev, L Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Computer &#8230;</proceeding>
        <year>2012</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">This paper presents an analysis of the power consumption of video data transmission with constant bit rate over 3G mobile wireless networks. The work includes the description of the radio resource control transition state machine in 3G networks, followed by a detailed ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:JN2snWYJHTsJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4259571158899023140&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4259571158899023140&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6196187</url>
        <title status="complete" source="scholar.google.com">Subjective Assessment of BitDetectâA Mechanism for Energy-Aware Multimedia Content Adaptation</title>
        <author status="complete" source="scholar.google.com">AN Moldovan, CH Muntean</author>
        <proceeding status="partial" source="scholar.google.com">Broadcasting, IEEE Transactions &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract As mobile devices are becoming more compact and powerful and as they start to be increasingly used for accessing power-hungry multimedia streaming applications, there is an increasing need for mechanisms to efficiently manage the limited battery power ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Nx48rhikcvMJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://uknowledge.uky.edu/gradschool_theses/275/</url>
        <title status="complete" source="scholar.google.com">POWER REDUCTION BY DYNAMICALLY VARYING SAMPLING RATE</title>
        <pdf>http://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1278&amp;context=gradschool_theses</pdf>
        <author status="complete" source="scholar.google.com">S Datta</author>
        <year>2006</year>
        <source>uknowledge.uky.edu</source>
        <snippet status="partial" source="scholar.google.com">Abstract In modern digital audio applications, a continuous audio signal stream is sampled at a fixed sampling rate, which is always greater than twice the highest frequency of the input signal, to prevent aliasing. A more energy efficient approach is to dynamically change the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:zZIOzF2HF9sJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15787235855695778509&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>http://www.cqvip.com/qk/96569x/200907/29990618.html</url>
        <title status="complete" source="scholar.google.com">ä¸ç§éå¯¹æ çº¿å¤åªä½ä¸å¡çåºäºèåçç¼å­åè°åº¦çåçèçç®æ³</title>
        <author status="complete" source="scholar.google.com">å´å®ï¼ çæï¼ ä¹ å</author>
        <proceeding status="complete" source="scholar.google.com">ç³»ç»ä»¿çå­¦æ¥</proceeding>
        <year>2009</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">ä¸ç§ææçèçç§»å¨ç»ç«¯åçæèçæ¹æ³æ¯å¨ä¿è¯ç¨æ·æå¡è´¨éçåæä¸, å°½å¯è½çå°æ çº¿ç½ç»æ¥å£(WNI) åæ¢è³ä¼ç ç¶æ. ç¶èæä»¬åç°, å¨å¤åªä½ä¸å¡æµçåç»å°è¾¾é´ééå¸¸ç­çæåµä¸, è¿ä¸ªç­ç¥å¹¶ä¸ååææ. æåºäºä¸ä¸ªæ°çåçèçç®æ³JBS, éè¿å¨åºç«è®¾ç½®ä¸ä¸ªæ´å½¢ä»£ç, å¯¹ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dS-jtc7FnIAJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://ppe.ufrj.br/ppe/production/tesis/amonteiro.pdf</url>
        <title status="partial" source="scholar.google.com">ESTRATÃGIA DE REDUÃÃO DE EMISSÃES DE POLUENTES NO SETOR DE TRANSPORTES POR MEIO DE SUBSTITUIÃÃO MODAL NA REGIÃO &#8230;</title>
        <pdf>http://ppe.ufrj.br/ppe/production/tesis/amonteiro.pdf</pdf>
        <author status="partial" source="scholar.google.com">TSAOCD DA, F COMO, P DOS REQUISITOS&#8230;</author>
        <year>1998</year>
        <source>ppe.ufrj.br</source>
        <snippet status="partial" source="scholar.google.com">Aos amigos, em especial Alexandre Szklo e Claude Cohen, professores e funcionÃ¡rios do Programa de Planejamento EnergÃ©tico e de Engenharia de Transportes que me ajudaram em tudo, nas discussÃµes de trabalhos e idÃ©ias, nas dicas de literatura, nas horas de ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:nUZI7lfhKAoJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:nUZI7lfhKAoJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>25</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=732082707223824029&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="3">
    <url>http://dl.acm.org/citation.cfm?id=1180777</url>
    <title status="complete" source="scholar.google.com">Syllabic level automatic synchronization of music signals and text lyrics</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2006_Syllabic_Level_Automatic_Synchronization_of_Music_Signals_and_Text_Lyrics.pdf</pdf>
    <author status="complete" source="scholar.google.com">D Iskandar, Y Wang, MY Kan, H Li</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 14th annual ACM &#8230;</proceeding>
    <year>2006</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Denny Iskandar Institute for Infocomm Research 21 Heng Mui Keng Terrace Singapore 119613idenny@i2r.a-star.edu.sg Ye Wang Min-Yen Kan School of Computing National University ofSingapore Singapore 177543 {wangye, kanmy}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:bHstnjZhhukJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>23</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=16827243944926346092&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>21</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=16827243944926346092&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="21">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432643</url>
        <title status="complete" source="scholar.google.com">LyricAlly: Automatic synchronization of textual lyrics to acoustic music signals</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.119.3200&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="partial" source="scholar.google.com">MY Kan, Y Wang, D Iskandar, TL New&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present LyricAlly, a prototype that automatically aligns acoustic musical signals with their corresponding textual lyrics, in a manner similar to manually-aligned karaoke. We tackle this problem based on a multimodal approach, using an appropriate pairing of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:UKV6kRlYEmEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6994750038097962320&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>27</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6994750038097962320&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1459359.1459382</url>
        <title status="complete" source="scholar.google.com">Combination of audio and lyrics features for genre classification in digital audio collections</title>
        <pdf>https://wikis.utexas.edu/download/attachments/4588896/may_acmmm08.pdf</pdf>
        <author status="complete" source="scholar.google.com">R Mayer, R Neumayer, A Rauber</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 16th ACM &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In many areas multimedia technology has made its way into mainstream. In the case of digital audio this is manifested in numerous online music stores having turned into profitable businesses. The widespread user adaption of digital audio both on home ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:JpIIj04pGOgJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>19</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16724162633624228390&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>16</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16724162633624228390&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.2653&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Relationships between lyrics and melody in popular music</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.2653&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">E Nichols, D Morris, S Basu, C Raphael</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 10&#39;th &#8230;</proceeding>
        <year>2009</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Composers of popular music weave lyrics, melody, and instrumentation together to create a consistent and compelling emotional scene. The relationships among these elements are critical to musical communication, and understanding the statistics ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:F1D2-7Fe3twJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:F1D2-7Fe3twJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>22</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15915262251703357463&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15915262251703357463&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.6633&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Multi-modal Analysis of Music: A large-scale Evaluation</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.6633&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">R Mayer, R Neumayer</author>
        <proceeding status="complete" source="scholar.google.com">Proc. WEMIS</proceeding>
        <year>2009</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">AbstractâMultimedia data by definition comprises several different types of content modalities. Music specifically inherits eg audio at its core, text in the form of lyrics, images by means of album covers, or video in the form of music videos. Yet, in many Music ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KcK3IYPgGk0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:KcK3IYPgGk0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5555999944125956649&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5555999944125956649&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5346497</url>
        <title status="complete" source="scholar.google.com">A novel framework for recognizing phonemes of singing voice in polyphonic music</title>
        <pdf>http://staff.aist.go.jp/h.fujihara/pdf/waspaa2009_fujihara.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Fujihara, M Goto, HG Okuno</author>
        <proceeding status="partial" source="scholar.google.com">Applications of Signal &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract A novel method is described that can be used to recognize the phoneme of a singing voice (vocal) in polyphonic music. Though we focus on the voiced phoneme in this paper, this method is design to concurrently recognize other elements of a singing voice ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:n85tdPiJTygJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2904691984875507359&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2904691984875507359&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf</url>
        <title status="complete" source="scholar.google.com">æ­å£°æå ±å¦çã®æè¿ã®ç ç©¶</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å°</author>
        <proceeding status="complete" source="scholar.google.com">æ¥æ¬é³é¿å­¦ä¼èª</proceeding>
        <year>2008</year>
        <source>staff.aist.go.jp</source>
        <snippet status="complete" source="scholar.google.com">â Recent studies on singing information processing. ââ Masataka Goto, Takeshi Saitou, Tomoyasu Nakano and Hiromasa Fujihara (National Institute of Ad- vanced Industrial Science and Technology (AIST), Tsukuba, 305â8568) e-mail: m.goto@aist.go.jp</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3835003938146142193&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>6</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3835003938146142193&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://proceedings.spiedigitallibrary.org/data/Conferences/SPIEP/17645/682004_1.pdf</url>
        <title status="complete" source="scholar.google.com">Enriching text with images and colored light</title>
        <pdf>http://144.206.159.178/FT/CONF/16408942/16408945.pdf</pdf>
        <author status="partial" source="scholar.google.com">D Sekulovski, G Geleijnse&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the is&amp;t/spie &#8230;</proceeding>
        <year>2008</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We present an unsupervised method to enrich textual applications with relevant images and colors. The images are collected by querying large image repositories and subsequently the colors are computed using image processing. A prototype system based ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:cCokTHsdtWsJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7761141948238539376&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7761141948238539376&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://www.springerlink.com/index/j181l048303427k1.pdf</url>
        <title status="complete" source="scholar.google.com">Multimodal Aspects of Music Retrieval: Audio, Song Lyricsâand Beyond?</title>
        <author status="complete" source="scholar.google.com">R Mayer, A Rauber</author>
        <proceeding status="complete" source="scholar.google.com">Advances in Music Information Retrieval</proceeding>
        <year>2010</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Music retrieval is predominantly seen as a problem to be tackled in the acoustic domain. With the exception of symbolic music retrieval and score-based systems, which form rather separate sub-disciplines on their own, most approaches to retrieve recordings of music by ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7RghtMuPw8UJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14250391750993582317&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14250391750993582317&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5876304</url>
        <title status="complete" source="scholar.google.com">Integrating Additional Chord Information into HMM-Based Lyrics-to-Audio Alignment</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/IEEETASLP201201mauch.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Mauch, H Fujihara, M Goto</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Aligning lyrics to audio has a wide range of applications such as the automatic generation of karaoke scores, song-browsing by lyrics, and the generation of audio thumbnails. Existing methods are restricted to using only lyrics and match them to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:4wt8-SvvXy0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3269594826642557923&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3269594826642557923&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <title status="complete" source="scholar.google.com">Singing Phoneme Class Detection In Polyphonic Music Recordings</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uJWqBAVmuaEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11653457682537027000&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11653457682537027000&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://dl.acm.org/citation.cfm?id=1363164</url>
        <title status="complete" source="scholar.google.com">Enriching music with synchronized lyrics, images and colored lights</title>
        <pdf>http://www.dse.nl/~gijsg/AmbiSys-GeleijnseEtAl.pdf</pdf>
        <author status="partial" source="scholar.google.com">G Geleijnse, D Sekulovski, J Korst, S Pauws&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 1st &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present a method to synchronize popular music with its lyrics at the stanza level. First we apply an algorithm to segment audio content into harmonically similar and/or contrasting progressions, ie the stanzas. We map the stanzas found to a sequence of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:GGSBRK85bhQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1472177553128121368&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1472177553128121368&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://dl.acm.org/citation.cfm?id=1823753</url>
        <title status="complete" source="scholar.google.com">Word level automatic alignment of music and lyrics using vocal synthesis</title>
        <author status="complete" source="scholar.google.com">NC Maddage, KC Sim, H Li</author>
        <proceeding status="partial" source="scholar.google.com">ACM Transactions on Multimedia &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a signal-based approach instead of the commonly used model-based approach, to automatically align vocal music with text lyrics at the word level. In this approach, we use a text-to-speech system to synthesize the singing voice according to the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:HJ9bzctkTegJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16739146216492474140&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16739146216492474140&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://eden.dei.uc.pt/~hroliv/pubs/GoncaloOliveiraMScThesis2007.pdf</url>
        <title status="complete" source="scholar.google.com">GeraÃ§ao de texto com base em ritmo</title>
        <pdf>http://eden.dei.uc.pt/~hroliv/pubs/GoncaloOliveiraMScThesis2007.pdf</pdf>
        <author status="complete" source="scholar.google.com">HRG Oliveira</author>
        <year>2007</year>
        <source>eden.dei.uc.pt</source>
        <snippet status="partial" source="scholar.google.com">Resumo Esta tese introduz o problema da geraÃ§Ã£o de texto baseado em ritmo, mais precisamente a geraÃ§Ã£o de uma letra de acordo com uma dada melodia. Numa primeira fase a base para o sistema foi preparada. Foram implementados algoritmos para obter a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:spooUTt_nQEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:spooUTt_nQEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=116389058122914482&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=116389058122914482&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://japanlinkcenter.org/JST.JSTAGE/jssst/26.1_4?from=Google</url>
        <title status="complete" source="scholar.google.com">é³æ¥½ã»é³å£°ã®é³é¿ä¿¡å·ã®èªè­ã»çè§£ç ç©¶ã®åå</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/JSSSTCS200902goto.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¾è¤çå­ï¼ ç·æ¹æ·³</author>
        <proceeding status="complete" source="scholar.google.com">ã³ã³ãã¥ã¼ã¿ ã½ããã¦ã§ã¢</proceeding>
        <year>2009</year>
        <source>J-STAGE</source>
        <snippet status="partial" source="scholar.google.com">æ¬è§£èª¬è«æã§ã¯, è¿å¹´ç®è¦ã¾ããç ç©¶ãé²å±ããé³æ¥½ã®é³é¿ä¿¡å·ã®èªè­ã»çè§£ã¨, é·å¹´ã®ç ç©¶ã«ããæ§è½ãåä¸ããé³å£°ã®é³é¿ä¿¡å·ã®èªè­ã»çè§£ã«ã¤ãã¦, ãã®ç ç©¶ã®ç¾ç¶ãç´¹ä»ãã. ã¤ã³ã¿ãã§ã¼ã¹ãã¤ã³ã¿ã©ã¯ã·ã§ã³ã«é¢é£ããå­¦ä¼ã«ããã¦ã, é³æ¥½ã»é³å£°ã«é¢ããç ç©¶ææã ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gQsFfFqphb0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13656507651163753345&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13656507651163753345&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf</url>
        <title status="complete" source="scholar.google.com">A repetition-based framework for lyric alignment in popular songs</title>
        <pdf>http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf</pdf>
        <author status="complete" source="scholar.google.com">LM Thang, KANM Yen</author>
        <proceeding status="partial" source="scholar.google.com">National University of Singapore &#8230;</proceeding>
        <year>2007</year>
        <source>w.comp.nus.edu</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We examine the problem of automatically aligning acoustic musical audio and textual lyric in popular songs. Existing works have tackled the problem using computationally-expensive audio processing techniques, resulting in solutions unsuitable ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>27</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17581284984694044119&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://drops.dagstuhl.de/opus/volltexte/2012/3464/</url>
        <title status="complete" source="scholar.google.com">Lyrics-to-Audio Alignment and its Application}}</title>
        <pdf>http://drops.dagstuhl.de/opus/volltexte/2012/3464/pdf/3.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Fujihara, M Goto</author>
        <proceeding status="complete" source="scholar.google.com">Multimodal Music Processing}</proceeding>
        <source>drops.dagstuhl.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic lyrics-to-audio alignment techniques have been drawing attention in the last years and various studies have been made in this field. The objective of lyrics-to-audio alignment is to estimate a temporal relationship between lyrics and musical audio signals ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pR0KaNFrpC0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3288872175025135013&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5946416</url>
        <title status="complete" source="scholar.google.com">Concurrent estimation of singing voice F0 and phonemes by using spectral envelopes estimated from polyphonic music</title>
        <pdf>http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202011/pdfs/0000365.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Fujihara, M Goto</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and Signal Processing &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The scarcity of available multi-track recordings constitutes a severe constraint on the training of probabilistic models for voice extraction from polyphonic music. We propose a novel training method to estimate a spectral envelope of a singing voice that makes it ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PRpLckBrSYIJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9388152822770113085&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://scholarbank.nus.sg/handle/10635/13382</url>
        <title status="partial" source="scholar.google.com">REFINING MUSIC SIGNAL TO LYRIC TEXT SYNCHRONIZATION FROM LINE-LEVEL TO SYLLABLE-LEVEL BY CONSTRAINING DYNAMIC TIME WARPING &#8230;</title>
        <pdf>http://scholarbank.nus.sg/bitstream/handle/10635/13382/thesis_21_amendment_2.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">D ISKANDAR</author>
        <year>2007</year>
        <source>scholarbank.nus.sg</source>
        <snippet status="partial" source="scholar.google.com">The problem we consider in this thesis is synchronization between lyric text and the corresponding singing voice recording. We limit the singing to the pop genre to make the problem manageable. The recordings we consider in this problem are those we can ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:q87ftenzmuwJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17049207524468706987&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://dl.acm.org/citation.cfm?id=2043615</url>
        <title status="complete" source="scholar.google.com">Beat space segmentation and octave scale cepstral feature for sung language recognition in pop music</title>
        <author status="complete" source="scholar.google.com">NC Maddage, H Li</author>
        <proceeding status="partial" source="scholar.google.com">ACM Transactions on Multimedia Computing, &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Sung language recognition relies on both effective feature extraction and acoustic modeling. In this paper, we study rhythm based music segmentation with the frame size being the duration of the smallest note in the music, as opposed to fixed length ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PDNw-UBaogIJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://staff.aist.go.jp/m.goto/PAPER/ASJ200909fujihara.pdf</url>
        <title status="complete" source="scholar.google.com">æ¥½æ²ä¸­ã®æ­å£°ã®åºæ¬å¨æ³¢æ°ã¨é³ç´ ãåææ¨å®å¯è½ãªãã¬ã¼ã ã¯ã¼ã¯</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/ASJ200909fujihara.pdf</pdf>
        <author status="complete" source="scholar.google.com">è¤åå¼å°ï¼ å¾è¤çå­ï¼ å¥¥ä¹å(äº¬å¤§</author>
        <proceeding status="complete" source="scholar.google.com">staff.aist.go.jp</proceeding>
        <snippet status="partial" source="scholar.google.com">é³æ¥½ã¯, ç£æ¥­çã«ãæåçã«ãéè¦ãªã³ã³ãã³ãã§ãã, ãã®ä¸­ã§ãæ­å£°ã¯éè¦ãªå½¹å²ãæããã¦ãã. æ¬ç¨¿ã§ã¯, æ··åé³ä¸­ã®æ­å£°ã®æ­è© (é³ç´ ) ã¨åºæ¬å¨æ³¢æ° (F0) ãåæã«èªè­ããããã®ææ³, WPST (Weighted composition of Probabilistic Spectral Template) æ³ãææ¡ã, F0 æ¨å®ã¨ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uGdvSfHJOboJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:uGdvSfHJOboJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007goto.pdf</url>
        <title status="complete" source="scholar.google.com">æ­å£°æå ±å¦ç: æ­å£°ãå¯¾è±¡ã¨ããé³æ¥½æå ±å¦ç</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007goto.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å°</author>
        <year>2010</year>
        <source>staff.aist.go.jp</source>
        <snippet status="partial" source="scholar.google.com">æ¬ç¨¿ã§ã¯,ãæ­å£°æå ±å¦çã ã¨åä»ããæ°ããç ç©¶é åã«ãããæãã®ç ç©¶äºä¾ãç´¹ä»ãã. ããã¯æ­å£°ã«å¯¾ããé³æ¥½æå ±å¦çã§ãã, ãã®ç ç©¶å¯¾è±¡ã¯å¤å²ã«æ¸¡ãã, æ¬ç¨¿ã§ã¯, æ­å£°çè§£ã·ã¹ãã , æ­å£°ã«åºã¥ãé³æ¥½æå ±æ¤ç´¢ã·ã¹ãã , æ­å£°åæã·ã¹ãã ã®ä¸ã¤ã®éè¦ãª ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uzd8aAf29qoJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:uzd8aAf29qoJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12319304342396745659&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="4">
    <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.5003&amp;rep=rep1&amp;type=pdf</url>
    <title status="complete" source="scholar.google.com">Low level descriptors for automatic violin transcription</title>
    <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.5003&amp;rep=rep1&amp;type=pdf</pdf>
    <author status="complete" source="scholar.google.com">A Loscos, Y Wang, WJJ Boo</author>
    <proceeding status="complete" source="scholar.google.com">Proc. of ISMIR2006</proceeding>
    <year>2006</year>
    <source>Citeseer</source>
    <snippet status="partial" source="scholar.google.com">Page 1. Low Level Descriptors for Automatic Violin Transcription Alex Loscos UniversitatPompeu Fabra / National University of Singapore Music Technology Group / Schoolof Computing aloscos@iua.upf.edu / loscos@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:2--epYjX8FMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:2--epYjX8FMJ:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <numVersions>25</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=6048571281452756955&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>12</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=6048571281452756955&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="12">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1290156</url>
        <title status="complete" source="scholar.google.com">Effective use of multimedia for computer-assisted musical instrument tutoring</title>
        <pdf>http://strum.googlecode.com/svn-history/r41/trunk/Research/CAMIT.pdf</pdf>
        <author status="complete" source="scholar.google.com">G Percival, Y Wang, G Tzanetakis</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a survey of recent work in computer-assisted musical instrumental tutoring and outlines several questions to consider when developing future projects. In particular, we suggest that the area ingreatest need of computer assistance is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:sSgpS4WrYA0J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>21</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=963958909237274801&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>18</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=963958909237274801&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=12</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1290154</url>
        <title status="complete" source="scholar.google.com">Educational violin transcription by fusing multimedia streams</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2007_Educational_Violin_Transcription_by_Fusing_Multimedia_Streams.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, B Zhang, O Schleusing</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Computer-assisted violin tutoring requires accurate violin transcription. For pitched non-percussive (PNP) sounds such as from the violin, note segmentation is a much more difficult task than pitch detection. This issue is accentuated when the audio is recorded ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:w23vslgkmBsJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1988379198861831619&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1988379198861831619&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=12</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.mistic.ece.uvic.ca/publications/2007_ismir_sitar.pdf</url>
        <title status="complete" source="scholar.google.com">Pedagogical transcription for multimodal sitar performance</title>
        <pdf>http://www.mistic.ece.uvic.ca/publications/2007_ismir_sitar.pdf</pdf>
        <author status="partial" source="scholar.google.com">A Kapur, G Percival, M Lagrange&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proc. Int&#39;l Conf. Music &#8230;</proceeding>
        <year>2007</year>
        <source>mistic.ece.uvic.ca</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Most automatic music transcription research is concerned with producing sheet music from the audio signal alone. However, the audio data does not include certain performance data which is vital for the preservation of instrument performance techniques ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Bo7GfV0RbCYJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Bo7GfV0RbCYJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>15</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2768606964165807622&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>6</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2768606964165807622&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=12</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4623948</url>
        <title status="complete" source="scholar.google.com">Application-specific music transcription for tutoring</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_Application_Specific_Music_Transcription_for_Tutoring.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, B Zhang</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic music transcription (AMT) refers to the ability of computers to write note information, such as the pitch, onset time, duration, and source of each sound, after listening to the music. Our application scenario is computer-assisted, musical-instrument tutoring, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vK-ug8uAcwIJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>18</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=176626421973561276&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=176626421973561276&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=12</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>https://dl.comp.nus.edu.sg/dspace/handle/1900.100/3056</url>
        <title status="complete" source="scholar.google.com">Automatic music transcription using audio-visual fusion for violin practice in home environment</title>
        <pdf>https://dl.comp.nus.edu.sg/dspace/bitstream/1900.100/3056/1/TRA7-09.pdf</pdf>
        <author status="complete" source="scholar.google.com">B Zhang, Y Wang</author>
        <year>2009</year>
        <source>dl.comp.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">Abstract: Violin practice in a home environment, where there is often no teacher available, can benefit from automatic music transcription to provide feedback to the student. This paper describes a high performance violin transcription system with three main contributions. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WDqOhljuXd0J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15951167519198165592&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15951167519198165592&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=12</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://www.springerlink.com/index/76x78554q0526132.pdf</url>
        <title status="complete" source="scholar.google.com">Adaptive music retrievalâa state of the art</title>
        <pdf>http://wwwiti.cs.uni-magdeburg.de/~stober/publ/mtap2012adaptiveMIR.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Stober, A NÃ¼rnberger</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Tools and Applications</proceeding>
        <year>2012</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract With the development of more and more sophisticated Music Information Retrieval approaches, aspects of adaptivity are becoming an increasingly important research topic. Even though, adaptive techniques have already found their way into Music Information ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:MvLbFPg5nPEJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17409853997172126258&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://digitallibrary.usc.edu/assetserver/controller/item/etd-Chuan-20080724.pdf</url>
        <title status="complete" source="scholar.google.com">Hybrid methods for music analysis and synthesis: Audio key finding and automatic style-specific accompaniment</title>
        <pdf>http://digitallibrary.usc.edu/assetserver/controller/item/etd-Chuan-20080724.pdf</pdf>
        <author status="complete" source="scholar.google.com">CH Chuan</author>
        <year>2008</year>
        <source>digitallibrary.usc.edu</source>
        <snippet status="partial" source="scholar.google.com">The Mirror of Erised at Hogwards that Harry Potter discovers in his first year there shows not your reflections, but your heart&#39;s desire. During the years of my doctoral study, I cannot remember just how many times I felt that if I looked into the Mirror of Erised, I would simply ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dkoAG5E2Ib8J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:dkoAG5E2Ib8J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13772349132326849142&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://ismir2012.ismir.net/event/papers/379-ismir-2012.pdf</url>
        <title status="complete" source="scholar.google.com">AUTOMATIC MUSIC TRANSCRIPTION: BREAKING THE GLASS CEILING</title>
        <pdf>http://ismir2012.ismir.net/event/papers/379-ismir-2012.pdf</pdf>
        <author status="partial" source="scholar.google.com">E Benetos, S Dixon, D Giannoulis, H Kirchhoff&#8230;</author>
        <year>2012</year>
        <source>ismir2012.ismir.net</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Automatic music transcription is considered by many to be the Holy Grail in the field of music signal analysis. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:XdGqHjeJcC0J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:XdGqHjeJcC0J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3274267798929068381&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://137.132.14.55/handle/10635/20949</url>
        <title status="complete" source="scholar.google.com">Adaptive multimodal fusion based similarity measures in music information retrieval</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/20949/ZhangBJ.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">Z BINGJUN</author>
        <year>2010</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">In the field of music information retrieval (MIR), one fundamental research problem is the measuring of the similarity between music documents. Based on a viable similarity measure, MIR systems can be made more effective to help users retrieve relevant music information. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9FjpHoxTlRIJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1339068325491726580&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://www.numediart.org/download/numediart_2012_s18_p3_report.pdf</url>
        <title status="complete" source="scholar.google.com">REALTIME TABLATURE</title>
        <pdf>http://www.numediart.org/download/numediart_2012_s18_p3_report.pdf</pdf>
        <author status="complete" source="scholar.google.com">L ReboursiÃ¨re, S Dupont</author>
        <proceeding status="complete" source="scholar.google.com">numediart.org</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In this project, we present a system which generates real-time and complete (ie with all guitar playing techniques) guitar tablature. It uses guitar playing techniques algorithms developed in previous Guitar As Controller numediart project and is ...</snippet>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:csITSlH5HCQJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://dl.acm.org/citation.cfm?id=2390855</url>
        <title status="complete" source="scholar.google.com">Learning and extraction of violin instrumental controls from audio signal</title>
        <pdf>http://up.stevetjoa.com/p25.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Perez Carrillo, MM Wanderley</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the second &#8230;</proceeding>
        <year>2012</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Acquisition of instrumental gestures in musical performances is an important task used in different fields ranging from acoustics and sound synthesis to motor learning or electroacoustic performances. The most common approach for acquiring gestures is by ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://www.sfu.ca/~eigenfel/ICMC08_Multiagent_Multimodal.pdf</url>
        <title status="complete" source="scholar.google.com">MULTI-AGENT MULTIMODAL PERFORMANCE ANALYSIS</title>
        <pdf>http://www.sfu.ca/~eigenfel/ICMC08_Multiagent_Multimodal.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Eigenfeldt, A Kapur</author>
        <year>2008</year>
        <source>sfu.ca</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper describes a custom built system which extracts high-level musical information from real-time multimodal gesture data. Data is collected from sensors during rehearsal using one program, GATHER, and, combined with audio analysis, is used to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KsPMitJqg0EJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:KsPMitJqg0EJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4720734286918763306&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="5">
    <url>http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202008/papers/ISMIR2008_127.pdf</url>
    <title status="complete" source="scholar.google.com">Multiple-feature fusion based onset detection for solo singing voice</title>
    <pdf>http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202008/papers/ISMIR2008_127.pdf</pdf>
    <author status="complete" source="scholar.google.com">CC Toh, B Zhang, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">&#8230; of the International Conference on Music &#8230;</proceeding>
    <year>2008</year>
    <source>mirlab.org</source>
    <snippet status="partial" source="scholar.google.com">... MULTIPLE-FEATURE FUSION BASED ONSET DETECTION FOR SOLO SINGING VOICE CheeChuan Toh Bingjun Zhang Ye Wang School of Computing National University of Singaporeu0403701@nus.edu.sg, {bingjun, wangye}@comp.nus.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:LV6xUoWvUdMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:LV6xUoWvUdMJ:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <numVersions>18</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=15227144802269224493&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>14</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=15227144802269224493&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="14">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5709966</url>
        <title status="complete" source="scholar.google.com">Signal processing for music analysis</title>
        <pdf>http://perso.telecom-paristech.fr/~grichard/Publications/2011_OverviewMusicSignalProcessing_IEEE-JSTSP.pdf</pdf>
        <author status="partial" source="scholar.google.com">M Muller, DPW Ellis, A Klapuri&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Selected Topics in &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music signal processing may appear to be the junior relation of the large and mature field of speech signal processing, not least because many techniques and representations originally developed for speech have been applied to music, often with ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:-24dnVOFQ9sJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15799618511910563579&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>30</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15799618511910563579&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5337997</url>
        <title status="complete" source="scholar.google.com">Three dimensions of pitched instrument onset detection</title>
        <pdf>http://www.ics.forth.gr/netlab/data/J12.pdf</pdf>
        <author status="partial" source="scholar.google.com">A Holzapfel, Y Stylianou, AC Gedik&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we suggest a novel group delay based method for the onset detection of pitched instruments. It is proposed to approach the problem of onset detection by examining three dimensions separately: phase (ie, group delay), magnitude and pitch. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_Pq3NteXXZkJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11051156011228855036&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>18</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11051156011228855036&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5654580</url>
        <title status="complete" source="scholar.google.com">Extracting predominant local pulse information from music recordings</title>
        <author status="complete" source="scholar.google.com">P Grosche, M Muller</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The extraction of tempo and beat information from music recordings constitutes a challenging task in particular for non-percussive music with soft note onsets and time-varying tempo. In this paper, we introduce a novel mid-level representation that captures ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:58s-5Z2nUIcJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9750477489854925799&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>13</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9750477489854925799&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5346500</url>
        <title status="complete" source="scholar.google.com">Improving MIDI-audio alignment with acoustic features</title>
        <pdf>http://academiccommons.columbia.edu/download/fedora_content/download/ac:148457/CONTENT/DevanME09-align.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Devaney, MI Mandel, DPW Ellis</author>
        <proceeding status="partial" source="scholar.google.com">Applications of Signal &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper describes a technique to improve the accuracy of dynamic time warping-based MIDI-audio alignment. The technique implements a hidden Markov model that uses aperiodicity and power estimates from the signal as observations and the results of a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:c83PruWVT2wJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>11</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7804621492969262451&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>9</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7804621492969262451&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://www.csd.uoc.gr/~hannover/MMILab-Andre_files/Holzapfel_dissertation.pdf</url>
        <title status="complete" source="scholar.google.com">Similarity methods for computational ethnomusicology</title>
        <pdf>http://www.csd.uoc.gr/~hannover/MMILab-Andre_files/Holzapfel_dissertation.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Holzapfel, Y Stylianou</author>
        <year>2010</year>
        <source>csd.uoc.gr</source>
        <snippet status="partial" source="scholar.google.com">Abstract The field of computational ethnomusicology has drawn growing attention by researchers in the music information retrieval community. In general, subjects are considered that are related to the processing of traditional forms of music, often with the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:DBTHiUqu2KoJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:DBTHiUqu2KoJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12310781216579589132&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12310781216579589132&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ismir2009.ismir.net/proceedings/ps1-11.pdf</url>
        <title status="complete" source="scholar.google.com">A Comparison of Score-level Fusion Rules for Onset Detection in Music Signals</title>
        <pdf>http://ismir2009.ismir.net/proceedings/ps1-11.pdf</pdf>
        <author status="partial" source="scholar.google.com">N Degara-Quintela, A Pena&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
        <year>2009</year>
        <source>ismir2009.ismir.net</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Finding automatically the starting time of audio events is a difficult process. A promising approach for onset detection lies in the combination of multiple algorithms. The goal of this paper is to compare score-level fusion rules that combine signal processing ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bHtS34r3l8cJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:bHtS34r3l8cJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14382236110855240556&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14382236110855240556&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://hal.archives-ouvertes.fr/tel-00457522/</url>
        <title status="complete" source="scholar.google.com">CaractÃ©risation de l&#39;environnement musical dans les documents audiovisuels</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/45/75/22/PDF/These_Helene_Lachambre.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Lachambre</author>
        <year>2009</year>
        <source>hal.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">Depuis plusieurs dizaines d&#39;annÃ©es, l&#39;indexation par le contenu des documents audiovisuels fait l&#39;objet de travaux de la part de nombreuses Ã©quipes de recherche:âLe mot Â«audiovisuelÂ» fait rÃ©fÃ©rencea un contenu Â«multimÃ©diaÂ» qui regroupea la fois des donnÃ©es ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Yp_9nKZh3GwJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7844252019198893922&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7844252019198893922&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://www.waset.ac.nz/journals/waset/v36/v36-51.pdf</url>
        <title status="complete" source="scholar.google.com">Practical Method for Digital Music Matching Robust to Various Sound Qualities</title>
        <pdf>http://www.waset.ac.nz/journals/waset/v36/v36-51.pdf</pdf>
        <author status="partial" source="scholar.google.com">B Sung, J Kim, J Kwun, J Park, J Ryeo&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">World Academy of Science, &#8230;</proceeding>
        <year>2009</year>
        <source>waset.ac.nz</source>
        <snippet status="partial" source="scholar.google.com">AbstractâIn this paper, we propose a practical digital music matching system that is robust to variation in sound qualities. The proposed system is subdivided into two parts: client and server. The client part consists of the input, preprocessing and feature extraction modules. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bBpgYCh3uI8J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:bBpgYCh3uI8J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10356158358437763692&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10356158358437763692&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://institut17-1.kug.ac.at/fileadmin/media/iem/projects/2011/gampp_ti.pdf</url>
        <title status="complete" source="scholar.google.com">Evaluation of Robust Features for Singing Voice Detection</title>
        <pdf>http://institut17-1.kug.ac.at/fileadmin/media/iem/projects/2011/gampp_ti.pdf</pdf>
        <author status="complete" source="scholar.google.com">P Gampp</author>
        <proceeding status="complete" source="scholar.google.com">institut17-1.kug.ac.at</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract The detection of singing voice segments within music signals is an important object of research in the field of music information retrieval, since it serves as an essential pre-stage for applications like singer identification, lyrics recognition, singing melody ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:CMMZACBTsKIJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:CMMZACBTsKIJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11722961226951148296&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6128359</url>
        <title status="complete" source="scholar.google.com">An Implementation method for converting the erhu music from wav to mid</title>
        <author status="complete" source="scholar.google.com">Z Nie, S Yang</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Intelligence and Security (CIS), 2011 Seventh &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This article mainly introduced the specific process of transforming an erhu (Chinese violin) music file from wav to mid. The process included abstracting the fundamental frequency of the notes by means of DWT and FFT analysis, then transforming the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:igwfnGd55zQJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3812149095500287114&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://www.springerlink.com/index/Y852J36287804304.pdf</url>
        <title status="complete" source="scholar.google.com">Note recognition from monophonic audio: a clustering approach</title>
        <author status="complete" source="scholar.google.com">R Typke</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Retrieval. Understanding Media and Adapting to the &#8230;</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">We describe a new method for recognizing notes from monophonic audio, such as sung or whistled queries. Our method achieves results similar to known methods, but without any probabilistic models that would need to be trained. Instead, we define a distance function ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vY2LpFYadaQJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11850406953925447101&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://www.gts.uvigo.es/~ndegara/Publications_files/Thesis_Degara.pdf</url>
        <title status="complete" source="scholar.google.com">Signal Processing Methods for Analyzing the Temporal Structure of Music Exploiting Rhythmic Knowledge</title>
        <pdf>http://www.gts.uvigo.es/~ndegara/Publications_files/Thesis_Degara.pdf</pdf>
        <author status="complete" source="scholar.google.com">ND Quintela</author>
        <year>2011</year>
        <source>gts.uvigo.es</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music is one of the most important sources of information on the Internet and the development of algorithms for searching, navigating, retrieving and organizing music has become a major challenge. This field of research is known as Music Information Retrieval ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Y9SE8c7Qmz4J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Y9SE8c7Qmz4J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://ismir2012.ismir.net/event/papers/511-ismir-2012.pdf</url>
        <title status="complete" source="scholar.google.com">A STUDY OF INTONATION IN THREE-PART SINGING USING THE AUTOMATIC MUSIC PERFORMANCE ANALYSIS AND COMPARISON TOOLKIT (AMPACT)</title>
        <pdf>http://ismir2012.ismir.net/event/papers/511-ismir-2012.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Devaney, M Mandel, I Fujinaga</author>
        <year>2012</year>
        <source>ismir2012.ismir.net</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper introduces the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT), is a MATLAB toolkit for accurately aligning monophonic audio to MIDI scores as well as extracting and analyzing timing-, pitch-, and dynamics- ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:SsY3-1nvxekJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:SsY3-1nvxekJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://www.cqvip.com/qk/97969a/201207/42545878.html</url>
        <title status="complete" source="scholar.google.com">ä¸ç§äºè¡é³ä¹ç WAVE è½¬ MIDI çè®¾è®¡æ¹æ³</title>
        <author status="complete" source="scholar.google.com">èå­å¿ï¼ æ¨å£«é¢</author>
        <proceeding status="complete" source="scholar.google.com">è®¡ç®æºææ¯ä¸åå±</proceeding>
        <year>2012</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">æä¸­çç®çæ¯ä»ç»ä¸ç§å°WAVE æ ¼å¼çäºè¡é³ä¹æä»¶è½¬æ¢æMIDI æ ¼å¼çè®¾è®¡è¿ç¨. å·ä½æ¹æ³å¦ä¸: é¦å, å©ç¨å°æ³¢åæ¢åå¿«éåéå¶åæ¢èåæååºé³ç¬¦çåºé¢; å¶æ¬¡, æ ¹æ®MIDI é³ä¹çæ¶æ¯æ ¼å¼, å°å¾å°çååºé¢è½¬æ¢æç¸åºçMII é³ç¬¦; åæ¬¡, æ ¹æ®äºè¡é³ä¹çç¹æ§, è®¾å®å ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:5xXH4OrTmbYJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13157780792086566375&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="6">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4037041</url>
    <title status="complete" source="scholar.google.com">A violin music transcriber for personalized learning</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2006_A_Violin_Music_Transcriber_for_Personalized_Learning.pdf</pdf>
    <author status="complete" source="scholar.google.com">WJJ Boo, Y Wang, A Loscos</author>
    <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2006 IEEE &#8230;</proceeding>
    <year>2006</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... LEARNING Wei Jie Jonathan Boo, Ye Wang, Alex Loscos Department of ComputerScience, School of Computing National University of Singapore, Singapore 117543{booweiji, wangye, loscos}@comp.nus.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:mCNMB2GpzukJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>13</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=16847589490238956440&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>8</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=16847589490238956440&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="8">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1290156</url>
        <title status="complete" source="scholar.google.com">Effective use of multimedia for computer-assisted musical instrument tutoring</title>
        <pdf>http://strum.googlecode.com/svn-history/r41/trunk/Research/CAMIT.pdf</pdf>
        <author status="complete" source="scholar.google.com">G Percival, Y Wang, G Tzanetakis</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a survey of recent work in computer-assisted musical instrumental tutoring and outlines several questions to consider when developing future projects. In particular, we suggest that the area ingreatest need of computer assistance is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:sSgpS4WrYA0J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>21</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=963958909237274801&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>18</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=963958909237274801&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202008/papers/ISMIR2008_127.pdf</url>
        <title status="complete" source="scholar.google.com">Multiple-feature fusion based onset detection for solo singing voice</title>
        <pdf>http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202008/papers/ISMIR2008_127.pdf</pdf>
        <author status="complete" source="scholar.google.com">CC Toh, B Zhang, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the International Conference on Music &#8230;</proceeding>
        <year>2008</year>
        <source>mirlab.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Onset detection is a challenging problem in automatic singing transcription. In this paper, we address singing onset detection with three main contributions. First, we outline the nature of a singing voice and present a new singing onset detection approach ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:LV6xUoWvUdMJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:LV6xUoWvUdMJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>18</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15227144802269224493&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>14</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15227144802269224493&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://dl.acm.org/citation.cfm?id=1290154</url>
        <title status="complete" source="scholar.google.com">Educational violin transcription by fusing multimedia streams</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2007_Educational_Violin_Transcription_by_Fusing_Multimedia_Streams.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, B Zhang, O Schleusing</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Computer-assisted violin tutoring requires accurate violin transcription. For pitched non-percussive (PNP) sounds such as from the violin, note segmentation is a much more difficult task than pitch detection. This issue is accentuated when the audio is recorded ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:w23vslgkmBsJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1988379198861831619&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1988379198861831619&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.5003&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Low level descriptors for automatic violin transcription</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.5003&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">A Loscos, Y Wang, WJJ Boo</author>
        <proceeding status="complete" source="scholar.google.com">Proc. of ISMIR2006</proceeding>
        <year>2006</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">Abstract On top of previous work in automatic violin transcription we present a set of straight forward low level descriptors for assisting the transcription techniques and saving computational cost. Proposed descriptors have been tested against a database of 1500 ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2--epYjX8FMJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:2--epYjX8FMJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>25</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6048571281452756955&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>12</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6048571281452756955&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4959552</url>
        <title status="complete" source="scholar.google.com">Transcription and expressiveness detection system for violin music</title>
        <author status="partial" source="scholar.google.com">I Barbancho, C de la Bandera&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; , Speech and Signal &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, a transcription system for music played by violin is presented. The transcription system not only detects the pitch and duration of the notes but also identifies successfully the employed technique to play each note: detache with and without accent ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Neb3DJIbwCkJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3008434865180239413&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>9</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3008434865180239413&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4623948</url>
        <title status="complete" source="scholar.google.com">Application-specific music transcription for tutoring</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_Application_Specific_Music_Transcription_for_Tutoring.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, B Zhang</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic music transcription (AMT) refers to the ability of computers to write note information, such as the pitch, onset time, duration, and source of each sound, after listening to the music. Our application scenario is computer-assisted, musical-instrument tutoring, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vK-ug8uAcwIJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>18</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=176626421973561276&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=176626421973561276&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://www.springerlink.com/index/Q7771330VV042511.pdf</url>
        <title status="complete" source="scholar.google.com">What signal processing can do for the music</title>
        <author status="partial" source="scholar.google.com">I Barbancho, L TardÃ³n, A Barbancho, A Ortiz&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Exploring Music &#8230;</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">In this paper, several examples of what signal processing can do in the music context will be presented. In this contribution, music content includes not only the audio files but also the scores. Using advanced signal processing techniques, we have developed new tools that ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:iAYTxWLb-FsJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6627288068935321224&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4517560</url>
        <title status="complete" source="scholar.google.com">Onset detection in pitched non-percussive music using warping-compensated correlation</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2008_Onset_Detection_in_Piteched_Non-Percussive_Music_Using_Warping_Compensated_Correlation.pdf</pdf>
        <author status="complete" source="scholar.google.com">O Schleusing, B Zhang, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatically extracting temporal information from musical recordings is inarguably one of the most critical subtasks of many music information retrieval systems. In this paper we present a system for automatic note onset detection in pitched non-percussive (PNP) ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Iyh_8psTDngJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>14</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8650873494734579747&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="7">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4959519</url>
    <title status="complete" source="scholar.google.com">Cultural style based music classification of audio signals</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2009_Cultural_Style_Based_Music_Classification_of_Audio_Signals.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Liu, Q Xiang, Y Wang, L Cai</author>
    <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and Signal &#8230;</proceeding>
    <year>2009</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... Tsinghua University, Beijing, China 2 Tsinghua National Laboratory for Information Science andTechnology, Beijing, China 3 School of Computing, National University of Singapore, Singaporeliuyuxiang06@mails.tsinghua.edu.cn, xiangqiaoliang@comp.nus.edu.sg, wangye ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:xFuLtQsJEi8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>18</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=3391783415258504132&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>14</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=3391783415258504132&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="14">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5495605</url>
        <title status="complete" source="scholar.google.com">Automatic state discovery for unstructured audio scene classification</title>
        <pdf>http://siddiqi.com/sajid/papers/ramos-siddiqi-etal.audioclassify.pdf</pdf>
        <author status="partial" source="scholar.google.com">J Ramos, S Siddiqi, A Dubrawski&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Speech and Signal &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper we present a novel scheme for unstructured audio scene classification that possesses three highly desirable and powerful features: autonomy, scalability, and robustness. Our scheme is based on our recently introduced machine learning algorithm ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Hz05YbNUP2kJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>14</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7583873426922224927&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7583873426922224927&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ismir2009.ismir.net/proceedings/OS9-5.pdf</url>
        <title status="complete" source="scholar.google.com">Music and geography: Content description of musical audio from different parts of the world</title>
        <pdf>http://ismir2009.ismir.net/proceedings/OS9-5.pdf</pdf>
        <author status="complete" source="scholar.google.com">E GÃ³mez, M Haro, P Herrera</author>
        <proceeding status="complete" source="scholar.google.com">Proc. of ISMIR</proceeding>
        <year>2009</year>
        <source>ismir2009.ismir.net</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper analyses how audio features related to different musical facets can be useful for the comparative analysis and classification of music from diverse parts of the world. The music collection under study gathers around 6,000 pieces, including traditional ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:isEy8B4nfaYJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:isEy8B4nfaYJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>14</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11996787996217229706&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11996787996217229706&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.music.mcgill.ca/~ich/classes/mumt621_09/presentations/germain/Final_Project_Report.pdf</url>
        <title status="complete" source="scholar.google.com">The wavelet transform Applications in Music Information Retrieval</title>
        <pdf>http://www.music.mcgill.ca/~ich/classes/mumt621_09/presentations/germain/Final_Project_Report.pdf</pdf>
        <author status="complete" source="scholar.google.com">F Germain</author>
        <proceeding status="complete" source="scholar.google.com">McGill University, Canada, December</proceeding>
        <year>2009</year>
        <source>music.mcgill.ca</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this report, we present an overview of existing literature about wavelet-based approaches in music information retrieval (MIR). Indeed, we wants to analyze the possibilities of this novel and popular transform in this particular field. in a first time, we ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jigZGN-ykKcJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:jigZGN-ykKcJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12074347272232052878&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12074347272232052878&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://dl.acm.org/citation.cfm?id=1874037</url>
        <title status="complete" source="scholar.google.com">A music search engine for therapeutic gait training</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/2.Applications_in_e-Health/2010_A_Music_Search_Engine_for_Therapeutic_Gait_Training.pdf</pdf>
        <author status="partial" source="scholar.google.com">Z Li, Q Xiang, J Hockman, J Yang, Y Yi&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract A music retrieval system is introduced that incorporate tempo, cultural, and beat strength features to help music therapists provide appropriate music for gait training for Parkinson&#39;s patients. Unlike current methods available to music therapists (eg, personal ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:CDDHVVzuQNoJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15726831979121291272&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15726831979121291272&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA515929</url>
        <title status="complete" source="scholar.google.com">Learning Latent Variable and Predictive Models of Dynamical Systems</title>
        <pdf>http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA515929</pdf>
        <author status="complete" source="scholar.google.com">SM Siddiqi</author>
        <year>2009</year>
        <source>DTIC Document</source>
        <snippet status="partial" source="scholar.google.com">Abstract: In this thesis we propose novel learning algorithms that address the issues of model selection, local minima and instability in learning latent variable models. We show that certain&#39;predictive&#39;latent variable model learning methods bridge the gap between ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:MX8UQIK_-_4J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18373489670929809201&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=18373489670929809201&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://lsas2008.dke-research.de/conftool/uploads/158/10-Sturm20121212.pdf</url>
        <title status="complete" source="scholar.google.com">A survey of evaluation in music genre recognition</title>
        <pdf>http://lsas2008.dke-research.de/conftool/uploads/158/10-Sturm20121212.pdf</pdf>
        <author status="complete" source="scholar.google.com">BL Sturm</author>
        <proceeding status="partial" source="scholar.google.com">Proc. Adaptive Multimedia Retrieval. &#8230;</proceeding>
        <year>2012</year>
        <source>lsas2008.dke-research.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract. Much work is focused upon music genre recognition (MGR) from audio recordings, symbolic data, and other modalities. While reviews have been written of some of this work before, no survey has been made of the approaches to evaluating approaches to MGR. ...</snippet>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:kE5jbYE3CTsJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4253992352053677712&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <title status="complete" source="scholar.google.com">Web Music Indexing and Search</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:r9qlLF4oNbMJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://dl.acm.org/citation.cfm?id=2160791</url>
        <title status="complete" source="scholar.google.com">Traditional and folk melody classifier on culture style using Markov models and neural networks</title>
        <author status="complete" source="scholar.google.com">C Karunatilake, S Nishimura, M Osano</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 2012 Joint &#8230;</proceeding>
        <year>2012</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music plays a vital role in any culture despite whether it is primary or modern and it is a good indicator reflecting the nature of the culture where it has been produced. Music traditions were developed even in the pre-historic periods when people did not have a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:AB-hS3XCY6AJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://www.aes.org/e-lib/browse.cfm?elib=15744</url>
        <title status="complete" source="scholar.google.com">Retargeting Expressive Musical Style from Classical Music Recordings Using a Support Vector Machine</title>
        <pdf>http://nomislui.net/resources/Paper/2010-AES.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Lui, A Horner, C So</author>
        <proceeding status="complete" source="scholar.google.com">Watermark</proceeding>
        <year>2012</year>
        <source>aes.org</source>
        <snippet status="partial" source="scholar.google.com">A method for transferring the expressive musical nuances of real recordings to a MIDI synthesized version was successfully demonstrated. Three features (dynamics, tempo, and articulation) were extracted from the recordings and then applied to the MIDI note list in ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KYuKWFEzljUJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3861330154987621161&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://www.ee.iitb.ac.in/web/files/publications/VidwansPJan2012.pdf</url>
        <title status="complete" source="scholar.google.com">Identifying Indian Classical Music Styles using Melodic Contours</title>
        <pdf>http://www.ee.iitb.ac.in/web/files/publications/VidwansPJan2012.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Vidwans, P Rao</author>
        <proceeding status="complete" source="scholar.google.com">ee.iitb.ac.in</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract A prominent categorization of Indian classical music is the Hindustani and Carnatic traditions. The distinction is geographical with the two styles having evolved under distinctly different historical and cultural influences. Both styles are grounded in the melodic and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:N-o5j0x54zcJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:N-o5j0x54zcJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4027195861526768183&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4027195861526768183&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://dl.acm.org/citation.cfm?id=2095679</url>
        <title status="complete" source="scholar.google.com">Artist filtering for non-western music classification</title>
        <author status="complete" source="scholar.google.com">A Kruspe, H Lukashevich, J AbeÃer</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the 6th Audio Mostly Conference &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The&quot; album effect&quot; is a known phenomenon in musical artist and genre recognition. Classification results are often better when songs from the same album are used in the training and evaluation data sets. Supposedly, this effect is caused by the production ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ZIgzlvMbfdMJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://dl.acm.org/citation.cfm?id=2396458</url>
        <title status="complete" source="scholar.google.com">A domain-specific music search engine for gait training</title>
        <author status="complete" source="scholar.google.com">Z Li, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 20th ACM international conference &#8230;</proceeding>
        <year>2012</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper demonstrates a domain-specific music retrieval system to help music therapists find appropriate music for Parkinson&#39;s disease patients in their gait training. Different from existing music search engines, this system incorporates multiple music ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://mtg.upf.edu/system/files/publications/Carlos-Vaquero-Master-Thesis-2012.pdf</url>
        <title status="complete" source="scholar.google.com">Improving the description of instrumental sounds by using ontologies and automatic content analysis</title>
        <pdf>http://mtg.upf.edu/system/files/publications/Carlos-Vaquero-Master-Thesis-2012.pdf</pdf>
        <author status="complete" source="scholar.google.com">CV Patricio</author>
        <year>2012</year>
        <source>mtg.upf.edu</source>
        <snippet status="partial" source="scholar.google.com">Abstract Browsing sound collections in a social database is a complex task when no uniformity in the classification of the sounds and tags is applied; the relation between tag concepts and sounds can vary extremely from one user to another, as well as the types of ...</snippet>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:wYmh7WF17u4J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://www.ee.iitb.ac.in/web/files/publications/VidwansPCJul2012.pdf</url>
        <title status="complete" source="scholar.google.com">CLASSIFICATION OF INDIAN CLASSICAL VOCAL STYLES FROM MELODIC CONTOURS</title>
        <pdf>http://www.ee.iitb.ac.in/web/files/publications/VidwansPCJul2012.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Vidwans, KK Ganguli, P Rao</author>
        <year>2012</year>
        <source>ee.iitb.ac.in</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT A prominent categorization of Indian classical music is the Hindustani and Carnatic traditions, the two styles having evolved under distinctly different historical and cultural influences. Both styles are grounded in the melodic and rhythmic framework of ...</snippet>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:qrz4-cKETN8J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="8">
    <url>http://dl.acm.org/citation.cfm?id=957013.957085</url>
    <title status="complete" source="scholar.google.com">Application of a content-based percussive sound synthesizer to packet loss recovery in music streaming</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/5.Error_Robust_Audio_Streaming/2003_Application_of_a_Content-Based_Percussive_Sound_Synthesizer_to_Packet_Loss_Recovery_in_Music_Streaming.pdf</pdf>
    <author status="complete" source="scholar.google.com">L Wyse, Y Wang, X Zhu</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the eleventh ACM international &#8230;</proceeding>
    <year>2003</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Lonce Wyse Institute for Incomm Research (I2R) Heng Mui Keng Terrace Singapore119613 lonce@zwhome.org Ye Wang National University of Singapore (NUS) 3Science Drive 2 Singapore 117543 wangye@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:HGKMS2M5poMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>13</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=9486332763734565404&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>9</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=9486332763734565404&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="9">
        <result id="0">
        <url>http://web.media.mit.edu/~tristan/Papers/PhD_Tristan.pdf</url>
        <title status="complete" source="scholar.google.com">Creating music by listening</title>
        <pdf>http://web.media.mit.edu/~tristan/Papers/PhD_Tristan.pdf</pdf>
        <author status="complete" source="scholar.google.com">T Jehan</author>
        <year>2005</year>
        <source>media.mit.edu</source>
        <snippet status="partial" source="scholar.google.com">Abstract Machines have the power and potential to make expressive music on their own. This thesis aims to computationally model the process of creating music using experience from listening to examples. Our unbiased signal-based solution models the life cycle of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:oZbrKLeWnSwJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:oZbrKLeWnSwJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3214891422422111905&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>109</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3214891422422111905&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dafx04.na.infn.it/WebProc/Proc/P_345.pdf</url>
        <title status="complete" source="scholar.google.com">Sound texture modeling and time-frequency LPC</title>
        <pdf>http://dafx04.na.infn.it/WebProc/Proc/P_345.pdf</pdf>
        <author status="complete" source="scholar.google.com">X Zhu, L Wyse</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 7th International Conference &#8230;</proceeding>
        <year>2004</year>
        <source>dafx04.na.infn.it</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper presents a method to model and synthesize the textures of sounds such as fire, footsteps and typewriters using time and frequency domain linear prediction coding (TFLPC). The common character of this class of sounds is that they have a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Uufok0W6nTgJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4079621645464561490&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>22</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4079621645464561490&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.sciencedirect.com/science/article/pii/S0164121204002298</url>
        <title status="complete" source="scholar.google.com">REDUP: a packet loss recovery scheme for real-time audio streaming over wireless IP networks</title>
        <pdf>http://ir.lib.stut.edu.tw/bitstream/987654321/9845/1/J4_REDUP%2520A%2520Packet-Loss%2520Recovery%2520Scheme%2520for%2520Real-Time%2520Audio%2520Streaming%2520over%2520Wireless%2520IP%2520Networks.pdf</pdf>
        <author status="complete" source="scholar.google.com">CM Huang, TH Hsu, YW Lin</author>
        <proceeding status="complete" source="scholar.google.com">Journal of Systems and Software</proceeding>
        <year>2006</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">Due to the characteristics of (1) smaller bandwidth and (2) unreliable transmission media, real-time audio streaming over wireless networks is not trivial. To have smooth audio streaming over wireless networks, we propose a scheme called REDUP in this paper. Two ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jI5tCkavSMAJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13855516968962854540&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13855516968962854540&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://eprints.ulster.ac.uk/21380</url>
        <title status="complete" source="scholar.google.com">Song form intelligence for streaming music across wireless bursty networks</title>
        <pdf>http://eprints.ulster.ac.uk/21380/1/sofiaics05.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Doherty, K Curran, P McKevitt</author>
        <proceeding status="partial" source="scholar.google.com">Irish Conference on Artificial &#8230;</proceeding>
        <year>2005</year>
        <source>eprints.ulster.ac.uk</source>
        <snippet status="partial" source="scholar.google.com">Preliminary research on the development of a system for streaming audio across a wireless network, whilst using Song Form Intelligence (SoFI) to correct bursty errors, is presented. Current problems identified with streaming audio across wireless networks are reviewed. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:iZkMCRPLCcEJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13909872206727125385&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13909872206727125385&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1386017</url>
        <title status="complete" source="scholar.google.com">Generative Sound Models</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.1292&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">L Wyse</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia Modelling Conference, 2005. MMM 2005. &#8230;</proceeding>
        <year>2005</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract An overview of generative sound models is presented. We discuss the benefits they offer in a variety of media contexts including indexing and retrieval, compression, sonification, traditional media and interactive media production. We examine ways in ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:NEFegJ4GjBsJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1984968813591806260&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1984968813591806260&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://www.springerlink.com/index/FT671731J0403662.pdf</url>
        <title status="complete" source="scholar.google.com">A Survey of Music Structure Analysis Techniques for Music Applications</title>
        <author status="complete" source="scholar.google.com">N Maddage, H Li, M Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">Recent Advances in Multimedia Signal &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Music carries multilayer information which forms different structures. The information embedded in the music can be categorized into time information, harmony/melody, music regions, music similarities, song structures and music semantics. In this chapter, we first ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3vo8SvZRZuUJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16529989600559299294&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1540230</url>
        <title status="complete" source="scholar.google.com">A method for separating drum objects from polyphonic musical signals</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_A_Method_for_Separating_Drum_Objects_from_Polyphonic_Musical_Signals.pdf</pdf>
        <author status="complete" source="scholar.google.com">W Huang, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Applications of Signal Processing to Audio &#8230;</proceeding>
        <year>2005</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract An additional coding of auditory objects for packet loss concealment has been proven to be effective in music streaming applications. This paper describes a new extension to our previous method in separating drum objects from polyphonic music ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:kmLNmfRrmJwJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11283887564673344146&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://www.scis.ulster.ac.uk/~jonathan/Research/papers/JDoherty1stYrRpt.doc</url>
        <title status="complete" source="scholar.google.com">Song Form Intelligence with Streaming Audio</title>
        <pdf>http://www.scis.ulster.ac.uk/~jonathan/Research/papers/JDoherty1stYrRpt.doc</pdf>
        <author status="complete" source="scholar.google.com">J Doherty, P Mc Kevitt, K Curran</author>
        <proceeding status="complete" source="scholar.google.com">scis.ulster.ac.uk</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract Streaming media across the Internet is still an unreliable and poor quality medium. Services such as audio-on-demand drastically increase the load on the networks therefore new, robust and highly efficient coding algorithms will be necessary. One overlooked ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BLvYyU2HRiYJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:BLvYyU2HRiYJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2758040589979663108&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://eprints.ulster.ac.uk/21381</url>
        <title status="complete" source="scholar.google.com">Error concealment for streaming audio across wireless bursty networks</title>
        <pdf>http://eprints.ulster.ac.uk/21381/1/sofipgnet05.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Doherty, K Curran, P McKevitt</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Postgraduate Symposium on &#8230;</proceeding>
        <year>2005</year>
        <source>eprints.ulster.ac.uk</source>
        <snippet status="partial" source="scholar.google.com">Preliminary research on the development of an application for streaming audio across a wireless network, whilst using song form intelligence (SoFI) to correct bursty errors, is presented. Current problems identified with streaming audio across wireless networks are ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gbHZ6IXhRi8J:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3406658133405839745&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="9">
    <url>http://dl.acm.org/citation.cfm?id=1027642</url>
    <title status="complete" source="scholar.google.com">The creation of a music-driven digital violinist</title>
    <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.9673&amp;rep=rep1&amp;type=pdf</pdf>
    <author status="complete" source="scholar.google.com">J Yin, A Dhanik, D Hsu, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th annual ACM &#8230;</proceeding>
    <year>2004</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. The Creation of a Music-Driven Digital Violinist Jun Yin+, Ankur Dhanik#, DavidHsu+, Ye Wang+ National University of Singapore, Singapore 117543 +: {yinjun, dyhsu,wangye} @comp.nus.edu.sg #: g0203706@nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:uqKUHFx83aUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>13</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=11951845721169502906&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>8</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=11951845721169502906&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="8">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1101353</url>
        <title status="complete" source="scholar.google.com">Digital violin tutor: an integrated system for beginning violin learners</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2005_Digital_Violin_Tutor-An_Integrated_System_for_Beginning_Violin_Learners.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Yin, Y Wang, D Hsu</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 13th annual ACM &#8230;</proceeding>
        <year>2005</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Prompt feedback is essential for beginning violin learners; however, most amateur learners can only meet with teachers and receive feedback once or twice a week. To help such learners, we have attempted an initial design of Digital Violin Tutor (DVT), an ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:keJO9X-wIy8J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3396752607590408849&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>22</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3396752607590408849&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1290156</url>
        <title status="complete" source="scholar.google.com">Effective use of multimedia for computer-assisted musical instrument tutoring</title>
        <pdf>http://strum.googlecode.com/svn-history/r41/trunk/Research/CAMIT.pdf</pdf>
        <author status="complete" source="scholar.google.com">G Percival, Y Wang, G Tzanetakis</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a survey of recent work in computer-assisted musical instrumental tutoring and outlines several questions to consider when developing future projects. In particular, we suggest that the area ingreatest need of computer assistance is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:sSgpS4WrYA0J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>21</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=963958909237274801&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>18</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=963958909237274801&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4062545</url>
        <title status="complete" source="scholar.google.com">Finger inverse kinematics using error model analysis for gesture enabled navigation in virtual environments</title>
        <author status="partial" source="scholar.google.com">A El-Sawah, ND Georganas&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Haptic Audio Visual &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper we provide a new method for solving the hand fingers inverse kinematics problem. Given the finger&#39;s end-effector position with respect to the finger&#39;s metacarpal joint, the finger&#39;s four degrees of freedom joint angles are uniquely solved ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:v0KRhwmwEpQJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10669784022169764543&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10669784022169764543&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://edisdat.ied.edu.hk/pubarch/b15907314/full_paper/989839790.pdf</url>
        <title status="complete" source="scholar.google.com">Seeds of harmony: Edutainment project on music creativity</title>
        <pdf>http://edisdat.ied.edu.hk/pubarch/b15907314/full_paper/989839790.pdf</pdf>
        <author status="complete" source="scholar.google.com">BW LEUNGï¼ æ¢å¯¶è¯</author>
        <proceeding status="partial" source="scholar.google.com">Asia-Pacific Educational Research &#8230;</proceeding>
        <year>2006</year>
        <source>edisdat.ied.edu.hk</source>
        <snippet status="partial" source="scholar.google.com">Abstract: This paper reports on the development of âSeeds of Harmonyâ, an online multi-user edutainment platform on music creativity. This project aimed to provide an immersive, interactive and inspirational virtual environment where user can interconnect to creative ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:crO0D2qeDksJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:crO0D2qeDksJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5408434380885767026&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://fivedots.coe.psu.ac.th/~montri/Research/Publications/ICEE2007_noterecognition.pdf</url>
        <title status="complete" source="scholar.google.com">NOTE RECOGNITION FOR THAI XYLOPHONES</title>
        <pdf>http://fivedots.coe.psu.ac.th/~montri/Research/Publications/ICEE2007_noterecognition.pdf</pdf>
        <author status="complete" source="scholar.google.com">C Janraksukhum, M Karnjanadecha</author>
        <proceeding status="complete" source="scholar.google.com">fivedots.coe.psu.ac.th</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract: This paper describes the design and implementation of note recognition for Thai xylophones. Thai xylophone is a percussion musical instrument which can be played monophonically and polyphonically. A set of 21 digital bandpass filters are used to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:I5fTiG8Ve4kJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:I5fTiG8Ve4kJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9906535374112659235&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://www.ingentaconnect.com/content/dav/aaua/2012/00000098/00000002/art00001</url>
        <title status="complete" source="scholar.google.com">Automatic Transcription of Recorded Music</title>
        <author status="partial" source="scholar.google.com">P Grosche, B Schuller, M Muller&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Acta Acustica united with &#8230;</proceeding>
        <year>2012</year>
        <source>ingentaconnect.com</source>
        <snippet status="partial" source="scholar.google.com">Abstract: The automatic transcription of music recordings with the objective to derive a score-like representation from a given audio representation is a fundamental and challenging task. In particular for polyphonic music recordings with overlapping sound sources, current ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jx1Jmi9BMX8J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9165178389383421327&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <title status="complete" source="scholar.google.com">6FKRRORI &amp;RPSXWLQJ 1DWLRQDO8QLYHUVLW\ RI 6LQJDSRUH^\ LQMXQ ZDQJ\ HG\ KVX# FRPS QXV HGX VJ</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:SzZzqC4R6hQJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <title status="complete" source="scholar.google.com">SEEDS OF HARMONY-EDUTAINMENT PLATFORM ON MUSIC CREATIVITY</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:i7_U6lIsPFAJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="10">
    <url>http://www.ee.columbia.edu/~dpwe/ismir2004/CRFILES/paper183.pdf</url>
    <title status="complete" source="scholar.google.com">Automatic detection of vocal segments in popular songs</title>
    <pdf>http://www.ee.columbia.edu/~dpwe/ismir2004/CRFILES/paper183.pdf</pdf>
    <author status="complete" source="scholar.google.com">TL Nwe, Y Wang</author>
    <proceeding status="complete" source="scholar.google.com">Proc. ISMIR</proceeding>
    <year>2004</year>
    <source>ee.columbia.edu</source>
    <snippet status="partial" source="scholar.google.com">... Tin Lay Nwe * Ye Wang School of Computing National University of Singapore 3 Science Drive2,Singapore 117543 tlnma@i2r.a-star.edu.sg School of Computing National University ofSingapore 3 Science Drive 2,Singapore 117543 wangye@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:pKUgJWqMSg4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:pKUgJWqMSg4J:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <numVersions>24</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=1029789852324898212&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>38</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=1029789852324898212&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="38">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4061176</url>
        <title status="complete" source="scholar.google.com">Automatic synchronization between lyrics and music CD recordings based on Viterbi alignment of segregated vocal signals</title>
        <pdf>http://staff.aist.go.jp/h.fujihara/pdf/ism2006_fujihara.pdf</pdf>
        <author status="partial" source="scholar.google.com">H Fujihara, M Goto, J Ogata, K Komatani&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; , 2006. ISM&#39;06. &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper describes a system that can automatically synchronize between polyphonic musical audio signals and corresponding lyrics. Although there were methods that can synchronize between monophonic speech signals and corresponding text ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0y_jQcZGvsAJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13888616118180065235&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>50</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13888616118180065235&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.4864&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">A query-by-example technique for retrieving cover versions of popular songs with similar melodies</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.4864&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">WH Tsai, HM Yu, HM Wang</author>
        <proceeding status="partial" source="scholar.google.com">Int. Symp. on Music Information Retrieval ( &#8230;</proceeding>
        <year>2005</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Retrieving audio material based on audio queries is an important and challenging issue in the research field of content-based access to popular music. As part of this research field, we present a preliminary investigation into retrieving cover versions of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Q9m-WVGs5eAJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Q9m-WVGs5eAJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16205548299560606019&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>38</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16205548299560606019&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.5510&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Separation of vocals from polyphonic audio recordings</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.5510&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">S Vembu, S Baumann</author>
        <proceeding status="complete" source="scholar.google.com">Proc. ISMIR</proceeding>
        <year>2005</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Source separation techniques like independent component analysis and the more recent non-negative matrix factorization are gaining widespread use for the monaural separation of individual tracks present in a music sample. The underlying principle behind ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tPH3PQYE3esJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:tPH3PQYE3esJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16995744993622094260&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>39</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16995744993622094260&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4067048</url>
        <title status="complete" source="scholar.google.com">Exploring vibrato-motivated acoustic features for singer identification</title>
        <pdf>http://www1.i2r.a-star.edu.sg/~hli/papers/101109TASL2006876756.pdf</pdf>
        <author status="complete" source="scholar.google.com">TL Nwe, H Li</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language Processing, IEEE &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Vibrato is a slightly tremulous effect imparted to vocal or instrumental tone for added warmth and expressiveness through slight variation in pitch. It corresponds to a periodic fluctuation of the fundamental frequency. It is common for a singer to develop a vibrato ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7yW25Nf-A5wJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11242109298055456239&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>32</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11242109298055456239&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1661260</url>
        <title status="complete" source="scholar.google.com">F0 estimation method for singing voice in polyphonic audio signal based on statistical vocal model and viterbi search</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/ICASSP2006fujihara.pdf</pdf>
        <author status="partial" source="scholar.google.com">H Fujihara, T Kitahara, M Goto&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; , Speech and Signal &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper describes a method for estimating F0s of vocal from polyphonic audio signals. Because melody is sung by a singer in many musical pieces, the estimation of F0s of the vocal part is useful for many applications. Based on existing multiple-F0 estimation ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:fvNJ6Xy-WtkJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15662040097833481086&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>32</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15662040097833481086&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4275079</url>
        <title status="complete" source="scholar.google.com">Exploring billions of audio features</title>
        <pdf>http://www.csl.sony.fr/downloads/papers/2007/pachet-07a.pdf</pdf>
        <author status="complete" source="scholar.google.com">F Pachet, P Roy</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; -Based Multimedia Indexing, 2007. CBMI&#39;07. &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Many works in audio and image signal analysis are based on the use of&quot; features&quot; to represent characteristics of sounds or images. Features are used in various ways, for instance as inputs to classifiers to categorize automatically objects, eg for audio scene ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:eJfgwfcowVMJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6035150020227929976&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>30</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6035150020227929976&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://dl.acm.org/citation.cfm?id=1592529</url>
        <title status="complete" source="scholar.google.com">Analytical features: a knowledge-based approach to audio feature generation</title>
        <pdf>http://downloads.hindawi.com/journals/asmp/2009/153017.pdf</pdf>
        <author status="complete" source="scholar.google.com">F Pachet, P Roy</author>
        <proceeding status="partial" source="scholar.google.com">EURASIP Journal on Audio, Speech, and Music &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present a feature generation system designed to create audio features for supervised classification tasks. The main contribution to feature generation studies is the notion of analytical features (AFs), a construct designed to support the representation of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:z8TEvEOZsIQJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>18</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9561310525116433615&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>17</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9561310525116433615&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255</url>
        <title status="complete" source="scholar.google.com">Singing voice detection for karaoke application</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Shenoy, Y Wu, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Visual &#8230;</proceeding>
        <year>2005</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">abstract We present a framework to detect the regions of singing voice in musical audio signals. This work is oriented towards the development of a robust transcriber of lyrics for karaoke applications. The technique leverages on a combination of low-level audio ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4518087</url>
        <title status="complete" source="scholar.google.com">On fusion of timbre-motivated features for singing voice detection and singer identification</title>
        <author status="complete" source="scholar.google.com">TL Nwe, H Li</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and Signal Processing, 2008. &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Timbre is the quality of sound which allows the ear to distinguish between musical sounds. In this paper, we study timbre effects in identification of singing voice segments in popular songs. Firstly, we identify between singing voice and instrumental segments in a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_MPT1b4jaOsJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16962847299029156860&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16962847299029156860&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://pdf.aminer.org/000/439/676/language_identification_in_vocal_music.pdf</url>
        <title status="complete" source="scholar.google.com">Language identification in vocal music</title>
        <pdf>http://pdf.aminer.org/000/439/676/language_identification_in_vocal_music.pdf</pdf>
        <author status="partial" source="scholar.google.com">J Schwenninger, R Brueckner, D Willett&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proc. of the 7th &#8230;</proceeding>
        <year>2006</year>
        <source>pdf.aminer.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Language identification is an important field in spoken language processing. The identification of the language sung or spoken in music, however, has attracted only minor attention so far. This, however, is an important task when it comes to categorizing, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:zBXdY2EXDqgJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:zBXdY2EXDqgJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12109642155100542412&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12109642155100542412&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5876296</url>
        <title status="complete" source="scholar.google.com">Lyricsynchronizer: Automatic synchronization system between musical audio signals and lyrics</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/IEEEJSTSP201110fujihara.pdf</pdf>
        <author status="partial" source="scholar.google.com">H Fujihara, M Goto, J Ogata&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Selected Topics in Signal &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper describes a system that can automatically synchronize polyphonic musical audio signals with their corresponding lyrics. Although methods for synchronizing monophonic speech signals and corresponding text transcriptions by using Viterbi ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:1VQohGZUJiAJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2316631857609331925&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2316631857609331925&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.9910&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Automatic Characterization of Music Complexity: a multifaceted approach</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.9910&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">S Streich</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Pompeu Fabra, Barcelona, Spain. Retrieved October</proceeding>
        <year>2005</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">Abstract The aim of this work is to present the concept of a multi-faceted music complexity descriptor set. The complexity of music is one of the less intensively researched areas in music information retrieval. Especially an automated estimation based on the audio ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:UeMxiMxm5s4J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:UeMxiMxm5s4J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14908716645100938065&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14908716645100938065&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://books.google.com/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA121&amp;ots=oDOSrGgwg1&amp;sig=gmPOPtfoAEYmBBWCaY2CfdhG-fs</url>
        <title status="complete" source="scholar.google.com">Vocal segment classification in popular music</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.3371&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">L Feng, AB Nielsen, LK Hansen</author>
        <proceeding status="partial" source="scholar.google.com">9th International Conference &#8230;</proceeding>
        <year>2008</year>
        <source>books.google.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper explores the vocal and non-vocal music classiï¬cation problem within popular songs. A newly built labeled database covering 147 popular songs is announced. It is designed for classifying signals from 1sec time windows. Features are selected for this ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:n_A8KLgNu24J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7978986249417191583&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7978986249417191583&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://www.springerlink.com/index/Q8205NH13L1RH912.pdf</url>
        <title status="complete" source="scholar.google.com">Effectiveness of signal segmentation for music content representation</title>
        <author status="complete" source="scholar.google.com">N Maddage, M Kankanhalli, H Li</author>
        <proceeding status="complete" source="scholar.google.com">Advances in Multimedia Modeling</proceeding>
        <year>2008</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">In this paper we compare the effectiveness of rhythm based signal segmentation technique with the traditional fixed length segmentation for music contents representation. We consider vocal regions, instrumental regions and chords which represent the harmony as different ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:u3oNKau615IJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10581131093821192891&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10581131093821192891&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf</url>
        <title status="complete" source="scholar.google.com">æ­å£°æå ±å¦çã®æè¿ã®ç ç©¶</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å°</author>
        <proceeding status="complete" source="scholar.google.com">æ¥æ¬é³é¿å­¦ä¼èª</proceeding>
        <year>2008</year>
        <source>staff.aist.go.jp</source>
        <snippet status="complete" source="scholar.google.com">â Recent studies on singing information processing. ââ Masataka Goto, Takeshi Saitou, Tomoyasu Nakano and Hiromasa Fujihara (National Institute of Ad- vanced Industrial Science and Technology (AIST), Tsukuba, 305â8568) e-mail: m.goto@aist.go.jp</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3835003938146142193&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>6</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3835003938146142193&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1661330</url>
        <title status="complete" source="scholar.google.com">Vibrato-Motivated Acoustic Features for Singger Identification</title>
        <author status="complete" source="scholar.google.com">H Li, TL Nwe</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and Signal Processing, 2006. &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract It is common that a singer develops a vibrato to personalize his/her singing style. In this paper, we explore the acoustic features that reflect vibrato information, to identify singers of popular music. We start with an enhanced vocal detection method that allows us to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:myn3WGtVk10J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6742826986646219163&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6742826986646219163&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607654</url>
        <title status="complete" source="scholar.google.com">Using dtw based unsupervised segmentation to improve the vocal part detection in pop music</title>
        <author status="complete" source="scholar.google.com">L Xiao, J Zhou, T Zhang</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2008 IEEE &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Vocal part detection, which plays an important role in music information retrieval, is still a tough task so far. Previous works focused on short time features, which cannot capture some essential long term characteristics of singing. In this paper, we propose a Dynamic ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:um2CZXTxNhkJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1816904981912120762&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5298607</url>
        <title status="complete" source="scholar.google.com">Pitch oriented automatic singer identification in pop music</title>
        <author status="complete" source="scholar.google.com">P Chang</author>
        <proceeding status="partial" source="scholar.google.com">Semantic Computing, 2009. ICSC&#39;09. IEEE &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we proposed two novel methods used to distinguish the singer of a pop music. We focused on a single singer and single track case. These two methods are ldquoPitch Extractionrdquo method and ldquo1/12 OFCCrdquo method. The Pitch ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:r1X3xpda2T0J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4456692914184476079&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4456692914184476079&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6022500</url>
        <title status="complete" source="scholar.google.com">Automatic singer identification based on auditory features</title>
        <author status="complete" source="scholar.google.com">W Cai, Q Li, X Guan</author>
        <proceeding status="partial" source="scholar.google.com">Natural Computation (ICNC), 2011 &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The paper describes a method of identifying singers&#39; voice from the monophonic music including sounds of various musical instruments based on auditory features. In this system, there are four problems to solve, vocal segment detection, feature extraction, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2fGI2y5MEzUJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3824484272703074777&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3824484272703074777&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://dl.acm.org/citation.cfm?id=1823753</url>
        <title status="complete" source="scholar.google.com">Word level automatic alignment of music and lyrics using vocal synthesis</title>
        <author status="complete" source="scholar.google.com">NC Maddage, KC Sim, H Li</author>
        <proceeding status="partial" source="scholar.google.com">ACM Transactions on Multimedia &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a signal-based approach instead of the commonly used model-based approach, to automatically align vocal music with text lyrics at the word level. In this approach, we use a text-to-speech system to synthesize the singing voice according to the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:HJ9bzctkTegJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16739146216492474140&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16739146216492474140&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4786093</url>
        <title status="complete" source="scholar.google.com">Semi-Supervised Classification of Speaker&#39;s Psychological Stress</title>
        <author status="partial" source="scholar.google.com">S Torabi, F AlmasGanj&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; , 2008. CIBEC 2008. &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract It is well known that speech signal is affected by speaker&#39;s stress. Some of the recent works have evaluated different acoustic features individually for the detection of stress from speech. In our previous work, a new mixed feature (TEO-Pch-LFPC) was ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:UcFm0ey31Q0J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=996905120285770065&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>http://staff.aist.go.jp/m.goto/PAPER/SIGMUS200608fujihara.pdf</url>
        <title status="complete" source="scholar.google.com">é³æ¥½é³é¿ä¿¡å·ã¨æ­è©ã®æéçå¯¾å¿ä»ãææ³: æ­å£°ã®åé¢ã¨æ¯é³ã® Viterbi ã¢ã©ã¤ã³ã¡ã³ã</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/SIGMUS200608fujihara.pdf</pdf>
        <author status="partial" source="scholar.google.com">è¤åå¼å°ï¼ å¾è¤çå­ï¼ ç·æ¹æ·³ï¼ é§è°·åç¯&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">æå ±å¦çå­¦ä¼ç ç©¶å ±å. &#8230;</proceeding>
        <year>2006</year>
        <source>staff.aist.go.jp</source>
        <snippet status="partial" source="scholar.google.com">æ¬ç¨¿ã§ã¯, ä¼´å¥é³ãå«ãé³æ¥½é³é¿ä¿¡å·ã¨å¯¾å¿ããæ­è©ã®æéçãªå¯¾å¿ä»ãææ³ã«ã¤ãã¦è¿°ã¹ã. ã¯ãªã¼ã³ãªé³å£°ä¿¡å·ã¨ãã®çºè©±åå®¹ã®æéçå¯¾å¿ä»ããæ¨å®ããã Viterbi ã¢ã©ã¤ã³ã¡ã³ãææ³ã¯ããã¾ã§ãå­å¨ããã, æ­å£°ã¨åæã«æ¼å¥ãããä¼´å¥é³ã®æªå½±é¿ã§å¸è²© CD ä¸­ã®æ­å£°ã«ã¯é©ç¨ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:MO-f7APWq5sJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11217294609239502640&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11217294609239502640&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5947660</url>
        <title status="complete" source="scholar.google.com">Automatic Language Identification in music videos with low level audio and visual features</title>
        <pdf>http://www.cs.utoronto.ca/~dross/ChandrasekharSarginRoss_ICASSP2011.pdf</pdf>
        <author status="partial" source="scholar.google.com">V Chandrasekhar, ME Sargin&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic Language Identification (LID) in music has received significantly less attention than LID in speech. Here, we study the problem of LID in music videos uploaded on YouTube. We use a&quot; bag-of-words&quot; approach based on state-of-the-art content based ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Gs_9FgBpIfoJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18023802632820084506&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="23">
        <url>http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf</url>
        <title status="complete" source="scholar.google.com">Melody Separation from Polyphonic Audio Recordings</title>
        <pdf>http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Vembu</author>
        <year>2005</year>
        <source>www-kd.iai.uni-bonn.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract In the field of music information retrieval, query-by-humming is an interesting application area in which humming sequences or melodies are matched with a database of songs to come up with a list of indexed songs that are similar in melodic content to the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16884184643397726041&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="24">
        <title status="complete" source="scholar.google.com">Identifying Singers of Popular Songs</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:nvwHI3QIotoJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15754163741392370846&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="25">
        <url>http://www.springerlink.com/index/FT671731J0403662.pdf</url>
        <title status="complete" source="scholar.google.com">A Survey of Music Structure Analysis Techniques for Music Applications</title>
        <author status="complete" source="scholar.google.com">N Maddage, H Li, M Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">Recent Advances in Multimedia Signal &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Music carries multilayer information which forms different structures. The information embedded in the music can be categorized into time information, harmony/melody, music regions, music similarities, song structures and music semantics. In this chapter, we first ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3vo8SvZRZuUJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16529989600559299294&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="26">
        <url>http://drops.dagstuhl.de/opus/volltexte/2012/3464/</url>
        <title status="complete" source="scholar.google.com">Lyrics-to-Audio Alignment and its Application}}</title>
        <pdf>http://drops.dagstuhl.de/opus/volltexte/2012/3464/pdf/3.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Fujihara, M Goto</author>
        <proceeding status="complete" source="scholar.google.com">Multimodal Music Processing}</proceeding>
        <source>drops.dagstuhl.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic lyrics-to-audio alignment techniques have been drawing attention in the last years and various studies have been made in this field. The objective of lyrics-to-audio alignment is to estimate a temporal relationship between lyrics and musical audio signals ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pR0KaNFrpC0J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3288872175025135013&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="27">
        <title status="complete" source="scholar.google.com">Singing Voice Detection in Western Popular Music</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dx-cdoqIEO4J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="28">
        <url>http://www.actapress.com/PaperInfo.aspx?PaperID=38587&amp;reason=500</url>
        <title status="complete" source="scholar.google.com">Automatic Labeling of Training Data for Singing Voice Detection in Musical Audio</title>
        <pdf>https://ccrma.stanford.edu/~kglee/pubs/klee-sppra09-final.pdf</pdf>
        <author status="complete" source="scholar.google.com">K Lee, M Cremer</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 6th IASTED International &#8230;</proceeding>
        <year>2009</year>
        <source>actapress.com</source>
        <snippet status="partial" source="scholar.google.com">AUTOMATIC LABELING OF TRAINING DATA FOR SINGING VOICE DETECTION IN MUSICALAUDIO Kyogu Lee Media Technology Lab, Gracenote 2000 Powell Street, Emeryville, CA94608,USA klee@gracenote.com Markus Cremer Media Technology Lab, Gracenote 2000 ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_npQe5m2DPoJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18017976979517635326&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="29">
        <title status="complete" source="scholar.google.com">VIBRATO-MOTIVATED ACOUSTIC FEATURES FOR SINGER IDENTIFICATION</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ZYc_l3H1LTQJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3759931132141864805&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="30">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4797191</url>
        <title status="complete" source="scholar.google.com">An Effective Vocal/Non-vocal Segmentation Approach for Embedded Music Retrieve System on Mobile Phone</title>
        <author status="complete" source="scholar.google.com">H Tuo, H Li, K Lei</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; and Mobile Computing, 2009. CMC&#39;09. WRI &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract With the growing bodies of MP3 songs in Internet, content-based analysis plays an important role for its retrieving and management. Due to most useful information is carried by vocal portions, it is necessary to separate the vocal segments from music. This paper ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:44sDuDX6SGMJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7154243116705483747&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="31">
        <url>http://www.springerlink.com/index/C0668J24363G3640.pdf</url>
        <title status="complete" source="scholar.google.com">Mining movie archives for song sequences</title>
        <pdf>http://www.iba-suk.edu.pk/ibasuk/faculty/MoviesMining.pdf</pdf>
        <author status="complete" source="scholar.google.com">SM Doudpota</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Tools and Applications</proceeding>
        <year>2012</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music and songs are integral parts of Bollywood movies. Every movie of two to three hours, contains three to ten songs, each song is 3â10 min long. Music lovers like to listen music and songs of a movie, however it is time consuming and error prone to search ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:I7AmwXJAY1wJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6657235535794712611&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="32">
        <url>http://dl.acm.org/citation.cfm?id=1631272.1631364</url>
        <title status="complete" source="scholar.google.com">Automatic and instant ring tone generation based on music structure analysis</title>
        <author status="complete" source="scholar.google.com">T Zhang, CK Fong, L Xiao, J Zhou</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 17th ACM &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Real tones, which are often excerpts from pop songs, have become popular as ring tones. This paper describes how a ring tone can be produced by analyzing the structure of music and selecting the most appropriate portion of the music. With audio feature analysis ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:YQzy0g82Tz8J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4561924389141089377&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="33">
        <url>http://jj-aucouturier.info/papers/PHD-2006.pdf</url>
        <title status="complete" source="scholar.google.com">Dix Experiences sur la Modelisation du Timbre Polyphonique</title>
        <pdf>http://jj-aucouturier.info/papers/PHD-2006.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Bengio</author>
        <proceeding status="complete" source="scholar.google.com">jj-aucouturier.info</proceeding>
        <snippet status="partial" source="scholar.google.com">Resume La grande majoritÃ© des systemes d&#39;extraction de metadonnÃ©es haut-niveaua partir de signaux musicaux repose sur un modele implicite de leur âsonâ ou timbre polyphonique. Ce modele reprÃ©sente le timbre comme la distribution statistique globale d&#39;attributs ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:IxbO610h6PwJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:IxbO610h6PwJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18223852579426539043&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="34">
        <url>https://www.uea.ac.uk/polopoly_fs/1.85551!steward_project2005.pdf</url>
        <title status="complete" source="scholar.google.com">FINAL YEAR PROJECT</title>
        <pdf>https://www.uea.ac.uk/polopoly_fs/1.85551!steward_project2005.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Steward</author>
        <year>2004</year>
        <source>uea.ac.uk</source>
        <snippet status="partial" source="scholar.google.com">Mobile devices are an area of technology which is making huge progress in terms of capabilities and size. Together with the advances in wireless technology, allowing some devices to almost be constantly in communication with other devices or the internet has ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:6BP-7JpLH-0J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:6BP-7JpLH-0J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17086458640040072168&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="35">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.6223&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Effectiveness of Signal Segmentation for Music Content Representation</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.6223&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">NCMMS Kankanhalli, H Li</author>
        <proceeding status="complete" source="scholar.google.com">Citeseer</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract. In this paper we compare the effectiveness of rhythm based signal segmentation technique with the traditional fixed length segmentation for music contents representation. We consider vocal regions, instrumental regions and chords which represent the harmony ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tis58lxAeiEJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:tis58lxAeiEJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2412311318355323830&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="36">
        <url>http://dl.acm.org/citation.cfm?id=2043615</url>
        <title status="complete" source="scholar.google.com">Beat space segmentation and octave scale cepstral feature for sung language recognition in pop music</title>
        <author status="complete" source="scholar.google.com">NC Maddage, H Li</author>
        <proceeding status="partial" source="scholar.google.com">ACM Transactions on Multimedia Computing, &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Sung language recognition relies on both effective feature extraction and acoustic modeling. In this paper, we study rhythm based music segmentation with the frame size being the duration of the smallest note in the music, as opposed to fixed length ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PDNw-UBaogIJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="37">
        <url>http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf</url>
        <title status="complete" source="scholar.google.com">DÃ©tection de la voix chantÃ©e dans un morceau de musique</title>
        <pdf>http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf</pdf>
        <author status="complete" source="scholar.google.com">L REGNIER</author>
        <proceeding status="complete" source="scholar.google.com">atiam.ircam.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© De tous les Ã©lÃ©ments constitutifs d&#39;un morceau de musique, la voix est sans conteste celui qui focalise le plus l&#39;attention de l&#39;auditeur. Ceci provient du fait que la voix est porteuse d&#39;un message (le texte) et d&#39;une identitÃ© (celle du chanteur). Pour ces raisons, de ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15229319373475501458&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="11">
    <url>http://dl.acm.org/citation.cfm?id=1027602</url>
    <title status="complete" source="scholar.google.com">Singing voice detection in popular music</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2004_Singing_Voice_Detection_in_Popular_Music.pdf</pdf>
    <author status="complete" source="scholar.google.com">TL Nwe, A Shenoy, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th annual ACM &#8230;</proceeding>
    <year>2004</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. Singing Voice Detection in Popular Music Tin Lay Nwe â Arun Shenoy Ye WangDepartment of Computer Science, School of Computing National University of Singapore,Singapore 117543 {nwetl, arunshen, wangye}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:bh6Y-g5TQzUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>22</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=3838002631248715374&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>34</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=3838002631248715374&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="34">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4244535</url>
        <title status="complete" source="scholar.google.com">Adaptation of Bayesian models for single-channel source separation and its application to voice/music separation in popular songs</title>
        <pdf>http://hal.inria.fr/docs/00/54/47/74/PDF/2007_IEEE_TASLP_OzerovEtAl_SingleChannelSourceSeparation.pdf</pdf>
        <author status="partial" source="scholar.google.com">A Ozerov, P Philippe, F Bimbot&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Probabilistic approaches can offer satisfactory solutions to source separation with a single channel, provided that the models of the sources match accurately the statistical properties of the mixed signals. However, it is not always possible to train such models. To ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:A4LdfBu5sCsJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3148219667242123779&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>85</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3148219667242123779&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4156205</url>
        <title status="complete" source="scholar.google.com">Separation of singing voice from music accompaniment for monaural recordings</title>
        <pdf>ftp://ftp.cse.ohio-state.edu/pub/tech-report/2005/TR61.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Li, DL Wang</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language Processing, &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Separating singing voice from music accompaniment is very useful in many applications, such as lyrics recognition and alignment, singer identification, and music information retrieval. Although speech separation has been extensively studied for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PJ4gQrBmOuoJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16877915460734066236&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>74</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16877915460734066236&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://dl.acm.org/citation.cfm?id=1027576</url>
        <title status="complete" source="scholar.google.com">LyricAlly: automatic synchronization of acoustic musical signals and textual lyrics</title>
        <pdf>http://www.comp.nus.edu/~kanmy/dossier/papers/p1568934817-wang.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, MY Kan, TL Nwe, A Shenoy, J Yin</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th &#8230;</proceeding>
        <year>2004</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present a prototype that automatically aligns acoustic musical signals with their corresponding textual lyrics, in a manner similar to manually-aligned karaoke. We tackle this problem using a multimodal approach, where the appropriate pairing of audio and text ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gj3dVP-I65UJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>36</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10802878761400089986&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>61</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10802878761400089986&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://dl.acm.org/citation.cfm?id=1112857</url>
        <title status="complete" source="scholar.google.com">Key, chord, and rhythm tracking of popular music recordings</title>
        <author status="complete" source="scholar.google.com">A Shenoy, Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">Computer Music Journal</proceeding>
        <year>2005</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">In this article, we propose a framework to analyze a musical audio signal (sampled from a popular music CD) and determine its key, provide usable chord transcriptions, and obtain the hierarchical rhythm structure representation comprising the quarternote, half-note, and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:33p2o7nmEuIJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16290336487138294495&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>22</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16290336487138294495&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ohm.fing.edu.uy/publicaciones/2007/RH07/RH07.pdf</url>
        <title status="complete" source="scholar.google.com">Comparing audio descriptors for singing voice detection in music audio files</title>
        <pdf>http://ohm.fing.edu.uy/publicaciones/2007/RH07/RH07.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Rocamora, P Herrera</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; on Computer Music, 11th. San Pablo, &#8230;</proceeding>
        <year>2007</year>
        <source>ohm.fing.edu.uy</source>
        <snippet status="partial" source="scholar.google.com">Abstract. Given the relevance of the singing voice in popular western music, a system able to reliable identify those portions of a music audio file containing vocals would be very useful. In this work, we explore already used descriptors to perform this task and compare the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:d-X3ceQPzT8J:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:d-X3ceQPzT8J:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4597348268454176119&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>23</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4597348268454176119&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://www.springerlink.com/index/j1h044n4v1767603.pdf</url>
        <title status="complete" source="scholar.google.com">Automatic lyrics alignment for Cantonese popular music</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.1526&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">CH Wong, WM Szeto, KH Wong</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Systems</proceeding>
        <year>2007</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract From lyrics-display on electronic music players and Karaoke videos to surtitles for live Chinese opera performance, one feature is common to all these everyday functionalities temporal: synchronization of the written text and its corresponding musical phrase. Our ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8ee-SWoY3QoJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=782808755015182321&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>16</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=782808755015182321&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4518002</url>
        <title status="complete" source="scholar.google.com">Vocal detection in music with support vector machines</title>
        <pdf>http://www.tsi.enst.fr/~grichard/Publications/Icassp08_ramona.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Ramona, G Richard, B David</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and Signal &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a statistical learning approach for the automatic detection of vocal regions in a polyphonic musical signal. A support vector model, based on a large feature set, is employed to discriminate accompanied singing voice from pure instrumental regions. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:t4S9h9gGkXEJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8183329524968948919&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>17</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8183329524968948919&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255</url>
        <title status="complete" source="scholar.google.com">Singing voice detection for karaoke application</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Shenoy, Y Wu, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Visual &#8230;</proceeding>
        <year>2005</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">abstract We present a framework to detect the regions of singing voice in musical audio signals. This work is oriented towards the development of a robust transcriber of lyrics for karaoke applications. The technique leverages on a combination of low-level audio ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://www.isca-speech.org/archive_open/archive_papers/sapa_2008/sap8_007.pdf</url>
        <title status="complete" source="scholar.google.com">Singing voice detection using modulation frequency features</title>
        <pdf>https://piglet2.ee.columbia.edu/2008/papers/122.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Markaki, A Holzapfel, Y Stylianou</author>
        <proceeding status="complete" source="scholar.google.com">Proc. SAPA</proceeding>
        <year>2008</year>
        <source>isca-speech.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, a feature set derived from modulation spectra is applied to the task of detecting singing voice in historical and recent recordings of Greek Rembetiko. A generalization of SVD to tensors, Higher Order SVD (HOSVD), is applied to reduce the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:c1bahNXRfKUJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>15</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11924636628357371507&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>9</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11924636628357371507&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://www.ee.iitb.ac.in/web/files/publications/RaoPS2008.pdf</url>
        <title status="complete" source="scholar.google.com">Singing voice detection in north indian classical music</title>
        <pdf>http://www.ee.iitb.ac.in/web/files/publications/RaoPS2008.pdf</pdf>
        <author status="complete" source="scholar.google.com">V Rao, S Ramakrishnan, P Rao</author>
        <proceeding status="partial" source="scholar.google.com">Proc. of the National Conference on &#8230;</proceeding>
        <year>2008</year>
        <source>ee.iitb.ac.in</source>
        <snippet status="partial" source="scholar.google.com">AbstractâSinging voice detection is essential for content-based applications such as those involving melody extraction and singer identification. This article is concerned with the accurate detection of singing voice phrases in north Indian classical vocal music. The ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2owjetZNGuAJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:2owjetZNGuAJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16148304997457824986&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16148304997457824986&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://hal.archives-ouvertes.fr/tel-00457522/</url>
        <title status="complete" source="scholar.google.com">CaractÃ©risation de l&#39;environnement musical dans les documents audiovisuels</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/45/75/22/PDF/These_Helene_Lachambre.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Lachambre</author>
        <year>2009</year>
        <source>hal.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">Depuis plusieurs dizaines d&#39;annÃ©es, l&#39;indexation par le contenu des documents audiovisuels fait l&#39;objet de travaux de la part de nombreuses Ã©quipes de recherche:âLe mot Â«audiovisuelÂ» fait rÃ©fÃ©rencea un contenu Â«multimÃ©diaÂ» qui regroupea la fois des donnÃ©es ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Yp_9nKZh3GwJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7844252019198893922&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7844252019198893922&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <title status="complete" source="scholar.google.com">Singing Phoneme Class Detection In Polyphonic Music Recordings</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uJWqBAVmuaEJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11653457682537027000&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11653457682537027000&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://dl.acm.org/citation.cfm?id=1363164</url>
        <title status="complete" source="scholar.google.com">Enriching music with synchronized lyrics, images and colored lights</title>
        <pdf>http://www.dse.nl/~gijsg/AmbiSys-GeleijnseEtAl.pdf</pdf>
        <author status="partial" source="scholar.google.com">G Geleijnse, D Sekulovski, J Korst, S Pauws&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 1st &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present a method to synchronize popular music with its lyrics at the stanza level. First we apply an algorithm to segment audio content into harmonically similar and/or contrasting progressions, ie the stanzas. We map the stanzas found to a sequence of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:GGSBRK85bhQJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1472177553128121368&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1472177553128121368&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://dl.acm.org/citation.cfm?id=2072366</url>
        <title status="complete" source="scholar.google.com">MUSIZ: a generic framework for music resizing with stretching and cropping</title>
        <author status="complete" source="scholar.google.com">Z Liu, C Wang, Y Bai, H Wang, J Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 19th ACM &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Content-aware music adaption, ie music resizing, in temporal constraints starts drawing attention from multimedia communities because of the need of real-world scenarios, eg animation production and radio advertisement production. The goal of music resizing is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:AFaRW2R9HeMJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16365374490920703488&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://www.springerlink.com/index/t3086h1747710650.pdf</url>
        <title status="complete" source="scholar.google.com">Impulsive Environment Sound Detection by Neural Classification of Spectrogram and Mel-Frequency Coefficient Images</title>
        <author status="complete" source="scholar.google.com">P Khunarsa, C Lursinsap, T Raicharoen</author>
        <proceeding status="partial" source="scholar.google.com">Advances in Neural Network &#8230;</proceeding>
        <year>2010</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">The problem of automatic detecting impulsive sounds such as human sound (screams, shout), gun shots, machine gun, thunder, fire alarm, and car horn are useful for hearing impairment person. In this paper, instead of filtering the frequency of each sound for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9iR6e_E8tMwJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14750481687401604342&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14750481687401604342&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/machine-learning-and-music.pdf#page=13</url>
        <title status="complete" source="scholar.google.com">Detecting key features in popular music: case studyâsinging voice detection</title>
        <pdf>http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/machine-learning-and-music.pdf#page=13</pdf>
        <author status="complete" source="scholar.google.com">R NÃ³brega, S Cavaco</author>
        <proceeding status="partial" source="scholar.google.com">Proc. of the Workshop on Machine &#8230;</proceeding>
        <year>2009</year>
        <source>ecmlpkdd2009.net</source>
        <snippet status="partial" source="scholar.google.com">Abstract. Detecting distinct features in modern pop music is an important problem that can have significant applications in areas such as multimedia entertainment. They can be used, for example, to give a visually coherent representation of the sound. We propose to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0oVzznXtnwkJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:0oVzznXtnwkJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=693533957868979666&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=693533957868979666&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://tel.archives-ouvertes.fr/pastel-00529331/</url>
        <title status="complete" source="scholar.google.com">Classification automatique de flux radiophoniques par Machines Ã  Vecteurs de Support</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/52/93/31/PDF/main.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Ramona</author>
        <year>2010</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">1.1 Vers une radio numÃ©rique......................... 9 1.2 Applications de l&#39;indexation audio pour la radio............ 10 1.3 Â«Qu&#39;est-ce que la musique?Â»...................... 11 1.4 Classification par Machines Ã  Vecteurs de Support.......... 12 1.5 ProblÃ©matiques................................ 13 1.6 ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:1n7-PBzLMbkJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13344670493018324694&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13344670493018324694&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=34</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://institut17-1.kug.ac.at/fileadmin/media/iem/projects/2011/gampp_ti.pdf</url>
        <title status="complete" source="scholar.google.com">Evaluation of Robust Features for Singing Voice Detection</title>
        <pdf>http://institut17-1.kug.ac.at/fileadmin/media/iem/projects/2011/gampp_ti.pdf</pdf>
        <author status="complete" source="scholar.google.com">P Gampp</author>
        <proceeding status="complete" source="scholar.google.com">institut17-1.kug.ac.at</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract The detection of singing voice segments within music signals is an important object of research in the field of music information retrieval, since it serves as an essential pre-stage for applications like singer identification, lyrics recognition, singing melody ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:CMMZACBTsKIJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:CMMZACBTsKIJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11722961226951148296&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf</url>
        <title status="complete" source="scholar.google.com">Melody Separation from Polyphonic Audio Recordings</title>
        <pdf>http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Vembu</author>
        <year>2005</year>
        <source>www-kd.iai.uni-bonn.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract In the field of music information retrieval, query-by-humming is an interesting application area in which humming sequences or melodies are matched with a database of songs to come up with a list of indexed songs that are similar in melodic content to the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16884184643397726041&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://www.springerlink.com/index/5017hhu836400623.pdf</url>
        <title status="complete" source="scholar.google.com">Adaptive music resizing with stretching, cropping and insertion</title>
        <author status="complete" source="scholar.google.com">Z Liu, C Wang, J Wang, H Wang, Y Bai</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Systems</proceeding>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Content-aware music adaption, ie music resizing, in temporal constraints starts drawing attention from multimedia communities, because there are plenty of real-world scenarios, eg animation production and radio advertisement production. The goal of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pgInTEFeqssJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <title status="complete" source="scholar.google.com">Monaural musical sound separation</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jpa0VTJdi4EJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9334657123423131278&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6337096</url>
        <title status="complete" source="scholar.google.com">Exploiting Semantic Content for Singing Voice Detection</title>
        <author status="complete" source="scholar.google.com">I Leonidas, JL Rouas</author>
        <proceeding status="partial" source="scholar.google.com">Semantic Computing (ICSC), 2012 IEEE &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper we propose a method for singing voice detection in popular music recordings. The method is based on statistical learning of spectral features extracted from the audio tracks. In our method we use Mel Frequency Cepstrum Coefficients (MFCC) to ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://tel.archives-ouvertes.fr/tel-00687475/</url>
        <title status="complete" source="scholar.google.com">Localisation, caractÃ©risation et reconnaissance de voix chantÃ©es</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/68/74/75/PDF/these.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Regnier</author>
        <year>2012</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">Many auditors have a remarkable ability to identify the singer of a new song as long as they have already heard some songs performed by the same singer. When the singer is unfamiliar it is common for listeners to attempt to characterize the singer&#39;s voice by finding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ObV6dGA6ysYJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14324325750750754105&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="23">
        <url>http://www.actapress.com/PaperInfo.aspx?PaperID=38587&amp;reason=500</url>
        <title status="complete" source="scholar.google.com">Automatic Labeling of Training Data for Singing Voice Detection in Musical Audio</title>
        <pdf>https://ccrma.stanford.edu/~kglee/pubs/klee-sppra09-final.pdf</pdf>
        <author status="complete" source="scholar.google.com">K Lee, M Cremer</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 6th IASTED International &#8230;</proceeding>
        <year>2009</year>
        <source>actapress.com</source>
        <snippet status="partial" source="scholar.google.com">AUTOMATIC LABELING OF TRAINING DATA FOR SINGING VOICE DETECTION IN MUSICALAUDIO Kyogu Lee Media Technology Lab, Gracenote 2000 Powell Street, Emeryville, CA94608,USA klee@gracenote.com Markus Cremer Media Technology Lab, Gracenote 2000 ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_npQe5m2DPoJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18017976979517635326&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="24">
        <url>http://193.136.122.112/img/files/archivemodule/@random49f9c25b31ba0/1241105061_AudioProjectFinalReport.pdf</url>
        <title status="complete" source="scholar.google.com">Detecting key features in popular music: case studyâvoice detection</title>
        <pdf>http://193.136.122.112/img/files/archivemodule/@random49f9c25b31ba0/1241105061_AudioProjectFinalReport.pdf</pdf>
        <author status="complete" source="scholar.google.com">R NÃ³brega</author>
        <proceeding status="complete" source="scholar.google.com">193.136.122.112</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Detecting distinct features in modern pop music is an important problem that can have significant applications in areas such as multimedia entertainment. They can be used, for example, to give a visually coherent representation of the sound. The work ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:UN2lj_WqVtcJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:UN2lj_WqVtcJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15516777537805344080&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="25">
        <url>http://www.ee.columbia.edu/~dpwe/e6820/proposals/felix.pdf</url>
        <title status="complete" source="scholar.google.com">Identifying singing segments in music</title>
        <pdf>http://www.ee.columbia.edu/~dpwe/e6820/proposals/felix.pdf</pdf>
        <author status="complete" source="scholar.google.com">FS Garcia</author>
        <proceeding status="complete" source="scholar.google.com">ee.columbia.edu</proceeding>
        <snippet status="partial" source="scholar.google.com">Page 1. Identifying singing segments in music Felix Sanchez Garcia Page 2. Objective â¢Given a music sample, identify singing segments Page 3. Difficulties â¢ It is hard tomodel singing voice, it differs from normal speech â Mixed ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_vPzlY0xKEAJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:_vPzlY0xKEAJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4622999501671756798&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="26">
        <url>http://hal.inria.fr/docs/00/68/74/75/PDF/these.pdf</url>
        <title status="complete" source="scholar.google.com">L&#39;UNIVERSITE PIERRE ET MARIE CURIE</title>
        <pdf>http://hal.inria.fr/docs/00/68/74/75/PDF/these.pdf</pdf>
        <author status="complete" source="scholar.google.com">ML Regnier</author>
        <proceeding status="complete" source="scholar.google.com">hal.inria.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">Many auditors have a remarkable ability to identify the singer of a new song as long as they have already heard some songs performed by the same singer. When the singer is unfamiliar it is common for listeners to attempt to characterize the singer&#39;s voice by finding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0HDPonzwK1wJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:0HDPonzwK1wJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="27">
        <url>https://repository.library.georgetown.edu/handle/10822/552965</url>
        <title status="complete" source="scholar.google.com">Learning techniques for identifying vocal regions in music using the wavelet transformation version 1.0</title>
        <pdf>https://repository.library.georgetown.edu/bitstream/handle/10822/552965/henryMichael.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">M Henry</author>
        <year>2012</year>
        <source>repository.library.georgetown.edu</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this research I present a machine learning method for the automatic detection of vocal regions in music. I employ the wavelet transformation to extract wavelet coefficients, from which I build feature sets capable of constructing a model that can distinguish ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PpPPvEB0KowJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10100012935726207806&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="28">
        <url>http://hal.inria.fr/hal-00759923/</url>
        <title status="complete" source="scholar.google.com">Exploiting Semantic Content for Singing Voice Detection</title>
        <pdf>http://hal.inria.fr/docs/00/75/99/23/PDF/2012_Ioannidis.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Ioannidis, JL Rouas</author>
        <proceeding status="partial" source="scholar.google.com">Sixth IEEE International Conference on &#8230;</proceeding>
        <year>2012</year>
        <source>hal.inria.fr</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ©: In this paper we propose a method for singing voice detection in popular music recordings. The method is based on statistical learning of spectral features extracted from the audio tracks. In our method we use Mel Frequency Cepstrum Coefficients (MFCC) to ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="29">
        <url>ftp://ftp.irisa.fr/techreports/theses/2006/ozerov.ps.gz</url>
        <title status="complete" source="scholar.google.com">devant l&#39;UniversitÃ© de Rennes</title>
        <pdf>ftp://ftp.irisa.fr/techreports/theses/2006/ozerov.ps.gz</pdf>
        <author status="complete" source="scholar.google.com">A Ozerov</author>
        <proceeding status="complete" source="scholar.google.com">irisa.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© La sÃ©paration de sources avec un seul capteur est un probleme tres rÃ©cent, qui attire de plus en plus d&#39;attention dans le monde scientifique. Cependant, il est loin d&#39;Ãªtre rÃ©solu et, mÃªme plus, il ne peut pas Ãªtre rÃ©solu en toute gÃ©nÃ©ralitÃ©. La difficultÃ© principale ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uf_1BBnHTmsJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:uf_1BBnHTmsJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7732336520513060793&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="30">
        <url>http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf</url>
        <title status="complete" source="scholar.google.com">DÃ©tection de la voix chantÃ©e dans un morceau de musique</title>
        <pdf>http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf</pdf>
        <author status="complete" source="scholar.google.com">L REGNIER</author>
        <proceeding status="complete" source="scholar.google.com">atiam.ircam.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© De tous les Ã©lÃ©ments constitutifs d&#39;un morceau de musique, la voix est sans conteste celui qui focalise le plus l&#39;attention de l&#39;auditeur. Ceci provient du fait que la voix est porteuse d&#39;un message (le texte) et d&#39;une identitÃ© (celle du chanteur). Pour ces raisons, de ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15229319373475501458&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="31">
        <url>http://eprints.fri.uni-lj.si/1169/</url>
        <title status="complete" source="scholar.google.com">HASH (0xbb3b6ea0)</title>
        <pdf>http://eprints.fri.uni-lj.si/1169/1/Strupeh_D._-UN.pdf</pdf>
        <author status="complete" source="scholar.google.com">D Strupeh</author>
        <year>2010</year>
        <source>eprints.fri.uni-lj.si</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this thesis the automatic recognition of groups in singing recordings is presented. The classification of audio recordings or their parts into defined classes is useful particularly at large record sets that carry a variety of useful research information. The manual record ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:XUEGKlAM1qEJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="32">
        <url>http://hal.inria.fr/tel-00564866/</url>
        <title status="complete" source="scholar.google.com">Adaptation de modÃ¨les statistiques pour la sÃ©paration de sources mono-capteur Texte imprimÃ©: application Ã  la sÃ©paration voix/musique dans les chansons</title>
        <pdf>http://hal.inria.fr/docs/00/56/48/66/PDF/ozerov_these.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Ozerov</author>
        <year>2006</year>
        <source>hal.inria.fr</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ©: La sÃ©paration de sources avec un seul capteur est un problÃ¨me trÃ¨s rÃ©cent, qui attire de plus en plus d&#39;attention dans le monde scientifique. Cependant, il est loin d&#39;Ãªtre rÃ©solu et, mÃªme plus, il ne peut pas Ãªtre rÃ©solu en toute gÃ©nÃ©ralitÃ©. La difficultÃ© principale ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:IqQDpNdyi08J:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5731801221254325282&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="33">
        <url>http://www.cqvip.com/qk/95659x/200905/30314191.html</url>
        <title status="complete" source="scholar.google.com">åºäºé«æ¯æ··åæ¨¡åæµè¡é³ä¹ä¸­æ­å±é¨åçæºè½æ£æµ</title>
        <author status="complete" source="scholar.google.com">æä¸½å¨ï¼ å¶èï¼ èµµæ¬£</author>
        <proceeding status="complete" source="scholar.google.com">å°åå¾®åè®¡ç®æºç³»ç»</proceeding>
        <year>2009</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">ææå°æ£æµåºæµè¡é³ä¹ä¸­çæ­å±é¨åå¯¹å¨æµ·éæ°æ®åºä¸­è¿è¡é³ä¹æ£ç´¢, æµè§, å½ç±», ä»¥åæå¾æååæ­å±å®¶è¯å«ç­æè¾å¤§çä»·å¼. æ¬æä½¿ç¨å¨è¯­é³ä¿¡å·å¤çä¸­å¹¿æ³ä½¿ç¨çåºäºæ¢å°é¢ççåè°±ç³»æ°(MFCC) ä½ä¸ºè¯­é³ç¹å¾æ¥åææè¦å¤ççé³ä¹ä¿¡å·, å¹¶éç¨é«æ¯æ··åæ¨¡å( ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:10Pnsd-bS1QJ:scholar.google.com/&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6074119907503981527&amp;hl=en&amp;num=34&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="12">
    <url>http://www.springerlink.com/index/J8Q46N41208P0507.pdf</url>
    <title status="complete" source="scholar.google.com">Speaker verification with adaptive spectral subband centroids</title>
    <pdf>http://pdf.aminer.org/000/274/265/spectral_subband_centroids_as_complementary_features_for_speaker_authentication.pdf</pdf>
    <author status="complete" source="scholar.google.com">T Kinnunen, B Zhang, J Zhu, Y Wang</author>
    <proceeding status="complete" source="scholar.google.com">Advances in Biometrics</proceeding>
    <year>2007</year>
    <source>Springer</source>
    <snippet status="partial" source="scholar.google.com">... 1 Speech and Dialogue Processing Lab Institution for Infocomm Research (I 2 R) 21 Heng MuiKeng Terrace, Singapore 119613 ktomi@i2r.a-star.edu.sg 2 Department of Computer ScienceSchool of Computing, National University of Singapore (NUS) 3 Science Drive 2 ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:-zU0VgAflcwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>20</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=14741723041573910011&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>5</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=14741723041573910011&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="5">
        <result id="0">
        <url>http://www.sciencedirect.com/science/article/pii/S0167639309001289</url>
        <title status="complete" source="scholar.google.com">An overview of text-independent speaker recognition: From features to supervectors</title>
        <pdf>http://peer.ccsd.cnrs.fr/docs/00/58/76/02/PDF/PEER_stage2_10.1016%252Fj.specom.2009.08.009.pdf</pdf>
        <author status="complete" source="scholar.google.com">T Kinnunen, H Li</author>
        <proceeding status="complete" source="scholar.google.com">Speech Communication</proceeding>
        <year>2010</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">This paper gives an overview of automatic speaker recognition technology, with an emphasis on text-independent recognition. Speaker recognition has been studied actively for several decades. We give an overview of both the classical and the state-of-the-art ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:rpY8siptCdMJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>25</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15206805646939559598&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>206</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15206805646939559598&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://www.isca-speech.org/archive_open/archive_papers/odyssey_2010/papers/od10_007.pdf</url>
        <title status="complete" source="scholar.google.com">Investigation of spectral centroid magnitude and frequency for speaker recognition</title>
        <author status="partial" source="scholar.google.com">JMK Kua, T Tharmarajah, M Nosratighods&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Odyssey Speaker and &#8230;</proceeding>
        <year>2010</year>
        <source>isca-speech.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Most conventional features used in speaker recognition are based on spectral envelope characterizations such as Mel-scale filterbank cepstrum coefficients (MFCC), Linear Prediction Cepstrum Coefficient (LPCC) and Perceptual Linear Prediction (PLP). ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:C-qXres6xe8J:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17277280329380915723&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.sciencedirect.com/science/article/pii/S0045790610000832</url>
        <title status="complete" source="scholar.google.com">Comparison of the impact of some Minkowski metrics on VQ/GMM based speaker recognition</title>
        <pdf>http://home.uludag.edu.tr/~chanilci/pdf/CEE2011.pdf</pdf>
        <author status="complete" source="scholar.google.com">C HanilÃ§i, F ErtaÅ</author>
        <proceeding status="complete" source="scholar.google.com">Computers &amp; Electrical Engineering</proceeding>
        <year>2011</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">This paper evaluates the impact of three special forms of the Minkowski metric (Euclidean, City Block, and Chebychev distances) on the performance of the conventional vector quantization (VQ) and Gaussian mixture model (GMM) based closed-set text-independent ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9Ryuu7Mj6RgJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1795005181338721525&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>6</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1795005181338721525&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <title status="complete" source="scholar.google.com">Text-Independent Speaker Recognition: Overview and State-of-the-Art</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8ob_rgjJm-sJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16977384259436119794&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://unsworks.unsw.edu.au/fapi/datastream/unsworks:10576/SOURCE02</url>
        <title status="complete" source="scholar.google.com">Improving Automatic Speaker Verification Using Front-end and Back-end diversity</title>
        <pdf>http://unsworks.unsw.edu.au/fapi/datastream/unsworks:10576/SOURCE02</pdf>
        <author status="complete" source="scholar.google.com">JMK Kua</author>
        <year>2012</year>
        <source>unsworks.unsw.edu.au</source>
        <snippet status="partial" source="scholar.google.com">Abstract Technologies that exploit biometrics can potentially be applied to the identification and verification of individuals for controlling access to secured areas or materials. Among these technologies, automatic speaker verification systems are of growing interest, as they ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jM7c4orDMF8J:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:jM7c4orDMF8J:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6859197233764290188&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="13">
    <url>http://dl.acm.org/citation.cfm?id=957099</url>
    <title status="complete" source="scholar.google.com">Content-based UEP: a new scheme for packet loss recovery in music streaming</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/5.Error_Robust_Audio_Streaming/2003_Content-based_UEP-A_New_Scheme_for_Packet_Loss_Recovery_in_Music_Streaming.pdf</pdf>
    <author status="partial" source="scholar.google.com">Y Wang, A Ahmaniemi, D Isherwood&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
    <year>2003</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Recovery in Music Streaming Ye Wang1,2, Ali Ahmaniemi2, David Isherwood2,Wendong Huang11School of Computing National University of Singapore 3 Science Drive 2, Singapore 117543wangye@comp.nus.edu.sg 2Audio and Visual Systems Laboratory, ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:x-zDQpyA1YMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>21</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=9499640397631319239&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>29</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=9499640397631319239&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="29">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1027549</url>
        <title status="complete" source="scholar.google.com">Content-based music structure analysis with applications to music semantics understanding</title>
        <pdf>http://pdf.aminer.org/000/502/603/content_based_music_structure_analysis_with_applications_to_music_semantics.pdf</pdf>
        <author status="partial" source="scholar.google.com">NC Maddage, C Xu, MS Kankanhalli&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th &#8230;</proceeding>
        <year>2004</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we present a novel approach for music structure analysis. A new segmentation method, beat space segmentation, is proposed and used for music chord detection and vocal/instrumental boundary detection. The wrongly detected chords in the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Tk9VPKYKLEcJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5128485784761225038&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>102</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5128485784761225038&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1580435</url>
        <title status="complete" source="scholar.google.com">Automatic structure detection for popular music</title>
        <pdf>http://www.comp.nus.edu.sg/~mohan/papers/music_struct_det.pdf</pdf>
        <author status="complete" source="scholar.google.com">NC Maddage</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Our proposed approach detects music structures by looking at beat-space segmentation, chords, singing-voice boundaries, and melody-and content-based similarity regions. Experiments illustrate that the proposed approach is capable of extracting useful ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pRwl89mT3mgJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7556639789070752933&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>36</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7556639789070752933&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7584495&amp;id=IGnKAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract</url>
        <title status="complete" source="scholar.google.com">Redundant stream alignment in IP datacasting over DVB-H</title>
        <author status="complete" source="scholar.google.com">M Hannuksela, R Jarvinen, I Bouazizi</author>
        <proceeding status="complete" source="scholar.google.com">US Patent 7,584,495</proceeding>
        <year>2009</year>
        <source>Google Patents</source>
        <snippet status="partial" source="scholar.google.com">A real-time program transmission system and method may receive a signal from a content source, and generate two different data streams. One stream may be of a higher quality than the other. The two streams may then be inserted into time slice frames, such that a single ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PCUwrUPJtGYJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7400761380182172988&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>14</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7400761380182172988&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://broadnets.org/2004/workshop-papers/Broadwim/broadwim2004-04-chesterfield-invite.pdf</url>
        <title status="complete" source="scholar.google.com">Experiences with multimedia streaming over 2.5 G and 3G Networks</title>
        <pdf>http://broadnets.org/2004/workshop-papers/Broadwim/broadwim2004-04-chesterfield-invite.pdf</pdf>
        <author status="partial" source="scholar.google.com">J Chesterfield, R Chakravorty, J Crowcroft&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Journal ACM/ &#8230;</proceeding>
        <year>2004</year>
        <source>broadnets.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract As third generation technologies become more widely deployed, mobile data users increasingly experience ubiquitous global network access across a variety of heterogeneous 2.5 and 3G technologies. Such provision of higher bandwidth in mobile ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:nQW94tf4hxYJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:nQW94tf4hxYJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1623539796796048797&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>16</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1623539796796048797&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.7973&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Transport level optimisations for streaming media over wideâ area wireless networks</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.7973&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="partial" source="scholar.google.com">J Chesterfield, R Chakravorty, S Banerjee, P Rodriguez&#8230;</author>
        <proceeding status="complete" source="scholar.google.com">WIOPT&#39;04</proceeding>
        <year>2004</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Wide-area cellular data networks (eg GPRS, CDMA 2000) are gaining popularity to provide âalways-onâ data connectivity to mobile users. However, the characteristics of the wide-area wireless links present several new challenges to different applications. In this ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:94r8oaPP6VkJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:94r8oaPP6VkJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6478937840653470455&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6478937840653470455&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1698477</url>
        <title status="complete" source="scholar.google.com">Perceptually controlled error protection for audio streaming over IP networks</title>
        <author status="partial" source="scholar.google.com">E Hellerud, JE Voldhaug&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; ,, 2006. ICDT&#39;06. &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents an unequal error protection (UEP) scheme for audio streaming over IP networks. The sender simulates packet loss by employing the same error concealment scheme as the receiver for each audio frame. The perceived quality of such ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0PPO0hA98oMJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9507728905798087632&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9507728905798087632&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5398918</url>
        <title status="complete" source="scholar.google.com">Index-based selective audio encryption for wireless multimedia sensor networks</title>
        <author status="partial" source="scholar.google.com">H Wang, M Hempel, D Peng, W Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia, IEEE &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Wireless multimedia sensor networks (WMSNs) support many acoustic applications for audio surveillance, animal tracking/vocalization, human health monitoring, etc. However, resource constraints in sensor networks (such as limited battery power, bandwidth/ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BrVo68ndvEsJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5457480707773871366&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5457480707773871366&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5398923</url>
        <title status="complete" source="scholar.google.com">Optimal audio transmission over error-prone wireless links</title>
        <author status="complete" source="scholar.google.com">A Khalifeh, H Yousefi&#39;zadeh</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE Transactions on</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we present an optimization framework for transmitting high quality audio sequences over error-prone wireless links. Our framework introduces apparatus and technique to optimally protect a stored audio sequence transmitted over a wireless link ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3s1AB2HLxK8J:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12665471669666631134&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12665471669666631134&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://www.sciencedirect.com/science/article/pii/S0164121204002298</url>
        <title status="complete" source="scholar.google.com">REDUP: a packet loss recovery scheme for real-time audio streaming over wireless IP networks</title>
        <pdf>http://ir.lib.stut.edu.tw/bitstream/987654321/9845/1/J4_REDUP%2520A%2520Packet-Loss%2520Recovery%2520Scheme%2520for%2520Real-Time%2520Audio%2520Streaming%2520over%2520Wireless%2520IP%2520Networks.pdf</pdf>
        <author status="complete" source="scholar.google.com">CM Huang, TH Hsu, YW Lin</author>
        <proceeding status="complete" source="scholar.google.com">Journal of Systems and Software</proceeding>
        <year>2006</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">Due to the characteristics of (1) smaller bandwidth and (2) unreliable transmission media, real-time audio streaming over wireless networks is not trivial. To have smooth audio streaming over wireless networks, we propose a scheme called REDUP in this paper. Two ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jI5tCkavSMAJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13855516968962854540&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13855516968962854540&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://www.ingentaconnect.com/content/dav/aaua/2008/00000094/00000001/art00002</url>
        <title status="complete" source="scholar.google.com">Influence of Sender Parameters and Network Architecture on Perceived Audio Quality</title>
        <author status="partial" source="scholar.google.com">J Erik Voldhaug, E Hellerud, A Undheim&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Acta Acustica united &#8230;</proceeding>
        <year>2008</year>
        <source>ingentaconnect.com</source>
        <snippet status="partial" source="scholar.google.com">Abstract: Network simulations and subjective testing have been used to evaluate the effects of frame lengths, packetization settings, and the advantages of using a Differentiated Services (DiffServ) network architecture for music streaming over Internet Protocol (IP) ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:d59fKWZUxYQJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9567145781176016759&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9567145781176016759&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://bit.kuas.edu.tw/~jihmsp/2010/vol1/JIH-MSP-2010-04-001.pdf</url>
        <title status="complete" source="scholar.google.com">Multiple Description Coding Using Time Domain Division for MP3 coded Sound Signal</title>
        <pdf>http://bit.kuas.edu.tw/~jihmsp/2010/vol1/JIH-MSP-2010-04-001.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Wey, A Ito, T Okamoto, Y Suzuki</author>
        <proceeding status="partial" source="scholar.google.com">Journal of Information Hiding &#8230;</proceeding>
        <year>2010</year>
        <source>bit.kuas.edu.tw</source>
        <snippet status="partial" source="scholar.google.com">Abstract. In audio communications over a lossy packet network, packet loss concealment techniques are needed to mitigate a user&#39;s frustration when perceiving the deterioration of the quality of the decoded signal. Multiple description coding (MDC) is a useful solution to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:UVbg5rc1loYJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:UVbg5rc1loYJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9697997911557690961&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9697997911557690961&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://www.aes.org/e-lib/browse.cfm?conv=121&amp;papernum=6893</url>
        <title status="complete" source="scholar.google.com">Error-Robust Frame Splitting For Audio Streaming Over the Lossy Packet Network</title>
        <author status="complete" source="scholar.google.com">JH Chang, JK Kim, JS Kim, NS Kim, HS Yun</author>
        <proceeding status="partial" source="scholar.google.com">Audio Engineering Society, &#8230;</proceeding>
        <year>2012</year>
        <source>aes.org</source>
        <snippet status="partial" source="scholar.google.com">In this paper, we propose a noble audio streaming scheme for the perceptual audio coder over the packet-based network. At first, a single frame is split into several subframes which are independently decoded based on the packet size for the robust error concealment. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8cG275ma6G4J:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7991807524711350769&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7991807524711350769&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://pages.cs.wisc.edu/~suman/pubs/monet04.pdf</url>
        <title status="complete" source="scholar.google.com">Transport Level Optimisations for Interactive Media Streaming Over Wide-Area Wireless Networks</title>
        <pdf>http://pages.cs.wisc.edu/~suman/pubs/monet04.pdf</pdf>
        <author status="partial" source="scholar.google.com">J Chesterfield, R Chakravorty, S Banerjee&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">WiOpt&#39;04: Modelling &#8230;</proceeding>
        <year>2004</year>
        <source>pages.cs.wisc.edu</source>
        <snippet status="partial" source="scholar.google.com">Abstract. Wide-area cellular data networks (eg GPRS, CDMA2000) are gaining popularity to provide&quot; always-on&quot; data connectivity to mobile users. However, the characteristics of the links present several new challenges to key applications which make use of them. In this ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:oRW0-RSPul8J:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:oRW0-RSPul8J:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6897983099522782625&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6897983099522782625&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://eprints.ulster.ac.uk/21380</url>
        <title status="complete" source="scholar.google.com">Song form intelligence for streaming music across wireless bursty networks</title>
        <pdf>http://eprints.ulster.ac.uk/21380/1/sofiaics05.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Doherty, K Curran, P McKevitt</author>
        <proceeding status="partial" source="scholar.google.com">Irish Conference on Artificial &#8230;</proceeding>
        <year>2005</year>
        <source>eprints.ulster.ac.uk</source>
        <snippet status="partial" source="scholar.google.com">Preliminary research on the development of a system for streaming audio across a wireless network, whilst using Song Form Intelligence (SoFI) to correct bursty errors, is presented. Current problems identified with streaming audio across wireless networks are reviewed. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:iZkMCRPLCcEJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13909872206727125385&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13909872206727125385&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://hi.snu.ac.kr/hm_j/public_html/upload_dir/pub/3_jkkim_Error-Robust%20Frame%20Splitting%20For%20Audio%20Streaming%20Over%20the%20Lossy%20Packet%20Network.pdf</url>
        <title status="complete" source="scholar.google.com">Errorrobust frame splitting for audio streaming over the lossy packet network</title>
        <pdf>http://hi.snu.ac.kr/hm_j/public_html/upload_dir/pub/3_jkkim_Error-Robust%20Frame%20Splitting%20For%20Audio%20Streaming%20Over%20the%20Lossy%20Packet%20Network.pdf</pdf>
        <author status="partial" source="scholar.google.com">JK Kim, HS Yun, JS Kim, JH Chang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">AES 121st Convention, &#8230;</proceeding>
        <year>2006</year>
        <source>hi.snu.ac.kr</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In this paper, we propose a novel audio streaming scheme for perceptual audio coder over the packet-switching network. Each frame is split into several subframes which are independently decoded based on the specified packet size for robust error ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:aCxOHUUmaPwJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:aCxOHUUmaPwJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18187829173422009448&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=18187829173422009448&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://electronicimaging.spiedigitallibrary.org/article.aspx?articleid=1098426%20</url>
        <title status="complete" source="scholar.google.com">Retransmission-based error control for scalable streaming media systems</title>
        <pdf>http://electronicimaging.spiedigitallibrary.org/article.aspx?articleid=1098426%20</pdf>
        <author status="complete" source="scholar.google.com">R Zimmermann, K Fu, F Liao</author>
        <proceeding status="partial" source="scholar.google.com">Journal of &#8230;</proceeding>
        <year>2005</year>
        <source>electronicimaging.spiedigitallibrary. &#8230;</source>
        <snippet status="partial" source="scholar.google.com">Large-scale continuous media (CM) system implementations require scalable servers most likely built from clusters of storage nodes. Across such nodes, random data placement is an attractive alternative to the traditional round-robin striping. One benefit of random ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:UjqwT9rMoyQJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2640179044581128786&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2640179044581128786&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://www.springerlink.com/index/h58mr71lmg58m615.pdf</url>
        <title status="complete" source="scholar.google.com">Frequency-Domain Stochastic Error Concealment for Wireless Audio Applications</title>
        <author status="complete" source="scholar.google.com">A Floros, M Avlonitis, P Vlamos</author>
        <proceeding status="complete" source="scholar.google.com">Mobile Networks and Applications</proceeding>
        <year>2008</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Real time digital audio delivery over Wireless Local Area Networks (WLANs) represents an attractive, flexible and cost effective framework for realizing high-quality, multichannel home audio applications. However, the unreliable nature of WLANs IP link ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:lj7fyReMhMUJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14232654756196597398&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14232654756196597398&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://dl.acm.org/citation.cfm?id=1385329</url>
        <title status="complete" source="scholar.google.com">Stochastic packet reconstruction for subjectively improved audio delivery over WLANs</title>
        <author status="complete" source="scholar.google.com">A Floros, M Avlonitis, P Vlamos</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the 3rd international conference on &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Real time digital audio delivery over Wireless Local Area Networks (WLANs) represents an attractive, flexible and cost effective framework for realizing high-quality, multichannel home audio applications. However, the unreliable nature of WLANs IP link ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_oTm_buPTF0J:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6722906381338707198&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://cdn.intechweb.org/pdfs/9261.pdf</url>
        <title status="complete" source="scholar.google.com">Music Structure Analysis Statistics for Popular Songs</title>
        <pdf>http://cdn.intechweb.org/pdfs/9261.pdf</pdf>
        <author status="complete" source="scholar.google.com">NC Maddage, L Haizhou, MS Kankanhalli</author>
        <proceeding status="complete" source="scholar.google.com">cdn.intechweb.org</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract In this chapter, we have proposed a better procedure for manual annotation of music information. The proposed annotation procedure involves carrying out listening tests and then incorporating music knowledge to iteratively refine the detected music ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7LRoGihTg1UJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:7LRoGihTg1UJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6161860146879837420&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6180097</url>
        <title status="complete" source="scholar.google.com">Radio Resource Allocation for OFDMA Systems in High Speed Environments</title>
        <author status="complete" source="scholar.google.com">H Zhu</author>
        <proceeding status="complete" source="scholar.google.com">Selected Areas in Communications, IEEE Journal on</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In high speed train (HST) system, real-time multimedia entertainments are very important applications in which a data stream often contains packets with different quality of service requirements. For example, video stream encoded with scalability contains the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ooz5LxSv--IJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16355858972965178530&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16355858972965178530&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5494001</url>
        <title status="complete" source="scholar.google.com">Adaptive Resource Management Based on Unequal Error Protection in OFDM Systems</title>
        <author status="complete" source="scholar.google.com">H Zhu</author>
        <proceeding status="partial" source="scholar.google.com">Vehicular Technology Conference (VTC 2010-Spring), &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In order to minimize the total transmit power in the downlink orthogonal frequency division multiplexing (OFDM) system, an adaptive resource management approach is proposed in this paper by considering that an application, such as real-time video and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:edliMoIpJBoJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1883676183315536249&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>http://www.springerlink.com/index/FT671731J0403662.pdf</url>
        <title status="complete" source="scholar.google.com">A Survey of Music Structure Analysis Techniques for Music Applications</title>
        <author status="complete" source="scholar.google.com">N Maddage, H Li, M Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">Recent Advances in Multimedia Signal &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Music carries multilayer information which forms different structures. The information embedded in the music can be categorized into time information, harmony/melody, music regions, music similarities, song structures and music semantics. In this chapter, we first ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3vo8SvZRZuUJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16529989600559299294&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://www.springerlink.com/index/424407J31N41P08X.pdf</url>
        <title status="complete" source="scholar.google.com">Content based packet loss recovery for classical music transmissions over the internet</title>
        <author status="complete" source="scholar.google.com">X Shao, C Zhou</author>
        <proceeding status="partial" source="scholar.google.com">Advances in Multimedia Information Processing-PCM &#8230;</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Various techniques have been proposed to deal with the problems created by packets loss. The error concealment is one of the possible techniques. Most of the researchers focused on error concealment techniques in general or specifically for human speech. These ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:DXaQjqMJMIwJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10101584562268173837&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="23">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1662532</url>
        <title status="complete" source="scholar.google.com">A novel no-latency simple-to-implement sender-based packet-loss recovery technique for multimedia streams</title>
        <author status="complete" source="scholar.google.com">B Rahnama, A Elci</author>
        <proceeding status="partial" source="scholar.google.com">Computer Networks, 2006 International &#8230;</proceeding>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We consider recovery techniques for packet-loss in the multimedia streams at both sender-and receiver-sides. We investigate recovery techniques as bases of useless packet transmission avoidance in order to release bandwidth by avoiding sending unintelligible ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:72FDm21jaggJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="24">
        <url>http://www.cl.cam.ac.uk/research/srg/netos/sla/mobileman/papers/WINET05_CHESTERFIELD.pdf</url>
        <title status="complete" source="scholar.google.com">Maximising Service Utilisation for Multimedia Streaming in the WWAN</title>
        <pdf>http://www.cl.cam.ac.uk/research/srg/netos/sla/mobileman/papers/WINET05_CHESTERFIELD.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Chesterfield, J Crowcroft</author>
        <proceeding status="complete" source="scholar.google.com">cl.cam.ac.uk</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract As third generation technologies become more widely deployed, mobile data users increasingly experience ubiquitous global network access across a variety of heterogeneous 2.5 and 3G technologies. Such provision of higher bandwidth in mobile ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:oyWcoMLvgHwJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:oyWcoMLvgHwJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="25">
        <url>http://www.scis.ulster.ac.uk/~jonathan/Research/papers/JDoherty1stYrRpt.doc</url>
        <title status="complete" source="scholar.google.com">Song Form Intelligence with Streaming Audio</title>
        <pdf>http://www.scis.ulster.ac.uk/~jonathan/Research/papers/JDoherty1stYrRpt.doc</pdf>
        <author status="complete" source="scholar.google.com">J Doherty, P Mc Kevitt, K Curran</author>
        <proceeding status="complete" source="scholar.google.com">scis.ulster.ac.uk</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract Streaming media across the Internet is still an unreliable and poor quality medium. Services such as audio-on-demand drastically increase the load on the networks therefore new, robust and highly efficient coding algorithms will be necessary. One overlooked ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BLvYyU2HRiYJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:BLvYyU2HRiYJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2758040589979663108&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="26">
        <url>http://eprints.ulster.ac.uk/21381</url>
        <title status="complete" source="scholar.google.com">Error concealment for streaming audio across wireless bursty networks</title>
        <pdf>http://eprints.ulster.ac.uk/21381/1/sofipgnet05.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Doherty, K Curran, P McKevitt</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Postgraduate Symposium on &#8230;</proceeding>
        <year>2005</year>
        <source>eprints.ulster.ac.uk</source>
        <snippet status="partial" source="scholar.google.com">Preliminary research on the development of an application for streaming audio across a wireless network, whilst using song form intelligence (SoFI) to correct bursty errors, is presented. Current problems identified with streaming audio across wireless networks are ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gbHZ6IXhRi8J:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3406658133405839745&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="27">
        <url>http://www.springerlink.com/index/4v4525654117554m.pdf</url>
        <title status="complete" source="scholar.google.com">Quality-driven secure audio transmissions in wireless multimedia sensor networks</title>
        <pdf>http://www.ece.ubc.ca/~minchen/min_paper/Min-0-JNLb-6-3-Honggang-MTAP2011.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Wang, W Wang, M Chen, X Yao</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Tools and Applications</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Many audio applications such as audio surveillance and human acoustic health monitoring require security protections for audio streaming over WSNs. The process of watermarking which embeds small amounts of data (ie, the watermark) into the original ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ZdcY9hC26ykJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3020708159046604645&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3020708159046604645&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=29</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="28">
        <url>http://www.cqvip.com/qk/95955b/201202/41765481.html</url>
        <title status="complete" source="scholar.google.com">åºäºå¤å¸é³ä¹ç Internet åç»å·®ééèæ¹æ¡</title>
        <author status="complete" source="scholar.google.com">ææåï¼ éµæ¦ï¼ ç³æ­£ç¨</author>
        <proceeding status="complete" source="scholar.google.com">åäº¬é®çµå¤§å­¦å­¦æ¥: èªç¶ç§å­¦ç</proceeding>
        <year>2012</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">å¸¸è§çInternet åç»å·®ééç¨éåä¸¢å¼æé»åéå¤, å¯¹æ³¨éå®æ¶æ§çVoIP æ¯ç®æ´èææç. ä½ä¸ºæ åªé³çæ³¨éä¹æåæå¾çå¤å¸é³ä¹, åæ´æ³¨éäººè³å¯¹é³ä¹çä¸ªä½æç¥. æä¸­ç»åºäºInternet æµåªä½æå¡å¨ä¼ è¾å¤å¸é³ä¹åºæ¯, æåºä¸ç§æ°çåç»ä¸¢å¤±éè(PEC) æ¹æ¡, ä¸ä»è§£å³åä¸ªåç» ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:F83_TW5Q_TQJ:scholar.google.com/&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3818296493765283095&amp;hl=en&amp;num=29&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="14">
    <url>https://jscholarship.library.jhu.edu/handle/1774.2/43</url>
    <title status="complete" source="scholar.google.com">A svm-based classification approach to musical audio</title>
    <pdf>https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/43/paper.pdf.txt</pdf>
    <author status="complete" source="scholar.google.com">NC Maddage, C Xu, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">&#8230; Conference on Music &#8230;</proceeding>
    <year>2003</year>
    <source>jscholarship.library.jhu.edu</source>
    <snippet status="partial" source="scholar.google.com">... 119613 maddage@i2r.a-star.edu.sg Changsheng Xu Institute for Inforcomm Research 21, HengMuikeng, Terrace Singapore 119613 xucs@i2r.a-star.edu.sg Ye Wang School of ComputingNational University of Singapore Singapore 117543 wangye@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:R5ivQM77y8IJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>22</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=14036589526897367111&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>28</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=14036589526897367111&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="28">
        <result id="0">
        <url>http://jjtok.io/papers/JNRSAS-2004.pdf</url>
        <title status="complete" source="scholar.google.com">Improving timbre similarity: How high is the sky?</title>
        <pdf>http://jjtok.io/papers/JNRSAS-2004.pdf</pdf>
        <author status="complete" source="scholar.google.com">F Pachet, JJ Aucouturier</author>
        <proceeding status="partial" source="scholar.google.com">Journal of negative results in speech and audio &#8230;</proceeding>
        <year>2004</year>
        <source>jjtok.io</source>
        <snippet status="partial" source="scholar.google.com">AbstractâWe report on experiments done in an attempt to improve the performance of a music similarity measure which we introduced in [2]. The technique aims at comparing music titles on the basis of their global âtimbreâ, which has many applications in the field of Music ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_WoNDU5s93YJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:_WoNDU5s93YJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8572439498205260541&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>285</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8572439498205260541&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4156205</url>
        <title status="complete" source="scholar.google.com">Separation of singing voice from music accompaniment for monaural recordings</title>
        <pdf>ftp://ftp.cse.ohio-state.edu/pub/tech-report/2005/TR61.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Li, DL Wang</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language Processing, &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Separating singing voice from music accompaniment is very useful in many applications, such as lyrics recognition and alignment, singer identification, and music information retrieval. Although speech separation has been extensively studied for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PJ4gQrBmOuoJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16877915460734066236&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>74</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16877915460734066236&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.ee.columbia.edu/~dpwe/ismir2004/CRFILES/paper183.pdf</url>
        <title status="complete" source="scholar.google.com">Automatic detection of vocal segments in popular songs</title>
        <pdf>http://www.ee.columbia.edu/~dpwe/ismir2004/CRFILES/paper183.pdf</pdf>
        <author status="complete" source="scholar.google.com">TL Nwe, Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">Proc. ISMIR</proceeding>
        <year>2004</year>
        <source>ee.columbia.edu</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper presents a technique for the automatic classification of vocal and non-vocal regions in an acoustic musical signal. The proposed technique uses acoustic features which are suitable to distinguish vocal and non-vocal signals. We employ the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pKUgJWqMSg4J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:pKUgJWqMSg4J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>24</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1029789852324898212&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>38</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1029789852324898212&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.5510&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Separation of vocals from polyphonic audio recordings</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.5510&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">S Vembu, S Baumann</author>
        <proceeding status="complete" source="scholar.google.com">Proc. ISMIR</proceeding>
        <year>2005</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Source separation techniques like independent component analysis and the more recent non-negative matrix factorization are gaining widespread use for the monaural separation of individual tracks present in a music sample. The underlying principle behind ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tPH3PQYE3esJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:tPH3PQYE3esJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16995744993622094260&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>39</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16995744993622094260&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1580435</url>
        <title status="complete" source="scholar.google.com">Automatic structure detection for popular music</title>
        <pdf>http://www.comp.nus.edu.sg/~mohan/papers/music_struct_det.pdf</pdf>
        <author status="complete" source="scholar.google.com">NC Maddage</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Our proposed approach detects music structures by looking at beat-space segmentation, chords, singing-voice boundaries, and melody-and content-based similarity regions. Experiments illustrate that the proposed approach is capable of extracting useful ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pRwl89mT3mgJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7556639789070752933&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>36</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7556639789070752933&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://dl.acm.org/citation.cfm?id=1027602</url>
        <title status="complete" source="scholar.google.com">Singing voice detection in popular music</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2004_Singing_Voice_Detection_in_Popular_Music.pdf</pdf>
        <author status="complete" source="scholar.google.com">TL Nwe, A Shenoy, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th annual ACM &#8230;</proceeding>
        <year>2004</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a novel technique for the automatic classification of vocal and non-vocal regions in an acoustic musical signal. Our technique uses a combination of harmonic content attenuation using higher level musical knowledge of key followed by sub-band ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bh6Y-g5TQzUJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>22</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3838002631248715374&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>34</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3838002631248715374&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5153305</url>
        <title status="complete" source="scholar.google.com">On the improvement of singing voice separation for monaural recordings using the MIR-1K dataset</title>
        <author status="complete" source="scholar.google.com">CL Hsu, JSR Jang</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language Processing, &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Monaural singing voice separation is an extremely challenging problem. While efforts in pitch-based inference methods have led to considerable progress in voiced singing voice separation, little attention has been paid to the incapability of such methods to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Qss1n-xSAnEJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8143162252476140354&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>32</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8143162252476140354&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://ohm.fing.edu.uy/publicaciones/2007/RH07/RH07.pdf</url>
        <title status="complete" source="scholar.google.com">Comparing audio descriptors for singing voice detection in music audio files</title>
        <pdf>http://ohm.fing.edu.uy/publicaciones/2007/RH07/RH07.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Rocamora, P Herrera</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; on Computer Music, 11th. San Pablo, &#8230;</proceeding>
        <year>2007</year>
        <source>ohm.fing.edu.uy</source>
        <snippet status="partial" source="scholar.google.com">Abstract. Given the relevance of the singing voice in popular western music, a system able to reliable identify those portions of a music audio file containing vocals would be very useful. In this work, we explore already used descriptors to perform this task and compare the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:d-X3ceQPzT8J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:d-X3ceQPzT8J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4597348268454176119&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>23</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4597348268454176119&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://www.mdpi.com/1999-4893/2/3/907</url>
        <title status="complete" source="scholar.google.com">Classification of echolocation calls from 14 species of bat by support vector machines and ensembles of neural networks</title>
        <pdf>http://www.mdpi.com/1999-4893/2/3/907/pdf</pdf>
        <author status="complete" source="scholar.google.com">RD Redgwell, JM Szewczak, G Jones, S Parsons</author>
        <proceeding status="complete" source="scholar.google.com">Algorithms</proceeding>
        <year>2009</year>
        <source>mdpi.com</source>
        <snippet status="partial" source="scholar.google.com">Abstract: Calls from 14 species of bat were classified to genus and species using discriminant function analysis (DFA), support vector machines (SVM) and ensembles of neural networks (ENN). Both SVMs and ENNs outperformed DFA for every species while ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:HKcXRKyRDtoJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15712656319005042460&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>12</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15712656319005042460&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://www.springerlink.com/index/FF2RACE6AEM0G0TJ.pdf</url>
        <title status="complete" source="scholar.google.com">Semantic region detection in acoustic music signals</title>
        <author status="complete" source="scholar.google.com">N Maddage, C Xu, A Shenoy, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Advances in Multimedia &#8230;</proceeding>
        <year>2005</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">We propose a novel approach to detect semantic regions (pure vocals, pure instrumental and instrumental mixed vocals) in acoustic music signals. The acoustic music signal is first segmented at the beat level based on our proposed rhythm tracking algorithm. Then for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9q_6ZQ2X8rAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12750419578840592374&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12750419578840592374&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://ismir2009.ismir.net/proceedings/PS4-22.pdf</url>
        <title status="complete" source="scholar.google.com">An integrated approach to music boundary detection</title>
        <pdf>http://ismir2009.ismir.net/proceedings/PS4-22.pdf</pdf>
        <author status="partial" source="scholar.google.com">MY Su, YH Yang, YC Lin&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 10th &#8230;</proceeding>
        <year>2009</year>
        <source>ismir2009.ismir.net</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Music boundary detection is a fundamental step of music analysis and summarization. Existing works use either unsupervised or supervised methodologies to detect boundary. In this paper, we propose an integrated approach that takes advantage ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:c0hZDRRZn9EJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:c0hZDRRZn9EJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15104889617882105971&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15104889617882105971&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <title status="complete" source="scholar.google.com">Singing voice detection in polyphonic music using predominant pitch</title>
        <pdf>http://www.ee.iitb.ac.in/daplab/publications/international-conference/papers/VRPR_ICSLP09.pdf</pdf>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:spNETHhXINsJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15789716467748213682&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15789716467748213682&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://www.ee.iitb.ac.in/web/files/publications/RaoPS2008.pdf</url>
        <title status="complete" source="scholar.google.com">Singing voice detection in north indian classical music</title>
        <pdf>http://www.ee.iitb.ac.in/web/files/publications/RaoPS2008.pdf</pdf>
        <author status="complete" source="scholar.google.com">V Rao, S Ramakrishnan, P Rao</author>
        <proceeding status="partial" source="scholar.google.com">Proc. of the National Conference on &#8230;</proceeding>
        <year>2008</year>
        <source>ee.iitb.ac.in</source>
        <snippet status="partial" source="scholar.google.com">AbstractâSinging voice detection is essential for content-based applications such as those involving melody extraction and singer identification. This article is concerned with the accurate detection of singing voice phrases in north Indian classical vocal music. The ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2owjetZNGuAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:2owjetZNGuAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16148304997457824986&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16148304997457824986&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://www.ee.iitb.ac.in/daplab/publications/international-conference/papers/PR_InvitedPaper_ncsipa09.pdf</url>
        <title status="complete" source="scholar.google.com">Musical information extraction from the singing voice</title>
        <pdf>http://www.ee.iitb.ac.in/daplab/publications/international-conference/papers/PR_InvitedPaper_ncsipa09.pdf</pdf>
        <author status="complete" source="scholar.google.com">P Rao</author>
        <proceeding status="complete" source="scholar.google.com">Proc. National Conf. Signal &amp; Image Process</proceeding>
        <year>2009</year>
        <source>ee.iitb.ac.in</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music information retrieval is currently an active research area that addresses the extraction of musically important information from audio signals, and the applications of such information. The extracted information can be used for search and retrieval of music in ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3-G36APBOeEJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:3-G36APBOeEJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16229214954739720671&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16229214954739720671&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607654</url>
        <title status="complete" source="scholar.google.com">Using dtw based unsupervised segmentation to improve the vocal part detection in pop music</title>
        <author status="complete" source="scholar.google.com">L Xiao, J Zhou, T Zhang</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2008 IEEE &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Vocal part detection, which plays an important role in music information retrieval, is still a tough task so far. Previous works focused on short time features, which cannot capture some essential long term characteristics of singing. In this paper, we propose a Dynamic ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:um2CZXTxNhkJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1816904981912120762&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://www.ee.iitb.ac.in/daplab/publications/vr-cg-pr-AMR-11.pdf</url>
        <title status="complete" source="scholar.google.com">Context-aware features for singing voice detection in polyphonic music</title>
        <pdf>http://www.ee.iitb.ac.in/daplab/publications/vr-cg-pr-AMR-11.pdf</pdf>
        <author status="complete" source="scholar.google.com">V Rao, C Gupta, P Rao</author>
        <proceeding status="complete" source="scholar.google.com">Proc. of Adaptive Multimedia Retrieval</proceeding>
        <year>2011</year>
        <source>ee.iitb.ac.in</source>
        <snippet status="partial" source="scholar.google.com">Abstract. The effectiveness of audio content analysis for music retrieval may be enhanced by the use of available metadata. In the present work, observed differences in singing style and instrumentation across genres are used to adapt acoustic features for the singing voice ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KEA0xlAZmokJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:KEA0xlAZmokJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9915265364322959400&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9915265364322959400&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <title status="complete" source="scholar.google.com">Singing Phoneme Class Detection In Polyphonic Music Recordings</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uJWqBAVmuaEJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11653457682537027000&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11653457682537027000&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://audiofingerprinting.googlecode.com/svn/trunk/Theory/NeuralMachinesForMusicRecognition.pdf</url>
        <title status="complete" source="scholar.google.com">Neural machines for music recognition</title>
        <pdf>http://audiofingerprinting.googlecode.com/svn/trunk/Theory/NeuralMachinesForMusicRecognition.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Pape</author>
        <year>2006</year>
        <source>audiofingerprinting.googlecode.com</source>
        <snippet status="partial" source="scholar.google.com">Abstract Since the early days of neural networks research, the focus has largely been aimed at forward processing of information. A recently developed theory called pattern theory, takes a different approach to solving pattern recognition problems. Rather than extracting ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:4D71JE-TIKAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:4D71JE-TIKAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11538384213454962400&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11538384213454962400&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://institut17-1.kug.ac.at/fileadmin/media/iem/projects/2011/gampp_ti.pdf</url>
        <title status="complete" source="scholar.google.com">Evaluation of Robust Features for Singing Voice Detection</title>
        <pdf>http://institut17-1.kug.ac.at/fileadmin/media/iem/projects/2011/gampp_ti.pdf</pdf>
        <author status="complete" source="scholar.google.com">P Gampp</author>
        <proceeding status="complete" source="scholar.google.com">institut17-1.kug.ac.at</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract The detection of singing voice segments within music signals is an important object of research in the field of music information retrieval, since it serves as an essential pre-stage for applications like singer identification, lyrics recognition, singing melody ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:CMMZACBTsKIJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:CMMZACBTsKIJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11722961226951148296&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://books.google.com/books?hl=en&amp;lr=&amp;id=huAzfJQhXDcC&amp;oi=fnd&amp;pg=PA99&amp;ots=_G5BM4g-li&amp;sig=36G2z5eAt4-5C_e0-IMJwuROluU</url>
        <title status="complete" source="scholar.google.com">Content-Based Music Summarization and Classification</title>
        <author status="complete" source="scholar.google.com">JS Jin</author>
        <proceeding status="complete" source="scholar.google.com">Managing Multimedia Semantics</proceeding>
        <year>2005</year>
        <source>books.google.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This chapter aims to provide a comprehensive survey of the technical achievements in the area of content-based music summarization and classification and to present our recent achievements. In order to give a full picture of the current status, the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:rJdeESIOowoJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf</url>
        <title status="complete" source="scholar.google.com">Melody Separation from Polyphonic Audio Recordings</title>
        <pdf>http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Vembu</author>
        <year>2005</year>
        <source>www-kd.iai.uni-bonn.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract In the field of music information retrieval, query-by-humming is an interesting application area in which humming sequences or melodies are matched with a database of songs to come up with a list of indexed songs that are similar in melodic content to the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16884184643397726041&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <title status="complete" source="scholar.google.com">A project report submitted for the award of BSc Computer Science with Artificial Intelligence</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dTdLIlVUU0oJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://mpac.ee.ntu.edu.tw/~yihsuan/pub/ISMIR09_segmentation.pdf</url>
        <title status="complete" source="scholar.google.com">AN INTEGRATED APPROACH TO MUSIC BOUNDARY DETECTION</title>
        <pdf>http://mpac.ee.ntu.edu.tw/~yihsuan/pub/ISMIR09_segmentation.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Min-Yian, YH Yang, YC Lin, H Chen</author>
        <year>2009</year>
        <source>mpac.ee.ntu.edu.tw</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Music boundary detection is a fundamental step of music analysis and summarization. Existing works either use unsupervised or supervised methods to detect boundary. In this paper, we propose an integrated approach that takes advantage of both ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:R-mRR-dw3RgJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:R-mRR-dw3RgJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="23">
        <title status="complete" source="scholar.google.com">Monaural musical sound separation</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jpa0VTJdi4EJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9334657123423131278&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="24">
        <url>http://www.ee.columbia.edu/~dpwe/e6820/proposals/felix.pdf</url>
        <title status="complete" source="scholar.google.com">Identifying singing segments in music</title>
        <pdf>http://www.ee.columbia.edu/~dpwe/e6820/proposals/felix.pdf</pdf>
        <author status="complete" source="scholar.google.com">FS Garcia</author>
        <proceeding status="complete" source="scholar.google.com">ee.columbia.edu</proceeding>
        <snippet status="partial" source="scholar.google.com">Page 1. Identifying singing segments in music Felix Sanchez Garcia Page 2. Objective â¢Given a music sample, identify singing segments Page 3. Difficulties â¢ It is hard tomodel singing voice, it differs from normal speech â Mixed ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_vPzlY0xKEAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:_vPzlY0xKEAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4622999501671756798&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="25">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4797191</url>
        <title status="complete" source="scholar.google.com">An Effective Vocal/Non-vocal Segmentation Approach for Embedded Music Retrieve System on Mobile Phone</title>
        <author status="complete" source="scholar.google.com">H Tuo, H Li, K Lei</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; and Mobile Computing, 2009. CMC&#39;09. WRI &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract With the growing bodies of MP3 songs in Internet, content-based analysis plays an important role for its retrieving and management. Due to most useful information is carried by vocal portions, it is necessary to separate the vocal segments from music. This paper ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:44sDuDX6SGMJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7154243116705483747&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="26">
        <url>http://ir.lib.szu.edu.cn:8080/handle/244041/2221</url>
        <title status="complete" source="scholar.google.com">åºäºç½®ä¿¡æµåº¦ç GMM æ¨¡åééç®æ³å¨é³ä¹åå²ä¸­çåºç¨</title>
        <pdf>http://ir.lib.szu.edu.cn:8080/bitstream/244041/2221/1/%E5%9F%BA%E4%BA%8E%E7%BD%AE%E4%BF%A1%E6%B5%8B%E5%BA%A6%E7%9A%84GMM%E6%A8%A1%E5%9E%8B%E9%80%82%E9%85%8D%E7%AE%97%E6%B3%95%E5%9C%A8%E9%9F%B3%E4%B9%90%E5%88%86%E5%89%B2%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¼ äºç£ï¼ éè½æï¼ æé</author>
        <year>2012</year>
        <source>ir.lib.szu.edu.cn</source>
        <snippet status="partial" source="scholar.google.com">æ¬ææåºäºä¸ç§åºäºç½®ä¿¡æµåº¦çèªéåºæ¨¡åééç®æ³ç¨äºé³ä¹åå². å¨ä¼ ç»çé³ä¹åå²ç®æ³åºç¡ä¸, éè¿ç½®ä¿¡æµåº¦éæ©å¯é çæ°æ®è¿è¡æ¨¡åçå¨çº¿éé, è·å¾è·å¾åå²çé³ä¹ä¿¡å·æ´å¹éçå£°ä¹/éå£°ä¹æ¨¡å, ä»èæé«é³ä¹åå²çåç¡®ç. ç¸å¯¹äºä¼ ç»çç®æ³, è¯¥ç®æ³å¨éè¯¯ç, èè­¦çåæ¼æ¥ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:52AkHppt1ccJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14399535892285120743&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="27">
        <url>http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf</url>
        <title status="complete" source="scholar.google.com">DÃ©tection de la voix chantÃ©e dans un morceau de musique</title>
        <pdf>http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf</pdf>
        <author status="complete" source="scholar.google.com">L REGNIER</author>
        <proceeding status="complete" source="scholar.google.com">atiam.ircam.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© De tous les Ã©lÃ©ments constitutifs d&#39;un morceau de musique, la voix est sans conteste celui qui focalise le plus l&#39;attention de l&#39;auditeur. Ceci provient du fait que la voix est porteuse d&#39;un message (le texte) et d&#39;une identitÃ© (celle du chanteur). Pour ces raisons, de ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15229319373475501458&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="15">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1394478</url>
    <title status="complete" source="scholar.google.com">Singing voice detection using twice-iterated composite fourier transform</title>
    <pdf>http://www.mirlab.org/conference_papers/International_Conference/ICME%202004/html/papers/P71285.pdf</pdf>
    <author status="partial" source="scholar.google.com">NC Maddage, K Wan, C Xu&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, &#8230;</proceeding>
    <year>2004</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... wangye@comp. nus. edu.sg I 2 Abstract In this paper, we propose a Twice-IteratedComposite Fourier Transform (TICFT) technique to detect the singing voiceboundaries from acoustical polyphonic music signals. We show ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:BBGwRYTisPEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>6</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=17415668816774435076&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>27</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=17415668816774435076&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="27">
        <result id="0">
        <url>http://www.ee.columbia.edu/~dpwe/ismir2004/CRFILES/paper183.pdf</url>
        <title status="complete" source="scholar.google.com">Automatic detection of vocal segments in popular songs</title>
        <pdf>http://www.ee.columbia.edu/~dpwe/ismir2004/CRFILES/paper183.pdf</pdf>
        <author status="complete" source="scholar.google.com">TL Nwe, Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">Proc. ISMIR</proceeding>
        <year>2004</year>
        <source>ee.columbia.edu</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper presents a technique for the automatic classification of vocal and non-vocal regions in an acoustic musical signal. The proposed technique uses acoustic features which are suitable to distinguish vocal and non-vocal signals. We employ the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pKUgJWqMSg4J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:pKUgJWqMSg4J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>24</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1029789852324898212&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>38</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1029789852324898212&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1394661</url>
        <title status="complete" source="scholar.google.com">Unsupervised classification of music genre using hidden markov model</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.4854&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">X Shao, C Xu, MS Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2004. &#8230;</proceeding>
        <year>2004</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music genre classification can be of great utility to musical database management. Most current classification methods are supervised and tend to be based on contrived taxonomies. However, due to the ambiguities and inconsistencies in the chosen ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KGj0Z9csuNoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15760396199656712232&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>38</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15760396199656712232&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4067048</url>
        <title status="complete" source="scholar.google.com">Exploring vibrato-motivated acoustic features for singer identification</title>
        <pdf>http://www1.i2r.a-star.edu.sg/~hli/papers/101109TASL2006876756.pdf</pdf>
        <author status="complete" source="scholar.google.com">TL Nwe, H Li</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language Processing, IEEE &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Vibrato is a slightly tremulous effect imparted to vocal or instrumental tone for added warmth and expressiveness through slight variation in pitch. It corresponds to a periodic fluctuation of the fundamental frequency. It is common for a singer to develop a vibrato ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7yW25Nf-A5wJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11242109298055456239&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>32</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11242109298055456239&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://dl.acm.org/citation.cfm?id=1027602</url>
        <title status="complete" source="scholar.google.com">Singing voice detection in popular music</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2004_Singing_Voice_Detection_in_Popular_Music.pdf</pdf>
        <author status="complete" source="scholar.google.com">TL Nwe, A Shenoy, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th annual ACM &#8230;</proceeding>
        <year>2004</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a novel technique for the automatic classification of vocal and non-vocal regions in an acoustic musical signal. Our technique uses a combination of harmonic content attenuation using higher level musical knowledge of key followed by sub-band ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bh6Y-g5TQzUJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>22</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3838002631248715374&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>34</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3838002631248715374&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1334225</url>
        <title status="complete" source="scholar.google.com">Singer identification based on vocal and instrumental models</title>
        <pdf>http://nguyendangbinh.org/Proceedings/ICPR/2004/DATA/V22_3_18.PDF</pdf>
        <author status="complete" source="scholar.google.com">NC Maddage, C Xu, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Pattern Recognition, 2004. ICPR &#8230;</proceeding>
        <year>2004</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a novel method to identify the singer of a query song from the audio database. The database contains over 100 popular songs of solo singers. The rhythm structure of the song is analyzed using our proposed rhythm tracking method and the song ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Ol18X4LG9LkJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13399553054584102202&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>22</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13399553054584102202&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ohm.fing.edu.uy/publicaciones/2007/RH07/RH07.pdf</url>
        <title status="complete" source="scholar.google.com">Comparing audio descriptors for singing voice detection in music audio files</title>
        <pdf>http://ohm.fing.edu.uy/publicaciones/2007/RH07/RH07.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Rocamora, P Herrera</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; on Computer Music, 11th. San Pablo, &#8230;</proceeding>
        <year>2007</year>
        <source>ohm.fing.edu.uy</source>
        <snippet status="partial" source="scholar.google.com">Abstract. Given the relevance of the singing voice in popular western music, a system able to reliable identify those portions of a music audio file containing vocals would be very useful. In this work, we explore already used descriptors to perform this task and compare the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:d-X3ceQPzT8J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:d-X3ceQPzT8J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4597348268454176119&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>23</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4597348268454176119&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://www.springerlink.com/index/j1h044n4v1767603.pdf</url>
        <title status="complete" source="scholar.google.com">Automatic lyrics alignment for Cantonese popular music</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.1526&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">CH Wong, WM Szeto, KH Wong</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Systems</proceeding>
        <year>2007</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract From lyrics-display on electronic music players and Karaoke videos to surtitles for live Chinese opera performance, one feature is common to all these everyday functionalities temporal: synchronization of the written text and its corresponding musical phrase. Our ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8ee-SWoY3QoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=782808755015182321&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>16</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=782808755015182321&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.9357&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Effective singing voice detection in popular music using arma filtering</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.9357&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">H Lukashevich, M Gruhne, C Dittmar</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; on Digital Audio Effects (DAFx&#39;07)</proceeding>
        <year>2007</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Locating singing voice segments is essential for convenient indexing, browsing and retrieval large music archives and catalogues. Furthermore, it is beneficial for automatic music transcription and annotations. The approach described in this paper uses Mel- ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bHRPZJbXzYsJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:bHRPZJbXzYsJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10073944982425662572&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>17</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10073944982425662572&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4518002</url>
        <title status="complete" source="scholar.google.com">Vocal detection in music with support vector machines</title>
        <pdf>http://www.tsi.enst.fr/~grichard/Publications/Icassp08_ramona.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Ramona, G Richard, B David</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and Signal &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a statistical learning approach for the automatic detection of vocal regions in a polyphonic musical signal. A support vector model, based on a large feature set, is employed to discriminate accompanied singing voice from pure instrumental regions. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:t4S9h9gGkXEJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8183329524968948919&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>17</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8183329524968948919&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255</url>
        <title status="complete" source="scholar.google.com">Singing voice detection for karaoke application</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Shenoy, Y Wu, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Visual &#8230;</proceeding>
        <year>2005</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">abstract We present a framework to detect the regions of singing voice in musical audio signals. This work is oriented towards the development of a robust transcriber of lyrics for karaoke applications. The technique leverages on a combination of low-level audio ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.184.6676&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Singing voice detection in monophonic and polyphonic contexts</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.184.6676&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">H Lachambre, R AndrÃ©-Obrecht, J Pinquier</author>
        <proceeding status="partial" source="scholar.google.com">15th European Signal &#8230;</proceeding>
        <year>2009</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In this article, we present an improvement of a previous singing voice detector. This new detector is in two steps. First, we distinguish monophonies from polyphonies. This distinction is based on the fact that the pitch estimated in a monophony is more reliable ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:qd8cvAJEkEoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:qd8cvAJEkEoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5372869133989633961&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5372869133989633961&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://hal.archives-ouvertes.fr/tel-00457522/</url>
        <title status="complete" source="scholar.google.com">CaractÃ©risation de l&#39;environnement musical dans les documents audiovisuels</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/45/75/22/PDF/These_Helene_Lachambre.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Lachambre</author>
        <year>2009</year>
        <source>hal.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">Depuis plusieurs dizaines d&#39;annÃ©es, l&#39;indexation par le contenu des documents audiovisuels fait l&#39;objet de travaux de la part de nombreuses Ã©quipes de recherche:âLe mot Â«audiovisuelÂ» fait rÃ©fÃ©rencea un contenu Â«multimÃ©diaÂ» qui regroupea la fois des donnÃ©es ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Yp_9nKZh3GwJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7844252019198893922&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7844252019198893922&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <title status="complete" source="scholar.google.com">Singing Phoneme Class Detection In Polyphonic Music Recordings</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uJWqBAVmuaEJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11653457682537027000&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11653457682537027000&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://www.springerlink.com/index/t3086h1747710650.pdf</url>
        <title status="complete" source="scholar.google.com">Impulsive Environment Sound Detection by Neural Classification of Spectrogram and Mel-Frequency Coefficient Images</title>
        <author status="complete" source="scholar.google.com">P Khunarsa, C Lursinsap, T Raicharoen</author>
        <proceeding status="partial" source="scholar.google.com">Advances in Neural Network &#8230;</proceeding>
        <year>2010</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">The problem of automatic detecting impulsive sounds such as human sound (screams, shout), gun shots, machine gun, thunder, fire alarm, and car horn are useful for hearing impairment person. In this paper, instead of filtering the frequency of each sound for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9iR6e_E8tMwJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14750481687401604342&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14750481687401604342&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5381403</url>
        <title status="complete" source="scholar.google.com">Vocal characteristics classification of audio segments: An investigation of the influence of accompaniment music on low-level features</title>
        <author status="complete" source="scholar.google.com">D Gartner, C Dittmar</author>
        <proceeding status="partial" source="scholar.google.com">Machine Learning and Applications, &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The characteristics of vocal segments in music are an important cue for automatic, content-based music recommendation, especially in the urban genre. In this paper, we investigate the classification of audio segments into singing and rap, using low-level ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:JjfSvDQsmK0J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12508796570039367462&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12508796570039367462&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://tel.archives-ouvertes.fr/pastel-00529331/</url>
        <title status="complete" source="scholar.google.com">Classification automatique de flux radiophoniques par Machines Ã  Vecteurs de Support</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/52/93/31/PDF/main.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Ramona</author>
        <year>2010</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">1.1 Vers une radio numÃ©rique......................... 9 1.2 Applications de l&#39;indexation audio pour la radio............ 10 1.3 Â«Qu&#39;est-ce que la musique?Â»...................... 11 1.4 Classification par Machines Ã  Vecteurs de Support.......... 12 1.5 ProblÃ©matiques................................ 13 1.6 ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:1n7-PBzLMbkJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13344670493018324694&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13344670493018324694&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://books.google.com/books?hl=en&amp;lr=&amp;id=huAzfJQhXDcC&amp;oi=fnd&amp;pg=PA99&amp;ots=_G5BM4g0fk&amp;sig=0B5R75tcSxDCFKy3Gq_DFBejE8Y</url>
        <title status="complete" source="scholar.google.com">Content-Based Music Summarization and Classification</title>
        <author status="complete" source="scholar.google.com">JS Jin</author>
        <proceeding status="complete" source="scholar.google.com">Managing Multimedia Semantics</proceeding>
        <year>2005</year>
        <source>books.google.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This chapter aims to provide a comprehensive survey of the technical achievements in the area of content-based music summarization and classification and to present our recent achievements. In order to give a full picture of the current status, the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:rJdeESIOowoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://tel.archives-ouvertes.fr/tel-00687475/</url>
        <title status="complete" source="scholar.google.com">Localisation, caractÃ©risation et reconnaissance de voix chantÃ©es</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/68/74/75/PDF/these.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Regnier</author>
        <year>2012</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">Many auditors have a remarkable ability to identify the singer of a new song as long as they have already heard some songs performed by the same singer. When the singer is unfamiliar it is common for listeners to attempt to characterize the singer&#39;s voice by finding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ObV6dGA6ysYJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14324325750750754105&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <title status="complete" source="scholar.google.com">Singing Voice Detection in Western Popular Music</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dx-cdoqIEO4J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://www.ee.columbia.edu/~dpwe/e6820/proposals/felix.pdf</url>
        <title status="complete" source="scholar.google.com">Identifying singing segments in music</title>
        <pdf>http://www.ee.columbia.edu/~dpwe/e6820/proposals/felix.pdf</pdf>
        <author status="complete" source="scholar.google.com">FS Garcia</author>
        <proceeding status="complete" source="scholar.google.com">ee.columbia.edu</proceeding>
        <snippet status="partial" source="scholar.google.com">Page 1. Identifying singing segments in music Felix Sanchez Garcia Page 2. Objective â¢Given a music sample, identify singing segments Page 3. Difficulties â¢ It is hard tomodel singing voice, it differs from normal speech â Mixed ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_vPzlY0xKEAJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:_vPzlY0xKEAJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4622999501671756798&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://hal.inria.fr/docs/00/68/74/75/PDF/these.pdf</url>
        <title status="complete" source="scholar.google.com">L&#39;UNIVERSITE PIERRE ET MARIE CURIE</title>
        <pdf>http://hal.inria.fr/docs/00/68/74/75/PDF/these.pdf</pdf>
        <author status="complete" source="scholar.google.com">ML Regnier</author>
        <proceeding status="complete" source="scholar.google.com">hal.inria.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">Many auditors have a remarkable ability to identify the singer of a new song as long as they have already heard some songs performed by the same singer. When the singer is unfamiliar it is common for listeners to attempt to characterize the singer&#39;s voice by finding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0HDPonzwK1wJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:0HDPonzwK1wJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>https://repository.library.georgetown.edu/handle/10822/552965</url>
        <title status="complete" source="scholar.google.com">Learning techniques for identifying vocal regions in music using the wavelet transformation version 1.0</title>
        <pdf>https://repository.library.georgetown.edu/bitstream/handle/10822/552965/henryMichael.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">M Henry</author>
        <year>2012</year>
        <source>repository.library.georgetown.edu</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this research I present a machine learning method for the automatic detection of vocal regions in music. I employ the wavelet transformation to extract wavelet coefficients, from which I build feature sets capable of constructing a model that can distinguish ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PpPPvEB0KowJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10100012935726207806&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5948861</url>
        <title status="complete" source="scholar.google.com">Research and Realization of a SVS Algorithm based on STFT and NDFT</title>
        <author status="partial" source="scholar.google.com">R Chen, Z Luo, Y Zhong, X Luo, Y Gao&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Security (NCIS), 2011 &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, a singing voice splitting (SVS) system is researched and implemented by short-time Fourier transform (STFT) and Nonuniform Discrete Fourier Transform (NDFT). Specifically, there are four processes: TF decomposition, main pitch detection, TF ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:r-Zd3Mb1Y9EJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15088173411070764719&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="23">
        <url>http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202011/papers/PS2-11.pdf</url>
        <title status="complete" source="scholar.google.com">Timbre and Melody Features for the Recognition of Vocal Activity and Instrumental Solos in Polyphonic Music</title>
        <pdf>http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202011/papers/PS2-11.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Mauch, H Fujihara, K Yoshii, M Goto</author>
        <year>2011</year>
        <source>mirlab.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We propose the task of detecting instrumental solos in polyphonic music recordings, and the usage of a set of four audio features for vocal and instrumental activity detection. Three of the features are based on the prior extraction of the predominant ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:6QHZ-0g_n38J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:6QHZ-0g_n38J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9196138546809340393&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="24">
        <url>http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf</url>
        <title status="complete" source="scholar.google.com">DÃ©tection de la voix chantÃ©e dans un morceau de musique</title>
        <pdf>http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf</pdf>
        <author status="complete" source="scholar.google.com">L REGNIER</author>
        <proceeding status="complete" source="scholar.google.com">atiam.ircam.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© De tous les Ã©lÃ©ments constitutifs d&#39;un morceau de musique, la voix est sans conteste celui qui focalise le plus l&#39;attention de l&#39;auditeur. Ceci provient du fait que la voix est porteuse d&#39;un message (le texte) et d&#39;une identitÃ© (celle du chanteur). Pour ces raisons, de ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15229319373475501458&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="25">
        <url>http://www.cqvip.com/qk/95659x/200905/30314191.html</url>
        <title status="complete" source="scholar.google.com">åºäºé«æ¯æ··åæ¨¡åæµè¡é³ä¹ä¸­æ­å±é¨åçæºè½æ£æµ</title>
        <author status="complete" source="scholar.google.com">æä¸½å¨ï¼ å¶èï¼ èµµæ¬£</author>
        <proceeding status="complete" source="scholar.google.com">å°åå¾®åè®¡ç®æºç³»ç»</proceeding>
        <year>2009</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">ææå°æ£æµåºæµè¡é³ä¹ä¸­çæ­å±é¨åå¯¹å¨æµ·éæ°æ®åºä¸­è¿è¡é³ä¹æ£ç´¢, æµè§, å½ç±», ä»¥åæå¾æååæ­å±å®¶è¯å«ç­æè¾å¤§çä»·å¼. æ¬æä½¿ç¨å¨è¯­é³ä¿¡å·å¤çä¸­å¹¿æ³ä½¿ç¨çåºäºæ¢å°é¢ççåè°±ç³»æ°(MFCC) ä½ä¸ºè¯­é³ç¹å¾æ¥åææè¦å¤ççé³ä¹ä¿¡å·, å¹¶éç¨é«æ¯æ··åæ¨¡å( ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:10Pnsd-bS1QJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6074119907503981527&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="26">
        <title status="complete" source="scholar.google.com">åºäº SVM çæµè¡é³ä¹ä¸­äººå£°çè¯å«</title>
        <pdf>http://www.ceaj.org/Jweb_gcyyy/EN/article/downloadArticleFile.do?attachType=PDF&amp;id=18706</pdf>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:wZEV1WsZ3HMJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8348575760165212609&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="16">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1394598</url>
    <title status="complete" source="scholar.google.com">Key determination of acoustic musical signals</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2004_Key_Determination_of_Acoustic_Musical_Signals.pdf</pdf>
    <author status="complete" source="scholar.google.com">A Shenoy, R Mohapatra, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2004. &#8230;</proceeding>
    <year>2004</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... Key Determination of Acoustic Musical Signals Arun Shenoy, Roshni Mohapatra,Ye Wang School of Computing, Naiional University of Singapore, Singapore I f 7543(arunshen, roshnimo, wangye) @comp.nus.edu.sg Abstract ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:xAgQjJbGkXMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>19</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=8327655535882012868&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>26</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=8327655535882012868&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="26">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1027549</url>
        <title status="complete" source="scholar.google.com">Content-based music structure analysis with applications to music semantics understanding</title>
        <pdf>http://pdf.aminer.org/000/502/603/content_based_music_structure_analysis_with_applications_to_music_semantics.pdf</pdf>
        <author status="partial" source="scholar.google.com">NC Maddage, C Xu, MS Kankanhalli&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th &#8230;</proceeding>
        <year>2004</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we present a novel approach for music structure analysis. A new segmentation method, beat space segmentation, is proposed and used for music chord detection and vocal/instrumental boundary detection. The wrongly detected chords in the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Tk9VPKYKLEcJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5128485784761225038&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>102</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5128485784761225038&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1027576</url>
        <title status="complete" source="scholar.google.com">LyricAlly: automatic synchronization of acoustic musical signals and textual lyrics</title>
        <pdf>http://www.comp.nus.edu/~kanmy/dossier/papers/p1568934817-wang.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, MY Kan, TL Nwe, A Shenoy, J Yin</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th &#8230;</proceeding>
        <year>2004</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present a prototype that automatically aligns acoustic musical signals with their corresponding textual lyrics, in a manner similar to manually-aligned karaoke. We tackle this problem using a multimodal approach, where the appropriate pairing of audio and text ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gj3dVP-I65UJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>36</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10802878761400089986&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>61</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10802878761400089986&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1580435</url>
        <title status="complete" source="scholar.google.com">Automatic structure detection for popular music</title>
        <pdf>http://www.comp.nus.edu.sg/~mohan/papers/music_struct_det.pdf</pdf>
        <author status="complete" source="scholar.google.com">NC Maddage</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Our proposed approach detects music structures by looking at beat-space segmentation, chords, singing-voice boundaries, and melody-and content-based similarity regions. Experiments illustrate that the proposed approach is capable of extracting useful ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pRwl89mT3mgJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7556639789070752933&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>36</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7556639789070752933&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://dl.acm.org/citation.cfm?id=1027602</url>
        <title status="complete" source="scholar.google.com">Singing voice detection in popular music</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2004_Singing_Voice_Detection_in_Popular_Music.pdf</pdf>
        <author status="complete" source="scholar.google.com">TL Nwe, A Shenoy, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th annual ACM &#8230;</proceeding>
        <year>2004</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a novel technique for the automatic classification of vocal and non-vocal regions in an acoustic musical signal. Our technique uses a combination of harmonic content attenuation using higher level musical knowledge of key followed by sub-band ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bh6Y-g5TQzUJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>22</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3838002631248715374&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>34</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3838002631248715374&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385971</url>
        <title status="complete" source="scholar.google.com">Music key detection for musical audio</title>
        <author status="complete" source="scholar.google.com">Y Zhu, MS Kankanhalli, S Gao</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Modelling Conference, 2005. &#8230;</proceeding>
        <year>2005</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The key or the scale information of a piece of music provides important clues on its high level musical content, like harmonic and melodic context, which can be useful for music classification, retrieval or further content analysis. Researchers have previously ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bfwho3gpLmcJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7434925632944995437&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>30</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7434925632944995437&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1632042</url>
        <title status="complete" source="scholar.google.com">Precise pitch profile feature extraction from musical audio for key detection</title>
        <author status="complete" source="scholar.google.com">Y Zhu, MS Kankanhalli</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE Transactions on</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The majority of pieces of music, including classical and popular music, are composed using music scales, such as keys. The key or the scale information of a piece provides important clues on its high level musical content, like harmonic and melodic ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8R7dg5_jAhgJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1730195481115303665&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>26</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1730195481115303665&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432643</url>
        <title status="complete" source="scholar.google.com">LyricAlly: Automatic synchronization of textual lyrics to acoustic music signals</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.119.3200&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="partial" source="scholar.google.com">MY Kan, Y Wang, D Iskandar, TL New&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We present LyricAlly, a prototype that automatically aligns acoustic musical signals with their corresponding textual lyrics, in a manner similar to manually-aligned karaoke. We tackle this problem based on a multimodal approach, using an appropriate pairing of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:UKV6kRlYEmEJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6994750038097962320&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>27</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6994750038097962320&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://dl.acm.org/citation.cfm?id=1112857</url>
        <title status="complete" source="scholar.google.com">Key, chord, and rhythm tracking of popular music recordings</title>
        <author status="complete" source="scholar.google.com">A Shenoy, Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">Computer Music Journal</proceeding>
        <year>2005</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">In this article, we propose a framework to analyze a musical audio signal (sampled from a popular music CD) and determine its key, provide usable chord transcriptions, and obtain the hierarchical rhythm structure representation comprising the quarternote, half-note, and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:33p2o7nmEuIJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16290336487138294495&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>22</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16290336487138294495&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4217450</url>
        <title status="complete" source="scholar.google.com">Pitch detection in polyphonic music using instrument tone models</title>
        <pdf>http://ispl.korea.ac.kr/conference/ICASSP2007/pdfs/0200481.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Li, DL Wang</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Speech and Signal Processing, 2007. ICASSP &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a hidden Markov model (HMM) based system to detect the pitch of an instrument in polyphonic music using an instrument tone model. Our system calculates at every time frame the salience of a pitch hypothesis based on the magnitudes of harmonics ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0E4vkjL8sCMJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2571832681359691472&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>21</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2571832681359691472&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255</url>
        <title status="complete" source="scholar.google.com">Singing voice detection for karaoke application</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Shenoy, Y Wu, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Visual &#8230;</proceeding>
        <year>2005</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">abstract We present a framework to detect the regions of singing voice in musical audio signals. This work is oriented towards the development of a robust transcriber of lyrics for karaoke applications. The technique leverages on a combination of low-level audio ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4564924</url>
        <title status="complete" source="scholar.google.com">Enhancing chord classification through neighbourhood histograms</title>
        <pdf>http://wwwiti.cs.uni-magdeburg.de/~stober/publ/cbmi2008.pdf</pdf>
        <author status="partial" source="scholar.google.com">J Reinhard, S Stober&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Content-Based Multimedia &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The chord progression of a song is an important high-level feature which enables indexing as well as deeper analysis of musical recordings. Different approaches to chord recognition have been suggested in the past. Though their performance increased, still ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:wAXIQHzDupYJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10861208389787583936&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10861208389787583936&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5742981</url>
        <title status="complete" source="scholar.google.com">Chord recognition by fitting rescaled chroma vectors to chord templates</title>
        <pdf>http://www.laurentoudre.fr/publis/OGF-IEEE-11.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Oudre, Y Grenier, C FÃ©votte</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we propose a simple and fast method for chord recognition in music signals. We extract a chromagram from the signal which transcribes the harmonic content of the piece over time. We introduce a set of chord templates taking into account one or more ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:h8JPXVxkH-wJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17014428265094300295&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17014428265094300295&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://www.mitpressjournals.org/doi/abs/10.1162/comj.2008.32.1.71</url>
        <title status="complete" source="scholar.google.com">Complexity-scalable beat detection with MP3 audio bitstreams</title>
        <author status="complete" source="scholar.google.com">J Zhu, Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">Computer Music Journal</proceeding>
        <year>2008</year>
        <source>MIT Press</source>
        <snippet status="partial" source="scholar.google.com">72 Computer Music Journal greatly limits their application, because it is not easy to obtain complete MIDI representations of real-world acoustic signals. Such models suffer from the scaling-up problem (Kitano 1993). Recent systems work with PCM audio bitstreams. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Zo7qHjhlQtwJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15871359328518311526&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15871359328518311526&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://www.springerlink.com/index/r8w711w22m350441.pdf</url>
        <title status="complete" source="scholar.google.com">Extracting the key from music</title>
        <author status="complete" source="scholar.google.com">S Pauws</author>
        <proceeding status="partial" source="scholar.google.com">Intelligent Algorithms in Ambient and Biomedical &#8230;</proceeding>
        <year>2006</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Extracting a sense of key from music audio is indispensable for various unequalled end-user applications dealing with music playback. This chapter presents an audio key extraction algorithm that is based on models of human auditory perception and music ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:cCbtO7Iq2k0J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5609843230841316976&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>https://qmro.qmul.ac.uk/xmlui/handle/123456789/534</url>
        <title status="complete" source="scholar.google.com">Towards automatic extraction of harmony information from music signals</title>
        <pdf>https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/534/HARTETowardsAutomatic2010.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">C Harte</author>
        <year>2010</year>
        <source>qmro.qmul.ac.uk</source>
        <snippet status="partial" source="scholar.google.com">In this thesis we address the subject of automatic extraction of harmony information from audio recordings. We focus on chord symbol recognition and methods for evaluating algorithms designed to perform that task. We present a novel six-dimensional model for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8ffUuo7AeeoJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16895747196309534705&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16895747196309534705&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://hal.inria.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf</url>
        <title status="complete" source="scholar.google.com">Joint estimation of musical content information from an audio signal</title>
        <pdf>http://hal.inria.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Papadopoulos</author>
        <year>2010</year>
        <source>hal.inria.fr</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© Dans cette these, nous nous intÃ©ressons au probleme de l&#39;extraction automatique d&#39;informations de contenu d&#39;un signal audio de musique. La plupart des travaux existants abordent ce probleme en considÃ©rant les attributs musicaux de maniere indÃ©pendante les ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Gsw65B4lZEwJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Gsw65B4lZEwJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5504565459161893914&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <title status="complete" source="scholar.google.com">Scalar theory and knowledge discovery in the recording industry</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:cOxlTvNjbjQJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3778067034053930096&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3778067034053930096&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://www.tandfonline.com/doi/abs/10.1080/09298215.2011.618543</url>
        <title status="complete" source="scholar.google.com">Music Theoretic and Perception-based Features for Audio Key Determination</title>
        <author status="complete" source="scholar.google.com">B Schuller, B Gollan</author>
        <proceeding status="complete" source="scholar.google.com">Journal of New Music Research</proceeding>
        <year>2012</year>
        <source>Taylor &amp; Francis</source>
        <snippet status="partial" source="scholar.google.com">Abstract The musical key of a piece is the fundamental knowledge for many Music Information Retrieval tasks as automatic transcription, chord detection or automatic play list generation. To this end, novel features are proposed and evaluated on the basis of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:k9rwCjMcUIoJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9966496980923374227&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6074928</url>
        <title status="complete" source="scholar.google.com">Local Key Estimation from an Audio Signal Relying on Harmonic and Metrical Structures</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/65/57/81/PDF/PapadopoulosPeeters_LcalKey_IEEE_2011.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Papadopoulos, G Peeters</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we present a method for estimating the progression of musical key from an audio signal. We address the problem of local key finding by investigating the possible combination and extension of different previously proposed approaches for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:M-uHMGyWElwJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6634530592485010227&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6634530592485010227&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf</url>
        <title status="complete" source="scholar.google.com">A repetition-based framework for lyric alignment in popular songs</title>
        <pdf>http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf</pdf>
        <author status="complete" source="scholar.google.com">LM Thang, KANM Yen</author>
        <proceeding status="partial" source="scholar.google.com">National University of Singapore &#8230;</proceeding>
        <year>2007</year>
        <source>w.comp.nus.edu</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We examine the problem of automatically aligning acoustic musical audio and textual lyric in popular songs. Existing works have tackled the problem using computationally-expensive audio processing techniques, resulting in solutions unsuitable ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>27</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17581284984694044119&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://www.springerlink.com/index/FT671731J0403662.pdf</url>
        <title status="complete" source="scholar.google.com">A Survey of Music Structure Analysis Techniques for Music Applications</title>
        <author status="complete" source="scholar.google.com">N Maddage, H Li, M Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">Recent Advances in Multimedia Signal &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Music carries multilayer information which forms different structures. The information embedded in the music can be categorized into time information, harmony/melody, music regions, music similarities, song structures and music semantics. In this chapter, we first ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3vo8SvZRZuUJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16529989600559299294&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <title status="complete" source="scholar.google.com">Singing Voice Detection in Western Popular Music</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dx-cdoqIEO4J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6012032</url>
        <title status="complete" source="scholar.google.com">Solo to a capella conversion-Synthesizing vocal harmony from lead vocals</title>
        <author status="partial" source="scholar.google.com">PY Chan, M Dong, SW Lee&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo ( &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents our work in the automatic synthesis of vocal harmony. Existing innovations either allow for dissonances (ie non-harmonious or clashing intervals) at various locations or require some musical ability of the user. We have developed a method that is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:fU1lqjRY0l8J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6904678161932701053&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="23">
        <url>http://laurentoudre.fr/publis/LO-PHDfr-10.pdf</url>
        <title status="complete" source="scholar.google.com">Laurent OUDRE</title>
        <pdf>http://laurentoudre.fr/publis/LO-PHDfr-10.pdf</pdf>
        <author status="complete" source="scholar.google.com">DE Rapporteurs, S Marchand</author>
        <proceeding status="complete" source="scholar.google.com">laurentoudre.fr</proceeding>
        <snippet status="complete" source="scholar.google.com">Ce premier chapitre vise Ã  introduire les principales notions musicales nÃ©cessaires Ã  la bonne comprÃ©hension du prÃ©sent document, mais aussi Ã  prÃ©ciser le contexte et les principales applications de ce travail de thÃ¨se.</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PPdVPuMCoxEJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:PPdVPuMCoxEJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1270862694875264828&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="24">
        <url>http://tel.archives-ouvertes.fr/pastel-00542840/</url>
        <title status="complete" source="scholar.google.com">Reconnaissance d&#39;accords Ã  partir de signaux audio par l&#39;utilisation de gabarits thÃ©oriques</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/54/28/40/PDF/These_fr_en.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Oudre</author>
        <year>2010</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© Cette thÃ¨se s&#39; inscrit dans le cadre du traitement du signal musical, en se focalisant plus particuliÃ¨rement sur la transcription automatique de signaux audio en accords. En effet, depuis une dizaine d&#39;annÃ©es, de nombreux travaux visent Ã  reprÃ©senter les signaux ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:afhmW5HEmVcJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6312292481319237737&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="25">
        <url>http://tel.archives-ouvertes.fr/tel-00548952/</url>
        <title status="complete" source="scholar.google.com">Estimation conjointe d&#39;information de contenu musical d&#39;un signal audio</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Papadopoulos</author>
        <year>2010</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© Dans cette these, nous nous intÃ©ressons au probleme de l&#39;extraction automatique d&#39;informations de contenu d&#39;un signal audio de musique. La plupart des travaux existants abordent ce probleme en considÃ©rant les attributs musicaux de maniere indÃ©pendante les ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:B-SObITZT2wJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7804695842036573191&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="17">
    <url>http://dl.acm.org/citation.cfm?id=1459477</url>
    <title status="complete" source="scholar.google.com">SenseCoding: accelerometer-assisted motion estimation for efficient video encoding</title>
    <pdf>http://edge.rit.edu/content/P11010/public/MultimediaSenseCoding.pdf</pdf>
    <author status="complete" source="scholar.google.com">G Hong, A Rahmati, Y Wang, L Zhong</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 16th ACM &#8230;</proceeding>
    <year>2008</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... 1School of Computing National University of Singapore, Singapore, 117590 {honggm,wangye}@comp.nus.edu.sg ... The work by the NUS team was supported by Singaporean MOEgrant with the WBS number of R-252-000-236-112. We thank Prof. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:0BTFNZ0Iv4cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>17</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=9781546386977002704&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>7</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=9781546386977002704&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="7">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5674049</url>
        <title status="complete" source="scholar.google.com">Richardson-lucy deblurring for scenes under a projective motion path</title>
        <author status="complete" source="scholar.google.com">YW Tai, P Tan, MS Brown</author>
        <proceeding status="partial" source="scholar.google.com">Pattern Analysis and Machine &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper addresses how to model and correct image blur that arises when a camera undergoes ego motion while observing a distant scene. In particular, we discuss how the blurred image can be modeled as an integration of the clear scene under a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ytAxdg8ExV0J:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6756811280393294026&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>40</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6756811280393294026&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1631272.1631325</url>
        <title status="complete" source="scholar.google.com">SaVE: sensor-assisted motion estimation for efficient h. 264/AVC video encoding</title>
        <pdf>http://www.ruf.rice.edu/~mobile/publications/chen09mm.pdf</pdf>
        <author status="partial" source="scholar.google.com">X Chen, Z Zhao, A Rahmati, Y Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 17th &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Motion estimation is a key component of modern video encoding and is very compute-intensive. We present a novel Sensor-assisted Video Encoding (SaVE) method to reduce the computational complexity of motion estimation in H. 264/AVC encoders, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:kqwH6-agbfkJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17973198601551588498&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17973198601551588498&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <title status="complete" source="scholar.google.com">Sensor-assisted Camera Motion Analysis and Motion Estimation Improvement for H. 264/AVC Video Encoding</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PTYtr4mKxf4J:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=18358231779999823421&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5711656</url>
        <title status="complete" source="scholar.google.com">Sensor-Assisted Video Encoding for Mobile Devices in Real-World Environments</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2011_Sensor-Assisted_Video_Encoding_for_Mobile_Devicesin_Real-World_Environments.pdf</pdf>
        <author status="partial" source="scholar.google.com">X Chen, Z Zhao, A Rahmati, Y Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Circuits and Systems &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we present a comprehensive study on sensor-assisted video encoding (SaVE) schemes for video capturing on mobile devices in real-world environments. Our purpose is to reduce the computational complexity of video encoding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gDmiExV2xjsJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4307259926522247552&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4307259926522247552&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://scien.stanford.edu/pages/labsite/2010/ee398a/projects/reports/andy.pdf</url>
        <title status="complete" source="scholar.google.com">Using Sensors for Efficient Video Coding in Hand-held Devices</title>
        <pdf>http://scien.stanford.edu/pages/labsite/2010/ee398a/projects/reports/andy.pdf</pdf>
        <author status="complete" source="scholar.google.com">AL Lin</author>
        <proceeding status="complete" source="scholar.google.com">scien.stanford.edu</proceeding>
        <snippet status="partial" source="scholar.google.com">One key challenge in video coding consists of minimizing computation, especially in hand-held devices. The sensors which are already present in hand-held devices can help simplify motion vector search and video coding. Keywords-motion estimation; mobile video coding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0i3hkw-ULfcJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:0i3hkw-ULfcJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17811054895946608082&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.404&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Richardson-Lucy Deblurring for Scenes Under A Projective Motion Path</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.404&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">YWTPT Michael, S Brown</author>
        <proceeding status="complete" source="scholar.google.com">Citeseer</proceeding>
        <snippet status="partial" source="scholar.google.com">AbstractâThis paper addresses how to model and correct image blur that arises when a camera undergoes ego motion while observing a distant scene. In particular, we discuss how the blurred image can be modeled as an integration of the clear scene under a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:MZO2zVCuwBsJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:MZO2zVCuwBsJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1999789896624411441&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="18">
    <url>http://144.206.159.178/ft/145/589081/12066673.pdf</url>
    <title status="complete" source="scholar.google.com">Synthesis and characterization of in situ grown carbon nanofiber/nanotube reinforced carbon/carbon composites</title>
    <pdf>http://144.206.159.178/ft/145/589081/12066673.pdf</pdf>
    <author status="complete" source="scholar.google.com">Q Gong, Z Li, X Zhou, J Wu, Y Wang, J Liang</author>
    <proceeding status="complete" source="scholar.google.com">Carbon</proceeding>
    <year>2005</year>
    <source>144.206.159.178</source>
    <snippet status="partial" source="scholar.google.com">... Acknowledgements This work was supported by ARF of NUS. We thank Dr. Li Xu for SAXSmeasurements. References [1] Stein A. Advances in microporous and mesoporous solidsâhighlights of recent progress. Adv Mater 2003;15:763â75. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:liY4gOw7lO0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:liY4gOw7lO0J:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <numVersions>4</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=17119373970491582102&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>24</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=17119373970491582102&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="24">
        <result id="0">
        <url>http://www.sciencedirect.com/science/article/pii/S0008622307001509</url>
        <title status="complete" source="scholar.google.com">Microstructures and mechanical properties of carbon/carbon composites reinforced with carbon nanofibers/nanotubes produced in situ</title>
        <pdf>http://144.206.159.178/ft/145/589069/12065836.pdf</pdf>
        <author status="complete" source="scholar.google.com">X Li, K Li, H Li, J Wei, C Wang</author>
        <proceeding status="complete" source="scholar.google.com">Carbon</proceeding>
        <year>2007</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">Using ferrocene as catalyst and toluene as the liquid precursor, carbon/carbon (C/C) composites were prepared by chemical liquidâvapor infiltration at 850â1100Â° C. The microstructures and properties of C/C composites obtained with different ferrocene ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PH5Ctx9mxD0J:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4450794618153500220&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>19</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4450794618153500220&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://www.sciencedirect.com/science/article/pii/S1359835X08001978</url>
        <title status="complete" source="scholar.google.com">Study of the mechanical properties of carbon nanofiber reinforced carbon/carbon composites</title>
        <pdf>http://144.206.159.178/FT/200/600849/12498626.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Li, R Luo</author>
        <proceeding status="partial" source="scholar.google.com">Composites Part A: Applied Science and &#8230;</proceeding>
        <year>2008</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">The effects of carbon nanofibers (CNFs) on the flexural properties and interlaminar shear strength of CNF-reinforced C/C composites are discussed. The results show that the flexural strength, modulus and interlaminar shear strength of the composite containing 5wt.% ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:aNusJWMMM-QJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16443500284433193832&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>10</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16443500284433193832&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.sciencedirect.com/science/article/pii/S0013468611001204</url>
        <title status="complete" source="scholar.google.com">Enhancement of electrosorption capacity of activated carbon fibers by grafting with carbon nanofibers</title>
        <author status="complete" source="scholar.google.com">Y Zhan, C Nie, H Li, L Pan, Z Sun</author>
        <proceeding status="complete" source="scholar.google.com">Electrochimica Acta</proceeding>
        <year>2011</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">The composite films of activated carbon fibers (ACFs) and carbon nanofibers (CNFs) are prepared via chemical vapor deposition of CNFs onto ACFs in different times from 0.5 to 2h and their electrosorption behaviors in NaCl solution are investigated. The morphology, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pWFDRsUBt4wJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10139575032846967205&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10139575032846967205&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.tnmsc.cn/upfile/soft/200858/200858162245512.pdf</url>
        <title status="complete" source="scholar.google.com">åä½çé¿ç¢³çº³ç±³ç®¡å¯¹ç­/ç­å¤åææå¯¼ç­æ§è½çå½±å</title>
        <pdf>http://www.tnmsc.cn/upfile/soft/200858/200858162245512.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¨å»ºä¼ï¼ å»å¯ä¹ï¼ çå é</author>
        <proceeding status="complete" source="scholar.google.com">The Chinese Journal of Nonferrous Metals</proceeding>
        <year>2008</year>
        <source>tnmsc.cn</source>
        <snippet status="partial" source="scholar.google.com">æè¦: ä»¥ç­çº¤ç»´è¡¨é¢åä½çé¿æç¢³çº³ç±³ç®¡(Carbon nanotubes, CNTs) çéåºæ¯¡ä½ä½ä¸ºåé©±ä½å¶å¤åºçé¿æCNTs çç­/ç­å¤åææ, å¹¶ä¸å¨åæ ·å·¥èºæ¡ä»¶ä¸éè¿è´å¯åæç»ç­å¤çå¾å°ççº¯ç­/ç­å¤åææè¿è¡å¯¹æ¯. ç»æè¡¨æ, å¨å¯åº¦å ä¹ç¸åçæåµä¸, çé¿æCNTs çç­/ç­å¤åææç ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:-BrwJ5n98TsJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:-BrwJ5n98TsJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4319512351866886904&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4319512351866886904&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://onlinelibrary.wiley.com/doi/10.1002/cvde.200806706/abstract</url>
        <title status="complete" source="scholar.google.com">Fabrication and Characteristics of Carbon NanofiberâReinforced Carbon/Carbon Composites by Fast Catalytic Infiltration Processes</title>
        <author status="complete" source="scholar.google.com">JC Zhang, RY Luo, XW Wu, Q Li</author>
        <proceeding status="complete" source="scholar.google.com">Chemical Vapor Deposition</proceeding>
        <year>2009</year>
        <source>Wiley Online Library</source>
        <snippet status="partial" source="scholar.google.com">Abstract The simultaneous in-situ growth of carbon nanofibers (CNFs) and densification of a CNFs/CF hybrid multiscale felt are accomplished in a single step by thermal gradient chemical vapor infiltration using Fe as the catalyst and vaporized kerosene under ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:n0IkXQu7wucJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16700116025724912287&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://file.lw23.com/9/9b/9b8/9b80e150-cda0-481b-aa54-5c664509af2d.pdf</url>
        <title status="complete" source="scholar.google.com">çº³ç±³ç¢³ç®¡å¢å¼ºç­/ç­å¤åææçç ç©¶è¿å±</title>
        <pdf>http://file.lw23.com/9/9b/9b8/9b80e150-cda0-481b-aa54-5c664509af2d.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¨æ¯ä¸­ï¼ æéèï¼ æå½©éï¼ å®åä¸¾</author>
        <proceeding status="complete" source="scholar.google.com">ææå¯¼æ¥</proceeding>
        <year>2006</year>
        <source>file.lw23.com</source>
        <snippet status="partial" source="scholar.google.com">æè¦çº³ç±³ç¢³ç®¡(CNTs) å·æç¬ç¹çç»æ, ä¼å¼çåå­¦æ§è½, ç­ç¨³å®æ§ä¸ä¼ å¯¼æ§è½, æ¯ç­/ç­(C/C) å¤åææçæ³çå¢å¼ºä½. ç»¼è¿°äºçº³ç±³ç¢³ç®¡å¢å¼ºç­/ç­(CNTs/C/C) å¤åææçå¶å¤æ¹æ³, è®¨è®ºäºè¯¥å¤åææçå¾®è§ç»æ, æ©æ¦å­¦æ§è½åä¼ å¯¼æ§è½, å¹¶å±æäºCNTs/C/C å¤åææçæ½å¨ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:r99J26wHqm4J:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:r99J26wHqm4J:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7974194529207705519&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7974194529207705519&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ecst.ecsdl.org/content/25/8/757.short</url>
        <title status="complete" source="scholar.google.com">Long and Aligned Multi-Walled Carbon Nanotubes Grown on Carbon and Metallic Substrates by Injection-CVD Process</title>
        <author status="partial" source="scholar.google.com">M Delmas, M Pinault, D Porterat, C Reynaud&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">ECS &#8230;</proceeding>
        <year>2009</year>
        <source>ecst.ecsdl.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Dense, aligned and clean multi-walled carbon nanotubes (MWCNT) can be obtained by aerosol-assisted catalytic CVD process. A successful extension of this method has been performed in order to obtain MWCNT growth even on carbon fibres and metallic ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:JFLaBcNr3QAJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=62324455214043684&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=62324455214043684&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://dl.acm.org/citation.cfm?id=1957967</url>
        <title status="complete" source="scholar.google.com">Progress in studies on carbon and silicon carbide nanocomposite materials</title>
        <pdf>http://downloads.hindawi.com/journals/jnm/2010/896389.pdf</pdf>
        <author status="complete" source="scholar.google.com">P Xiao, J Chen, X Xu</author>
        <proceeding status="complete" source="scholar.google.com">Journal of Nanomaterials</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Silicon carbide nanofiber and carbon nanotubes are introduced. The structure and application of nanotubers (nanofibers) in carbon/carbon composites are emphatically presented. Due to the unique structure of nanotubers (nanofibers), they can modify the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:eaOkDQn9BBcJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1658728778104611705&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1658728778104611705&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://iopscience.iop.org/0957-4484/23/10/105604</url>
        <title status="complete" source="scholar.google.com">Growth of long and aligned multi-walled carbon nanotubes on carbon and metal substrates</title>
        <pdf>http://atmsp.whut.edu.cn/resource/pdf/14133.pdf</pdf>
        <author status="partial" source="scholar.google.com">M Delmas, M Pinault, S Patel, D Porterat&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230;</proceeding>
        <year>2012</year>
        <source>iopscience.iop.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Well aligned, long and dense multi-walled carbon nanotubes (CNT) can be grown on both carbon fibres and any metal substrates compatible with the CNT synthesis temperature. The injection-CVD process developed involves two stages, including fibre ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:yXw6EDgA224J:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7987978604862143689&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7987978604862143689&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://www.sciencedirect.com/science/article/pii/S0008622312000942</url>
        <title status="complete" source="scholar.google.com">Transmission electron microscopy study of the microstructure of carbon/carbon composites reinforced with in situ grown carbon nanofibers</title>
        <author status="complete" source="scholar.google.com">Y Liu, LL He, XF Lu, P Xiao</author>
        <proceeding status="complete" source="scholar.google.com">Carbon</proceeding>
        <year>2012</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">Introduction of carbon nanofibers (CNFs) into carbon/carbon (C/C) composites is an effective method to improve the mechanical properties of C/C composites. In situ grown CNFs reinforced C/C composites as well as conventional C/C composites without CNFs were ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7J2pscEnM64J:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12552420299274558956&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12552420299274558956&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6179252</url>
        <title status="complete" source="scholar.google.com">Analyses of reinforcing effects of in situ grown CNTs on carbon fibre fabric/epoxy composites at micro-and macroscale</title>
        <author status="partial" source="scholar.google.com">C Ren, Q Gong, L Guo, X Zhao&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Micro &amp; Nano Letters, &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Carbon nanotubes (CNTs) have been in situ grown on the surface of carbon fibre fabric (CFF) by electrochemical oxidation, urea hydrolysis and chemical vapour deposition process. The reinforcing efficiency of the in situ grown CNTs (I-CNTs) was analysed at ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:DPYKCrkZLpIJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10533384860994303500&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://versita.metapress.com/index/4370P157827312K6.pdf</url>
        <title status="complete" source="scholar.google.com">Synthesis of carbon nanotubes via chemical vapor deposition by using rareearth metals as catalysts</title>
        <author status="partial" source="scholar.google.com">A JÄdrzejewska, K Wnuk, RJ KaleÅczuk&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Polish Journal of &#8230;</proceeding>
        <year>2010</year>
        <source>Versita</source>
        <snippet status="partial" source="scholar.google.com">This work presents the results of the synthesis of carbon nanotubes using the CVD method. Fe: MgO catalyst was used, also in combination with rare earth elements (gadolinium (Gd), dysprosium (Dy)), which when used alone, are not efficient as catalysts in nanotube ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ZEFoDK-fRbgJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13278194650548027748&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://www.sciencedirect.com/science/article/pii/S0008622312007336</url>
        <title status="complete" source="scholar.google.com">Transmission electron microscopy study of the microstructure of unidirectional C/C composites fabricated by catalytic chemical vapor infiltration</title>
        <author status="complete" source="scholar.google.com">Y Liu, L He, X Lu, P Xiao</author>
        <proceeding status="complete" source="scholar.google.com">Carbon</proceeding>
        <year>2012</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">Abstract Unidirectional carbon/carbon (C/C) composites were fabricated by catalytic chemical vapor infiltration, using electroless Ni-P as catalyst. Transmission electron microscopy (TEM) investigations indicate that the catalyst particles (100-800 nm) in the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:B3jwHJMn0NIJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <title status="complete" source="scholar.google.com">Simulation of Thermal Transport in a Nanocomposite Blow Mold</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:UwCGIvO3KWwJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7793962885025497171&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6036030</url>
        <title status="complete" source="scholar.google.com">Non-destructive and controllable catalyst deposition of in situ growing carbon nanotubes onto the carbon fibre fabric</title>
        <author status="complete" source="scholar.google.com">C Ren, Q Gong, J Liang</author>
        <proceeding status="complete" source="scholar.google.com">Micro &amp; Nano Letters, IET</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract To in situ grow carbon nanotubes (CNTs) on the surface of carbon fibre fabric (CFF) with suitable diameter and uniform distribution, controllable catalyst deposition is the key process. A controllable and non-destructive pre-treatment, that is, electrochemical ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:24ScRfngUZIJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10543455563870405851&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://www.sciencedirect.com/science/article/pii/S0008622312002710</url>
        <title status="complete" source="scholar.google.com">Grafting straight carbon nanotubes radially onto carbon fibers and their effect on the mechanical properties of carbon/carbon composites</title>
        <author status="complete" source="scholar.google.com">Q Song, K Li, H Li, H Li, C Ren</author>
        <proceeding status="complete" source="scholar.google.com">Carbon</proceeding>
        <year>2012</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">Straight carbon nanotubes (CNTs) were grafted radially onto carbon fibers to produce hybrid materials that were used to reinforce carbon/carbon (C/C) composites. Mechanical property tests indicated that these C/C composites have improvements in out-of-plane and in-plane ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:GauAfZKvi7MJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12937627398257552153&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12937627398257552153&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=24</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://pubs.acs.org/doi/abs/10.1021/ie100295d</url>
        <title status="complete" source="scholar.google.com">Effect of Post Production Processing on Dispersion of Carbon Nanofibers in Water</title>
        <author status="complete" source="scholar.google.com">J Zhao</author>
        <proceeding status="complete" source="scholar.google.com">Industrial &amp; Engineering Chemistry Research</proceeding>
        <year>2011</year>
        <source>ACS Publications</source>
        <snippet status="partial" source="scholar.google.com">Dispersion of carbon nanofibers subjected to post production processing, heat treatment (HT) and pyrolytical stripping (PS), is investigated in aqueous suspension using small-angle light scattering. Both samples exhibit a hierarchical morphology consisting of small-scale ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:yx2gyJZPvtoJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15762123254872939979&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://onlinelibrary.wiley.com/doi/10.1002/9781118097298.weoc067/full</url>
        <title status="complete" source="scholar.google.com">Design with Composites</title>
        <author status="complete" source="scholar.google.com">P Kere</author>
        <proceeding status="complete" source="scholar.google.com">Wiley Encyclopedia of Composites</proceeding>
        <year>2012</year>
        <source>Wiley Online Library</source>
        <snippet status="partial" source="scholar.google.com">The use of fiber-reinforced composite materials in the manufacture of lightweight structures has increased steadily since the early days of composite materials. Composite materials are being employed in many different applications in aerospace, automotive, off-shore, and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8erw7bUjzVIJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5966464345623816945&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://www.intechopen.com/source/pdfs/8648/InTech-Morphology_and_dispersion_of_pristine_and_modified_carbon_nanofibers_in_water.pdf</url>
        <title status="complete" source="scholar.google.com">Morphology and Dispersion of Pristine and Modified Carbon Nanofibers in Water</title>
        <pdf>http://www.intechopen.com/source/pdfs/8648/InTech-Morphology_and_dispersion_of_pristine_and_modified_carbon_nanofibers_in_water.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Zhao</author>
        <proceeding status="complete" source="scholar.google.com">intechopen.com</proceeding>
        <snippet status="partial" source="scholar.google.com">Carbon nanofibers have been of great interest due to their extraordinary mechanical and electronic properties. Carbon nanofibers (CNF) are different from carbon nanotubes in that they have many more walls of crystalline carbon and usually have more structural defects ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0R1C8LWPD1kJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:0R1C8LWPD1kJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6417506005608963537&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://d.wanfangdata.com.cn/periodical_gsyxb201201024.aspx</url>
        <title status="complete" source="scholar.google.com">çº³ç±³æ¶å°æ¶ç³-ç¢³å¤åç²ä½çæºæ¢°åéåæ³å¶å¤</title>
        <author status="complete" source="scholar.google.com">åªæå¨¥ï¼ æ¯ææï¼ é©åµå¼ºï¼ ææ¥</author>
        <proceeding status="complete" source="scholar.google.com">ç¡é¸çå­¦æ¥</proceeding>
        <year>2012</year>
        <source>ä¸æ¹æ°æ®èµæºç³»ç»</source>
        <snippet status="partial" source="scholar.google.com">æè¦: ä»¥ç¢±å¼ç¢³é¸éåéå±éç²ä¸ºåæ, Ni (OH) 2 ä¸ºæ·»å å, éç¨X å°çº¿è¡å°åå·®ç¤ºæ«æéç­åæ, ä»¥åææ¼åè°±, é«åè¾¨éå°çµéç ç©¶äºçº³ç±³æ¶MgAl2O4-C å¤åç²ä½çæºæ¢°åéåæ³å¶å¤. ç»æè¡¨æ: éç¨æºæ¢°åéåçæ¹æ³, ç»è¿15 h çç ç£¨åå¯ç´æ¥å¶å¤é«åº¦åæ£ççº³ç±³æ¶å°æ¶ç³-ç¢³ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:JRVGFWF4pNoJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15754849754812912933&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://www.cqvip.com/qk/90370x/201024/36076369.html</url>
        <title status="complete" source="scholar.google.com">ææºåå¯¼ä½çèªææ³¨å¥åèªæè¾è¿</title>
        <author status="complete" source="scholar.google.com">ç±³ä»ªç³ï¼ èµµå°éï¼ çå°</author>
        <proceeding status="complete" source="scholar.google.com">ææå¯¼æ¥</proceeding>
        <year>2010</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">å©ç¨èªææ¼ç§»-æ©æ£æ¹ç¨èªæ´½å°å¾å°äºéç£/ææºåå¯¼ä½èªææ³¨å¥ç»æä¸­æåå­çµå¯¼çèªæç¸å³æ§å¯¹èªææ³¨å¥æççå½±å. è®¡ç®ç»æè¡¨æ, èªææ³¨å¥æçææ¾ä¾èµäºæåå­çµå¯¼çèªæç¸å³æ§. å¨èªææ³¨å¥æçå¢å 30% çæåµä¸, æåå­çµå¯¼çèªæç¸å³æ§T= 5K æ¶å°å¢å¤§4 ä¸ªæ°éçº§å·¦å³ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:IUZkI6PRPckJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14500976873806513697&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <title status="complete" source="scholar.google.com">Nano-C/C composite materials</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:LDBgM6BU3bMJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://www.cqvip.com/qk/90370x/201024/36076372.html</url>
        <title status="complete" source="scholar.google.com">å¬åè£è§£å¤©ç¶æ°äºç¢³æ¯¡åä½ææ¬åæç¢³çº³ç±³ç®¡</title>
        <author status="complete" source="scholar.google.com">å®å¼ºï¼ æåæºï¼ è®¸å ä½ï¼ å¼ºçª</author>
        <proceeding status="complete" source="scholar.google.com">ææå¯¼æ¥</proceeding>
        <year>2010</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">åºäºå¶å¤ç¢³/ç¢³(C/C) å¤åææçç­æ¸©åå­¦æ°ç¸æ¸é(ICVI) ææ¯, å¨1010-1100â ç¨Fe å¬åè£è§£å·¥ä¸å¤©ç¶æ°å¯å¨ç¢³æ¯¡ååä½åæåºç¢³çº³ç±³ç®¡(CNTs). æ«æçµé(SEM) è§å¯ç»æè¡¨æ, 1060â åæçCNTs å·æè¾å¥½çè¦çå½¢è²åååç®¡å¾(110~ 120nm) ä¸çº¯ååº¦é«. é«åè¾¨ç ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gfOZQDtsceIJ:scholar.google.com/&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16316941926682719105&amp;hl=en&amp;num=24&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="23">
        <url>http://d.wanfangdata.com.cn/periodical_nhcl201205002.aspx</url>
        <title status="complete" source="scholar.google.com">ä¸åç¢³é¸çæºæ¢°åéåæ³å¶å¤å°æ¶ç³-ç¢³å¤åç²ä½</title>
        <author status="complete" source="scholar.google.com">åªæå¨¥ï¼ æ¯ææï¼ ææ¯åï¼ ææ¥</author>
        <proceeding status="complete" source="scholar.google.com">èç«ææ</proceeding>
        <year>2012</year>
        <source>ä¸æ¹æ°æ®èµæºç³»ç»</source>
        <snippet status="partial" source="scholar.google.com">æè¦: éç¨ç²åº¦â¤ 74 Î¼m, Ï (MgCO3)= 98.84% çå·¥ä¸çº¯è±éç¿, ç²åº¦&lt; 74 Î¼m, Ï (MgO) ä¸º40.0%~ 44.5% çåæçº¯ç¢±å¼ç¢³é¸éMg (OH) 2Â· 4MgCO3Â· 5H2O, ä»¥åç²åº¦â¤ 74 Î¼m, Ï (A1)â¥ 99.9% çéå±éç²ä¸ºåæ, æç§n (MgCO3): n (Al)= 3: 4 ån [Mg (OH) 2Â· 4MgCO3Â· ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="19">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1334225</url>
    <title status="complete" source="scholar.google.com">Singer identification based on vocal and instrumental models</title>
    <pdf>http://nguyendangbinh.org/Proceedings/ICPR/2004/DATA/V22_3_18.PDF</pdf>
    <author status="complete" source="scholar.google.com">NC Maddage, C Xu, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Pattern Recognition, 2004. ICPR &#8230;</proceeding>
    <year>2004</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... 1 Institute for Infocomm Research, 21 Heng Mui Keng Terrace, Singapore 119613{maddage, xucs}@i2r.a-star.edu.sg 2 School of Computing, National University ofSingapore, Singapore 117543 wangye@comp.nus.edu.sg Abstract ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:Ol18X4LG9LkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>12</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=13399553054584102202&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>22</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=13399553054584102202&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="22">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4067048</url>
        <title status="complete" source="scholar.google.com">Exploring vibrato-motivated acoustic features for singer identification</title>
        <pdf>http://www1.i2r.a-star.edu.sg/~hli/papers/101109TASL2006876756.pdf</pdf>
        <author status="complete" source="scholar.google.com">TL Nwe, H Li</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language Processing, IEEE &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Vibrato is a slightly tremulous effect imparted to vocal or instrumental tone for added warmth and expressiveness through slight variation in pitch. It corresponds to a periodic fluctuation of the fundamental frequency. It is common for a singer to develop a vibrato ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7yW25Nf-A5wJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11242109298055456239&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>32</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11242109298055456239&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255</url>
        <title status="complete" source="scholar.google.com">Singing voice detection for karaoke application</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Shenoy, Y Wu, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Visual &#8230;</proceeding>
        <year>2005</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">abstract We present a framework to detect the regions of singing voice in musical audio signals. This work is oriented towards the development of a robust transcriber of lyrics for karaoke applications. The technique leverages on a combination of low-level audio ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4518087</url>
        <title status="complete" source="scholar.google.com">On fusion of timbre-motivated features for singing voice detection and singer identification</title>
        <author status="complete" source="scholar.google.com">TL Nwe, H Li</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and Signal Processing, 2008. &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Timbre is the quality of sound which allows the ear to distinguish between musical sounds. In this paper, we study timbre effects in identification of singing voice segments in popular songs. Firstly, we identify between singing voice and instrumental segments in a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_MPT1b4jaOsJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16962847299029156860&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16962847299029156860&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://cjc.ict.ac.cn/quanwenjiansuo/2007-05/zyb.zip</url>
        <title status="complete" source="scholar.google.com">åºäºåå®¹çé³é¢ä¸é³ä¹åæç»¼è¿° [J]</title>
        <pdf>http://cjc.ict.ac.cn/quanwenjiansuo/2007-05/zyb.zip</pdf>
        <author status="complete" source="scholar.google.com">å¼ ä¸å½¬ï¼ å¨æ°ï¼ è¾¹èç¥ºï¼ é­å</author>
        <proceeding status="complete" source="scholar.google.com">è®¡ç®æºå­¦æ¥</proceeding>
        <year>2007</year>
        <source>cjc.ict.ac.cn</source>
        <snippet status="partial" source="scholar.google.com">æè¦æºå¨å¬è§åæ¬ä¸å¤§ç ç©¶é¢å: è¯­é³ä¿¡å·å¤çä¸è¯å«, ä¸è¬é³é¢ä¿¡å·åæ, åºäºåå®¹çé³ä¹ä¿¡å·åæ. å¶ä¸­, è¯­é³ä¿¡å·å¤çä¸è¯å«æ©å·²æä¸ºä¸ä¸ªä¼ ç»çç ç©¶ç­ç¹. éçä¿¡æ¯ç§å­¦ä¸ææ¯çè¿éåå±, åºäºåå®¹çé³é¢ä¸é³ä¹ä¿¡å·åæä¹éæ¸æä¸ºä¸ä¸ªæ°çç ç©¶ç­ç¹, è¿å å¹´æ¥åå¾äºå¤§éç ç©¶ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:HriVNRqQrCwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:HriVNRqQrCwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3219106275905615902&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>14</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3219106275905615902&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://www.music-ir.org/mirex/abstracts/2005/logan.pdf</url>
        <title status="complete" source="scholar.google.com">Nearest-neighbor artist identification</title>
        <pdf>http://www.music-ir.org/mirex/abstracts/2005/logan.pdf</pdf>
        <author status="complete" source="scholar.google.com">B Logan</author>
        <proceeding status="partial" source="scholar.google.com">Proceeding of Music Information Retrieval Evaluation &#8230;</proceeding>
        <source>music-ir.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We propose and investigate the performance of a simple artist identification system. Our approach learns a set of signatures for songs by known artists and then given an new song chooses the nearest neighbor from amongst these to identify the artist. The ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:DnpTRy0-8qsJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:DnpTRy0-8qsJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12390033889040759310&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12390033889040759310&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf</url>
        <title status="complete" source="scholar.google.com">æ­å£°æå ±å¦çã®æè¿ã®ç ç©¶</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å°</author>
        <proceeding status="complete" source="scholar.google.com">æ¥æ¬é³é¿å­¦ä¼èª</proceeding>
        <year>2008</year>
        <source>staff.aist.go.jp</source>
        <snippet status="complete" source="scholar.google.com">â Recent studies on singing information processing. ââ Masataka Goto, Takeshi Saitou, Tomoyasu Nakano and Hiromasa Fujihara (National Institute of Ad- vanced Industrial Science and Technology (AIST), Tsukuba, 305â8568) e-mail: m.goto@aist.go.jp</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3835003938146142193&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>6</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3835003938146142193&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5604657</url>
        <title status="complete" source="scholar.google.com">Background Music Removal Based on Cepstrum Transformation for Popular Singer Identification</title>
        <author status="complete" source="scholar.google.com">WH Tsai, HP Lin</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language Processing, &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract One major challenge of identifying singers in popular music recordings lies in how to reduce the interference of background accompaniment in trying to characterize the singer voice. Although a number of studies on automatic Singer IDentification (SID) from acoustic ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_Ay7Rh7NeBAJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1186924031731502332&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1186924031731502332&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6022500</url>
        <title status="complete" source="scholar.google.com">Automatic singer identification based on auditory features</title>
        <author status="complete" source="scholar.google.com">W Cai, Q Li, X Guan</author>
        <proceeding status="partial" source="scholar.google.com">Natural Computation (ICNC), 2011 &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The paper describes a method of identifying singers&#39; voice from the monophonic music including sounds of various musical instruments based on auditory features. In this system, there are four problems to solve, vocal segment detection, feature extraction, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2fGI2y5MEzUJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3824484272703074777&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3824484272703074777&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://hal.archives-ouvertes.fr/tel-00457522/</url>
        <title status="complete" source="scholar.google.com">CaractÃ©risation de l&#39;environnement musical dans les documents audiovisuels</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/45/75/22/PDF/These_Helene_Lachambre.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Lachambre</author>
        <year>2009</year>
        <source>hal.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">Depuis plusieurs dizaines d&#39;annÃ©es, l&#39;indexation par le contenu des documents audiovisuels fait l&#39;objet de travaux de la part de nombreuses Ã©quipes de recherche:âLe mot Â«audiovisuelÂ» fait rÃ©fÃ©rencea un contenu Â«multimÃ©diaÂ» qui regroupea la fois des donnÃ©es ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Yp_9nKZh3GwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7844252019198893922&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7844252019198893922&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5583066</url>
        <title status="complete" source="scholar.google.com">Popular singer identification based on cepstrum transformation</title>
        <author status="complete" source="scholar.google.com">WH Tsai, HP Lin</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo (ICME), 2010 IEEE &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract A prerequisite for identifying the singers in popular music recordings is to reduce the interference of background accompaniment when trying to characterize the singer voice. This study proposes a background music removal approach for singer identification (SID) ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7D45AbbgAKYJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11961807682605235948&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11961807682605235948&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://cjc.ict.ac.cn/eng/qwjse/view.asp?id=2381</url>
        <title status="complete" source="scholar.google.com">A Review of Content-Based Audio and Music Analysis</title>
        <author status="complete" source="scholar.google.com">YB Zhang, J Zhou, ZQ Bian, J Guo</author>
        <proceeding status="partial" source="scholar.google.com">CHINESE JOURNAL OF &#8230;</proceeding>
        <year>2007</year>
        <source>cjc.ict.ac.cn</source>
        <snippet status="partial" source="scholar.google.com">Abstract Machine hearing includes three fields: Speech signal processing and recognition, general audio signal processing, and content-based music analysis. Speech signal processing and recognition has been a traditional research field for many years. There are ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dO9jGiU6e6UJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11924188369424478068&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11924188369424478068&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5381403</url>
        <title status="complete" source="scholar.google.com">Vocal characteristics classification of audio segments: An investigation of the influence of accompaniment music on low-level features</title>
        <author status="complete" source="scholar.google.com">D Gartner, C Dittmar</author>
        <proceeding status="partial" source="scholar.google.com">Machine Learning and Applications, &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The characteristics of vocal segments in music are an important cue for automatic, content-based music recommendation, especially in the urban genre. In this paper, we investigate the classification of audio segments into singing and rap, using low-level ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:JjfSvDQsmK0J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12508796570039367462&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12508796570039367462&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://tel.archives-ouvertes.fr/pastel-00529331/</url>
        <title status="complete" source="scholar.google.com">Classification automatique de flux radiophoniques par Machines Ã  Vecteurs de Support</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/52/93/31/PDF/main.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Ramona</author>
        <year>2010</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">1.1 Vers une radio numÃ©rique......................... 9 1.2 Applications de l&#39;indexation audio pour la radio............ 10 1.3 Â«Qu&#39;est-ce que la musique?Â»...................... 11 1.4 Classification par Machines Ã  Vecteurs de Support.......... 12 1.5 ProblÃ©matiques................................ 13 1.6 ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:1n7-PBzLMbkJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13344670493018324694&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13344670493018324694&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://institut17-1.kug.ac.at/fileadmin/media/iem/projects/2011/gampp_ti.pdf</url>
        <title status="complete" source="scholar.google.com">Evaluation of Robust Features for Singing Voice Detection</title>
        <pdf>http://institut17-1.kug.ac.at/fileadmin/media/iem/projects/2011/gampp_ti.pdf</pdf>
        <author status="complete" source="scholar.google.com">P Gampp</author>
        <proceeding status="complete" source="scholar.google.com">institut17-1.kug.ac.at</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract The detection of singing voice segments within music signals is an important object of research in the field of music information retrieval, since it serves as an essential pre-stage for applications like singer identification, lyrics recognition, singing melody ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:CMMZACBTsKIJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:CMMZACBTsKIJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11722961226951148296&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://books.google.com/books?hl=en&amp;lr=&amp;id=huAzfJQhXDcC&amp;oi=fnd&amp;pg=PA99&amp;ots=_G5BM4g1mk&amp;sig=LD_EGU-pe3iWvkG5dX7f0PE9jZE</url>
        <title status="complete" source="scholar.google.com">Content-Based Music Summarization and Classification</title>
        <author status="complete" source="scholar.google.com">JS Jin</author>
        <proceeding status="complete" source="scholar.google.com">Managing Multimedia Semantics</proceeding>
        <year>2005</year>
        <source>books.google.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This chapter aims to provide a comprehensive survey of the technical achievements in the area of content-based music summarization and classification and to present our recent achievements. In order to give a full picture of the current status, the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:rJdeESIOowoJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <title status="complete" source="scholar.google.com">Identifying Singers of Popular Songs</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:nvwHI3QIotoJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15754163741392370846&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://tel.archives-ouvertes.fr/tel-00687475/</url>
        <title status="complete" source="scholar.google.com">Localisation, caractÃ©risation et reconnaissance de voix chantÃ©es</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/68/74/75/PDF/these.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Regnier</author>
        <year>2012</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">Many auditors have a remarkable ability to identify the singer of a new song as long as they have already heard some songs performed by the same singer. When the singer is unfamiliar it is common for listeners to attempt to characterize the singer&#39;s voice by finding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ObV6dGA6ysYJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14324325750750754105&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://thesis.lib.ncu.edu.tw/ETD-db/ETD-search/view_etd?URN=975201012</url>
        <title status="complete" source="scholar.google.com">Design and Implementation for Content-based Singer Classification on Compressed Domain Audio Data</title>
        <author status="complete" source="scholar.google.com">Y Huang</author>
        <year>2010</year>
        <source>thesis.lib.ncu.edu.tw</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this thesis we proposed a singer classification approach to automatically identify the singer of an unknown MP3 or AAC audio data. Differing from previous researches for singer identification in MP3 compressed domain, we use Mel-Frequency Cepstral ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:er8DnWzEJtoJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://hal.inria.fr/docs/00/68/74/75/PDF/these.pdf</url>
        <title status="complete" source="scholar.google.com">L&#39;UNIVERSITE PIERRE ET MARIE CURIE</title>
        <pdf>http://hal.inria.fr/docs/00/68/74/75/PDF/these.pdf</pdf>
        <author status="complete" source="scholar.google.com">ML Regnier</author>
        <proceeding status="complete" source="scholar.google.com">hal.inria.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">Many auditors have a remarkable ability to identify the singer of a new song as long as they have already heard some songs performed by the same singer. When the singer is unfamiliar it is common for listeners to attempt to characterize the singer&#39;s voice by finding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0HDPonzwK1wJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:0HDPonzwK1wJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6205358</url>
        <title status="complete" source="scholar.google.com">Singer Identification Based on Spoken Data in Voice Characterization</title>
        <author status="complete" source="scholar.google.com">WH Tsai, HC Lee</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language Processing, &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Currently existing singer identification (SID) methods follow the framework of speaker identification (SPID), which requires that singing data be collected beforehand to establish each singer&#39;s voice characteristics. This framework, however, is unsuitable for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:TuwLwTRRgoAJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://hal.archives-ouvertes.fr/hal-00662341/</url>
        <title status="complete" source="scholar.google.com">Combining Classification based on Local and Global Features: Application to Singer Identification</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/66/23/41/PDF/75_e.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Regnier, G Peeters</author>
        <year>2011</year>
        <source>hal.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In this paper we investigate the problem of singer identification on acapella recordings of isolated notes. Most of studies on singer identification describe the content of signals of singing voice with features related to the timbre (such as MFCC or LPC). These ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pKq6xxG2xuAJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16196833297366166180&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007goto.pdf</url>
        <title status="complete" source="scholar.google.com">æ­å£°æå ±å¦ç: æ­å£°ãå¯¾è±¡ã¨ããé³æ¥½æå ±å¦ç</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007goto.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å°</author>
        <year>2010</year>
        <source>staff.aist.go.jp</source>
        <snippet status="partial" source="scholar.google.com">æ¬ç¨¿ã§ã¯,ãæ­å£°æå ±å¦çã ã¨åä»ããæ°ããç ç©¶é åã«ãããæãã®ç ç©¶äºä¾ãç´¹ä»ãã. ããã¯æ­å£°ã«å¯¾ããé³æ¥½æå ±å¦çã§ãã, ãã®ç ç©¶å¯¾è±¡ã¯å¤å²ã«æ¸¡ãã, æ¬ç¨¿ã§ã¯, æ­å£°çè§£ã·ã¹ãã , æ­å£°ã«åºã¥ãé³æ¥½æå ±æ¤ç´¢ã·ã¹ãã , æ­å£°åæã·ã¹ãã ã®ä¸ã¤ã®éè¦ãª ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uzd8aAf29qoJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:uzd8aAf29qoJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12319304342396745659&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="20">
    <url>http://dl.acm.org/citation.cfm?id=1112857</url>
    <title status="complete" source="scholar.google.com">Key, chord, and rhythm tracking of popular music recordings</title>
    <author status="complete" source="scholar.google.com">A Shenoy, Y Wang</author>
    <proceeding status="complete" source="scholar.google.com">Computer Music Journal</proceeding>
    <year>2005</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... com wangye@comp.nus.edu.sg Key, Chord, and Rhythm Tracking of Popular Music RecordingsIn this article, we propose a framework to analyze a musical audio signal (sampled from a popularmusic CD) and determine its key, provide usable chord transcriptions, and obtain ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:33p2o7nmEuIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>6</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=16290336487138294495&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>22</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=16290336487138294495&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="22">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4275055</url>
        <title status="complete" source="scholar.google.com">Large-scale study of chord estimation algorithms based on chroma representation and HMM</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/51/14/37/PDF/Papadopoulos_Chord_CBMI_2007.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Papadopoulos, G Peeters</author>
        <proceeding status="partial" source="scholar.google.com">Content-Based Multimedia &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper deals with the automatic estimation of chord progression over time of an audio file. From the audio signal, a set of chroma vectors representing the pitch content of the file over time is extracted. From these observations the chord progression is then ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BUvWHNkTDwkJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=652762294204648197&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>59</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=652762294204648197&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4517561</url>
        <title status="complete" source="scholar.google.com">Simultaneous estimation of chord progression and downbeats from an audio file</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/51/14/45/PDF/Papadopoulos_ChordDownbeat_ICASSP_2008.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Papadopoulos, G Peeters</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and Signal &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Harmony and metrical structure are some of the most important attributes of Western tonal music. In this paper, we present a new method for simultaneously estimating the chord progression and the downbeats from an audio file. For this, we propose a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Rgv-ltW9WzsJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>11</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4277220996177333062&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>35</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4277220996177333062&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.springerlink.com/index/G840778660242162.pdf</url>
        <title status="complete" source="scholar.google.com">A probabilistic framework for audio-based tonal key and chord recognition</title>
        <author status="complete" source="scholar.google.com">B Catteau, JP Martens, M Leman</author>
        <proceeding status="complete" source="scholar.google.com">Advances in Data Analysis</proceeding>
        <year>2007</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">A unified probabilistic framework for audio-based chord and tonal key recognition is described and evaluated. The proposed framework embodies an acoustic observation likelihood model and key &amp; chord transition models. It is shown how to conceive these ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:yPLvR7oxZ50J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11342088862696469192&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>15</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11342088862696469192&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>https://biblio.ugent.be/publication/967938</url>
        <title status="complete" source="scholar.google.com">Integrating musicological knowledge into a probabilistic framework for chord and key extraction</title>
        <pdf>https://biblio.ugent.be/publication/967938/file/978931.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Pauwels, J Martens</author>
        <year>2010</year>
        <source>biblio.ugent.be</source>
        <snippet status="partial" source="scholar.google.com">abstract In this contribution a formerly developed probabilistic framework for the simultaneous detection of chords and keys in polyphonic audio is further extended and validated. The system behaviour is controlled by a small set of carefully defined free ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gZn3pPN_T8UJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14217723233053415809&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>9</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14217723233053415809&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4564924</url>
        <title status="complete" source="scholar.google.com">Enhancing chord classification through neighbourhood histograms</title>
        <pdf>http://wwwiti.cs.uni-magdeburg.de/~stober/publ/cbmi2008.pdf</pdf>
        <author status="partial" source="scholar.google.com">J Reinhard, S Stober&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Content-Based Multimedia &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The chord progression of a song is an important high-level feature which enables indexing as well as deeper analysis of musical recordings. Different approaches to chord recognition have been suggested in the past. Though their performance increased, still ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:wAXIQHzDupYJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10861208389787583936&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10861208389787583936&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5664772</url>
        <title status="complete" source="scholar.google.com">Probabilistic template-based chord recognition</title>
        <pdf>http://laurentoudre.fr/publis/OFG-IEEE-11.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Oudre, C FÃ©votte, Y Grenier</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper describes a probabilistic approach to template-based chord recognition in music signals. The algorithm only takes chromagram data and a user-defined dictionary of chord templates as input data. No training or musical information such as key, rhythm, or ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dONbRdf-OF4J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6789456638796096372&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6789456638796096372&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://books.google.com/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA319&amp;ots=oDOSrGgB95&amp;sig=sBWhBDxXRIXKa14TnKOuDPdbnKo</url>
        <title status="complete" source="scholar.google.com">Clustering music recordings by their keys</title>
        <pdf>http://www.cs.fiu.edu/~lli003/Music/clu/5.pdf</pdf>
        <author status="partial" source="scholar.google.com">Y Liu, Y Wang, A Shenoy, WH Tsai&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
        <year>2008</year>
        <source>books.google.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Music key, a high level feature of musical audio, is an effective tool for structural analysis of musical works. This paper presents a novel unsupervised approach for clustering music recordings by their keys. Based on chroma-based features extracted from acoustic ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:upv83jIWit4J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>24</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16035653830951345082&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16035653830951345082&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://hal.inria.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf</url>
        <title status="complete" source="scholar.google.com">Joint estimation of musical content information from an audio signal</title>
        <pdf>http://hal.inria.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Papadopoulos</author>
        <year>2010</year>
        <source>hal.inria.fr</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© Dans cette these, nous nous intÃ©ressons au probleme de l&#39;extraction automatique d&#39;informations de contenu d&#39;un signal audio de musique. La plupart des travaux existants abordent ce probleme en considÃ©rant les attributs musicaux de maniere indÃ©pendante les ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Gsw65B4lZEwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Gsw65B4lZEwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5504565459161893914&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5610710</url>
        <title status="complete" source="scholar.google.com">Distinguishing monophonies from polyphonies using Weibull bivariate distributions</title>
        <pdf>http://www.helene.lachambre.eu/files/lachambre_TASLP_2011.pdf</pdf>
        <author status="partial" source="scholar.google.com">H Lachambre, R AndrÃ©-Obrecht&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In the context of music indexation, it would be useful to have a precise information about the number of sources performing; a source is a solo voice or an isolated instrument which produces a single note at any time. This correspondence discusses the automatic ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:wgTohkjSeSsJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3132766224755721410&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3132766224755721410&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6012146</url>
        <title status="complete" source="scholar.google.com">Improving the key extraction performance of a simultaneous local key and chord estimation system</title>
        <pdf>https://biblio.ugent.be/publication/1944757/file/1944775.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Pauwels, JP Martens, M Leman</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo (ICME &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, significant improvements of a previously developed key and chord extraction system are proposed. The major improvement is the introduction of a separate acoustic model, designed to verify local key hypotheses. The conducted experimental ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:aBPy1WCBU_YJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17749672809303511912&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17749672809303511912&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://hal.archives-ouvertes.fr/tel-00457522/</url>
        <title status="complete" source="scholar.google.com">CaractÃ©risation de l&#39;environnement musical dans les documents audiovisuels</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/45/75/22/PDF/These_Helene_Lachambre.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Lachambre</author>
        <year>2009</year>
        <source>hal.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">Depuis plusieurs dizaines d&#39;annÃ©es, l&#39;indexation par le contenu des documents audiovisuels fait l&#39;objet de travaux de la part de nombreuses Ã©quipes de recherche:âLe mot Â«audiovisuelÂ» fait rÃ©fÃ©rencea un contenu Â«multimÃ©diaÂ» qui regroupea la fois des donnÃ©es ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Yp_9nKZh3GwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7844252019198893922&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7844252019198893922&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284586</url>
        <title status="complete" source="scholar.google.com">Pop music beat detection in the huffman coded domain</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2007_Pop_Music_Beat_Detection_in_the_Huffman_Coded_Domain.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Zhu, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2007 IEEE International &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a novel beat detector that operates in the Huffman coded domain of a MP3 audio bitstream. We seek to answer two main questions. First, whether it is possible to extract beats without even partial decoding of a MP3 audio. Second, how to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:4gW-SQhyFW0J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7860314104567563746&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7860314104567563746&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5685054</url>
        <title status="complete" source="scholar.google.com">Design of an architecture for a MIDI based music e-tutor</title>
        <author status="complete" source="scholar.google.com">S Mammen, I Krishnamurthi</author>
        <proceeding status="partial" source="scholar.google.com">Audio Language and Image &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The main goal of our work is to develop a framework which can train students on musical instruments. The student&#39;s musical instrument can be connected to the framework through MIDI connection. The framework receives input both from the MIDI instrument and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:XSvA2Fm7uBoJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1925494835263843165&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://www.music-ir.org/mirex/abstracts/2009/ACD_SS_mauch.pdf</url>
        <title status="complete" source="scholar.google.com">MIREX submissions for audio chord detection (no training) and structural segmentation</title>
        <pdf>http://www.music-ir.org/mirex/abstracts/2009/ACD_SS_mauch.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Mauch, K Noland, S Dixon</author>
        <proceeding status="complete" source="scholar.google.com">MIREX Submission Abstracts</proceeding>
        <year>2009</year>
        <source>music-ir.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper describes our approach to chord extraction from audio, a variant of which was submitted to the 2009 MIREX Chord Detection Task (No Training), and achieved the top ranking of 71.2%. The structural segmentation algorithm is a pre-processing step ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:JLmx-Wv3dCUJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:JLmx-Wv3dCUJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2699054119824505124&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2699054119824505124&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://www.springerlink.com/index/76x78554q0526132.pdf</url>
        <title status="complete" source="scholar.google.com">Adaptive music retrievalâa state of the art</title>
        <pdf>http://wwwiti.cs.uni-magdeburg.de/~stober/publ/mtap2012adaptiveMIR.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Stober, A NÃ¼rnberger</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Tools and Applications</proceeding>
        <year>2012</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract With the development of more and more sophisticated Music Information Retrieval approaches, aspects of adaptivity are becoming an increasingly important research topic. Even though, adaptive techniques have already found their way into Music Information ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:MvLbFPg5nPEJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17409853997172126258&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5662016</url>
        <title status="complete" source="scholar.google.com">Probabilistic framework for template-based chord recognition</title>
        <pdf>http://laurentoudre.fr/publis/OFG-MMSP-10.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Oudre, C FÃ©votte, Y Grenier</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia Signal Processing ( &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper describes a method for chord recognition from audio signals. Our method provides a coherent and relevant probabilistic framework for template-based transcription. The only information needed for the transcription is the definition of the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dnxlM2X4ymwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7839351214929443958&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://scholarbank.nus.sg/handle/10635/13382</url>
        <title status="partial" source="scholar.google.com">REFINING MUSIC SIGNAL TO LYRIC TEXT SYNCHRONIZATION FROM LINE-LEVEL TO SYLLABLE-LEVEL BY CONSTRAINING DYNAMIC TIME WARPING &#8230;</title>
        <pdf>http://scholarbank.nus.sg/bitstream/handle/10635/13382/thesis_21_amendment_2.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">D ISKANDAR</author>
        <year>2007</year>
        <source>scholarbank.nus.sg</source>
        <snippet status="partial" source="scholar.google.com">The problem we consider in this thesis is synchronization between lyric text and the corresponding singing voice recording. We limit the singing to the pop genre to make the problem manageable. The recordings we consider in this problem are those we can ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:q87ftenzmuwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17049207524468706987&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5571465</url>
        <title status="complete" source="scholar.google.com">Design of an Architecture for</title>
        <author status="partial" source="scholar.google.com">S Mammen, I Krishnamurthi&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Computing (ICIIC), 2010 &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The main goal of our work is to develop a framework which can train students on musical instruments. The student&#39;s musical instrument can be connected to the framework through MIDI connection. The framework receives input both from the MIDI instrument and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:T8Bj_VEd0CEJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2436479636388167759&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://laurentoudre.fr/publis/LO-PHDfr-10.pdf</url>
        <title status="complete" source="scholar.google.com">Laurent OUDRE</title>
        <pdf>http://laurentoudre.fr/publis/LO-PHDfr-10.pdf</pdf>
        <author status="complete" source="scholar.google.com">DE Rapporteurs, S Marchand</author>
        <proceeding status="complete" source="scholar.google.com">laurentoudre.fr</proceeding>
        <snippet status="complete" source="scholar.google.com">Ce premier chapitre vise Ã  introduire les principales notions musicales nÃ©cessaires Ã  la bonne comprÃ©hension du prÃ©sent document, mais aussi Ã  prÃ©ciser le contexte et les principales applications de ce travail de thÃ¨se.</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PPdVPuMCoxEJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:PPdVPuMCoxEJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1270862694875264828&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://www.db-thueringen.de/servlets/DerivateServlet/Derivate-23720/ilm1-2011000164.pdf</url>
        <title status="complete" source="scholar.google.com">Ein Beitrag zur tonraumbasierten Analyse und Synthese musikalischer Audiosignale</title>
        <pdf>http://www.db-thueringen.de/servlets/DerivateServlet/Derivate-23720/ilm1-2011000164.pdf</pdf>
        <author status="complete" source="scholar.google.com">IT Sporer, T KrÃ¤mer</author>
        <proceeding status="complete" source="scholar.google.com">db-thueringen.de</proceeding>
        <snippet status="partial" source="scholar.google.com">Im ersten Teil, der die Kapitel 2 bis 6 enthÃ¤lt und von Gabriel Gatzsche verfasst wurde, erfolgt die mathematisch-geometrische Beschreibung der TonalitÃ¤t auf verschiedenen hierarchischen Ebenen angelehnt an Fred Lerdahls Tonal Pitch Space, David Gatzsches Kadenzkreis ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KJPIThOZopUJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:KJPIThOZopUJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10782348766083584808&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://tel.archives-ouvertes.fr/pastel-00542840/</url>
        <title status="complete" source="scholar.google.com">Reconnaissance d&#39;accords Ã  partir de signaux audio par l&#39;utilisation de gabarits thÃ©oriques</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/54/28/40/PDF/These_fr_en.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Oudre</author>
        <year>2010</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© Cette thÃ¨se s&#39; inscrit dans le cadre du traitement du signal musical, en se focalisant plus particuliÃ¨rement sur la transcription automatique de signaux audio en accords. En effet, depuis une dizaine d&#39;annÃ©es, de nombreux travaux visent Ã  reprÃ©senter les signaux ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:afhmW5HEmVcJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6312292481319237737&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>http://tel.archives-ouvertes.fr/tel-00548952/</url>
        <title status="complete" source="scholar.google.com">Estimation conjointe d&#39;information de contenu musical d&#39;un signal audio</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Papadopoulos</author>
        <year>2010</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© Dans cette these, nous nous intÃ©ressons au probleme de l&#39;extraction automatique d&#39;informations de contenu d&#39;un signal audio de musique. La plupart des travaux existants abordent ce probleme en considÃ©rant les attributs musicaux de maniere indÃ©pendante les ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:B-SObITZT2wJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7804695842036573191&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="21">
    <url>http://dl.acm.org/citation.cfm?id=1101209</url>
    <title status="complete" source="scholar.google.com">Using offline bitstream analysis for power-aware video decoding in portable devices</title>
    <pdf>http://kusu.comp.nus.edu/proceedings/mm05/docs/mm299.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Huang, S Chakraborty, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 13th annual ACM &#8230;</proceeding>
    <year>2005</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Yicheng Huang Samarjit Chakraborty Ye Wang Department of Computer Science, NationalUniversity of Singapore E-mail: {huangyic, samarjit, wangye}@comp.nus.edu.sg ABSTRACTDynamic voltage/frequency scheduling algorithms for multimedia applications have recently ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:xDEpsov8ugAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>23</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=52632022587879876&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>23</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=52632022587879876&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="23">
        <result id="0">
        <url>http://www.springerlink.com/index/f6505jp679313874.pdf</url>
        <title status="complete" source="scholar.google.com">Scenario selection and prediction for DVS-aware scheduling of multimedia applications</title>
        <pdf>http://www.ics.ele.tue.nl/~tbasten/papers/jvlsi_scenarios.pdf</pdf>
        <author status="complete" source="scholar.google.com">SV Gheorghita, T Basten, H Corporaal</author>
        <proceeding status="partial" source="scholar.google.com">Journal of Signal Processing &#8230;</proceeding>
        <year>2008</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Modern multimedia applications usually have real-time constraints and they are implemented using application-domain specific embedded processors. Dimensioning a system requires accurate estimations of resources needed by the applications. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9pawwfWXc80J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14804343481869309686&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>19</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14804343481869309686&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4084751</url>
        <title status="complete" source="scholar.google.com">Profiling driven scenario detection and prediction for multimedia applications</title>
        <pdf>http://www.es.ele.tue.nl/~premadona/publications/GBC06a.pdf</pdf>
        <author status="partial" source="scholar.google.com">SV Gheorghita, T Basten&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Modeling and Simulation &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Modern multimedia applications usually have real-time constraints and they are implemented using heterogeneous multiprocessor systems-on-chip. Dimensioning a system requires accurate estimations of resources needed by the applications. Overestimation ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Q-DWY2OkLAIJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=156680833834868803&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>12</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=156680833834868803&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.springerlink.com/index/51J2T304232575V0.pdf</url>
        <title status="complete" source="scholar.google.com">Exploiting video stream similarity for energy-efficient decoding</title>
        <author status="complete" source="scholar.google.com">J Hamers, L Eeckhout, K De Bosschere</author>
        <proceeding status="partial" source="scholar.google.com">Advances in Multimedia &#8230;</proceeding>
        <year>2006</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Energy consumption is a key issue in modern microprocessor system design in general, and in the design of mobile computing devices more in particular. This paper introduces a novel approach to energy-efficient media stream decoding that is based on the notion of media ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:5cWJWn-s2bYJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13175751847828899301&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>10</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13175751847828899301&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4211863</url>
        <title status="complete" source="scholar.google.com">Resource prediction for media stream decoding</title>
        <pdf>http://dent.cecs.uci.edu/~papers/date07/PAPERS/2007/DATE07/PDFFILES/04.5_3.PDF</pdf>
        <author status="complete" source="scholar.google.com">J Hamers, L Eeckhout</author>
        <proceeding status="partial" source="scholar.google.com">Design, Automation &amp; Test in Europe &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Resource prediction refers to predicting required compute power and energy resources for consuming a service on a device. Resource prediction is extremely useful in a client-server setup where the client requests a media service from the server or content ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Fr7hXYupwpQJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>19</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10719316479135825430&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>10</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10719316479135825430&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4711231</url>
        <title status="complete" source="scholar.google.com">Adaptive disk power management for portable media players</title>
        <pdf>http://dspace.inha.ac.kr/bitstream/10505/1887/1/Adaptive.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Go, M Song</author>
        <proceeding status="complete" source="scholar.google.com">Consumer Electronics, IEEE Transactions on</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract To support the large storage requirements, consumer electronics for video playback are increasingly being equipped with hard disk drives (HDD) that consume a significant amount of energy. A video player may prefetch many frames to give disk an opportunity to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ldM0RwYkFzgJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4041738799971357589&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4041738799971357589&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://dl.acm.org/citation.cfm?id=1629337</url>
        <title status="complete" source="scholar.google.com">Aggressive dynamic voltage scaling for energy-aware video playback based on decoding time estimation</title>
        <author status="complete" source="scholar.google.com">A Yang, M Song</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the seventh ACM international &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract An effective way for reducing CPU power consumption is to reduce its operating frequency. But this slows down program execution, which may violate the real-time requirements of video playback. What is worse, it is difficult to predict future decoding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:fzAWSYUg_7cJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13258351584830959743&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13258351584830959743&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://dl.acm.org/citation.cfm?id=2146419</url>
        <title status="complete" source="scholar.google.com">Exploiting media stream similarity for energy-efficient decoding and resource prediction</title>
        <author status="complete" source="scholar.google.com">J Hamers, L Eeckhout</author>
        <proceeding status="partial" source="scholar.google.com">ACM Transactions on Embedded Computing &#8230;</proceeding>
        <year>2012</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This article introduces a novel approach to energy-efficient media stream decoding that is based on the notion of media stream similarity. The key idea is that platform-independent scenarios with similar decoding complexity can be identified within and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:sE6FXavKTcEJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13929012059884048048&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://dl.acm.org/citation.cfm?id=1730851</url>
        <title status="complete" source="scholar.google.com">Achieving viewing time scalability in mobile video streaming using scalable video coding</title>
        <pdf>https://www.cs.sfu.ca/~mhefeeda/Papers/mmsys10_viewingTime.pdf</pdf>
        <author status="complete" source="scholar.google.com">CH Hsu, M Hefeeda</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the first annual ACM SIGMM &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a general quality-power adaptation framework that controls the perceived video quality and the length of viewing time on battery-powered video receivers. The framework can be used for standalone video devices (eg, DVD players and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:HWS2KFyvn4AJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9268319368506663965&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9268319368506663965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://dl.acm.org/citation.cfm?id=1391710</url>
        <title status="complete" source="scholar.google.com">Automated hardware-independent scenario identification</title>
        <author status="complete" source="scholar.google.com">J Hamers, L Eeckhout</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 45th annual Design &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Scenario-based design exploits the time-varying execution behavior of applications by dynamically adapting the system on which they run. This is a particularly interesting design methodology for media applications with soft realtime constraints such as ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:qDa_r7v71uoJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16921989433417217704&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16921989433417217704&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://www.es.ele.tue.nl/esreports/esr-2010-02.pdf</url>
        <title status="complete" source="scholar.google.com">Predicting the throughput of multiprocessor applications under dynamic workload</title>
        <pdf>http://www.es.ele.tue.nl/esreports/esr-2010-02.pdf</pdf>
        <author status="complete" source="scholar.google.com">P Poplavko, M Geilen, T Basten</author>
        <proceeding status="complete" source="scholar.google.com">Int. Conf. on Computer Design, ICCD</proceeding>
        <year>2010</year>
        <source>es.ele.tue.nl</source>
        <snippet status="partial" source="scholar.google.com">AbstractâThis work contributes to throughput calculation for real-time multiprocessor applications experiencing dynamic workload variations. We focus on a method to predict the system throughput when processing an arbitrarily long data frame given the meta- ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8xy4x7YwUfsJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:8xy4x7YwUfsJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18109309138226388211&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=18109309138226388211&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5551101</url>
        <title status="complete" source="scholar.google.com">Scenario-based resource prediction for QoS-aware media processing</title>
        <author status="complete" source="scholar.google.com">J Hamers, L Eeckhout</author>
        <proceeding status="complete" source="scholar.google.com">Computer</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Media streams can be annotated with platform-independent scenario information to reflect frame-level decode complexity. This enables energy-efficient decoding, resource prediction, and quality-of-service management on single-core as well as multicore ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:l2I2IlJ5i6YJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12000819025727087255&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12000819025727087255&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://alexandria.tue.nl/repository/books/710997.pdf</url>
        <title status="complete" source="scholar.google.com">An Overview of Application Scenario Usage in Streaming-Oriented Embedded System Design</title>
        <pdf>http://alexandria.tue.nl/repository/books/710997.pdf</pdf>
        <author status="partial" source="scholar.google.com">SV Gheorghita, T Basten&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Eindhoven University of &#8230;</proceeding>
        <year>2006</year>
        <source>alexandria.tue.nl</source>
        <snippet status="partial" source="scholar.google.com">Abstract In the past years real-time embedded systems became more and more complex. From the user perspective, these systems have stringent requirements regarding size, performance and power consumption, and due to business competition, their time-to- ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:QgwRQ1d5tJQJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:QgwRQ1d5tJQJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10715322829115558978&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10715322829115558978&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://alexandria.tue.nl/extra2/200811801.pdf</url>
        <title status="complete" source="scholar.google.com">An accurate analysis for guaranteed performance of multiprocessor streaming applications</title>
        <pdf>http://alexandria.tue.nl/extra2/200811801.pdf</pdf>
        <author status="complete" source="scholar.google.com">P Poplavko</author>
        <year>2008</year>
        <source>alexandria.tue.nl</source>
        <snippet status="partial" source="scholar.google.com">Already for more than a decade, consumer electronic devices have been available for entertainment, educational, or telecommunication tasks based on multimedia streaming applications, ie, applications that process streams of audio and video samples in digital ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:EsP8vh2dwMMJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:EsP8vh2dwMMJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14105446784008241938&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14105446784008241938&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://scholarbank.nus.sg/handle/10635/35830</url>
        <title status="complete" source="scholar.google.com">Quality-aware performance analysis for multimedia MPSoC platforms</title>
        <pdf>http://scholarbank.nus.sg/bitstream/handle/10635/35830/GangadharanD.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">D GANGADHARAN</author>
        <year>2012</year>
        <source>scholarbank.nus.sg</source>
        <snippet status="partial" source="scholar.google.com">State-of-the-art embedded devices (eg mobile devices) run multiple applications on multiprocessor system-on-chip (MPSoC) platforms. MPSoC platforms are becoming popular due to the increasing number and complexity of target applications. Although there is a ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://dl.acm.org/citation.cfm?id=1629492</url>
        <title status="complete" source="scholar.google.com">Fast model-based test case classification for performance analysis of multimedia MPSoC platforms</title>
        <author status="partial" source="scholar.google.com">D Gangadharan, S Chakraborty&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 7th &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Currently, performance analysis of multimedia-MPSoC platforms largely rely on simulation. The execution of one or more applications on such a platform is simulated for a library of test video clips. If all specified performance constraints are satisfied for this ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:e3WsWVpdpl4J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6820241328332109179&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6820241328332109179&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>https://scholarbank.nus.edu.sg/handle/10635/16736</url>
        <title status="complete" source="scholar.google.com">Workload model for video decoding and its applications</title>
        <pdf>https://scholarbank.nus.edu.sg/bitstream/handle/10635/16736/Thesis_HuangYicheng.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">H Yicheng</author>
        <year>2008</year>
        <source>scholarbank.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">In this thesis, we study the relationship between decoding workload and video quality. Based on the analysis of video structure and decoder implementations, we propose a decoding workload model. Given a video clip, the model can accurately estimate the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:SWymZq2_0vUJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17713431035874012233&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://scholarbank.nus.edu.sg/handle/10635/16660</url>
        <title status="complete" source="scholar.google.com">Perception-aware low-power audio processing techniques for portable devices</title>
        <pdf>http://scholarbank.nus.edu.sg/bitstream/handle/10635/16660/HuangWendong_PhD_thesis.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">H WENDONG</author>
        <year>2009</year>
        <source>scholarbank.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">In this thesis, we study perception-aware low power audio processing techniques for portable device. These works are mainly motivated by the fact that the audio decoding application is a significant source of energy consumption in context of portable devices. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BzEVahxPeIQJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9545466393669218567&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6390931</url>
        <title status="complete" source="scholar.google.com">Equivalent decoding complexity based DVS algorithm for video decoding</title>
        <author status="complete" source="scholar.google.com">Y Cao, J Yang, C Yun</author>
        <proceeding status="partial" source="scholar.google.com">Control Conference (CCC), 2012 31st &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Video decoding belongs to a high energy consuming task, which poses a great challenge on energy-constraint devices. To overcome the challenge, lots of dynamic voltages scaling (DVS) algorithms have been proposed. However, an online DVS ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://scholarbank.nus.edu.sg/handle/10635/16102</url>
        <title status="complete" source="scholar.google.com">Power management for interactive 3D games</title>
        <pdf>http://scholarbank.nus.edu.sg/bitstream/handle/10635/16102/GuYan.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">YAN GU</author>
        <year>2008</year>
        <source>scholarbank.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">Interactive 3D games are now widely available on a variety of mobile devices for which battery-life is a major concern. Many of these devices support voltage/frequency-scalable processors and dynamic voltage scaling (DVS) has emerged as a powerful technique for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9UJ8bAxwr68J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12659460276223754997&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>https://wwwp.cs.unc.edu/~bipasa/projects/UROP.pdf</url>
        <title status="complete" source="scholar.google.com">Design Space Exploration of Multiprocessor System-on-Chip Architectures for Real-Time Multimedia Applications</title>
        <pdf>https://wwwp.cs.unc.edu/~bipasa/projects/UROP.pdf</pdf>
        <author status="complete" source="scholar.google.com">B Chattopadhyay, S Chakraborty</author>
        <year>2009</year>
        <source>wwwp.cs.unc.edu</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We have identified the performance impact associated with different design parameters of an on-chip ARM processor in the specific context of implementing Real-Time Multimedia applications. We have mapped different tasks on to a processor, scheduled ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ucN4ihMchlsJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:ucN4ihMchlsJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6594989574583796665&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.114.6550&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Application Scenarios</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.114.6550&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">SV Gheorghita</author>
        <proceeding status="complete" source="scholar.google.com">Citeseer</proceeding>
        <snippet status="partial" source="scholar.google.com">In the past decade, real-time embedded systems became more and more complex and pervasive. From the user perspective, these systems have stringent requirements regarding size, performance and energy consumption, and due to business competition, their time-to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mMFKue8s1SIJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mMFKue8s1SIJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2509961775446409624&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://dl.acm.org/citation.cfm?id=1459619</url>
        <title status="complete" source="scholar.google.com">Multimedia power management on a platter: from audio to video &amp; games</title>
        <author status="complete" source="scholar.google.com">S Chakraborty, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 16th ACM international &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Today, battery-life is a major design concern for all portable devices ranging from cell phones to PDAs and portable game consoles. The purpose of this tutorial will be to give an overview of power management techniques that are applicable to multimedia ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_MHf5UBV64YJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9721957957832262140&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="22">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1415685</url>
    <title status="complete" source="scholar.google.com">Music transcription using an instrument model</title>
    <pdf>http://ispl.korea.ac.kr/conference/icassp2005/pdfs/0300217.pdf</pdf>
    <author status="complete" source="scholar.google.com">J Yin, T Sim, Y Wang, A Shenoy</author>
    <proceeding status="partial" source="scholar.google.com">Acoustics, Speech, and &#8230;</proceeding>
    <year>2005</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. MUSIC TRANSCRIPTION USING AN INSTRUMENT MODEL Jun Yin, Terence Sim,Ye Wang, Arun Shenoy National University of Singapore, School of Computing, Singapore117543 {yinjun, tsim, wangye, arunshen}@comp.nus.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:BAeAPu-9I50J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>19</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=11323102723382970116&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>21</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=11323102723382970116&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="21">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1101353</url>
        <title status="complete" source="scholar.google.com">Digital violin tutor: an integrated system for beginning violin learners</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2005_Digital_Violin_Tutor-An_Integrated_System_for_Beginning_Violin_Learners.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Yin, Y Wang, D Hsu</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 13th annual ACM &#8230;</proceeding>
        <year>2005</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Prompt feedback is essential for beginning violin learners; however, most amateur learners can only meet with teachers and receive feedback once or twice a week. To help such learners, we have attempted an initial design of Digital Violin Tutor (DVT), an ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:keJO9X-wIy8J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3396752607590408849&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>22</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3396752607590408849&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1290156</url>
        <title status="complete" source="scholar.google.com">Effective use of multimedia for computer-assisted musical instrument tutoring</title>
        <pdf>http://strum.googlecode.com/svn-history/r41/trunk/Research/CAMIT.pdf</pdf>
        <author status="complete" source="scholar.google.com">G Percival, Y Wang, G Tzanetakis</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international &#8230;</proceeding>
        <year>2007</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a survey of recent work in computer-assisted musical instrumental tutoring and outlines several questions to consider when developing future projects. In particular, we suggest that the area ingreatest need of computer assistance is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:sSgpS4WrYA0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>21</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=963958909237274801&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>18</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=963958909237274801&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://winnie.kuis.kyoto-u.ac.jp/~kitahara/papers/d-thesis-kitahara.pdf</url>
        <title status="complete" source="scholar.google.com">Computational musical instrument recognition and its application to content-based music information retrieval</title>
        <pdf>http://winnie.kuis.kyoto-u.ac.jp/~kitahara/papers/d-thesis-kitahara.pdf</pdf>
        <author status="complete" source="scholar.google.com">T Kitahara</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; PhD Thesis, Kyoto University, Kyoto, Japan. &#8230;</proceeding>
        <year>2007</year>
        <source>winnie.kuis.kyoto-u.ac.jp</source>
        <snippet status="partial" source="scholar.google.com">Abstract The current capability of computers to recognize auditory events is severely limited when compared to human ability. Although computers can accurately recognize sounds that are sufficiently close to those trained in advance and that occur without other sounds ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PMzNjLYBNY0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:PMzNjLYBNY0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10175040816671476796&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>10</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10175040816671476796&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5356228</url>
        <title status="complete" source="scholar.google.com">Music scene-adaptive harmonic dictionary for unsupervised note-event detection</title>
        <author status="partial" source="scholar.google.com">JJ Carabias-Orti, P Vera-Candeas&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Harmonic decompositions are a powerful tool dealing with polyphonic music signals in some potential applications such as music visualization, music transcription and instrument recognition. The usefulness of a harmonic decomposition relies on the design ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:xtR60XG4XRIJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1323416664502097094&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1323416664502097094&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://www.mistic.ece.uvic.ca/publications/2007_ismir_sitar.pdf</url>
        <title status="complete" source="scholar.google.com">Pedagogical transcription for multimodal sitar performance</title>
        <pdf>http://www.mistic.ece.uvic.ca/publications/2007_ismir_sitar.pdf</pdf>
        <author status="partial" source="scholar.google.com">A Kapur, G Percival, M Lagrange&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proc. Int&#39;l Conf. Music &#8230;</proceeding>
        <year>2007</year>
        <source>mistic.ece.uvic.ca</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Most automatic music transcription research is concerned with producing sheet music from the audio signal alone. However, the audio data does not include certain performance data which is vital for the preservation of instrument performance techniques ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Bo7GfV0RbCYJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Bo7GfV0RbCYJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>15</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2768606964165807622&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>6</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2768606964165807622&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4623948</url>
        <title status="complete" source="scholar.google.com">Application-specific music transcription for tutoring</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_Application_Specific_Music_Transcription_for_Tutoring.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, B Zhang</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic music transcription (AMT) refers to the ability of computers to write note information, such as the pitch, onset time, duration, and source of each sound, after listening to the music. Our application scenario is computer-assisted, musical-instrument tutoring, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vK-ug8uAcwIJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>18</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=176626421973561276&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=176626421973561276&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5303851</url>
        <title status="complete" source="scholar.google.com">Multiple fundamental frequency estimation based on harmonic structure model</title>
        <author status="complete" source="scholar.google.com">L Shi, J Zhang, G Han</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; and Signal Processing, 2009. CISP&#39;09. &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Multiple-f0 estimation refers to the core part of music signal analysis. Although it has been considerably developed, current techniques are still not accurate and robust. Partial overlapping is one of the most difficult problem, all algorithms have troubles in processing ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BNmmNC71a7gJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13288984704287430916&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://paper.ijcsns.org/07_book/200709/20070928.pdf</url>
        <title status="complete" source="scholar.google.com">Analysis/Synthesis of Stringed Instrument Using Formant Structure</title>
        <pdf>http://paper.ijcsns.org/07_book/200709/20070928.pdf</pdf>
        <author status="complete" source="scholar.google.com">K Yasuda, H Hama</author>
        <proceeding status="complete" source="scholar.google.com">IJCSNS</proceeding>
        <year>2007</year>
        <source>paper.ijcsns.org</source>
        <snippet status="partial" source="scholar.google.com">Summary In this paper, timbre of stringed instrument is analyzed and the timbre sound is synthesized. Timbre is familiar and looks well known, but it has been unknown even now what an essential factor is. And it is the most important factor for human to distinguish ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jufuPZ2wGmgJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:jufuPZ2wGmgJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7501502318734403470&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7501502318734403470&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://www.cqvip.com/qk/94913x/200612/23426065.html</url>
        <title status="complete" source="scholar.google.com">åºäºè°æ³¢ç»æä¿¡æ¯çèªå¨é³ä¹æ æ³¨æ¹æ³</title>
        <author status="complete" source="scholar.google.com">éè´µæ»¨ï¼ é©çºªåº</author>
        <proceeding status="complete" source="scholar.google.com">è®¡ç®æºç ç©¶ä¸åå±</proceeding>
        <year>2007</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">æ ¹æ®åç±»ä¹å¨é³è²ç¸ä¼¼çç¹ç¹, æåºäºä¸ç§åºäºè°æ³¢ç»æä¿¡æ¯çèªå¨é³ä¹æ æ³¨æ¹æ³. è¯¥æ¹æ³äºåæåä¸ç±»ä¹å¨ä¸­æä¸ä»¶ä¹å¨çè°æ³¢ç»æä¿¡æ¯. æ ¹æ®è¾å¥ä¿¡å·éæ©åéçä¸è°åç³»æ°, é¢çè¯¯å·®ç³»æ°, ç»åè°æ³¢ç»æä¿¡æ¯æé åç±»æªç¥ä¹å¨çååº¦è°±, ç¶åéç¨æªæ­å®å¨æå°äºä¹æ³ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:43RE2ItV4lIJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5972430114963879139&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5972430114963879139&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4303965</url>
        <title status="complete" source="scholar.google.com">Automatic Transcription Method for Polyphonic Music Based on Adaptive Comb Filter and Neural Network</title>
        <author status="complete" source="scholar.google.com">Z Guibin, L Sheng</author>
        <proceeding status="partial" source="scholar.google.com">Mechatronics and Automation, 2007. ICMA &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a method to transcribe polyphonic music based on adaptive comb filter and neural network. In the method, the input audio is firstly divided into snapshots by a BP neural network, and then comb filters of different notes are used to calculate ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:GVD1cUqKdVQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6085922523805208601&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://www.aes.org/e-lib/browse.cfm?conv=126&amp;papernum=7819</url>
        <title status="complete" source="scholar.google.com">Estimating Instrument Spectral Envelopes for Polyphonic Music Transcription in a Music Scene-Adaptive Approach</title>
        <author status="partial" source="scholar.google.com">JJ Carabias-Orti, P Vera-Candeas, N Ruiz-Reyes&#8230;</author>
        <proceeding status="complete" source="scholar.google.com">Watermark</proceeding>
        <year>2012</year>
        <source>aes.org</source>
        <snippet status="partial" source="scholar.google.com">We propose a method for estimating the spectral envelope pattern of musical instruments in an musical scene-adaptive scheme, without having any prior knowledge about the real transcription. A musical note is defined as stable when variations between its harmonic ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gghaq8Y6TqcJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12055637877470136450&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12055637877470136450&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://www.hindawi.com/journals/aaa/2012/302958/abs/</url>
        <title status="complete" source="scholar.google.com">A Combined Mathematical Treatment for a Special Automatic Music Transcription System</title>
        <pdf>http://downloads.hindawi.com/journals/aaa/2012/302958.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Guo, J Tang</author>
        <proceeding status="complete" source="scholar.google.com">Abstract and Applied Analysis</proceeding>
        <year>2012</year>
        <source>hindawi.com</source>
        <snippet status="partial" source="scholar.google.com">This paper presents a combined mathematical treatment for a special automatic music transcription system. This system is specially made for computer-synthesized music. The combined mathematical treatment includes harmonic selection, matrix analysis, and ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <title status="complete" source="scholar.google.com">A Compositional Automatic Music Transcription System for Computer-synthesized Music</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:q7i8KOrlg_kJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6060196</url>
        <title status="complete" source="scholar.google.com">Study on accurate estimation of musical information in musical performance</title>
        <author status="complete" source="scholar.google.com">N Funakoshi, T Tanaka</author>
        <proceeding status="partial" source="scholar.google.com">SICE Annual Conference (SICE), 2011 &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Although the recent research in the field of Automatic music transcription has been active, there are still outstanding problems. Amongst, we focused on note-length and aim to estimate note-length exactly. In the past, Fourier or Wavelet transform are used to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Sk7vLW1qpAsJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.184.7941&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">OVERLAPPED EVENT-NOTE SEPARATION BASED ON PARTIALS AMPLITUDE AND PHASE ESTIMATION FOR POLYPHONIC MUSIC TRANSCRIPTION</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.184.7941&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="partial" source="scholar.google.com">JJ Carabias-Orti, P Vera-Candeas, N Ruiz-Reyes&#8230;</author>
        <year>2009</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We propose a discriminative model for polyphonic music transcription that deals with the well-known overlapped partial problem by taking into account the instrument envelope pattern for each note. The process to obtain the music scene-adaptive envelope ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:D9-LkvBnjdQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:D9-LkvBnjdQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15316012190705377039&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <title status="complete" source="scholar.google.com">6FKRRORI &amp;RPSXWLQJ 1DWLRQDO8QLYHUVLW\ RI 6LQJDSRUH^\ LQMXQ ZDQJ\ HG\ KVX# FRPS QXV HGX VJ</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:SzZzqC4R6hQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5336070</url>
        <title status="complete" source="scholar.google.com">Note Recognition of Polyphonic Music Based on Timbre Model</title>
        <author status="complete" source="scholar.google.com">L Shi, J Zhang, M Li</author>
        <proceeding status="partial" source="scholar.google.com">Intelligent Human-Machine Systems and &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Note recognition has been studied for many years, and current techniques are still not accurate and robust. Partial overlapping is one of the most difficult problems, all algorithms have trouble in processing the shared partial. In this paper, we analyze the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:U2Bm4q0oY_sJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18114366853507080275&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://www.springerlink.com/index/9860642482586476.pdf</url>
        <title status="complete" source="scholar.google.com">Automatic music transcription based on wavelet transform</title>
        <pdf>http://rahati.mshdiau.ac.ir/docs/azizi-faez-rezaeeian-rahati-ICIC2009.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Azizi, K Faez, A Delui, S Rahati</author>
        <proceeding status="partial" source="scholar.google.com">Emerging Intelligent Computing &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract. In this paper, we introduce a method which uses a note model and signal post processing for a musical instrument to make a piece of music. one of the important issues in note transcription is extraction of multiple pitches. Most of the examined methods face ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:a0Gc_4CXsmYJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7400143718023184747&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://www.cqvip.com/qk/95033x/201013/34734262.html</url>
        <title status="complete" source="scholar.google.com">åºäºé³è²æ¨¡åçå¤åºé¢ä¼°è®¡æ¹æ³</title>
        <author status="complete" source="scholar.google.com">ç³ç«æ°ï¼ å¼ ä¿æ</author>
        <proceeding status="complete" source="scholar.google.com">è®¡ç®æºå·¥ç¨ä¸è®¾è®¡</proceeding>
        <year>2010</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">ä¸ºäºææè§£å³å¤è°é³ä¹æ³é³éå é®é¢, æåºäºä¸ç§åºäºé³è²æ¨¡åçå¤åºé¢ä¼°è®¡æ¹æ³. åæäºé³è²çæ¶é¢ç¹å¾, æåºè¡°éåç»´æé¶æ®µçè°æ³¢ç»æç¨³å®å¯é , éååºé¢ä¼°è®¡. æ ¹æ®è°æ³¢å¹éçåå¼ºåº¦ç­éåéåºé¢, éç¨é¢è°±è¿­ä»£å é¤çæ¹æ³ç¡®å®ç¡®ååºé¢. ä¾æ®æå°è°æ³¢ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:kNzoiv2TkN8J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16109538584270789776&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://dialnet.unirioja.es/servlet/tesis?codigo=20907</url>
        <title status="complete" source="scholar.google.com">Computationally efficient methods for polyphonic music transcription</title>
        <pdf>http://rua.ua.es/dspace/bitstream/10045/18326/1/Tesis_Pertusa.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Pertusa-IbÃ¡Ã±ez</author>
        <year>2010</year>
        <source>dialnet.unirioja.es</source>
        <snippet status="partial" source="scholar.google.com">Resumen: Automatic music transcription is a music information retrieval (MIR) task which involves many different disciplines, such as audio signal processing, machine learning, computer science, psychoacoustics and music perception, music theory, and music ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2JJri16MFxAJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1159549766765089496&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://www.iit.upcomillas.es/pfc/resumenes/4c27a624eb042.pdf</url>
        <title status="partial" source="scholar.google.com">CODIFICACIÃN AUTOMÃTICA DE MELODÃAS Y APLICACIÃN DE TÃCNICAS DE CLUSTERING PARA EL ANÃLISIS DE ARCHIVOS DE AUDIO &#8230;</title>
        <pdf>http://www.iit.upcomillas.es/pfc/resumenes/4c27a624eb042.pdf</pdf>
        <author status="complete" source="scholar.google.com">MMR Calvo</author>
        <proceeding status="complete" source="scholar.google.com">iit.upcomillas.es</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Always was heard referenced to art that&quot; no accounting for taste is nothing in writing&quot;, but when you study the popularity ratings of some songs, the conclusion is that some composers and musicians have had an extraordinary ability to generate melodies ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:W-f1qAxPts8J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:W-f1qAxPts8J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14967237327405311835&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="23">
    <url>http://dl.acm.org/citation.cfm?id=1572011</url>
    <title status="complete" source="scholar.google.com">CompositeMap: a novel framework for music similarity measure</title>
    <pdf>http://dblab.cs.nccu.edu.tw/presentation../981210/CompositeMap.pdf</pdf>
    <author status="complete" source="scholar.google.com">B Zhang, J Shen, Q Xiang, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 32nd &#8230;</proceeding>
    <year>2009</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Measure Bingjun Zhang1, Jialie Shen2, Qiaoliang Xiang1, Ye Wang1 1School of Computing,National University of Singapore 2School of Information Systems, Singapore ManagementUniversity 1{bingjun,xiangqiaoliang,wangye}@comp.nus.edu.sg, 2jlshen@smu.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:Jsb4vgd7pmwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>16</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=7829080275429148198&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>21</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=7829080275429148198&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="21">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1631303</url>
        <title status="complete" source="scholar.google.com">Comprehensive query-dependent fusion using regression-on-folksonomies: a case study of multimodal music search</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2009_Comprehensive_Query-Dependent_Fusion_using_Regression-on-Folksonomies-A_Case_Study_of_Multimodal_Music_Search.pdf</pdf>
        <author status="complete" source="scholar.google.com">B Zhang, Q Xiang, H Lu, J Shen, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 17th ACM &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The combination of heterogeneous knowledge sources has been widely regarded as an effective approach to boost retrieval accuracy in many information retrieval domains. While various technologies have been recently developed for information retrieval, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Vz0tAKThYoYJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>11</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9683550243293838679&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9683550243293838679&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1993038</url>
        <title status="complete" source="scholar.google.com">Exploring the music similarity space on the web</title>
        <author status="complete" source="scholar.google.com">M Schedl, T Pohle, P Knees, G Widmer</author>
        <proceeding status="partial" source="scholar.google.com">ACM Transactions on &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This article comprehensively addresses the problem of similarity measurement between music artists via text-based features extracted from Web pages. To this end, we present a thorough evaluation of different term-weighting strategies, normalization ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BH3f10dpo8AJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13881054233771343108&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13881054233771343108&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://dl.acm.org/citation.cfm?id=1874006</url>
        <title status="complete" source="scholar.google.com">Large-scale music tag recommendation with explicit multiple attributes</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2010_Large-scale_Music_Tag_Recommendation_with_Explicit_Multiple_Attributes.pdf</pdf>
        <author status="partial" source="scholar.google.com">Z Zhao, X Wang, Q Xiang, AM Sarroff, Z Li&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Social tagging can provide rich semantic information for large-scale retrieval in music discovery. Such collaborative intelligence, however, also generates a high degree of tags unhelpful to discovery, some of which obfuscate critical information. Towards ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tjNWBquXKB8J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2245211175045706678&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2245211175045706678&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.biomedcentral.com/1471-2105/11/423/</url>
        <title status="complete" source="scholar.google.com">Ranked retrieval of Computational Biology models</title>
        <pdf>http://www.biomedcentral.com/1471-2105/11/423/</pdf>
        <author status="partial" source="scholar.google.com">R Henkel, L Endler, A Peters, N Le NovÃ¨re&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">BMC &#8230;</proceeding>
        <year>2010</year>
        <source>biomedcentral.com</source>
        <snippet status="partial" source="scholar.google.com">Background The study of biological systems demands computational support. If targeting a biological problem, the reuse of existing computational models can save time and effort. Deciding for potentially suitable models, however, becomes more challenging with the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:78by5gGaKmsJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7722153844040451823&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>9</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7722153844040451823&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://dl.acm.org/citation.cfm?id=1835555</url>
        <title status="complete" source="scholar.google.com">Effective music tagging through advanced statistical modeling</title>
        <pdf>http://research.microsoft.com/en-us/um/people/xshua/publications/pdf/2010_sigir_musictagging.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Shen, W Meng, S Yan, HH Pang, X Hua</author>
        <proceeding status="partial" source="scholar.google.com">Proceeding of the 33rd &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music information retrieval (MIR) holds great promise as a technology for managing large music archives. One of the key components of MIR that has been actively researched into is music tagging. While significant progress has been achieved, most of the existing ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7YFYRdl9o-kJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16835438204165849581&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16835438204165849581&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://dl.acm.org/citation.cfm?id=1631474</url>
        <title status="complete" source="scholar.google.com">CompositeMap: a novel music similarity measure for personalized multimodal music search</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2009_CompositeMap-A_Novel_Music_Similarity_Measure_for_Personalized_Multimodal_Music_Search.pdf</pdf>
        <author status="complete" source="scholar.google.com">B Zhang, Q Xiang, Y Wang, J Shen</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 17th ACM &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract How to measure and model the similarity between different music items is one of the most fundamental yet challenging research problems in music information retrieval. This paper demonstrates a novel multimodal and adaptive music similarity measure ( ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:hpOvLkNk2JoJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>14</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11157778316519248774&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11157778316519248774&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://www.springerlink.com/index/K6112U27W8235M32.pdf</url>
        <title status="complete" source="scholar.google.com">Personalized video similarity measure</title>
        <pdf>http://posgrado.escom.ipn.mx/biblioteca/Personalized%20video%20similarity%20measure.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Shen, Z Cheng</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia systems</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract As an effective technique to manage and explore large scale of video collections, personalized video search has received great attentions in recent years. One of the key problems in the related technique development is how to design and evaluate the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:SoBYnIDPEpoJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11102164182707503178&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11102164182707503178&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://www.cp.jku.at/research/papers/Schedl_Knees_amr_2011.pdf</url>
        <title status="complete" source="scholar.google.com">Personalization in Multimodal Music Retrieval</title>
        <pdf>http://www.cp.jku.at/research/papers/Schedl_Knees_amr_2011.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Schedl, P Knees</author>
        <proceeding status="complete" source="scholar.google.com">Proc. AMR</proceeding>
        <year>2011</year>
        <source>cp.jku.at</source>
        <snippet status="partial" source="scholar.google.com">Abstract. This position paper provides an overview of current research endeavors and existing solutions in multimodal music retrieval, where the term âmultimodalâ relates to two aspects. The first one is taking into account the music context of a piece of music or an ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:NYKzUtOiYAYJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:NYKzUtOiYAYJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=459546190501085749&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=459546190501085749&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://drops.dagstuhl.de/opus/volltexte/2012/3470/</url>
        <title status="complete" source="scholar.google.com">User-Aware Music Retrieval</title>
        <pdf>http://drops.dagstuhl.de/opus/volltexte/2012/3470/pdf/9.pdf</pdf>
        <author status="partial" source="scholar.google.com">M Schedl, S Stober, E GÃ³mez, N Orio&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimodal Music &#8230;</proceeding>
        <year>2012</year>
        <source>drops.dagstuhl.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract Personalized and user-aware systems for retrieving multimedia items are becoming increasingly important as the amount of available multimedia data has been spiraling. A personalized system is one that incorporates information about the user into its data ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:XogDQo61VyMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2546703736898816094&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2546703736898816094&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://dl.acm.org/citation.cfm?id=2071949</url>
        <title status="complete" source="scholar.google.com">Document dependent fusion in multimodal music retrieval</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2011_Document_Dependent_Fusion_in_Multimodal_Music_Retrieval.pdf</pdf>
        <author status="complete" source="scholar.google.com">Z Li, B Zhang, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 19th ACM international &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we propose a novel multimodal fusion framework, document dependent fusion (DDF), which derives the optimal combination strategy for each individual document in the fusion process. For each document, we derive a document weight vector ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:JyU9BhZI5j4J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4532389334426133799&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6103017</url>
        <title status="complete" source="scholar.google.com">An Interactive Music Learning System in Ensemble Performance Class</title>
        <author status="complete" source="scholar.google.com">K Takano, S Sasaki</author>
        <proceeding status="partial" source="scholar.google.com">Broadband and Wireless Computing, &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract It is a significant research area in computer-assisted music learning to develop interactive learning materials to cultivate learner&#39;s performing abilities of musical instruments. Especially in the ensemble lesson at school, it is pointed out that not only the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:z3l8C9DS4_sJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18150582714220968399&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5580673</url>
        <title status="complete" source="scholar.google.com">Dictionary based inverted index for music information retrieval</title>
        <author status="complete" source="scholar.google.com">XH Yang, QC Chen, XL Wang</author>
        <proceeding status="partial" source="scholar.google.com">Machine Learning and &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract With the rapid progress in data storage and communication technology, there has been an explosive growth of music information. As the traditional metadata-based search engine can not provide natural and intuitive way to retrieve music, content-based music ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:93d5DN3i9sMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://137.132.14.55/handle/10635/20949</url>
        <title status="complete" source="scholar.google.com">Adaptive multimodal fusion based similarity measures in music information retrieval</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/20949/ZhangBJ.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">Z BINGJUN</author>
        <year>2010</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">In the field of music information retrieval (MIR), one fundamental research problem is the measuring of the similarity between music documents. Based on a viable similarity measure, MIR systems can be made more effective to help users retrieve relevant music information. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9FjpHoxTlRIJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1339068325491726580&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://137.132.14.55/handle/10635/19070</url>
        <title status="complete" source="scholar.google.com">Utilizing EEG Signal in Music Information Retrieval</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/19070/ZhaoWEI.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">Z WEI</author>
        <year>2010</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">Despite significant progresses in the field of music information retrieval (MIR), grand challenges such as the intention gap and the semantic gap still exist. Inspired by the current successes in the Brain Computer Interface (BCI), how to utilize electroencephalography ( ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:646kKzTsLiAJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2319050566957043435&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://hub.hku.hk/handle/10722/131820</url>
        <title status="complete" source="scholar.google.com">Third-order tensor decomposition for search in social</title>
        <pdf>http://hub.hku.hk/bitstream/10722/131820/1/FullText.pdf?accept=1</pdf>
        <author status="complete" source="scholar.google.com">B Biï¼ é­å½¬</author>
        <proceeding status="complete" source="scholar.google.com">SIAM Review</proceeding>
        <year>2009</year>
        <source>hub.hku.hk</source>
        <snippet status="partial" source="scholar.google.com">1.1 Background We are witnessing the recent proliferation of the World Wide Web. As online popularity increased, an increasing number of electronic documents started appearing on the web. Google announced that its crawler found 1 trillion unique URLs and that the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7lPTPNlJ5_4J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://ismir2012.ismir.net/event/papers/385-ismir-2012.pdf</url>
        <title status="complete" source="scholar.google.com">PUTTING THE USER IN THE CENTER OF MUSIC INFORMATION RETRIEVAL</title>
        <pdf>http://ismir2012.ismir.net/event/papers/385-ismir-2012.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Schedl, A Flexer</author>
        <proceeding status="complete" source="scholar.google.com">ismir2012.ismir.net</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Personalized and context-aware music retrieval and recommendation algorithms ideally provide music that perfectly fits the individual listener in each imaginable situation and for each of her information or entertainment need. Although first steps ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:YjALPQTcqDEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:YjALPQTcqDEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3578351814707982434&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3578351814707982434&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://www.springerlink.com/index/J816R27044174R55.pdf</url>
        <title status="complete" source="scholar.google.com">Effective bitmap indexing for non-metric similarities</title>
        <author status="partial" source="scholar.google.com">C Jensen, E Mungure, T Pedersen, K SÃ¸rensen&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Database and Expert &#8230;</proceeding>
        <year>2010</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">An increasing number of applications include recommender systems that have to perform search in a non-metric similarity space, thus creating an increasing demand for efficient yet flexible indexing techniques to facilitate similarity search. This demand is further fueled by ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tLvpSxbb_x4J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2233744829008427956&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2233744829008427956&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://dl.acm.org/citation.cfm?id=2043705</url>
        <title status="complete" source="scholar.google.com">Probabilistic indexing of media sequences</title>
        <author status="complete" source="scholar.google.com">J Shen, M Wang, S Yan, Q Tian</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the Third International &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Accurate and fast nearest neighbor search is often required in applications involving media sequences, such as duplicate detection in video collections, music retrieval in digital libraries, and event discovery in streaming documents. Among various related ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:d05zk1V1wT0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4449966917225238135&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://www.sciencedirect.com/science/article/pii/S016516841200326X</url>
        <title status="complete" source="scholar.google.com">A query by humming system based on locality sensitive hashing indexes</title>
        <author status="complete" source="scholar.google.com">Q Wang, Z Guo, J Guo, G Liu</author>
        <proceeding status="complete" source="scholar.google.com">Signal Processing</proceeding>
        <year>2012</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">Abstract Recently developed query by humming (QBH) system, which uses the humming clip to find the wanted song, has become a hot topic in the area of music retrieval. At present, the challenging issue is how to quickly and accurately find the song in a large scale ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:hWsu3YHBNz0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://dl.acm.org/citation.cfm?id=2348346</url>
        <title status="complete" source="scholar.google.com">Modeling concept dynamics for large scale music search</title>
        <author status="complete" source="scholar.google.com">J Shen, HH Pang, M Wang, S Yan</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the 35th international ACM SIGIR &#8230;</proceeding>
        <year>2012</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Continuing advances in data storage and communication technologies have led to an explosive growth in digital music collections. To cope with their increasing scale, we need effective Music Information Retrieval (MIR) capabilities like tagging, concept search ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:xMPb53iszVQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://www.tandfonline.com/doi/abs/10.1080/09298215.2012.749919</url>
        <title status="complete" source="scholar.google.com">Algorithmic Prediction of Inter-song Similarity in Western Popular Music</title>
        <author status="partial" source="scholar.google.com">A Novello, S van de Par, MMF McKinney&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Journal of New Music &#8230;</proceeding>
        <year>2013</year>
        <source>Taylor &amp; Francis</source>
        <snippet status="partial" source="scholar.google.com">Abstract We investigate a method for automatic extraction of inter-song similarity for songs selected from several genres of Western popular music. The specific purpose of this approach is to evaluate the predictive power of different feature extraction sets based on ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="24">
    <url>http://dl.acm.org/citation.cfm?id=1027554</url>
    <title status="complete" source="scholar.google.com">A framework for robust and scalable audio streaming</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/5.Error_Robust_Audio_Streaming/2004_A_Framework_for_Robust_and_Scalable_Audio_Streaming.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Wang, W Huang, J Korhonen</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th annual ACM &#8230;</proceeding>
    <year>2004</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. A Framework for Robust and Scalable Audio Streaming Ye Wang, WendongHuang, Jari Korhonen School of Computing, National University of Singapore{wangye, huangwd, jari}@comp.nus.edu.sg ABSTRACT We ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:A8yetuDRh0QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>17</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=4938146279501777923&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>19</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=4938146279501777923&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="19">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1065994</url>
        <title status="complete" source="scholar.google.com">Power-efficient streaming for mobile terminals</title>
        <pdf>https://www-new.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2005_Power-Efficient_Streaming_for_Mobile_Terminals.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Korhonen, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the international workshop on &#8230;</proceeding>
        <year>2005</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Wireless Network Interface (WNI) is one of the most critical components for power efficiency in multimedia streaming to mobile devices. A common strategy to save power is to switch WNI to active mode only when network activity is expected. In streaming systems, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:AdDmBoq27LkJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13397283695457914881&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>23</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13397283695457914881&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1065993</url>
        <title status="complete" source="scholar.google.com">Weather forecasting: predicting performance for streaming video over wireless LANs</title>
        <pdf>http://web.cs.wpi.edu/~claypool/papers/weather/wlan_paper.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Li, F Li, M Claypool, R Kinicki</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; systems support for digital audio and &#8230;</proceeding>
        <year>2005</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The growth of wireless LANs has brought the expectation for high-bitrate streaming video to wireless PCs. However, it remains unclear how wireless channel characteristics impact the quality of streaming video sent over wireless LANs. This paper presents results ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_3ZVr6Zi4yYJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>20</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2802191861200287487&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>21</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2802191861200287487&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://dl.acm.org/citation.cfm?id=1180845</url>
        <title status="complete" source="scholar.google.com">Modelling dependency in multimedia streams</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.72.4690&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">A Eichhorn</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 14th annual ACM international &#8230;</proceeding>
        <year>2006</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Expressing and analysing data dependency in multimedia streams is promising, since content-aware policies at a transport level would benefit from such services. In this paper we present a format-independent dependency model aimed at specifying, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:lJuv1HIAlHIJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8256224510071511956&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>19</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8256224510071511956&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1521388</url>
        <title status="complete" source="scholar.google.com">Optimization of source and channel coding for voice over IP</title>
        <pdf>http://www.switzernet.com/1/people/emin-gabrielyan/060708-thesis-ref/papers/Huang05.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Huang, J Korhonen, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2005. &#8230;</proceeding>
        <year>2005</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Voice over Internet protocol (VoIP) applications must typically choose a tradeoff between the bits allocated for forward error correcting (FEC) and that for the source coding to achieve the best speech quality at a given packet loss rate. In this paper, we present a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:qS7qTafhnVsJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>28</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6601680737477996201&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>18</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6601680737477996201&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://www.springerlink.com/index/18231153T6838HU7.pdf</url>
        <title status="complete" source="scholar.google.com">Generic forward error correction of short frames for IP streaming applications</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/5.Error_Robust_Audio_Streaming/2006_Generic_Forward_Error_Correction_of_Short_Frames_for_IP_Streaming_Applications.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Korhonen, Y Huang, Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Tools and Applications</proceeding>
        <year>2006</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract If the frame size of a multimedia encoder is small, Internet Protocol (IP) streaming applications need to pack many encoded media frames in each Real-time Transport Protocol (RTP) packet to avoid unnecessary header overhead. The generic forward error ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:h-flwEfZJiIJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2460893148606752647&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>9</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2460893148606752647&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1518060</url>
        <title status="complete" source="scholar.google.com">A perception-aware low-power software audio decoder for portable devices</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.4492&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="partial" source="scholar.google.com">S Chakraborty, Y Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Embedded Systems for &#8230;</proceeding>
        <year>2005</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a new software audio decoder for processors supporting multiple discrete voltage-frequency operating points. The proposed decoding scheme allows the user to switch between multiple output quality levels, where each level is associated with ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:oD7YdVw5A6MJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11746295322389266080&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11746295322389266080&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://dl.acm.org/citation.cfm?id=1101207</url>
        <title status="complete" source="scholar.google.com">Power-aware bandwidth and stereo-image scalable audio decoding</title>
        <pdf>http://www.mirlab.org/conference_papers/International_Conference/ACM%202005/docs/mm291.pdf</pdf>
        <author status="complete" source="scholar.google.com">W Huang, Y Wang, S Chakraborty</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 13th annual ACM &#8230;</proceeding>
        <year>2005</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a new workload-scalable audio decoding scheme that would enable users to control the tradeoff between playback quality and power consumption in battery-powered portable audio players. Our objective is to give users a control at the decoder ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:qosghxX2cK4J:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12569817132312857514&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12569817132312857514&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1465888</url>
        <title status="complete" source="scholar.google.com">Progressive scrambling for MP3 audio</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.4568&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">WG Fu, WQ Yan, MS Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">Circuits and Systems, 2005. &#8230;</proceeding>
        <year>2005</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Audio scrambling can be employed in audio distribution for the purpose of guaranteeing the confidentiality. Electronic commerce in audio products would be facilitated by the development of solutions which can ensure security/privacy, have efficiency in the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:cUFcUOY8EK8J:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12614649516153127281&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12614649516153127281&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://www.ewp.rpi.edu/hartford/~rhb/cs_seminar_2005/SessionB3/lipka.pdf</url>
        <title status="complete" source="scholar.google.com">An analysis of error handling techniques in voice over IP</title>
        <pdf>http://www.ewp.rpi.edu/hartford/~rhb/cs_seminar_2005/SessionB3/lipka.pdf</pdf>
        <author status="complete" source="scholar.google.com">MJ Lipka</author>
        <proceeding status="complete" source="scholar.google.com">Proceedings of 21st Computer Science Seminar</proceeding>
        <year>2005</year>
        <source>ewp.rpi.edu</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT The use of Voice over IP (VoIP) has been growing in popularity, but unlike its wired circuit-switched telephone network predecessor, this technology runs over a packet-switched network. It is therefore vulnerable to changes in underlying network conditions ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ItsHfQzTMFsJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:ItsHfQzTMFsJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6570983906924288802&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6570983906924288802&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4657456</url>
        <title status="complete" source="scholar.google.com">Progressive Audio Scrambling in Compressed Domain</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.151.6649&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">WQ Yan, WG Fu, MS Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; , IEEE Transactions on</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Audio scrambling can be employed to ensure confidentiality in audio distribution. We first describe scrambling for raw audio using the discrete wavelet transform (DWT) first and then focus on MP3 audio scrambling. We perform scrambling based on a set of keys ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ZRBv2WVsF7oJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13409305600214372453&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13409305600214372453&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284586</url>
        <title status="complete" source="scholar.google.com">Pop music beat detection in the huffman coded domain</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2007_Pop_Music_Beat_Detection_in_the_Huffman_Coded_Domain.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Zhu, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2007 IEEE International &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a novel beat detector that operates in the Huffman coded domain of a MP3 audio bitstream. We seek to answer two main questions. First, whether it is possible to extract beats without even partial decoding of a MP3 audio. Second, how to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:4gW-SQhyFW0J:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7860314104567563746&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7860314104567563746&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1633700</url>
        <title status="complete" source="scholar.google.com">Scalable audio coding for compression and loss resilient streaming</title>
        <author status="complete" source="scholar.google.com">M Sandler, D Black</author>
        <proceeding status="partial" source="scholar.google.com">Vision, Image and Signal Processing, IEE &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Current popular Internet audio streaming solutions impose a division between source coding (provided, for example, by MPEG layer III) and channel coding, typically accomplished in the server by means of packet re-transmission. A novel joint source and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2GBMYM4klCoJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3068117714943631576&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3068117714943631576&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://www.springerlink.com/index/FT671731J0403662.pdf</url>
        <title status="complete" source="scholar.google.com">A Survey of Music Structure Analysis Techniques for Music Applications</title>
        <author status="complete" source="scholar.google.com">N Maddage, H Li, M Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">Recent Advances in Multimedia Signal &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Music carries multilayer information which forms different structures. The information embedded in the music can be categorized into time information, harmony/melody, music regions, music similarities, song structures and music semantics. In this chapter, we first ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3vo8SvZRZuUJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16529989600559299294&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://scholarbank.nus.edu.sg/handle/10635/16660</url>
        <title status="complete" source="scholar.google.com">Perception-aware low-power audio processing techniques for portable devices</title>
        <pdf>http://scholarbank.nus.edu.sg/bitstream/handle/10635/16660/HuangWendong_PhD_thesis.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">H WENDONG</author>
        <year>2009</year>
        <source>scholarbank.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">In this thesis, we study perception-aware low power audio processing techniques for portable device. These works are mainly motivated by the fact that the audio decoding application is a significant source of energy consumption in context of portable devices. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BzEVahxPeIQJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9545466393669218567&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8036900&amp;id=qv73AQAAEBAJ&amp;oi=fnd&amp;printsec=abstract</url>
        <title status="complete" source="scholar.google.com">Device and a method of playing audio clips</title>
        <author status="complete" source="scholar.google.com">Y Wang, W Huang, S Chakroborty</author>
        <proceeding status="complete" source="scholar.google.com">US Patent 8,036,900</proceeding>
        <year>2011</year>
        <source>Google Patents</source>
        <snippet status="partial" source="scholar.google.com">A device for playing audio clips, a method of playing audio clips and a data storage medium having stored thereon computer code means for instructing a computer system to execute a method of playing audio clips. The device comprising a processor scalable in voltage, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:qH8_k2TalooJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9986409349242650536&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://137.132.14.55/handle/10635/16008</url>
        <title status="complete" source="scholar.google.com">Packet prioritizing and delivering for multimedia streaming</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/16008/2007%20-%20Nguyen%20Vu%20Thanh%20-%20PhD%20thesis.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">NVU THANH</author>
        <year>2008</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">In this thesis, we investigated the problems of prioritizing and delivering packets in multimedia streaming. Under a lossy network, the sender has to decide which packets are to be further protected from losses, which packets are to be sent, how to send them, and ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:P9WDqejwpbYJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13161190368127210815&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://www.opus.ub.uni-erlangen.de/opus/volltexte/2008/1192/</url>
        <title status="partial" source="scholar.google.com">Format independence provision of audio and video data in multimedia database management systems Bereitstellung der FormatunabhÃ¤ngigkeit von Audio- &#8230;</title>
        <pdf>http://www.opus.ub.uni-erlangen.de/opus/volltexte/2008/1192/</pdf>
        <author status="complete" source="scholar.google.com">M Suchomski</author>
        <proceeding status="complete" source="scholar.google.com">opus.ub.uni-erlangen.de</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Since late 90s there is a noticeable revolution in the consumption of multimedia data being analogical to the electronic data processing revolution in 80s and 90s. The multimedia revolution covers different aspects such as multimedia production, storage, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:-7lY1r3NPH4J:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://d-nb.info/991601661/34</url>
        <title status="complete" source="scholar.google.com">Format Independence Provision of Audio and Video Data in Multimedia Database Management Systems</title>
        <pdf>http://d-nb.info/991601661/34</pdf>
        <author status="complete" source="scholar.google.com">M Suchomski</author>
        <year>2008</year>
        <source>d-nb.info</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Since late 90s there is a noticeable revolution in the consumption of multimedia data being analogical to the electronic data processing revolution in 80s and 90s. The multimedia revolution covers different aspects such as multimedia production, storage, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:DbqE6G4-pZYJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:DbqE6G4-pZYJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://dl.acm.org/citation.cfm?id=1858493</url>
        <title status="complete" source="scholar.google.com">Improving audio files availability in file sharing networks</title>
        <author status="partial" source="scholar.google.com">M Dahia, G Ramalho, F Trinta, G Cabral&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the XV &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract File sharing networks are one of the most popular Internet applications nowadays. Despite its success, these networks have two main issues: the ineffectiveness in downloading content that few users in the network have and the high incidence of getting ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:wdK5AuKRgeUJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="25">
    <url>http://dl.acm.org/citation.cfm?id=1290156</url>
    <title status="complete" source="scholar.google.com">Effective use of multimedia for computer-assisted musical instrument tutoring</title>
    <pdf>http://strum.googlecode.com/svn-history/r41/trunk/Research/CAMIT.pdf</pdf>
    <author status="complete" source="scholar.google.com">G Percival, Y Wang, G Tzanetakis</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the international &#8230;</proceeding>
    <year>2007</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Graham Percival Departments of Computer Science and Music University of Victoria BritishColumbia, Canada gperciva@uvic.ca Ye Wang School of Computing (SoC) NationalUniversity of Singapore Singapore 117543 wangye@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:sSgpS4WrYA0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>21</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=963958909237274801&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>18</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=963958909237274801&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="18">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1979018</url>
        <title status="complete" source="scholar.google.com">PossessedHand: techniques for controlling human hands using electrical muscles stimuli</title>
        <author status="complete" source="scholar.google.com">E Tamaki, T Miyaki, J Rekimoto</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the 2011 annual conference on &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract If a device can control human hands, the device can be useful for HCI and tangible application&#39;s output. To aid the controlling of finger movement, we present PossessedHand, a device with a forearm belt that can inform when and which fingers should be moved. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:cL0pXsdLxbYJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13170016004965252464&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=18</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1952245</url>
        <title status="complete" source="scholar.google.com">Who makes what sound?: supporting real-time musical improvisations of electroacoustic ensembles</title>
        <author status="complete" source="scholar.google.com">T Merritt, W Kow, C Ng, K McGee, L Wyse</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 22nd &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Coordination between ensembles of improvising electroacoustic musicians is a special case of the larger HCI problem of coordinating joint, real-time activity; one that involves some interesting additional and different challenges. This paper reports on ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ht3YeD2TPggJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=594074093089250694&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=18</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.178.2918&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Generating targeted rhythmic exercises for music students with constraint satisfaction programming</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.178.2918&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">G Percival, T Anders, G Tzanetakis</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 2008 International &#8230;</proceeding>
        <year>2008</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Generating technical exercises for various levels of playing ability is important for any instrument method book. Writing exercises by hand can be quite tedious, and severely limits the number of exercises which could be created. This is particularly ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tONQ5cOF4-YJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:tONQ5cOF4-YJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16637288524849144756&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16637288524849144756&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=18</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.editlib.org/view/33012</url>
        <title status="complete" source="scholar.google.com">Effectiveness of telepresence learning environment for opera singing: First results from a case study</title>
        <pdf>http://www.editlib.org/p/33012/proceeding_33012.pdf</pdf>
        <author status="partial" source="scholar.google.com">T Rojas-Rajs, F Alpiste, P Lorente, F Iglesias&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">World Conference on E &#8230;</proceeding>
        <year>2009</year>
        <source>editlib.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This brief paper shows the first results of a case study on a remote learning telepresence environment, specialized on lyric singing at higher education level. Opera eLearning study uses high bandwidth for delivering quality audio and video experience, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:T3A5LKgp_pUJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10808121958008975439&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10808121958008975439&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=18</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://www.emeraldinsight.com/journals.htm?articleid=1875554&amp;show=abstract</url>
        <title status="complete" source="scholar.google.com">Impression-oriented music courseware and its application in elementary schools</title>
        <author status="partial" source="scholar.google.com">S Sasaki, K Watagoshi, K Takano&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; and Smart Education</proceeding>
        <year>2010</year>
        <source>emeraldinsight.com</source>
        <snippet status="partial" source="scholar.google.com">PurposeâThe purpose of this paper is to present the design and implementation of music courseware that features a music search system that uses impression keywords. The paper applies the courseware to Kansei (sensibility) development for elementary and junior high ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:xnPWPVeicUgJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5220131938682434502&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5220131938682434502&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=18</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://socrs.cdm.depaul.edu/2011/program/SOCRS-11.pdf#page=55</url>
        <title status="complete" source="scholar.google.com">Leveraging Game Design to Enhance Motivation and Learning</title>
        <pdf>http://socrs.cdm.depaul.edu/2011/program/SOCRS-11.pdf#page=55</pdf>
        <author status="complete" source="scholar.google.com">B Grey, A Alkhafaji</author>
        <proceeding status="complete" source="scholar.google.com">General Co-Chairs</proceeding>
        <year>2011</year>
        <source>socrs.cdm.depaul.edu</source>
        <snippet status="partial" source="scholar.google.com">Abstract Video games can be highly engaging and motivating environments whose success is dependent upon teaching players how to play well. For many years, researchers have been exploring how to leverage this motivation for educational purposes. This article ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Q_dZeMALWiYJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Q_dZeMALWiYJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2763534242626008899&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2763534242626008899&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=18</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://tuprints.ulb.tu-darmstadt.de/2797/</url>
        <title status="complete" source="scholar.google.com">Sensor-Based Feedback for Piano Pedagogy</title>
        <pdf>http://tuprints.ulb.tu-darmstadt.de/2797/</pdf>
        <author status="complete" source="scholar.google.com">A Hadjakos</author>
        <year>2011</year>
        <source>tuprints.ulb.tu-darmstadt.de</source>
        <snippet status="partial" source="scholar.google.com">Recent advances in sensor technology provide new opportunities for applications that utilize the user&#39;s movement as an input source. This thesis focuses on movement analysis, which is a sub-area of a larger field concerned with the interpretation of sensor signals of human ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:NK7pUurz2xMJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1431005494359207476&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=18</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://www.mitpressjournals.org/doi/abs/10.1162/COMJ_a_00119</url>
        <title status="complete" source="scholar.google.com">A High-Fidelity Orchestra Simulator for Individual Musicians&#39; Practice</title>
        <pdf>http://130.102.44.246/journals/computer_music_journal/v036/36.2.olmos.pdf</pdf>
        <author status="partial" source="scholar.google.com">A Olmos, N Bouillot, T Knight, N Mabire, J Redel&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Computer Music &#8230;</proceeding>
        <year>2012</year>
        <source>MIT Press</source>
        <snippet status="partial" source="scholar.google.com">We developed the Open Orchestra system to provide individual musicians with a high-fidelity experience of ensemble rehearsal or performance, combined with the convenience and flexibility of solo study. This builds on the theme of an immersive orchestral simulator ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KputC2FCj3gJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8687235190796163882&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6266249</url>
        <title status="complete" source="scholar.google.com">Real-Time Pitch Training System for Violin Learners</title>
        <pdf>http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Workshops/data/4729a163.pdf</pdf>
        <author status="partial" source="scholar.google.com">JH Wang, SA Wang, WC Chen&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper specifically targets violin learners who are working on their pitch accuracy. We employ a pitch tracking algorithm to extract the pitch played. Through volume thresholding and region detection, only parts of frames are processed. So our system can ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gMp1DiddLkkJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5273254636025137792&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://strum.googlecode.com/svn/trunk/Report/Thesis.pdf</url>
        <title status="complete" source="scholar.google.com">Guitar Tuition Using Real-Time Pitch Analysis</title>
        <pdf>http://strum.googlecode.com/svn/trunk/Report/Thesis.pdf</pdf>
        <author status="complete" source="scholar.google.com">R Toner</author>
        <year>2009</year>
        <source>strum.googlecode.com</source>
        <snippet status="partial" source="scholar.google.com">Executive Summary This project aims to enhance the experience of individual guitar practice through virtual tuition. Daily practice is vital to a musician&#39;s progression and unfortunately the prospect of this sort of commitment has become a deterrent in the eyes of an easily bored ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Lugacuy3N_cJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Lugacuy3N_cJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17813909077241817134&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://pdf.aminer.org/000/287/524/cognitively_oriented_design_of_a_multimedia_system_to_learn_guitar.pdf</url>
        <title status="complete" source="scholar.google.com">Graeme Smith</title>
        <pdf>http://pdf.aminer.org/000/287/524/cognitively_oriented_design_of_a_multimedia_system_to_learn_guitar.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Johnston</author>
        <proceeding status="complete" source="scholar.google.com">pdf.aminer.org</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper we present software designed to help address problems encountered by beginning guitarists, using interactive software to find effective solutions to enhance the learning process. Software can be utilised to improve a player&#39;s ability to hear mistakes in ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Eh0tglG-JuIJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Eh0tglG-JuIJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16295921558972341522&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://essay.utwente.nl/61665/</url>
        <title status="complete" source="scholar.google.com">Drum assistant</title>
        <pdf>http://essay.utwente.nl/61665/1/MSc_A_Zanderink.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Zanderink</author>
        <year>2012</year>
        <source>essay.utwente.nl</source>
        <snippet status="partial" source="scholar.google.com">In this research, an application is designed for pupils that attend drumming lessons, on which they can practice their home study. Normally pupils study on their own drum at home without the assistance of a teacher. The added value of using this application is that they ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:U-N8lTIBbmMJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://www-apps.cs.colorado.edu/department/publications/theses/docs/bs/ransom_christofferson.pdf</url>
        <title status="complete" source="scholar.google.com">Digital Drum Tutor</title>
        <pdf>http://www-apps.cs.colorado.edu/department/publications/theses/docs/bs/ransom_christofferson.pdf</pdf>
        <author status="complete" source="scholar.google.com">R Christofferson</author>
        <proceeding status="complete" source="scholar.google.com">www-apps.cs.colorado.edu</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract This thesis investigates issues surrounding the design and implementation of a digital drum tutor. People have been developing digital tutors in all fields of study in an attempt to replace or complement human instructors. This drum tutor aims to replace a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tN8inIVJijYJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:tN8inIVJijYJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3930034463033188276&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://www.tandfonline.com/doi/abs/10.1080/10494820.2011.584322</url>
        <title status="complete" source="scholar.google.com">A telepresence learning environment for opera singing: distance lessons implementations over Internet2</title>
        <author status="partial" source="scholar.google.com">FA Penalba, T Rojas-Rajs, P Lorente, F Iglesias&#8230;</author>
        <year>2011</year>
        <source>Taylor &amp; Francis</source>
        <snippet status="partial" source="scholar.google.com">Abstract The Opera eLearning project developed a solution for opera singing distance lessons at the graduate level, using high bandwidth to deliver a quality audio and video experience that has been evaluated by singing teachers, chorus and orchestra directors, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:NJy5qMgukTsJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://dl.acm.org/citation.cfm?id=1979762</url>
        <title status="complete" source="scholar.google.com">Duet for solo piano: MirrorFugue for single user playing with recorded performances</title>
        <author status="complete" source="scholar.google.com">X Xiao, H Ishii</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 2011 annual conference extended &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract MirrorFugue is an interface that supports symmetric, real-time collaboration on the piano using spatial metaphors to communicate the hand gesture of collaborators. In this paper, we present an extension of MirrorFugue to support single-user interactions with ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:rZBgukwmaKYJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://vcu.sagepub.com/content/10/2/238.short</url>
        <title status="complete" source="scholar.google.com">Musical Goals, Graphical Lure and Narrative Drive: VisualAudio in Games</title>
        <author status="complete" source="scholar.google.com">J d&#39;EscrivÃ¡n, N Collins</author>
        <proceeding status="complete" source="scholar.google.com">Journal of Visual Culture</proceeding>
        <year>2011</year>
        <source>vcu.sagepub.com</source>
        <snippet status="partial" source="scholar.google.com">Abstract Musical computer games and their reward structures are transforming solitary and participative music making. Visuals in musical games tend to assume the role of music in video games as they become incidental to the gameplay or provide graphical aid for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:4t3T0OyiLy4J:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://www.mitpressjournals.org/doi/abs/10.1162/COMJ_a_00005</url>
        <title status="complete" source="scholar.google.com">Intune: A system to support an instrumentalist&#39;s visualization of intonation</title>
        <pdf>http://www.music.informatics.indiana.edu/~craphael/papers/CMJ_InTune_submission_final.pdf</pdf>
        <author status="complete" source="scholar.google.com">KA Lim, C Raphael</author>
        <proceeding status="complete" source="scholar.google.com">Computer Music Journal</proceeding>
        <year>2010</year>
        <source>MIT Press</source>
        <snippet status="partial" source="scholar.google.com">One of our most beloved music teachers emphasized the importance of âfacing the music,â by which he meant listening to recordings of our playing. As with the first hearing of one&#39;s voice on a recording, many of us were both surprised by and suspicious of this external ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3CNe6US_is8J:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14954975815331095516&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14954975815331095516&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=18</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://147.83.15.91/Doc/cols_new/contenidos/data/154749301109ProyectoTesis.docx</url>
        <title status="complete" source="scholar.google.com">OeL.</title>
        <pdf>http://147.83.15.91/Doc/cols_new/contenidos/data/154749301109ProyectoTesis.docx</pdf>
        <author status="complete" source="scholar.google.com">TR Rajs</author>
        <proceeding status="complete" source="scholar.google.com">147.83.15.91</proceeding>
        <snippet status="partial" source="scholar.google.com">El proyecto Opera eLearning proporciona un sistema para la enseÃ±anza a distancia del canto lÃ­rico profesional por medio de un entorno de telepresencia. Provee las herramientas bÃ¡sicas para la funcionalidad del MÃ¡ster Internacional de Canto LÃ­rico planteado por la ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:k622Vel6YTwJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:k622Vel6YTwJ:scholar.google.com/&amp;hl=en&amp;num=18&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="26">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1326813</url>
    <title status="complete" source="scholar.google.com">Automatic music summarization in compressed domain</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2004_Automatic_Music_Summarization_in_Compressed_Domain.pdf</pdf>
    <author status="partial" source="scholar.google.com">X Shao, C Xu, Y Wang&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Acoustics, Speech, and &#8230;</proceeding>
    <year>2004</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... Xi Shao# *, Changsheng Xu#, Ye Wang* , Mohan S Kankanhalli* #Institute for Infocomm Research,21 Heng Mui Keng Terrace, Singapore 119613 {shaoxi, xucs}@i2r.a-star.edu.sg *School ofComputing, National University of Singapore {wangye, mohan}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:Zx3rMQJxkl0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>20</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=6742575846368419175&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>17</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=6742575846368419175&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="17">
        <result id="0">
        <url>http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2006.00964.x/full</url>
        <title status="complete" source="scholar.google.com">DancingâtoâMusic Character Animation</title>
        <pdf>http://www.cvl.iis.u-tokyo.ac.jp/~siratori/pub/EG2006shiratori.pdf</pdf>
        <author status="complete" source="scholar.google.com">T Shiratori, A Nakazawa, K Ikeuchi</author>
        <proceeding status="complete" source="scholar.google.com">Computer Graphics Forum</proceeding>
        <year>2006</year>
        <source>Wiley Online Library</source>
        <snippet status="partial" source="scholar.google.com">Abstract In computer graphics, considerable research has been conducted on realistic human motion synthesis. However, most research does not consider human emotional aspects, which often strongly affect human motion. This paper presents a new approach ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:yeTmRF_HyggJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=633537909590779081&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>44</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=633537909590779081&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1561266</url>
        <title status="complete" source="scholar.google.com">Modeling timbre distance with temporal statistics from polyphonic music</title>
        <pdf>http://www.mybytes.de/papers/moerchen06modelling.pdf</pdf>
        <author status="partial" source="scholar.google.com">F Morchen, A Ultsch, M Thies&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Timbre distance and similarity are expressions of the phenomenon that some music appears similar while other songs sound very different to us. The notion of genre is often used to categorize music, but songs from a single genre do not necessarily sound similar ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:K1suPc_0paYJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12008273152375151403&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>39</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12008273152375151403&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5410060</url>
        <title status="complete" source="scholar.google.com">Audio signal representations for indexing in the transform domain</title>
        <pdf>http://gigapaper.ir/Articles/Most_Downloaded_Papers_from_all_IEEE_Journals/Audio_Speech_and_Language_Pr/Audio_Signal_Representations_for_Indexing_in_the_Transform_Domain-gze.pdf</pdf>
        <author status="complete" source="scholar.google.com">E Ravelli, G Richard, L Daudet</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Indexing audio signals directly in the transform domain can potentially save a significant amount of computation when working on a large database of signals stored in a lossy compression format, without having to fully decode the signals. Here, we show that ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Q1H8fN9YvwkJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>14</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=702377783790948675&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>15</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=702377783790948675&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://hal.archives-ouvertes.fr/hal-00604389/</url>
        <title status="complete" source="scholar.google.com">Automatic Segmentation of the Temporal Evolution of Isolated Acoustic Musical Instrument Sounds Using Spectro-Temporal Cues</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/60/43/89/PDF/caetano_segmentation_DAFx2010.pdf</pdf>
        <author status="partial" source="scholar.google.com">M Freitas Caetano, JJ Burred&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proc. of the 13th Int. &#8230;</proceeding>
        <year>2010</year>
        <source>hal.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT The automatic segmentation of isolated musical instrument sounds according to the temporal evolution is not a trivial task. It requires a model capable of capturing regions such as the attack, decay, sustain and release accurately for many types of instruments ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:-3cUKUiTuUkJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5312439173596346363&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>6</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5312439173596346363&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://cjc.ict.ac.cn/quanwenjiansuo/2007-05/zyb.zip</url>
        <title status="complete" source="scholar.google.com">åºäºåå®¹çé³é¢ä¸é³ä¹åæç»¼è¿° [J]</title>
        <pdf>http://cjc.ict.ac.cn/quanwenjiansuo/2007-05/zyb.zip</pdf>
        <author status="complete" source="scholar.google.com">å¼ ä¸å½¬ï¼ å¨æ°ï¼ è¾¹èç¥ºï¼ é­å</author>
        <proceeding status="complete" source="scholar.google.com">è®¡ç®æºå­¦æ¥</proceeding>
        <year>2007</year>
        <source>cjc.ict.ac.cn</source>
        <snippet status="partial" source="scholar.google.com">æè¦æºå¨å¬è§åæ¬ä¸å¤§ç ç©¶é¢å: è¯­é³ä¿¡å·å¤çä¸è¯å«, ä¸è¬é³é¢ä¿¡å·åæ, åºäºåå®¹çé³ä¹ä¿¡å·åæ. å¶ä¸­, è¯­é³ä¿¡å·å¤çä¸è¯å«æ©å·²æä¸ºä¸ä¸ªä¼ ç»çç ç©¶ç­ç¹. éçä¿¡æ¯ç§å­¦ä¸ææ¯çè¿éåå±, åºäºåå®¹çé³é¢ä¸é³ä¹ä¿¡å·åæä¹éæ¸æä¸ºä¸ä¸ªæ°çç ç©¶ç­ç¹, è¿å å¹´æ¥åå¾äºå¤§éç ç©¶ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:HriVNRqQrCwJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:HriVNRqQrCwJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3219106275905615902&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>14</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3219106275905615902&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://dl.acm.org/citation.cfm?id=1835554</url>
        <title status="complete" source="scholar.google.com">Robust audio identification for mp3 popular music</title>
        <pdf>http://homepage.fudan.edu.cn/weili/files/2011/06/LW-2010SIGIRFull.pdf</pdf>
        <author status="complete" source="scholar.google.com">W Li, Y Liu, X Xue</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 33rd international ACM SIGIR &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Audio identification via fingerprint has been an active research field with wide applications for years. Many technical papers were published and commercial software systems were also employed. However, most of these previously reported methods work ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:cGoqDT1eYesJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16960941287960898160&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16960941287960898160&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4023803</url>
        <title status="complete" source="scholar.google.com">Music motive extraction through hanson intervallic analysis</title>
        <author status="complete" source="scholar.google.com">JF Serrano, JM Inesta</author>
        <proceeding status="partial" source="scholar.google.com">Computing, 2006. CIC&#39;06. 15th &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music motive extraction is an important concept to consider in music information retrieval. Among the possible applications are the creations of music databases that need of indexing tools and access in a dynamic way, copyright management and plagiarism ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:kl0Gvcxo0IQJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9570264436716232082&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9570264436716232082&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://search.ieice.org/bin/gbpdf.php?category=D&amp;lang=J&amp;year=2007&amp;fname=j90-d_8_2242&amp;abst=</url>
        <title status="complete" source="scholar.google.com">é³æ¥½ç¹å¾´ãèæ®ããèè¸åä½ã®èªåçæ</title>
        <pdf>http://search.ieice.org/bin/gbpdf.php?category=D&amp;lang=J&amp;year=2007&amp;fname=j90-d_8_2242&amp;abst=</pdf>
        <author status="complete" source="scholar.google.com">ç½é³¥è²´äº®ï¼ ä¸­æ¾¤ç¯¤å¿ï¼ æ± ååå²</author>
        <proceeding status="complete" source="scholar.google.com">é»å­æå ±éä¿¡å­¦ä¼è«æèª D</proceeding>
        <year>2007</year>
        <source>search.ieice.org</source>
        <snippet status="partial" source="scholar.google.com">ããã¾ã è¿å¹´ã³ã³ãã¥ã¼ã¿ã°ã©ãã£ãã¯ã¹ã®åéã§ã¯, èªç¶ãªã­ã£ã©ã¯ã¿ã®ã¢ãã¡ã¼ã·ã§ã³ãçæããææ³ãæ°å¤ãææ¡ããã¦ãã¦ãã. ããã, äººéã®æ¯èãã®å°è±¡ãå¤§ããå·¦å³ããè¡¨ç¾ãèæ®ããææ³ã¯ã»ã¨ãã©ææ¡ããã¦ããªã. ããã§æ¬è«æã§ã¯, è¡¨ç¾ãéè¦ãªè¦å ã¨ãªãèè¸åä½ãå¯¾è±¡ã¨ãã¦, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:5B6z-ZOWrEAJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:5B6z-ZOWrEAJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4660265276715245284&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4660265276715245284&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://cjc.ict.ac.cn/eng/qwjse/view.asp?id=2381</url>
        <title status="complete" source="scholar.google.com">A Review of Content-Based Audio and Music Analysis</title>
        <author status="complete" source="scholar.google.com">YB Zhang, J Zhou, ZQ Bian, J Guo</author>
        <proceeding status="partial" source="scholar.google.com">CHINESE JOURNAL OF &#8230;</proceeding>
        <year>2007</year>
        <source>cjc.ict.ac.cn</source>
        <snippet status="partial" source="scholar.google.com">Abstract Machine hearing includes three fields: Speech signal processing and recognition, general audio signal processing, and content-based music analysis. Speech signal processing and recognition has been a traditional research field for many years. There are ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:dO9jGiU6e6UJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11924188369424478068&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11924188369424478068&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5521945</url>
        <title status="complete" source="scholar.google.com">Automatic scene change detection for composed speech and music sound under low snr in compressed domain</title>
        <author status="complete" source="scholar.google.com">X Yu, C Li, X Xu, S Yang, W Wan</author>
        <proceeding status="partial" source="scholar.google.com">Wireless Mobile and &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract With the amount of MP3 compressed data increasing, automatic scene change detection is becoming more and more important. Several studies have proposed some interesting approaches. However, none of these techniques analyze the audio signals in a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:x2fZ0f6KM3kJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8733476929448536007&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8733476929448536007&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://www.cvl.iis.u-tokyo.ac.jp/papers/all/743.pdf</url>
        <title status="complete" source="scholar.google.com">è¦³å¯ã«åºã¥ãé³æ¥½ããã³ã¢ã¼ã·ã§ã³ã­ã£ããã£ãã¼ã¿ããã®èè¸åä½çæææ³</title>
        <pdf>http://www.cvl.iis.u-tokyo.ac.jp/papers/all/743.pdf</pdf>
        <author status="complete" source="scholar.google.com">ä¸­æ¾¤ç¯¤å¿ï¼ ç½é³¥è²´äº®ï¼ æ± ååå²</author>
        <proceeding status="partial" source="scholar.google.com">ç»åã®èªè­ã»çè§£ã·ã³ãã¸ã¦ã  ( &#8230;</proceeding>
        <year>2005</year>
        <source>cvl.iis.u-tokyo.ac.jp</source>
        <snippet status="partial" source="scholar.google.com">ããã¾ã æ¬è«æã¯, é³æ¥½ãå¥åããã¨, ããã«åã£ãã­ã£ã©ã¯ã¿ã¼ã¢ãã¡ã¼ã·ã§ã³ãçæããææ³ãææ¡ãã¦ãã. æ¬ææ³ã¯ã¢ã¼ã·ã§ã³ã­ã£ããã£ãã¼ã¿ã®è¦³å¯ã«åºã¥ãåä½è§£æ, é³æ¥½è§£æ, ããã³è§£æã«åºã¥ãåä½çæææ³ã«ãã£ã¦æ§æããã. åä½è§£æé¨åã§ã¯, ã¢ã¼ã·ã§ã³ã­ã£ããã£ã¼ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:XlA0wFIJP0gJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:XlA0wFIJP0gJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5205889945280204894&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <title status="complete" source="scholar.google.com">Music-Driven Dance Synthesis by Multimodal Dance Performance Analysis</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7dn-VyF6fckJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5301778</url>
        <title status="complete" source="scholar.google.com">Audio segmentation in AAC domain for content analysis</title>
        <author status="complete" source="scholar.google.com">R Zhu, H Ai, R Hu</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; and Mobile Computing, 2009. WiCom&#39;09. &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We focus the attention on the audio scene segmentation in AAC domain for audio-based multimedia indexing and retrieval applications. In particular, a MFCC extraction method is proposed, which is adaptive to the window switch in AAC encoding process, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:kSUFcO8CYngJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8674499059668821393&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8013229&amp;id=c0jtAQAAEBAJ&amp;oi=fnd&amp;printsec=abstract</url>
        <title status="complete" source="scholar.google.com">Automatic creation of thumbnails for music videos</title>
        <author status="complete" source="scholar.google.com">C Xu</author>
        <proceeding status="complete" source="scholar.google.com">US Patent 8,013,229</proceeding>
        <year>2011</year>
        <source>Google Patents</source>
        <snippet status="partial" source="scholar.google.com">There is provided a method for automatically creating a music video thumbnail (50) from a music video signal (12). The music video signal is separated into a music signal (16) and a video signal (18). The music signal is analysed by detecting similarity regions and the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:joE1fTcSfmEJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7025072498277712270&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5784292</url>
        <title status="complete" source="scholar.google.com">Fast Audio Feature Extraction From Compressed Audio Data</title>
        <author status="complete" source="scholar.google.com">G Schuller, M Gruhne, T Friedrich</author>
        <proceeding status="partial" source="scholar.google.com">Selected Topics in Signal &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We describe an efficient system, which directly extracts features from compressed audio material. It consists of a time-frequency conversion method and a feature extraction algorithm. The conversion method provides the feature extraction algorithm with a suitable ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bJNfz_MGBFMJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5981913849280828268&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://www.cqvip.com/qk/95389x/1997008/2608268.html</url>
        <title status="complete" source="scholar.google.com">ä½¿ç¨åçè°±ç¸åçå®æ¶èååæ¢ç¸å³å¨ä½å¤ç®æ æ£æµ</title>
        <author status="complete" source="scholar.google.com">é»ç®çï¼ èµè¹å¯</author>
        <proceeding status="complete" source="scholar.google.com">ä¸­å½æ¿å</proceeding>
        <year>1997</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">æåºä¸ç§ä½¿ç¨åçè°±ç¸åçå®æ¶èååæ¢ç¸å³å¨ä½ç®æ æ£æµ. ä½¿ç¨è¿ç§æ¹æ³, è¾å¥é¢æä¸¤å¥èåå¾å. å¨ç¬¬äºèåå¾åä¸­, åèå¾åæ¯å¯¹æ¯åº¦åè½¬ç. ä½¿è¡å°åæ å¯¹è¿ä¸¤èååçè°±ä½ç¸åå¤ç, è¿ç§æ¹æ³å¯å¤§å¤§æå¶è¾åºé¢çç´æµé¡¹åä¸ç¸è¦çä¼ªç¸å³å³°, å¢å¼ºç¸å³å³°ç ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:EutYCy41UGsJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7732739032070286098&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://www.joca.cn/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=11396</url>
        <title status="complete" source="scholar.google.com">MP3 åç¼©åä¸­è¯­é³åå²çç ç©¶ä¸å®ç°</title>
        <pdf>http://www.joca.cn/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=11396</pdf>
        <author status="complete" source="scholar.google.com">å¸¸è¾½è±«ï¼ å¤¯å°æ¸ï¼ ä¸æºæ ¹ï¼ ææè²ï¼ è®¸éªç¼</author>
        <proceeding status="partial" source="scholar.google.com">Journal of ComputeT &#8230;</proceeding>
        <year>2009</year>
        <source>joca.cn</source>
        <snippet status="partial" source="scholar.google.com">æè¦: éå¯¹è¯´è¯äººæ¹åç¹æ£æµé®é¢, å¨MP3 æ ¼å¼ä¸ç¨æ¹è¿åBIC ç®æ³å®ç°äºå¤è¯èæ¹åç¹çæ£æµo æ ¹æ®éåç¼©åä¸­MFCC çæ±åè¿ç¨, æåºäºä¸ç§å¨åç¼©åMP3 æ ¼å¼ä¸å©ç¨MDCT ç³»æ°è®¡ç®MFCC ç¹å¾åæ°çæ°æ¹æ³o å¨æ­¤åºç¡ä¸, ä½¿ç¨æ¹è¿åBIC æ¹åç¹æ£æµç®æ³æ£æµè¯´è¯äºº ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9OVIkVtEC2wJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7785391540910220788&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="27">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4623948</url>
    <title status="complete" source="scholar.google.com">Application-specific music transcription for tutoring</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_Application_Specific_Music_Transcription_for_Tutoring.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Wang, B Zhang</author>
    <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
    <year>2008</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... Ye Wang is an assistant professor in the department of computer science at the National Universityof Singapore. His research inter- ests include music transcription and its applications to musicedu- cation (see http://www.comp.nus. ... Contact him at wangye@comp.nus.edu.sg. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:vK-ug8uAcwIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>18</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=176626421973561276&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>5</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=176626421973561276&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="5">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5431024</url>
        <title status="complete" source="scholar.google.com">Vocal melody extraction in the presence of pitched accompaniment in polyphonic music</title>
        <pdf>http://www.ee.iitb.ac.in/web/files/publications/RaoPNov2010.pdf</pdf>
        <author status="complete" source="scholar.google.com">V Rao, P Rao</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language Processing, &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Melody extraction algorithms for single-channel polyphonic music typically rely on the salience of the lead melodic instrument, considered here to be the singing voice. However the simultaneous presence of one or more pitched instruments in the polyphony ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:G_EYgKFr_ZEJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10519682645990371611&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>17</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10519682645990371611&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5430181</url>
        <title status="complete" source="scholar.google.com">A melody detection user interface for polyphonic music</title>
        <pdf>http://www.ee.iitb.ac.in/uma/~daplab/publications/sp-vr-pr-ncc10.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Pant, V Rao, P Rao</author>
        <proceeding status="partial" source="scholar.google.com">Communications (NCC), 2010 National &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The automatic extraction of the melody of the music from polyphonic recordings is a challenging problem for which no general solutions currently exist. We present a novel interface for semi-automatic melody extraction with the goal to provide highly accurate ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_p6UuVUQQPsJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18104488462401183486&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=18104488462401183486&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.eurasip.org/Proceedings/Eusipco/Eusipco2012/Conference/papers/1569573877.pdf</url>
        <title status="complete" source="scholar.google.com">Score-informed transcription for automatic piano tutoring</title>
        <pdf>http://www.eurasip.org/Proceedings/Eusipco/Eusipco2012/Conference/papers/1569573877.pdf</pdf>
        <author status="complete" source="scholar.google.com">E Benetos, A Klapuri, S Dixon</author>
        <year>2012</year>
        <source>eurasip.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In this paper, a score-informed transcription method for automatic piano tutoring is proposed. The method takes as input a recording made by a student which may contain mistakes, along with a reference score. The recording and the aligned synthesized score ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:7Z6ne08y6XcJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:7Z6ne08y6XcJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8640492677045395181&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8640492677045395181&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ismir2012.ismir.net/event/papers/379-ismir-2012.pdf</url>
        <title status="complete" source="scholar.google.com">AUTOMATIC MUSIC TRANSCRIPTION: BREAKING THE GLASS CEILING</title>
        <pdf>http://ismir2012.ismir.net/event/papers/379-ismir-2012.pdf</pdf>
        <author status="partial" source="scholar.google.com">E Benetos, S Dixon, D Giannoulis, H Kirchhoff&#8230;</author>
        <year>2012</year>
        <source>ismir2012.ismir.net</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Automatic music transcription is considered by many to be the Holy Grail in the field of music signal analysis. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:XdGqHjeJcC0J:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:XdGqHjeJcC0J:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3274267798929068381&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <title status="complete" source="scholar.google.com">Improving the TANSEN Query by Humming System</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:hZIEzFer7MEJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="28">
    <url>http://dl.acm.org/citation.cfm?id=1291361</url>
    <title status="complete" source="scholar.google.com">Visual analysis of fingering for pedagogical violin transcription</title>
    <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.169.8333&amp;rep=rep1&amp;type=pdf</pdf>
    <author status="complete" source="scholar.google.com">B Zhang, J Zhu, Y Wang, WK Leow</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 15th &#8230;</proceeding>
    <year>2007</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. Visual Analysis of Fingering for Pedagogical Violin Transcription Bingjun Zhang JiaZhu Ye Wang Wee Kheng Leow Department of Computer Science, National University ofSingapore E-mail:{bingjun,zhujia,wangye,leowwk}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:NXVIML9pAOIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>13</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=16285132522441438517&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>15</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=16285132522441438517&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="15">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4711699</url>
        <title status="complete" source="scholar.google.com">A multimodal approach to music transcription</title>
        <pdf>http://www.eurecom.fr/~schutz/publications/ICIP08%20-%20Eurecom%20-%20A%20Multimodal%20Approach%20to%20Music%20Transcription.pdf</pdf>
        <author status="partial" source="scholar.google.com">M Paleari, B Huet, A Schutz&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Image Processing, 2008. &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music transcription refers to extraction of a human readable and interpretable description from a recording of a music performance. Automatic music transcription remains, nowadays, a challenging research problem when dealing with polyphonic sounds or ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:M1EdVpqFsAAJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=49686493817295155&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>13</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=49686493817295155&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4623948</url>
        <title status="complete" source="scholar.google.com">Application-specific music transcription for tutoring</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_Application_Specific_Music_Transcription_for_Tutoring.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, B Zhang</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic music transcription (AMT) refers to the ability of computers to write note information, such as the pitch, onset time, duration, and source of each sound, after listening to the music. Our application scenario is computer-assisted, musical-instrument tutoring, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vK-ug8uAcwIJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>18</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=176626421973561276&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=176626421973561276&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://numediart.org/docs/numediart_2009_s07_p1_report.pdf</url>
        <title status="complete" source="scholar.google.com">Multimodal Guitar: Performance Toolbox and Study Workbench</title>
        <pdf>http://numediart.org/docs/numediart_2009_s07_p1_report.pdf</pdf>
        <author status="partial" source="scholar.google.com">C Frisson, L ReboursiÃ¨re, WY Chu&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">QPSR of the numediart &#8230;</proceeding>
        <year>2009</year>
        <source>numediart.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This project aims at studying how recent interactive and interaction technologies would help extend how we play the guitar, thus defining the âmultimodal guitarâ. We investigate two axes, 1)âA gestural/polyphonic sensing/processing toolbox to augment ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Nnqcy-L_rTMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Nnqcy-L_rTMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3723913816448989750&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3723913816448989750&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>https://dl.comp.nus.edu.sg/dspace/handle/1900.100/3056</url>
        <title status="complete" source="scholar.google.com">Automatic music transcription using audio-visual fusion for violin practice in home environment</title>
        <pdf>https://dl.comp.nus.edu.sg/dspace/bitstream/1900.100/3056/1/TRA7-09.pdf</pdf>
        <author status="complete" source="scholar.google.com">B Zhang, Y Wang</author>
        <year>2009</year>
        <source>dl.comp.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">Abstract: Violin practice in a home environment, where there is often no teacher available, can benefit from automatic music transcription to provide feedback to the student. This paper describes a high performance violin transcription system with three main contributions. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WDqOhljuXd0J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15951167519198165592&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15951167519198165592&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://www.tandfonline.com/doi/abs/10.1080/09298215.2012.708048</url>
        <title status="complete" source="scholar.google.com">Temporal Differences in String Bowing of Symphony Orchestra Players</title>
        <author status="complete" source="scholar.google.com">J PÃ¤tynen, S Tervo, T Lokki</author>
        <proceeding status="complete" source="scholar.google.com">Journal of New Music Research</proceeding>
        <year>2012</year>
        <source>Taylor &amp; Francis</source>
        <snippet status="partial" source="scholar.google.com">Abstract A study on the temporal differences between the bowing of string instrument players in an orchestra is presented. A high quality video of a professional symphony orchestra was recorded high above the first rows of the audience area. The movement of the instrument ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3MDuCziL-cIJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14049413585393598684&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6266249</url>
        <title status="complete" source="scholar.google.com">Real-Time Pitch Training System for Violin Learners</title>
        <pdf>http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Workshops/data/4729a163.pdf</pdf>
        <author status="partial" source="scholar.google.com">JH Wang, SA Wang, WC Chen&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper specifically targets violin learners who are working on their pitch accuracy. We employ a pitch tracking algorithm to extract the pitch played. Through volume thresholding and region detection, only parts of frames are processed. So our system can ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gMp1DiddLkkJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5273254636025137792&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://137.132.14.55/handle/10635/20949</url>
        <title status="complete" source="scholar.google.com">Adaptive multimodal fusion based similarity measures in music information retrieval</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/20949/ZhangBJ.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">Z BINGJUN</author>
        <year>2010</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">In the field of music information retrieval (MIR), one fundamental research problem is the measuring of the similarity between music documents. Based on a viable similarity measure, MIR systems can be made more effective to help users retrieve relevant music information. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9FjpHoxTlRIJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1339068325491726580&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.3219&amp;rep=rep1&amp;type=pdf#page=26</url>
        <title status="complete" source="scholar.google.com">Project# 03 Multimodal Guitar: Performance Toolbox and Study Workbench</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.3219&amp;rep=rep1&amp;type=pdf#page=26</pdf>
        <author status="partial" source="scholar.google.com">C Frisson, L Reboursiere, WY Chu, O LÃ¤hdeoja&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">on Multimodal Interfaces &#8230;</proceeding>
        <year>2009</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">AbstractâThis project aims at studying how recent interactive and interaction technologies would help extend how we play the guitar, thus defining the âmultimodal guitarâ. We investigate two axes, 1)âA gestural/polyphonic sensing/processing toolbox to augment ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9IzfhnW3LSQJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:9IzfhnW3LSQJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://www.nickgillian.com/papers/Gillian_Digito.pdf</url>
        <title status="complete" source="scholar.google.com">Digito: A Fine-Grain Gesturally Controlled Virtual Musical Instrument</title>
        <pdf>http://www.nickgillian.com/papers/Gillian_Digito.pdf</pdf>
        <author status="complete" source="scholar.google.com">N Gillian, JA Paradiso</author>
        <proceeding status="complete" source="scholar.google.com">nickgillian.com</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper presents Digito, a gesturally controlled virtual musical instrument. Digito is controlled through a number of intricate hand gestures, providing both discrete and continuous control of Digito&#39;s sound engine; with the finegrain hand gestures captured by ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:e-lUqt60e3wJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:e-lUqt60e3wJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8969961951271905659&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://www.mitpressjournals.org/doi/abs/10.1162/COMJ_a_00129</url>
        <title status="complete" source="scholar.google.com">Automated Violin Fingering Transcription Through Analysis of an Audio Recording</title>
        <pdf>http://130.102.44.246/journals/computer_music_journal/v036/36.3.maezawa.pdf</pdf>
        <author status="partial" source="scholar.google.com">A Maezawa, K Itoyama, K Komatani, T Ogata&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Computer Music &#8230;</proceeding>
        <year>2012</year>
        <source>MIT Press</source>
        <snippet status="partial" source="scholar.google.com">We present a method to recuperate fingerings for a given piece of violin music in order to recreate the timbre of a given audio recording of the piece. This is achieved by first analyzing an audio signal to determine the most likely sequence of two-dimensional ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:toQ_tQ8Bi2oJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7677231156760118454&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://www.eecs.umich.edu/nime2012/Proceedings/papers/82_Final_Manuscript.pdf</url>
        <title status="complete" source="scholar.google.com">Drum Stroke Computing: Multimodal Signal Processing for Drum Stroke Identification and Performance Metrics</title>
        <pdf>http://www.eecs.umich.edu/nime2012/Proceedings/papers/82_Final_Manuscript.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Hochenbaum, A Kapur</author>
        <proceeding status="complete" source="scholar.google.com">eecs.umich.edu</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In this paper we present a multimodal system for analyzing drum performance. In the first example we perform automatic drum hand recognition utilizing a technique for automatic labeling of training data using direct sensors, and only indirect sensors (eg a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:nftu-m8sjg0J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:nftu-m8sjg0J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://hal-institut-mines-telecom.archives-ouvertes.fr/docs/00/57/64/71/PDF/thesis_Schutz.pdf</url>
        <title status="complete" source="scholar.google.com">Antony Schutz</title>
        <pdf>http://hal-institut-mines-telecom.archives-ouvertes.fr/docs/00/57/64/71/PDF/thesis_Schutz.pdf</pdf>
        <author status="complete" source="scholar.google.com">SM eta la SÃ©paration</author>
        <year>2011</year>
        <source>hal-institut-mines-telecom.archives- &#8230;</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© Pour les Ãªtres humains, le son n&#39;a d&#39;importance que pour son contenu. La voie est un langage parlÃ©, la musique une intention artistique. Le processus physiologique est hautement dÃ©veloppÃ©, tout comme notre capacitÃ©a comprendre les processus sous-jacent ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:-IK9Kr9m-LcJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:-IK9Kr9m-LcJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13256458474406904568&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://hal-institut-mines-telecom.archives-ouvertes.fr/docs/00/55/93/01/PDF/Paleari_-_PhD_Thesis.pdf</url>
        <title status="complete" source="scholar.google.com">Marco PALEARI</title>
        <pdf>http://hal-institut-mines-telecom.archives-ouvertes.fr/docs/00/55/93/01/PDF/Paleari_-_PhD_Thesis.pdf</pdf>
        <author status="complete" source="scholar.google.com">O des Ãmotions</author>
        <year>2009</year>
        <source>hal-institut-mines-telecom.archives- &#8230;</source>
        <snippet status="partial" source="scholar.google.com">Abstract Affective Computing refers to computing that relates to, arises from, or deliberately influences emotions and has is natural application domain in highly abstracted humancomputer interactions. Affective computing can be divided into three main parts, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:TNQWKyO5Ep8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:TNQWKyO5Ep8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11462427562309243980&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://pastel.archives-ouvertes.fr/pastel-00005615/</url>
        <title status="complete" source="scholar.google.com">Informatique Affective: Affichage, Reconnaissance, et SynthÃ¨se par Ordinateur des Ãmotions</title>
        <pdf>http://pastel.archives-ouvertes.fr/docs/00/55/93/01/PDF/Paleari_-_PhD_Thesis.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Paleari</author>
        <year>2009</year>
        <source>pastel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">Abstract Affective Computing refers to computing that relates to, arises from, or deliberately influences emotions and has is natural application domain in highly abstracted humancomputer interactions. Affective computing can be divided into three main parts, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:upMHWFSL_a4J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12609387726077662138&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://pastel.archives-ouvertes.fr/pastel-00576471/</url>
        <title status="complete" source="scholar.google.com">Quelques Contributions au Traitement de Signal Musical et Ã  la SÃ©paration Aveugle de Source Audio Mono-Microphone</title>
        <pdf>http://pastel.archives-ouvertes.fr/docs/00/57/64/71/PDF/thesis_Schutz.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Schutz</author>
        <year>2010</year>
        <source>pastel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© Pour les Ãªtres humains, le son n&#39;a d&#39;importance que pour son contenu. La voie est un langage parlÃ©, la musique une intention artistique. Le processus physiologique est hautement dÃ©veloppÃ©, tout comme notre capacitÃ©a comprendre les processus sous-jacent ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Iah5e2Ivl8gJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14454073629093767201&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="29">
    <url>http://dl.acm.org/citation.cfm?id=1291443</url>
    <title status="complete" source="scholar.google.com">A workload prediction model for decoding mpeg video and its application to workload-scalable transcoding</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2007_A_Workload_Prediction_Model_for_Decoding_MPEG_Video_and_its_Application_to_Workload-Scalable_Transcoding.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Huang, AV Tran, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">&#8230; of the 15th international conference on &#8230;</proceeding>
    <year>2007</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... and its Application to Workload-scalable Transcoding Yicheng Huang, Vu An Tran, and YeWang School of Computing National University of Singapore 3 Science Drive 2, Singapore117543 {huangyic,tranvuan,wangye}@comp.nus.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:nm7plHvEHqoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>12</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=12258451270807416478&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>15</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=12258451270807416478&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="15">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5651500</url>
        <title status="complete" source="scholar.google.com">Hilbert transform based workload estimation for low power surveillance video compression</title>
        <author status="complete" source="scholar.google.com">X Jin, S Goto</author>
        <proceeding status="partial" source="scholar.google.com">Image Processing (ICIP), 2010 17th IEEE &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, a workload estimation scheme is proposed for surveillance video encoding by using difference detection and Hilbert transform-based workload estimation model. Difference detection distributes the input video data according to their content ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vExOyH1UgtwJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15889355334524751036&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15889355334524751036&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1450135.1450195</url>
        <title status="complete" source="scholar.google.com">Power reduction via macroblock prioritization for power aware H. 264 video applications</title>
        <pdf>http://pdf.aminer.org/000/330/436/a_distortion_control_scheme_for_allocating_constant_distortion_in_fd.pdf</pdf>
        <author status="partial" source="scholar.google.com">MA Baker, V Parameswaran, KS Chatha&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the 6th IEEE/ACM/IFIP &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract As the importance of multimedia applications in hand-held devices increases, the computational strain and corresponding demand for energy in such devices continues to grow. Portable multimedia devices with inherently limited energy supplies face tight ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9v5izPdcqv8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18422639445068152566&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>6</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=18422639445068152566&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://dl.acm.org/ft_gateway.cfm?id=1879059&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Dynamic tuning of feature set in highly variant interactive applications</title>
        <pdf>http://romain.hopto.me/files/papers/slidesEMSOFT2010.pdf</pdf>
        <author status="complete" source="scholar.google.com">T Kumar, RE Cledat, S Pande</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the tenth ACM international conference &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT For important classes of interactive consumer applications, such as gaming and video, the Quality-of-Service requirement is to create a maximally immersive experience for the interactive user. This necessitates a trade-off between maximizing the computational ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:6APdNb_A5T4J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4532240527493235688&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4532240527493235688&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.es.ele.tue.nl/esreports/esr-2010-02.pdf</url>
        <title status="complete" source="scholar.google.com">Predicting the throughput of multiprocessor applications under dynamic workload</title>
        <pdf>http://www.es.ele.tue.nl/esreports/esr-2010-02.pdf</pdf>
        <author status="complete" source="scholar.google.com">P Poplavko, M Geilen, T Basten</author>
        <proceeding status="complete" source="scholar.google.com">Int. Conf. on Computer Design, ICCD</proceeding>
        <year>2010</year>
        <source>es.ele.tue.nl</source>
        <snippet status="partial" source="scholar.google.com">AbstractâThis work contributes to throughput calculation for real-time multiprocessor applications experiencing dynamic workload variations. We focus on a method to predict the system throughput when processing an arbitrarily long data frame given the meta- ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8xy4x7YwUfsJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:8xy4x7YwUfsJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18109309138226388211&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=18109309138226388211&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5656651</url>
        <title status="complete" source="scholar.google.com">Linear modeling for MPEG-4 intra frame decoding complexity prediction based on statistical analysis</title>
        <author status="complete" source="scholar.google.com">T Tian, S Yu, H Guo</author>
        <proceeding status="partial" source="scholar.google.com">Signal Processing (ICSP), 2010 IEEE &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Video decoding complexity prediction plays an important role in energy efficient applications, such as dynamic voltage scaling and workload reshaping. This paper presents a novel linear model for MPEG-4 intra frame decoding complexity prediction. Detailed ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:81p4LMaVGdYJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15427526676916493043&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://dl.acm.org/citation.cfm?id=1496057</url>
        <title status="complete" source="scholar.google.com">Decoding-workload-aware video encoding</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2008_Decoding_workload_aware_Video_Encoding.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Huang, G Hong, VA Tran, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 18th &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a novel decoding-workload-aware video encoding scheme. It takes raw video data and decoding workload constraint of a mobile client as input and generates a video bitstream which matches such a constraint while striving to achieve the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_8f7hM8JCUwJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>10</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5478921208566892543&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=5478921208566892543&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://alexandria.tue.nl/extra2/200811801.pdf</url>
        <title status="complete" source="scholar.google.com">An accurate analysis for guaranteed performance of multiprocessor streaming applications</title>
        <pdf>http://alexandria.tue.nl/extra2/200811801.pdf</pdf>
        <author status="complete" source="scholar.google.com">P Poplavko</author>
        <year>2008</year>
        <source>alexandria.tue.nl</source>
        <snippet status="partial" source="scholar.google.com">Already for more than a decade, consumer electronic devices have been available for entertainment, educational, or telecommunication tasks based on multimedia streaming applications, ie, applications that process streams of audio and video samples in digital ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:EsP8vh2dwMMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:EsP8vh2dwMMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14105446784008241938&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14105446784008241938&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6011998</url>
        <title status="complete" source="scholar.google.com">Linear model-based adaptive prediction for video decoding complexity</title>
        <author status="complete" source="scholar.google.com">T Tian, H Guo, S Yu</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo (ICME), 2011 IEEE &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper proposes a novel approach to predict the video decoding computational complexity. The decoding complexity of each frame is found having an approximate linear relationship with the frame length, whereas the motion Information (MI) and the amount of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:hzI8E9qKx2oJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7694271157615997575&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7694271157615997575&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://scholarbank.nus.sg/handle/10635/35830</url>
        <title status="complete" source="scholar.google.com">Quality-aware performance analysis for multimedia MPSoC platforms</title>
        <pdf>http://scholarbank.nus.sg/bitstream/handle/10635/35830/GangadharanD.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">D GANGADHARAN</author>
        <year>2012</year>
        <source>scholarbank.nus.sg</source>
        <snippet status="partial" source="scholar.google.com">State-of-the-art embedded devices (eg mobile devices) run multiple applications on multiprocessor system-on-chip (MPSoC) platforms. MPSoC platforms are becoming popular due to the increasing number and complexity of target applications. Although there is a ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://dl.acm.org/citation.cfm?id=1950871</url>
        <title status="complete" source="scholar.google.com">Fast hybrid simulation for accurate decoded video quality assessment on MPSoC platforms with resource constraints</title>
        <pdf>http://www.aspdac.com/aspdac2011/archive/pdf/3A-3.pdf</pdf>
        <author status="partial" source="scholar.google.com">D Gangadharan, S Chakraborty&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 16th &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Multimedia decoders mapped onto MPSoC platforms exhibit degraded video quality when the critical system resources such as buffer and processor frequency are constrained. Hence, it is essential for system designers to find the appropriate mix of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Y99ZQnc8r3oJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8840351076463533923&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8840351076463533923&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6186860</url>
        <title status="complete" source="scholar.google.com">Hilbert Transform-Based Workload Prediction and Dynamic Frequency Scaling for Power-Efficient Video Encoding</title>
        <author status="complete" source="scholar.google.com">X Jin, S Goto</author>
        <proceeding status="partial" source="scholar.google.com">Computer-Aided Design of Integrated Circuits &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract With the popularity of mobile devices with embedded video cameras, real-time video encoding on hand-held devices becomes increasingly popular. Reducing the power consumption during real-time video encoding to suspend the battery life with the same ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PjlRSKs3Fg4J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://dl.acm.org/citation.cfm?id=1629492</url>
        <title status="complete" source="scholar.google.com">Fast model-based test case classification for performance analysis of multimedia MPSoC platforms</title>
        <author status="partial" source="scholar.google.com">D Gangadharan, S Chakraborty&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 7th &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Currently, performance analysis of multimedia-MPSoC platforms largely rely on simulation. The execution of one or more applications on such a platform is simulated for a library of test video clips. If all specified performance constraints are satisfied for this ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:e3WsWVpdpl4J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6820241328332109179&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6820241328332109179&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <url>http://dl.acm.org/citation.cfm?id=1459619</url>
        <title status="complete" source="scholar.google.com">Multimedia power management on a platter: from audio to video &amp; games</title>
        <author status="complete" source="scholar.google.com">S Chakraborty, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 16th ACM international &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Today, battery-life is a major design concern for all portable devices ranging from cell phones to PDAs and portable game consoles. The purpose of this tutorial will be to give an overview of power management techniques that are applicable to multimedia ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_MHf5UBV64YJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9721957957832262140&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://www.cqvip.com/qk/90818x/201210/43733927.html</url>
        <title status="complete" source="scholar.google.com">è§é¢è§£ç è®¡ç®å¤æåº¦ççº¿æ§å»ºæ¨¡çè®ºåå¨çº¿é¢æµæ¹æ³</title>
        <author status="complete" source="scholar.google.com">ç°å©·ï¼ ä½èçï¼ é­çº¢æï¼ èæå</author>
        <proceeding status="complete" source="scholar.google.com">è®¡ç®æºå­¦æ¥</proceeding>
        <year>2012</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">è§é¢è§£ç æ¯ä¸ç±»æå¸åçå¤åªä½åºç¨, å¶è®¡ç®éå¤§, èè½é«. ç°ä»£å¤åªä½è®¡ç®å¹³å°å¯å©ç¨è§é¢è§£ç è®¡ç®å¤æåº¦åºæçå¨æååç¹å¾æ¥èªéåºå°è°æ´æéè®¡ç®èµæº, ä»èèçè½è, å¶åææ¯å¯¹è§é¢è§£ç è®¡ç®å¤æåº¦è¿è¡åç¡®ä¼°è®¡. ä½èåºäºè§£ç è®¡ç®å¤æåº¦ä¸å¸§é¿ä¹é´ççº¿æ§ ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="30">
    <url>http://dl.acm.org/citation.cfm?id=2072533</url>
    <title status="complete" source="scholar.google.com">A tempo-sensitive music search engine with multimodal inputs</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2011_A_Tempo-Sensitive_Music_Search_Engine.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Yi, Y Zhou, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 1st international ACM &#8230;</proceeding>
    <year>2011</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Yinsheng Zhou School of Computing National University of Singapore 117590Singapore yzhou86@comp.nus.edu.sg Ye Wang School of Computing NationalUniversity of Singapore 117590 Singapore wangye@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:1siliOrsv2QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>5</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=7259781616403597526&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>2</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=7259781616403597526&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="2">
        <result id="0">
        <url>http://137.132.14.55/handle/10635/29967</url>
        <title status="complete" source="scholar.google.com">A tempo-based music search engine with multimodal query</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/29967/YiY.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">YI YU</author>
        <year>2011</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">This thesis presents TMSE: a novel Tempo-sensitive Music Search Engine with multimodal inputs for wellness and therapeutic applications. TMSE integrates six different interaction modes, Query-by-Number, Query-by-Sliding, Query-by-Example, Query-by-Tapping, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2BCtKWxScuMJ:scholar.google.com/&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16389252618463809752&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=2072386</url>
        <title status="complete" source="scholar.google.com">1st international ACM workshop on music information retrieval with user-centered and multimodal strategies (MIRUM)</title>
        <author status="complete" source="scholar.google.com">C Liem, M MÃ¼ller, D Eck, G Tzanetakis</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 19th ACM &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The 1st International ACM Workshop on Music Information Retrieval with User-Centered and Multimodal Strategies (MIRUM) at ACM Multimedia was proposed in order to gather experts from the Music and Multimedia Information Retrieval communities, as well ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:zoielqzuba8J:scholar.google.com/&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="31">
    <url>http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255</url>
    <title status="complete" source="scholar.google.com">Singing voice detection for karaoke application</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf</pdf>
    <author status="complete" source="scholar.google.com">A Shenoy, Y Wu, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Visual &#8230;</proceeding>
    <year>2005</year>
    <source>proceedings.spiedigitallibrary.org</source>
    <snippet status="partial" source="scholar.google.com">... of rhythm and tonality. * â  School of Computing, National University of Singapore,3 Science Drive 2, Singapore 117543 * arun@arunshenoy.com, â {wuyuansh,wangye}@comp.nus.edu.sg Visual Communications and Image ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>17</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>11</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="11">
        <result id="0">
        <url>http://ohm.fing.edu.uy/publicaciones/2007/RH07/RH07.pdf</url>
        <title status="complete" source="scholar.google.com">Comparing audio descriptors for singing voice detection in music audio files</title>
        <pdf>http://ohm.fing.edu.uy/publicaciones/2007/RH07/RH07.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Rocamora, P Herrera</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; on Computer Music, 11th. San Pablo, &#8230;</proceeding>
        <year>2007</year>
        <source>ohm.fing.edu.uy</source>
        <snippet status="partial" source="scholar.google.com">Abstract. Given the relevance of the singing voice in popular western music, a system able to reliable identify those portions of a music audio file containing vocals would be very useful. In this work, we explore already used descriptors to perform this task and compare the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:d-X3ceQPzT8J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:d-X3ceQPzT8J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4597348268454176119&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>23</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4597348268454176119&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://hal.inria.fr/inria-00544277/</url>
        <title status="complete" source="scholar.google.com">Transcription of vocal melodies using voice characteristics and algorithm fusion</title>
        <pdf>http://hal.inria.fr/docs/00/54/42/77/PDF/sutton_MIREX06.pdf</pdf>
        <author status="complete" source="scholar.google.com">C Sutton, E Vincent, MD Plumbley, JP Bello</author>
        <proceeding status="partial" source="scholar.google.com">2006 Music Information &#8230;</proceeding>
        <year>2006</year>
        <source>hal.inria.fr</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ©: This paper deals with the transcription of vocal melodies in music recordings. The proposed system relies on two distinct pitch estimators which exploit characteristics of the human singing voice. A Hidden Markov Model (HMM) is used to fuse the pitch estimates ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_zGZrpWM-AoJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>19</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=790536309110813183&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=790536309110813183&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <title status="complete" source="scholar.google.com">Singing voice detection in polyphonic music using predominant pitch</title>
        <pdf>http://www.ee.iitb.ac.in/daplab/publications/international-conference/papers/VRPR_ICSLP09.pdf</pdf>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:spNETHhXINsJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15789716467748213682&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15789716467748213682&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.ee.iitb.ac.in/daplab/publications/vr-cg-pr-AMR-11.pdf</url>
        <title status="complete" source="scholar.google.com">Context-aware features for singing voice detection in polyphonic music</title>
        <pdf>http://www.ee.iitb.ac.in/daplab/publications/vr-cg-pr-AMR-11.pdf</pdf>
        <author status="complete" source="scholar.google.com">V Rao, C Gupta, P Rao</author>
        <proceeding status="complete" source="scholar.google.com">Proc. of Adaptive Multimedia Retrieval</proceeding>
        <year>2011</year>
        <source>ee.iitb.ac.in</source>
        <snippet status="partial" source="scholar.google.com">Abstract. The effectiveness of audio content analysis for music retrieval may be enhanced by the use of available metadata. In the present work, observed differences in singing style and instrumentation across genres are used to adapt acoustic features for the singing voice ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KEA0xlAZmokJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:KEA0xlAZmokJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9915265364322959400&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9915265364322959400&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5298607</url>
        <title status="complete" source="scholar.google.com">Pitch oriented automatic singer identification in pop music</title>
        <author status="complete" source="scholar.google.com">P Chang</author>
        <proceeding status="partial" source="scholar.google.com">Semantic Computing, 2009. ICSC&#39;09. IEEE &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we proposed two novel methods used to distinguish the singer of a pop music. We focused on a single singer and single track case. These two methods are ldquoPitch Extractionrdquo method and ldquo1/12 OFCCrdquo method. The Pitch ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:r1X3xpda2T0J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4456692914184476079&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4456692914184476079&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6022500</url>
        <title status="complete" source="scholar.google.com">Automatic singer identification based on auditory features</title>
        <author status="complete" source="scholar.google.com">W Cai, Q Li, X Guan</author>
        <proceeding status="partial" source="scholar.google.com">Natural Computation (ICNC), 2011 &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The paper describes a method of identifying singers&#39; voice from the monophonic music including sounds of various musical instruments based on auditory features. In this system, there are four problems to solve, vocal segment detection, feature extraction, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2fGI2y5MEzUJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3824484272703074777&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3824484272703074777&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <title status="complete" source="scholar.google.com">Singing Phoneme Class Detection In Polyphonic Music Recordings</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uJWqBAVmuaEJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11653457682537027000&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11653457682537027000&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://sonictruths.net/old/pub/Christopher%20Sutton%20-%20Transcription%20of%20Vocal%20Melodies%20in%20Popular%20Music.pdf</url>
        <title status="complete" source="scholar.google.com">Transcription of vocal melodies in popular music</title>
        <pdf>http://sonictruths.net/old/pub/Christopher%20Sutton%20-%20Transcription%20of%20Vocal%20Melodies%20in%20Popular%20Music.pdf</pdf>
        <author status="complete" source="scholar.google.com">C Sutton</author>
        <year>2006</year>
        <source>sonictruths.net</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This report details the design, implementation and evaluation of a system for the transcription of vocal melodies in polyphonic recordings of popular music. The system operates on monaural sampled audio to produce a transcription indicating which time ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_7_UEpvfOSkJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:_7_UEpvfOSkJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2970651286356410367&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2970651286356410367&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://tel.archives-ouvertes.fr/tel-00687475/</url>
        <title status="complete" source="scholar.google.com">Localisation, caractÃ©risation et reconnaissance de voix chantÃ©es</title>
        <pdf>http://tel.archives-ouvertes.fr/docs/00/68/74/75/PDF/these.pdf</pdf>
        <author status="complete" source="scholar.google.com">L Regnier</author>
        <year>2012</year>
        <source>tel.archives-ouvertes.fr</source>
        <snippet status="partial" source="scholar.google.com">Many auditors have a remarkable ability to identify the singer of a new song as long as they have already heard some songs performed by the same singer. When the singer is unfamiliar it is common for listeners to attempt to characterize the singer&#39;s voice by finding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ObV6dGA6ysYJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14324325750750754105&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://hal.inria.fr/docs/00/68/74/75/PDF/these.pdf</url>
        <title status="complete" source="scholar.google.com">L&#39;UNIVERSITE PIERRE ET MARIE CURIE</title>
        <pdf>http://hal.inria.fr/docs/00/68/74/75/PDF/these.pdf</pdf>
        <author status="complete" source="scholar.google.com">ML Regnier</author>
        <proceeding status="complete" source="scholar.google.com">hal.inria.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">Many auditors have a remarkable ability to identify the singer of a new song as long as they have already heard some songs performed by the same singer. When the singer is unfamiliar it is common for listeners to attempt to characterize the singer&#39;s voice by finding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0HDPonzwK1wJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:0HDPonzwK1wJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf</url>
        <title status="complete" source="scholar.google.com">DÃ©tection de la voix chantÃ©e dans un morceau de musique</title>
        <pdf>http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf</pdf>
        <author status="complete" source="scholar.google.com">L REGNIER</author>
        <proceeding status="complete" source="scholar.google.com">atiam.ircam.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© De tous les Ã©lÃ©ments constitutifs d&#39;un morceau de musique, la voix est sans conteste celui qui focalise le plus l&#39;attention de l&#39;auditeur. Ceci provient du fait que la voix est porteuse d&#39;un message (le texte) et d&#39;une identitÃ© (celle du chanteur). Pour ces raisons, de ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15229319373475501458&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="32">
    <url>http://dl.acm.org/citation.cfm?id=1290154</url>
    <title status="complete" source="scholar.google.com">Educational violin transcription by fusing multimedia streams</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2007_Educational_Violin_Transcription_by_Fusing_Multimedia_Streams.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Wang, B Zhang, O Schleusing</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the international &#8230;</proceeding>
    <year>2007</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Streams Ye Wang Bingjun Zhang Olaf Schleusing Department of Computer Science, NationalUniversity of Singapore E-mail:{wangye,bingjun,olaf}@comp.nus.edu.sg ABSTRACTComputer-assisted violin tutoring requires accurate violin transcription. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:w23vslgkmBsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>16</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=1988379198861831619&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>11</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=1988379198861831619&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="11">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4623948</url>
        <title status="complete" source="scholar.google.com">Application-specific music transcription for tutoring</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_Application_Specific_Music_Transcription_for_Tutoring.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, B Zhang</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic music transcription (AMT) refers to the ability of computers to write note information, such as the pitch, onset time, duration, and source of each sound, after listening to the music. Our application scenario is computer-assisted, musical-instrument tutoring, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vK-ug8uAcwIJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>18</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=176626421973561276&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=176626421973561276&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_iDVT-An_Interactive_Digital_Violin_Tutoring_System_Based_on_Audio-Visual_Fusion.pdf</url>
        <title status="complete" source="scholar.google.com">iDVT: an interactive digital violin tutoring system based on audio-visual fusion</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_iDVT-An_Interactive_Digital_Violin_Tutoring_System_Based_on_Audio-Visual_Fusion.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Lu, B Zhang, Y Wang, WK Leow</author>
        <proceeding status="partial" source="scholar.google.com">Austria Association for &#8230;</proceeding>
        <year>2008</year>
        <source>comp.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">Computer-assisted musical instrumental tutoring (CAMIT) is catching eyes of many researchers and musicologist in recent years. Projects providing general instructions are of special interest since they are intended for self-learning and practice, the most frequent ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Q-5pHbonLxQJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Q-5pHbonLxQJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>20</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1454424884974841411&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1454424884974841411&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4623944</url>
        <title status="complete" source="scholar.google.com">Educational Multimedia</title>
        <pdf>http://www.icsi.berkeley.edu/pubs/speech/educationalmultimedia08.pdf</pdf>
        <author status="complete" source="scholar.google.com">G Friedland, W Hurst, L Knipping</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia, IEEE</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Making education more engaging, more enjoyable, and, in the end, more effective through the use of multimedia technology has been the goal of many researchers during the past few years. Encouraging results have been achieved so far. There are many examples ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:CbQwa70k2-IJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>14</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16346699668437120009&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16346699668437120009&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://numediart.org/docs/numediart_2009_s07_p1_report.pdf</url>
        <title status="complete" source="scholar.google.com">Multimodal Guitar: Performance Toolbox and Study Workbench</title>
        <pdf>http://numediart.org/docs/numediart_2009_s07_p1_report.pdf</pdf>
        <author status="partial" source="scholar.google.com">C Frisson, L ReboursiÃ¨re, WY Chu&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">QPSR of the numediart &#8230;</proceeding>
        <year>2009</year>
        <source>numediart.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This project aims at studying how recent interactive and interaction technologies would help extend how we play the guitar, thus defining the âmultimodal guitarâ. We investigate two axes, 1)âA gestural/polyphonic sensing/processing toolbox to augment ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Nnqcy-L_rTMJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Nnqcy-L_rTMJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3723913816448989750&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3723913816448989750&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>https://dl.comp.nus.edu.sg/dspace/handle/1900.100/3056</url>
        <title status="complete" source="scholar.google.com">Automatic music transcription using audio-visual fusion for violin practice in home environment</title>
        <pdf>https://dl.comp.nus.edu.sg/dspace/bitstream/1900.100/3056/1/TRA7-09.pdf</pdf>
        <author status="complete" source="scholar.google.com">B Zhang, Y Wang</author>
        <year>2009</year>
        <source>dl.comp.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">Abstract: Violin practice in a home environment, where there is often no teacher available, can benefit from automatic music transcription to provide feedback to the student. This paper describes a high performance violin transcription system with three main contributions. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WDqOhljuXd0J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15951167519198165592&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15951167519198165592&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://tuprints.ulb.tu-darmstadt.de/2797/</url>
        <title status="complete" source="scholar.google.com">Sensor-Based Feedback for Piano Pedagogy</title>
        <pdf>http://tuprints.ulb.tu-darmstadt.de/2797/</pdf>
        <author status="complete" source="scholar.google.com">A Hadjakos</author>
        <year>2011</year>
        <source>tuprints.ulb.tu-darmstadt.de</source>
        <snippet status="partial" source="scholar.google.com">Recent advances in sensor technology provide new opportunities for applications that utilize the user&#39;s movement as an input source. This thesis focuses on movement analysis, which is a sub-area of a larger field concerned with the interpretation of sensor signals of human ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:NK7pUurz2xMJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1431005494359207476&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6266249</url>
        <title status="complete" source="scholar.google.com">Real-Time Pitch Training System for Violin Learners</title>
        <pdf>http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Workshops/data/4729a163.pdf</pdf>
        <author status="partial" source="scholar.google.com">JH Wang, SA Wang, WC Chen&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper specifically targets violin learners who are working on their pitch accuracy. We employ a pitch tracking algorithm to extract the pitch played. Through volume thresholding and region detection, only parts of frames are processed. So our system can ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gMp1DiddLkkJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5273254636025137792&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://137.132.14.55/handle/10635/20949</url>
        <title status="complete" source="scholar.google.com">Adaptive multimodal fusion based similarity measures in music information retrieval</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/20949/ZhangBJ.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">Z BINGJUN</author>
        <year>2010</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">In the field of music information retrieval (MIR), one fundamental research problem is the measuring of the similarity between music documents. Based on a viable similarity measure, MIR systems can be made more effective to help users retrieve relevant music information. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9FjpHoxTlRIJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1339068325491726580&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.3219&amp;rep=rep1&amp;type=pdf#page=26</url>
        <title status="complete" source="scholar.google.com">Project# 03 Multimodal Guitar: Performance Toolbox and Study Workbench</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.3219&amp;rep=rep1&amp;type=pdf#page=26</pdf>
        <author status="partial" source="scholar.google.com">C Frisson, L Reboursiere, WY Chu, O LÃ¤hdeoja&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">on Multimodal Interfaces &#8230;</proceeding>
        <year>2009</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">AbstractâThis project aims at studying how recent interactive and interaction technologies would help extend how we play the guitar, thus defining the âmultimodal guitarâ. We investigate two axes, 1)âA gestural/polyphonic sensing/processing toolbox to augment ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9IzfhnW3LSQJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:9IzfhnW3LSQJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://www.actapress.com/PDFViewer.aspx?paperId=452988</url>
        <title status="complete" source="scholar.google.com">A Context-based Emotion-Analyzer for Teaching Tonality in Music Courses</title>
        <author status="complete" source="scholar.google.com">A Ichinose, S Kurabayashi, Y Kiyoki</author>
        <proceeding status="partial" source="scholar.google.com">Technology for Education/758: &#8230;</proceeding>
        <year>2011</year>
        <source>actapress.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT This paper presents a context-based emotion-analyzer dedicated for supporting to learn tonality in music courses. This emotion-analyzer realizes a new music retrieval environment to find and visualize music items with considering genre dependent ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0297oUbKgIgJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9836083990882906067&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://www.lps.usp.br/~magno/papers/SBGames10.pdf</url>
        <title status="complete" source="scholar.google.com">Rainbow Strings: Jogo para aprendizado de violino com processamento de audio</title>
        <pdf>http://www.lps.usp.br/~magno/papers/SBGames10.pdf</pdf>
        <author status="partial" source="scholar.google.com">PRR Vianna, R Nakamura, EMF Mesquita, MTM Silva&#8230;</author>
        <proceeding status="complete" source="scholar.google.com">lps.usp.br</proceeding>
        <snippet status="partial" source="scholar.google.com">Resumo Neste artigo discute-se o desenvolvimento do jogo âRainbow Stringsâ, com finalidades lÃºdica e didÃ¡tica. Trata-se de um jogo para computadores que apresenta uma partitura musical simplificada, ao mesmo tempo em que o som de um violino tocado pelo ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mHNpPmXYeEgJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mHNpPmXYeEgJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5222161697286484888&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="33">
    <url>http://dl.acm.org/citation.cfm?id=1874037</url>
    <title status="complete" source="scholar.google.com">A music search engine for therapeutic gait training</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/2.Applications_in_e-Health/2010_A_Music_Search_Engine_for_Therapeutic_Gait_Training.pdf</pdf>
    <author status="partial" source="scholar.google.com">Z Li, Q Xiang, J Hockman, J Yang, Y Yi&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
    <year>2010</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Qiaoliang Xiang1, Jason Hockman2, Jianqing Yang1, Yu Yi1 Ichiro Fujinaga2, Ye Wang1 1Schoolof Computing, National University of Singapore, Singapore 2Schulich School of Music, McGillUniversity, Canada {lizhongh, xiangqiaoliang, yiyu09, wangye}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:CDDHVVzuQNoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>5</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=15726831979121291272&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>3</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=15726831979121291272&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="3">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=2072533</url>
        <title status="complete" source="scholar.google.com">A tempo-sensitive music search engine with multimodal inputs</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2011_A_Tempo-Sensitive_Music_Search_Engine.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Yi, Y Zhou, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 1st international ACM &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents TMSE: a novel Tempo-sensitive Music Search Engine with multimodal inputs for wellness and therapeutic applications. TMSE integrates six different interaction modes, Query-by-Number, Query-by-Sliding, Query-by-Example, Query-by- ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:1siliOrsv2QJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7259781616403597526&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7259781616403597526&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=3</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://137.132.14.55/handle/10635/29967</url>
        <title status="complete" source="scholar.google.com">A tempo-based music search engine with multimodal query</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/29967/YiY.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">YI YU</author>
        <year>2011</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">This thesis presents TMSE: a novel Tempo-sensitive Music Search Engine with multimodal inputs for wellness and therapeutic applications. TMSE integrates six different interaction modes, Query-by-Number, Query-by-Sliding, Query-by-Example, Query-by-Tapping, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2BCtKWxScuMJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16389252618463809752&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://dl.acm.org/citation.cfm?id=2396458</url>
        <title status="complete" source="scholar.google.com">A domain-specific music search engine for gait training</title>
        <author status="complete" source="scholar.google.com">Z Li, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 20th ACM international conference &#8230;</proceeding>
        <year>2012</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper demonstrates a domain-specific music retrieval system to help music therapists find appropriate music for Parkinson&#39;s disease patients in their gait training. Different from existing music search engines, this system incorporates multiple music ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="34">
    <url>http://www.springerlink.com/index/31155K667467331K.pdf</url>
    <title status="complete" source="scholar.google.com">An optimal speed control scheme supported by media servers for low-power multimedia applications</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2009_An_Optimal_Speed_Control_Scheme_Supported_by_Media_Servers_for_Low-Power_Multimedia_Applications.pdf</pdf>
    <author status="complete" source="scholar.google.com">W Huang, Y Wang</author>
    <proceeding status="complete" source="scholar.google.com">Multimedia systems</proceeding>
    <year>2009</year>
    <source>Springer</source>
    <snippet status="partial" source="scholar.google.com">... Communicated by Ralf Steinmetz. W. Huang Â· Y. Wang (B) School of Computing, NationalUniversity of Singapore, Computing 1, 13 Computing Drive, Singapore 117417, Singaporee-mail: wangye@comp.nus.edu.sg W. Huang e-mail: huangwd@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:_gCcqK6vKokJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>11</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=9883905496905941246&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>3</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=9883905496905941246&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="3">
        <result id="0">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://essay.utwente.nl/61528/</url>
        <title status="complete" source="scholar.google.com">Optimal Dynamic Voltage and Frequency Scaling for Multimedia Devices</title>
        <pdf>http://essay.utwente.nl/61528/1/MSc_M_Gerards.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Gerards</author>
        <year>2011</year>
        <source>essay.utwente.nl</source>
        <snippet status="partial" source="scholar.google.com">Embedded systems like cellular phones, portable media players, etc. are often used for multimedia and high speed communication applications. The complexity of these applications increases rapidly. Because of this, faster devices are required, while the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:wDfqYRCmarwJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://dl.acm.org/citation.cfm?id=2400700</url>
        <title status="complete" source="scholar.google.com">Optimal DPM and DVFS for frame-based real-time systems</title>
        <author status="complete" source="scholar.google.com">MET Gerards, J Kuper</author>
        <proceeding status="partial" source="scholar.google.com">ACM Transactions on Architecture and Code &#8230;</proceeding>
        <year>2013</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Dynamic Power Management (DPM) and Dynamic Voltage and Frequency Scaling (DVFS) are popular techniques for reducing energy consumption. Algorithms for optimal DVFS exist, but optimal DPM and the optimal combination of DVFS and DPM are not yet ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="35">
    <url>http://dl.acm.org/citation.cfm?id=1874006</url>
    <title status="complete" source="scholar.google.com">Large-scale music tag recommendation with explicit multiple attributes</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2010_Large-scale_Music_Tag_Recommendation_with_Explicit_Multiple_Attributes.pdf</pdf>
    <author status="partial" source="scholar.google.com">Z Zhao, X Wang, Q Xiang, AM Sarroff, Z Li&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
    <year>2010</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Zhendong Zhao, Xinxi Wang, Qiaoliang Xiang, Andy M Sarroff, Zhonghua Li, Ye Wang Schoolof Computing National University of Singapore {zhaozhendong, andy660, qiaoliangxiang}@gmail.com andy.sarroff@nyu.edu, {lizhongh, wangye}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:tjNWBquXKB8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>6</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=2245211175045706678&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>8</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=2245211175045706678&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="8">
        <result id="0">
        <url>http://www.inescporto.pt/~fgouyon/docs/MarquesDominguesLangloisGouyon_ISMIR2011.pdf</url>
        <title status="complete" source="scholar.google.com">Three current issues in music autotagging</title>
        <pdf>http://www.inescporto.pt/~fgouyon/docs/MarquesDominguesLangloisGouyon_ISMIR2011.pdf</pdf>
        <author status="complete" source="scholar.google.com">G Marques, M Domingues, T Langlois, F Gouyon</author>
        <proceeding status="complete" source="scholar.google.com">Proc. of ISMIR</proceeding>
        <year>2011</year>
        <source>inescporto.pt</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT The purpose of this paper is to address several aspects of music autotagging. We start by presenting autotagging experiments conducted with two different systems and show performances on a par with a method representative of the state-of-the-art. Beyond ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KnSGekrwpsAJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:KnSGekrwpsAJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13882047104090010666&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13882047104090010666&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://onlinelibrary.wiley.com/doi/10.1002/cpe.1846/full</url>
        <title status="complete" source="scholar.google.com">Multimedia Applications and Security in MapReduce: Opportunities and Challenges</title>
        <author status="partial" source="scholar.google.com">Z Yu, C Wang, C Thomborson, J Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Concurrency and &#8230;</proceeding>
        <year>2011</year>
        <source>Wiley Online Library</source>
        <snippet status="partial" source="scholar.google.com">SUMMARY Cloud computing has recently attracted great attention, both commercially and academically. MapReduce is a popular programming model for distributed storage and computation in the cloud. In this paper, we survey cloud-based multimedia applications, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vMNf9iZqnDgJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4079252078065599420&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4079252078065599420&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ismir2011.ismir.net/papers/PS6-11.pdf</url>
        <title status="complete" source="scholar.google.com">AN EMPIRICAL STUDY OF MULTI-LABEL CLASSIFIERS FOR MUSIC TAG</title>
        <pdf>http://ismir2011.ismir.net/papers/PS6-11.pdf</pdf>
        <author status="complete" source="scholar.google.com">C Sanden, AB Lethbridge, JZ Zhang</author>
        <proceeding status="complete" source="scholar.google.com">ismir2011.ismir.net</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In this paper we study the problem of automatic music tag annotation. Treating tag annotation as a computational classification process, we attempt to explore the relationship between acoustic features and music tags. Toward this end, we conduct a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uY7ohaRI0DQJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:uY7ohaRI0DQJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3805621556586516153&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.springerlink.com/index/W187W56N17647177.pdf</url>
        <title status="complete" source="scholar.google.com">A survey of tagging techniques for music, speech and environmental sound</title>
        <pdf>http://eprints.qut.edu.au/56097/1/56097.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Duan, J Zhang, P Roe, M Towsey</author>
        <proceeding status="complete" source="scholar.google.com">Artificial Intelligence Review</proceeding>
        <year>2012</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Sound tagging has been studied for years. Among all sound types, music, speech, and environmental sound are three hottest research areas. This survey aims to provide an overview about the state-of-the-art development in these areas. We discuss about the ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://www.thinkmind.org/index.php?view=article&amp;articleid=afin_2011_3_20_70029</url>
        <title status="complete" source="scholar.google.com">Tag Relevancy for Similar Artists</title>
        <pdf>http://www.thinkmind.org/download.php?articleid=afin_2011_3_20_70029</pdf>
        <author status="complete" source="scholar.google.com">B Marshall</author>
        <proceeding status="partial" source="scholar.google.com">AFIN 2011, The Third International Conference on &#8230;</proceeding>
        <year>2011</year>
        <source>thinkmind.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract: In music information retrieval, the rank position in similar artist lists have gained a lot of attention due to the surge in online music listening and semi-automated song recommendation approaches. Artist tags with respect to genre, style and mood are critical ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:r_54yQcoc7AJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://www.dbpia.co.kr/Journal/ArticleDetail/1512645</url>
        <title status="complete" source="scholar.google.com">íë¡ì ì´ì©í ë´ì©ê¸°ë° ìì ê²ì ìì¤í ì¤ê³</title>
        <author status="complete" source="scholar.google.com">ì íì©ï¼ ê¹ì¤íï¼ ë°íë¯¼ï¼ ì´ì ì¤</author>
        <proceeding status="complete" source="scholar.google.com">íêµ­ì ë³´ê³¼íí íì ë°íë¼ë¬¸ì§</proceeding>
        <year>2011</year>
        <source>dbpia.co.kr</source>
        <snippet status="partial" source="scholar.google.com">ììì ì¸ë¥ì ëíì ì¸ ìì ë¡ì ì¤ë ì¸ìëì ì¬ëì ë°ììë¤. ê·¸ ì¤ëë ì¸ìë§í¼ì´ë ì¸ë¥ê° ë§ë¤ì´ì¨ ììì ìë ë°©ëíë¤. ë°©ëí ììì´ IT ê¸°ì ì ë°ë¬ê³¼ ì¸í°ë·ì íì°ì íµíì¬ ì¨ë¼ì¸ ìììì¥ì íì±íìê³  ììì ëì§í¸ ììì¼ë¡ ê´ë¦¬ëê² ëìë¤. ì´ë¬í ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2CTyRoMRIXsJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://www.cairn.info/resume.php?ID_ARTICLE=LCN_072_0135</url>
        <title status="complete" source="scholar.google.com">Reconnaissance des Ã©motions dans la musique</title>
        <pdf>http://hal.archives-ouvertes.fr/docs/00/68/24/96/PDF/Debaecker-J_rem_draft.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Debaecker</author>
        <proceeding status="complete" source="scholar.google.com">Les Cahiers du numÃ©rique</proceeding>
        <year>2012</year>
        <source>cairn.info</source>
        <snippet status="partial" source="scholar.google.com">RÃ©sumÃ© La reconnaissance des Ã©motions dans la musique est un challenge industriel et acadÃ©mique. Ã l&#39;heure de l&#39;explosion des contenus multimÃ©dias, il devient nÃ©cessaire de concevoir des ensembles structurÃ©s de termes et concepts facilitant l&#39;organisation et l&#39; ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gJTZa73VwGEJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7043864826742412416&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://archivesic.ccsd.cnrs.fr/docs/00/68/24/96/DOC/Debaecker-J_rem_draft.docx</url>
        <title status="complete" source="scholar.google.com">LES CAHIERS DU NUMÃRIQUE Consignes Ã©ditoriales Ã  l&#39;usage des auteurs</title>
        <pdf>http://archivesic.ccsd.cnrs.fr/docs/00/68/24/96/DOC/Debaecker-J_rem_draft.docx</pdf>
        <author status="complete" source="scholar.google.com">D Jean</author>
        <proceeding status="complete" source="scholar.google.com">archivesic.ccsd.cnrs.fr</proceeding>
        <snippet status="partial" source="scholar.google.com">La reconnaissance des Ã©motions dans la musique est un challenge industriel et acadÃ©mique. Ã l&#39;heure de l&#39;explosion des contenus multimÃ©dias, il devient nÃ©cessaire de concevoir des ensembles structurÃ©s de termes et concepts facilitant l&#39;organisation et l&#39; ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PHbMgomWYBgJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:PHbMgomWYBgJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="36">
    <url>http://dl.acm.org/citation.cfm?id=1101207</url>
    <title status="complete" source="scholar.google.com">Power-aware bandwidth and stereo-image scalable audio decoding</title>
    <pdf>http://www.mirlab.org/conference_papers/International_Conference/ACM%202005/docs/mm291.pdf</pdf>
    <author status="complete" source="scholar.google.com">W Huang, Y Wang, S Chakraborty</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 13th annual ACM &#8230;</proceeding>
    <year>2005</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. Power-aware Bandwidth and Stereo-image Scalable Audio Decoding WendongHuang, Ye Wang and Samarjit Chakraborty School of Computing National Universityof Singapore {huangwd, wangye, samarjit}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:qosghxX2cK4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>17</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=12569817132312857514&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>7</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=12569817132312857514&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="7">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036746</url>
        <title status="complete" source="scholar.google.com">Efficient Partial Spectrum Reconstruction using an Asymmetric PQMF Algorithm for MPEG-Coded Stereo Audio</title>
        <pdf>http://cecs.uci.edu/~papers/icme06/pdfs/0000901.pdf</pdf>
        <author status="complete" source="scholar.google.com">W Huang, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2006 IEEE &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a novel algorithm of a scalable and efficient pseudo-quadrature mirror filters (PQMF), which is employed for partial decoding a single-layer audio bitstream such as MP3, typically coded in joint/MS mode. The proposed algorithm is a new ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:eybAX40jtswJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>16</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=14751016719602427515&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=14751016719602427515&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://www.springerlink.com/index/B054378M53388853.pdf</url>
        <title status="complete" source="scholar.google.com">A joint encoderâdecoder framework for supporting energy efficient audio decoding</title>
        <pdf>https://www-new.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2009_A_Joint_Encoder-Decoder_Framework_for_Supprting_Energy_Efficient_Audio_Decoding.pdf</pdf>
        <author status="complete" source="scholar.google.com">W Huang, Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia systems</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract In comparison with the relatively slow progress of battery technology, semiconductor memory has improved much more rapidly, making storage a less critical limiting factor in designing low power embedded systems such as PDAs. To exploit such ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:S_7a7PXw8iMJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>11</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2590397674722885195&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2590397674722885195&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://scholarbank.nus.edu.sg/handle/10635/16660</url>
        <title status="complete" source="scholar.google.com">Perception-aware low-power audio processing techniques for portable devices</title>
        <pdf>http://scholarbank.nus.edu.sg/bitstream/handle/10635/16660/HuangWendong_PhD_thesis.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">H WENDONG</author>
        <year>2009</year>
        <source>scholarbank.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">In this thesis, we study perception-aware low power audio processing techniques for portable device. These works are mainly motivated by the fact that the audio decoding application is a significant source of energy consumption in context of portable devices. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BzEVahxPeIQJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9545466393669218567&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://uknowledge.uky.edu/gradschool_theses/275/</url>
        <title status="complete" source="scholar.google.com">POWER REDUCTION BY DYNAMICALLY VARYING SAMPLING RATE</title>
        <pdf>http://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1278&amp;context=gradschool_theses</pdf>
        <author status="complete" source="scholar.google.com">S Datta</author>
        <year>2006</year>
        <source>uknowledge.uky.edu</source>
        <snippet status="partial" source="scholar.google.com">Abstract In modern digital audio applications, a continuous audio signal stream is sampled at a fixed sampling rate, which is always greater than twice the highest frequency of the input signal, to prevent aliasing. A more energy efficient approach is to dynamically change the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:zZIOzF2HF9sJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15787235855695778509&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://dl.acm.org/citation.cfm?id=1459619</url>
        <title status="complete" source="scholar.google.com">Multimedia power management on a platter: from audio to video &amp; games</title>
        <author status="complete" source="scholar.google.com">S Chakraborty, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 16th ACM international &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Today, battery-life is a major design concern for all portable devices ranging from cell phones to PDAs and portable game consoles. The purpose of this tutorial will be to give an overview of power management techniques that are applicable to multimedia ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_MHf5UBV64YJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9721957957832262140&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://dl.acm.org/citation.cfm?id=1858493</url>
        <title status="complete" source="scholar.google.com">Improving audio files availability in file sharing networks</title>
        <author status="partial" source="scholar.google.com">M Dahia, G Ramalho, F Trinta, G Cabral&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the XV &#8230;</proceeding>
        <year>2009</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract File sharing networks are one of the most popular Internet applications nowadays. Despite its success, these networks have two main issues: the ineffectiveness in downloading content that few users in the network have and the high incidence of getting ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:wdK5AuKRgeUJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="37">
    <url>http://www.springerlink.com/index/FF2RACE6AEM0G0TJ.pdf</url>
    <title status="complete" source="scholar.google.com">Semantic region detection in acoustic music signals</title>
    <author status="complete" source="scholar.google.com">N Maddage, C Xu, A Shenoy, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Advances in Multimedia &#8230;</proceeding>
    <year>2005</year>
    <source>Springer</source>
    <snippet status="partial" source="scholar.google.com">... 1 Institute for Infocomm Research, 21 Heng Mui Keng Terrace, Singapore 119613{maddage,xucs}@i2r.a-star.edu.sg 2 School of Computing, National University ofSingapore,Singapore 117543 {arunshen,wangye}@comp.nus.edu.sg Abstract. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:9q_6ZQ2X8rAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>2</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=12750419578840592374&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>5</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=12750419578840592374&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="5">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1394661</url>
        <title status="complete" source="scholar.google.com">Unsupervised classification of music genre using hidden markov model</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.4854&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">X Shao, C Xu, MS Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2004. &#8230;</proceeding>
        <year>2004</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music genre classification can be of great utility to musical database management. Most current classification methods are supervised and tend to be based on contrived taxonomies. However, due to the ambiguities and inconsistencies in the chosen ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:KGj0Z9csuNoJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>13</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15760396199656712232&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>38</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15760396199656712232&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255</url>
        <title status="complete" source="scholar.google.com">Singing voice detection for karaoke application</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Shenoy, Y Wu, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Visual &#8230;</proceeding>
        <year>2005</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">abstract We present a framework to detect the regions of singing voice in musical audio signals. This work is oriented towards the development of a robust transcriber of lyrics for karaoke applications. The technique leverages on a combination of low-level audio ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>11</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.springerlink.com/index/424407J31N41P08X.pdf</url>
        <title status="complete" source="scholar.google.com">Content based packet loss recovery for classical music transmissions over the internet</title>
        <author status="complete" source="scholar.google.com">X Shao, C Zhou</author>
        <proceeding status="partial" source="scholar.google.com">Advances in Multimedia Information Processing-PCM &#8230;</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Various techniques have been proposed to deal with the problems created by packets loss. The error concealment is one of the possible techniques. Most of the researchers focused on error concealment techniques in general or specifically for human speech. These ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:DXaQjqMJMIwJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10101584562268173837&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.cqvip.com/qk/95955b/201202/41765481.html</url>
        <title status="complete" source="scholar.google.com">åºäºå¤å¸é³ä¹ç Internet åç»å·®ééèæ¹æ¡</title>
        <author status="complete" source="scholar.google.com">ææåï¼ éµæ¦ï¼ ç³æ­£ç¨</author>
        <proceeding status="complete" source="scholar.google.com">åäº¬é®çµå¤§å­¦å­¦æ¥: èªç¶ç§å­¦ç</proceeding>
        <year>2012</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">å¸¸è§çInternet åç»å·®ééç¨éåä¸¢å¼æé»åéå¤, å¯¹æ³¨éå®æ¶æ§çVoIP æ¯ç®æ´èææç. ä½ä¸ºæ åªé³çæ³¨éä¹æåæå¾çå¤å¸é³ä¹, åæ´æ³¨éäººè³å¯¹é³ä¹çä¸ªä½æç¥. æä¸­ç»åºäºInternet æµåªä½æå¡å¨ä¼ è¾å¤å¸é³ä¹åºæ¯, æåºä¸ç§æ°çåç»ä¸¢å¤±éè(PEC) æ¹æ¡, ä¸ä»è§£å³åä¸ªåç» ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:F83_TW5Q_TQJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3818296493765283095&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://www.uni-weimar.de/medien/webis/teaching/theses/wall_2006.pdf</url>
        <title status="complete" source="scholar.google.com">Problemklassen bei Automatisierungsaufgaben in der Musik</title>
        <pdf>http://www.uni-weimar.de/medien/webis/teaching/theses/wall_2006.pdf</pdf>
        <author status="complete" source="scholar.google.com">B Stein, O Kramer, J Wall</author>
        <proceeding status="complete" source="scholar.google.com">uni-weimar.de</proceeding>
        <snippet status="partial" source="scholar.google.com">Ich versichere, dass ich die vorliegende Arbeit selbststÃ¤ndig und ohne unerlaubte fremde Hilfe, sowie ohne Benutzung anderer als der angegebenen Quellen angefertigt habe. Alle AusfÃ¼hrungen, die wÃ¶rtlich oder sinngemÃ¤Ã Ã¼bernommen worden sind, sind als solche ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:MD5FjpnS_y8J:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:MD5FjpnS_y8J:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3458714595802562096&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="38">
    <url>http://dl.acm.org/citation.cfm?id=1631272.1631325</url>
    <title status="complete" source="scholar.google.com">SaVE: sensor-assisted motion estimation for efficient h. 264/AVC video encoding</title>
    <pdf>http://www.ruf.rice.edu/~mobile/publications/chen09mm.pdf</pdf>
    <author status="partial" source="scholar.google.com">X Chen, Z Zhao, A Rahmati, Y Wang&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 17th &#8230;</proceeding>
    <year>2009</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... H.264/AVC Video Encoding Xiaoming Chen1, Zhendong Zhao1, Ahmad Rahmati2,Ye Wang1 and Lin Zhong2 1School of Computing National University of Singapore117590 Singapore {chenxm, zhaozd, wangye}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:kqwH6-agbfkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>13</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=17973198601551588498&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>8</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=17973198601551588498&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="8">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1989260</url>
        <title status="complete" source="scholar.google.com">GPU-based fast motion estimation for on-the-fly encoding of computer-generated video streams</title>
        <author status="complete" source="scholar.google.com">J Taibo, VM Gulias, P Montero, S Rivas</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 21st &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Motion estimation is known to be one of the most expensive tasks in video coding as it is usually performed through blind search-based methods. However, in the particular case of computer-generated video, the rendering stage provides useful information to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bizyB4aYkWIJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7102625788766334062&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <title status="complete" source="scholar.google.com">Sensor-assisted Camera Motion Analysis and Motion Estimation Improvement for H. 264/AVC Video Encoding</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:PTYtr4mKxf4J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=18358231779999823421&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.ruf.rice.edu/~mobile/publications/lin10reflex.pdf</url>
        <title status="complete" source="scholar.google.com">Reflex: managing sensor data processing in mobile systems</title>
        <pdf>http://www.ruf.rice.edu/~mobile/publications/lin10reflex.pdf</pdf>
        <author status="complete" source="scholar.google.com">X Lin, L Zhong</author>
        <year>2010</year>
        <source>ruf.rice.edu</source>
        <snippet status="partial" source="scholar.google.com">Abstract Many emerging mobile services leverage sensors available on mobile systems to acquire information regarding the physical world through sensory data processing, in order to serve human users intelligently and ubiquitously. While the sensors themselves can be ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:v4Qf1R1iPb8J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:v4Qf1R1iPb8J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13780278315092772031&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13780278315092772031&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5711656</url>
        <title status="complete" source="scholar.google.com">Sensor-Assisted Video Encoding for Mobile Devices in Real-World Environments</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2011_Sensor-Assisted_Video_Encoding_for_Mobile_Devicesin_Real-World_Environments.pdf</pdf>
        <author status="partial" source="scholar.google.com">X Chen, Z Zhao, A Rahmati, Y Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Circuits and Systems &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we present a comprehensive study on sensor-assisted video encoding (SaVE) schemes for video capturing on mobile devices in real-world environments. Our purpose is to reduce the computational complexity of video encoding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gDmiExV2xjsJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4307259926522247552&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4307259926522247552&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5961220</url>
        <title status="complete" source="scholar.google.com">Image Pixel Comparison Using Block Based Positioning Subtraction Technique for Motion Estimation</title>
        <author status="partial" source="scholar.google.com">S Jitvinder, SSS Ranjit, KC Lim&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Symposium (AMS), 2011 &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Images are usually consists of motion translation from one frame to another frame. Motion translation is represented by the pixels value changes based on the pixel density. The motion transition process from one frame to another frame is called motion estimation. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:e6lCGTkD-PcJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17868035065364261243&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://ijcit.com/archives/volume2/issue1/Paper020125.pdf</url>
        <title status="complete" source="scholar.google.com">Medical Images Inter Frame Motion Analysis via Block Positioning Pixel Subtraction Technique</title>
        <pdf>http://ijcit.com/archives/volume2/issue1/Paper020125.pdf</pdf>
        <author status="complete" source="scholar.google.com">SSS Ranjit, H Jitvinder, KC Lim, SA Anas</author>
        <proceeding status="complete" source="scholar.google.com">analysis</proceeding>
        <year>2013</year>
        <source>ijcit.com</source>
        <snippet status="partial" source="scholar.google.com">AbstractâThis paper presents a study on image pixels using block positioning pixel subtraction technique to analyse the changes that occur in that region of interest. The changes that happen in the pixels represent the motion translation. Processing a whole ...</snippet>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:F8toOe9UEjUJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://scien.stanford.edu/pages/labsite/2010/ee398a/projects/reports/andy.pdf</url>
        <title status="complete" source="scholar.google.com">Using Sensors for Efficient Video Coding in Hand-held Devices</title>
        <pdf>http://scien.stanford.edu/pages/labsite/2010/ee398a/projects/reports/andy.pdf</pdf>
        <author status="complete" source="scholar.google.com">AL Lin</author>
        <proceeding status="complete" source="scholar.google.com">scien.stanford.edu</proceeding>
        <snippet status="partial" source="scholar.google.com">One key challenge in video coding consists of minimizing computation, especially in hand-held devices. The sensors which are already present in hand-held devices can help simplify motion vector search and video coding. Keywords-motion estimation; mobile video coding ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0i3hkw-ULfcJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:0i3hkw-ULfcJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17811054895946608082&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="39">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1518060</url>
    <title status="complete" source="scholar.google.com">A perception-aware low-power software audio decoder for portable devices</title>
    <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.4492&amp;rep=rep1&amp;type=pdf</pdf>
    <author status="partial" source="scholar.google.com">S Chakraborty, Y Wang&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Embedded Systems for &#8230;</proceeding>
    <year>2005</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... Samarjit Chakraborty Ye Wang Wendong Huang Department of Computer Science NationalUniversity of Singapore email: {samarjit, wangye, huangwd}@comp.nus.edu.sg ... http://www.comp.nus.edu.sg/Ësamarjit/pl-mp3/. [5] K. Choi, K. Dantu, W.-C. Cheng, and M. Pedram. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:oD7YdVw5A6MJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>17</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=11746295322389266080&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>8</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=11746295322389266080&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="8">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1101207</url>
        <title status="complete" source="scholar.google.com">Power-aware bandwidth and stereo-image scalable audio decoding</title>
        <pdf>http://www.mirlab.org/conference_papers/International_Conference/ACM%202005/docs/mm291.pdf</pdf>
        <author status="complete" source="scholar.google.com">W Huang, Y Wang, S Chakraborty</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 13th annual ACM &#8230;</proceeding>
        <year>2005</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We propose a new workload-scalable audio decoding scheme that would enable users to control the tradeoff between playback quality and power consumption in battery-powered portable audio players. Our objective is to give users a control at the decoder ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:qosghxX2cK4J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12569817132312857514&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12569817132312857514&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4115452</url>
        <title status="complete" source="scholar.google.com">High-level power management of audio power amplifiers for portable multimedia applications</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.7905&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">K Lee, Y Cho, N Chang</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; , Proceedings of the 2006 IEEE/ACM/ &#8230;</proceeding>
        <year>2006</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Modern hand-held multimedia devices are equipped with high-power audio subsystems with built-in loudspeakers. These audio subsystems consume around 30% of the total system power, due to the poor efficiency of loudspeakers, even though high- ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:nSaJfCQdjfEJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17405600177402226333&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17405600177402226333&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7945448&amp;id=HAvgAQAAEBAJ&amp;oi=fnd&amp;printsec=abstract</url>
        <title status="complete" source="scholar.google.com">Perception-aware low-power audio decoder for portable devices</title>
        <pdf>http://scholarbank.nus.edu.sg/bitstream/handle/10635/32793/US7945448.PDF?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">Y Wang, S Chakraborty, W Huang</author>
        <proceeding status="complete" source="scholar.google.com">US Patent 7,945,448</proceeding>
        <year>2011</year>
        <source>Google Patents</source>
        <snippet status="partial" source="scholar.google.com">A method of decoding audio data representing an audio clip, said method comprising the steps of selecting one of a predetermined number of frequency bands; decoding a portion of the audio data representing said audio clip according to the selected frequency band, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:k6pKxrDv_p4J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11456858044597185171&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11456858044597185171&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4697164</url>
        <title status="complete" source="scholar.google.com">A low power MPEG-1/2 Layer I/II/III audio decoder with downsampling mode</title>
        <author status="complete" source="scholar.google.com">Z Zhu, T Lin</author>
        <proceeding status="partial" source="scholar.google.com">Signal Processing, 2008. ICSP 2008. 9th &#8230;</proceeding>
        <year>2008</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper presents a low cost and low power MPEG-1/2 Lay I/II/III audio decoder. The decoder consists of a software core and a hardware core. By the corporation between the two parts, it can real-timely decode MPEG-1/2 Layer I/II/III audio bit-stream at a low ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:GADxHTQ4m1YJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://scholarbank.nus.edu.sg/handle/10635/16660</url>
        <title status="complete" source="scholar.google.com">Perception-aware low-power audio processing techniques for portable devices</title>
        <pdf>http://scholarbank.nus.edu.sg/bitstream/handle/10635/16660/HuangWendong_PhD_thesis.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">H WENDONG</author>
        <year>2009</year>
        <source>scholarbank.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">In this thesis, we study perception-aware low power audio processing techniques for portable device. These works are mainly motivated by the fact that the audio decoding application is a significant source of energy consumption in context of portable devices. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:BzEVahxPeIQJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9545466393669218567&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8036900&amp;id=qv73AQAAEBAJ&amp;oi=fnd&amp;printsec=abstract</url>
        <title status="complete" source="scholar.google.com">Device and a method of playing audio clips</title>
        <author status="complete" source="scholar.google.com">Y Wang, W Huang, S Chakroborty</author>
        <proceeding status="complete" source="scholar.google.com">US Patent 8,036,900</proceeding>
        <year>2011</year>
        <source>Google Patents</source>
        <snippet status="partial" source="scholar.google.com">A device for playing audio clips, a method of playing audio clips and a data storage medium having stored thereon computer code means for instructing a computer system to execute a method of playing audio clips. The device comprising a processor scalable in voltage, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:qH8_k2TalooJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9986409349242650536&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://dl.acm.org/citation.cfm?id=1459619</url>
        <title status="complete" source="scholar.google.com">Multimedia power management on a platter: from audio to video &amp; games</title>
        <author status="complete" source="scholar.google.com">S Chakraborty, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 16th ACM international &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Today, battery-life is a major design concern for all portable devices ranging from cell phones to PDAs and portable game consoles. The purpose of this tutorial will be to give an overview of power management techniques that are applicable to multimedia ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_MHf5UBV64YJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9721957957832262140&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://www.ecice06.com/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=11260</url>
        <title status="complete" source="scholar.google.com">å¤å·¥ä½æ¨¡å¼ MPEG é³é¢è§£ç å¨</title>
        <pdf>http://www.ecice06.com/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=11260</pdf>
        <author status="complete" source="scholar.google.com">æ±å­åï¼ ææ¶</author>
        <proceeding status="complete" source="scholar.google.com">Computer Engineering</proceeding>
        <year>2009</year>
        <source>ecice06.com</source>
        <snippet status="partial" source="scholar.google.com">æè¦: è®¾è®¡ä¸ä¸ªä½åè, å¯éç½®, ä½ææ¬çMPEG-1/2 Layer I/II/III é³é¢è§£ç å¨. è¯¥è§£ç å¨åå«è½¯ä»¶é¨ååç¡¬ä»¶å éé¨å, å¯ä»¥éæ©å·¥ä½å¨âé«åè´¨â åâä½åèâ 2 ç§å·¥ä½æ¨¡å¼ä¸. å¨âé«åè´¨â å·¥ä½æ¨¡å¼ä¸, è¯¥è§£ç å¨æ¯å¨å¼å®¹ç. å¨âä½åèâ å·¥ä½æ¨¡å¼ä¸, è®¾è®¡ä¸ä¸ªæªæ­¢ç»¼åå­å¸¦ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Rq9z8m-j9IcJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Rq9z8m-j9IcJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9796634790642036550&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="40">
    <url>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_iDVT-An_Interactive_Digital_Violin_Tutoring_System_Based_on_Audio-Visual_Fusion.pdf</url>
    <title status="complete" source="scholar.google.com">iDVT: an interactive digital violin tutoring system based on audio-visual fusion</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_iDVT-An_Interactive_Digital_Violin_Tutoring_System_Based_on_Audio-Visual_Fusion.pdf</pdf>
    <author status="complete" source="scholar.google.com">H Lu, B Zhang, Y Wang, WK Leow</author>
    <proceeding status="partial" source="scholar.google.com">Austria Association for &#8230;</proceeding>
    <year>2008</year>
    <source>comp.nus.edu.sg</source>
    <snippet status="partial" source="scholar.google.com">Page 1. iDVT: An Interactive Digital Violin Tutoring System Based on Audio-Visual FusionHuanhuan Lu, Bingjun Zhang, Ye Wang and Wee Kheng Leow School of Computing, NationalUniversity of Singapore {luhuan, bingjun, wangye, leowwk}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:Q-5pHbonLxQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Q-5pHbonLxQJ:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <numVersions>20</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=1454424884974841411&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>4</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=1454424884974841411&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="4">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5365381</url>
        <title status="complete" source="scholar.google.com">Bowed String Sequence Estimation of a Violin Based on Adaptive Audio Signal Classification and Context-Dependent Error Correction</title>
        <author status="partial" source="scholar.google.com">A Maezawa, K Itoyama, T Takahashi&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; , 2009. ISM&#39;09. 11th &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The sequence of strings played on a bowed string instrument is essential to understanding of the fingering. Thus, its estimation is required for machine understanding of violin playing. Audio-based identification is the only viable way to realize this goal for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:leN0XXC8qVMJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6028556767002420117&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6028556767002420117&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=4</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://proceedings.spiedigitallibrary.org/data/Conferences/SPIEP/64101/82940A_1.pdf</url>
        <title status="complete" source="scholar.google.com">Visualization feedback for musical ensemble practice: A case study on phrase articulation and dynamics</title>
        <pdf>http://www.cim.mcgill.ca/sre/publications/2012-VDA.pdf</pdf>
        <author status="partial" source="scholar.google.com">T Knight, N Bouillot&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of &#8230;</proceeding>
        <year>2012</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We consider the possible advantages of visualization in supporting musical interpretation. Specifically, we investigate the use of visualizations in making a subjective judgement of a student&#39;s performance compared to reference âexpertâ performance for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:VVWYIlCLw9MJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15259193138755425621&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15259193138755425621&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=4</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6266249</url>
        <title status="complete" source="scholar.google.com">Real-Time Pitch Training System for Violin Learners</title>
        <pdf>http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Workshops/data/4729a163.pdf</pdf>
        <author status="partial" source="scholar.google.com">JH Wang, SA Wang, WC Chen&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper specifically targets violin learners who are working on their pitch accuracy. We employ a pitch tracking algorithm to extract the pitch played. Through volume thresholding and region detection, only parts of frames are processed. So our system can ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gMp1DiddLkkJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5273254636025137792&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.mitpressjournals.org/doi/abs/10.1162/COMJ_a_00129</url>
        <title status="complete" source="scholar.google.com">Automated Violin Fingering Transcription Through Analysis of an Audio Recording</title>
        <pdf>http://130.102.44.246/journals/computer_music_journal/v036/36.3.maezawa.pdf</pdf>
        <author status="partial" source="scholar.google.com">A Maezawa, K Itoyama, K Komatani, T Ogata&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Computer Music &#8230;</proceeding>
        <year>2012</year>
        <source>MIT Press</source>
        <snippet status="partial" source="scholar.google.com">We present a method to recuperate fingerings for a given piece of violin music in order to recreate the timbre of a given audio recording of the piece. This is achieved by first analyzing an audio signal to determine the most likely sequence of two-dimensional ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:toQ_tQ8Bi2oJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=7677231156760118454&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="41">
    <url>http://www.mitpressjournals.org/doi/abs/10.1162/comj.2008.32.1.71</url>
    <title status="complete" source="scholar.google.com">Complexity-scalable beat detection with MP3 audio bitstreams</title>
    <author status="complete" source="scholar.google.com">J Zhu, Y Wang</author>
    <proceeding status="complete" source="scholar.google.com">Computer Music Journal</proceeding>
    <year>2008</year>
    <source>MIT Press</source>
    <snippet status="partial" source="scholar.google.com">... Jia Zhu and Ye Wang Department of Computer Science National University of Singapore 3Science Drive 2, Singapore 117543 {zhujia, wangye}@comp.nus.edu.sg Complexity- ScalableBeat Detection with MP3 Audio Bitstreams Computer Music Journal, 32:1, pp. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:Zo7qHjhlQtwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>4</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=15871359328518311526&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>4</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=15871359328518311526&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="4">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5410060</url>
        <title status="complete" source="scholar.google.com">Audio signal representations for indexing in the transform domain</title>
        <pdf>http://gigapaper.ir/Articles/Most_Downloaded_Papers_from_all_IEEE_Journals/Audio_Speech_and_Language_Pr/Audio_Signal_Representations_for_Indexing_in_the_Transform_Domain-gze.pdf</pdf>
        <author status="complete" source="scholar.google.com">E Ravelli, G Richard, L Daudet</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Indexing audio signals directly in the transform domain can potentially save a significant amount of computation when working on a large database of signals stored in a lossy compression format, without having to fully decode the signals. Here, we show that ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Q1H8fN9YvwkJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>14</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=702377783790948675&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>15</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=702377783790948675&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=4</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://mue.music.miami.edu/wp-content/uploads/2010/03/SantoroThesis.pdf</url>
        <title status="complete" source="scholar.google.com">Multiple F0 Estimation in the Transform Domain</title>
        <pdf>http://mue.music.miami.edu/wp-content/uploads/2010/03/SantoroThesis.pdf</pdf>
        <author status="complete" source="scholar.google.com">CA Santoro</author>
        <year>2009</year>
        <source>mue.music.miami.edu</source>
        <snippet status="partial" source="scholar.google.com">The successes of perceptual audio coding can be seen everywhere today. Virtually all modern music collections are stored in some perceptually coded format. Perceptual coding techniques greatly reduce the bandwidth and disk space required to convey and store ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ibsKpStvPK0J:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:ibsKpStvPK0J:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12482974500407393161&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=12482974500407393161&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=4</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5784292</url>
        <title status="complete" source="scholar.google.com">Fast Audio Feature Extraction From Compressed Audio Data</title>
        <author status="complete" source="scholar.google.com">G Schuller, M Gruhne, T Friedrich</author>
        <proceeding status="partial" source="scholar.google.com">Selected Topics in Signal &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We describe an efficient system, which directly extracts features from compressed audio material. It consists of a time-frequency conversion method and a feature extraction algorithm. The conversion method provides the feature extraction algorithm with a suitable ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bJNfz_MGBFMJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5981913849280828268&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="42">
    <url>http://dl.acm.org/citation.cfm?id=1631474</url>
    <title status="complete" source="scholar.google.com">CompositeMap: a novel music similarity measure for personalized multimodal music search</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2009_CompositeMap-A_Novel_Music_Similarity_Measure_for_Personalized_Multimodal_Music_Search.pdf</pdf>
    <author status="complete" source="scholar.google.com">B Zhang, Q Xiang, Y Wang, J Shen</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 17th ACM &#8230;</proceeding>
    <year>2009</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Bingjun Zhang1, Qiaoliang Xiang1, Ye Wang1, Jialie Shen2 1School of Computing, NationalUniversity of Singapore 2School of Information Systems, Singapore Management University1{bingjun, xiangqiaoliang, wangye}@comp.nus.edu.sg 2jlshen@smu.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:hpOvLkNk2JoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>14</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=11157778316519248774&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>4</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=11157778316519248774&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="4">
        <result id="0">
        <url>http://www.cp.jku.at/research/papers/Schedl_Knees_amr_2011.pdf</url>
        <title status="complete" source="scholar.google.com">Personalization in Multimodal Music Retrieval</title>
        <pdf>http://www.cp.jku.at/research/papers/Schedl_Knees_amr_2011.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Schedl, P Knees</author>
        <proceeding status="complete" source="scholar.google.com">Proc. AMR</proceeding>
        <year>2011</year>
        <source>cp.jku.at</source>
        <snippet status="partial" source="scholar.google.com">Abstract. This position paper provides an overview of current research endeavors and existing solutions in multimodal music retrieval, where the term âmultimodalâ relates to two aspects. The first one is taking into account the music context of a piece of music or an ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:NYKzUtOiYAYJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:NYKzUtOiYAYJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=459546190501085749&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=459546190501085749&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=4</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://drops.dagstuhl.de/opus/volltexte/2012/3470/</url>
        <title status="complete" source="scholar.google.com">User-Aware Music Retrieval</title>
        <pdf>http://drops.dagstuhl.de/opus/volltexte/2012/3470/pdf/9.pdf</pdf>
        <author status="partial" source="scholar.google.com">M Schedl, S Stober, E GÃ³mez, N Orio&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimodal Music &#8230;</proceeding>
        <year>2012</year>
        <source>drops.dagstuhl.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract Personalized and user-aware systems for retrieving multimedia items are becoming increasingly important as the amount of available multimedia data has been spiraling. A personalized system is one that incorporates information about the user into its data ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:XogDQo61VyMJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2546703736898816094&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2546703736898816094&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=4</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.ic.unicamp.br/~jsantos/pdf/fedel2012iccsa.pdf</url>
        <title status="complete" source="scholar.google.com">Sinimbu-Multimodal queries to support biodiversity studies</title>
        <pdf>http://www.ic.unicamp.br/~jsantos/pdf/fedel2012iccsa.pdf</pdf>
        <author status="complete" source="scholar.google.com">GS Fedel, CB Medeiros, JA dos Santos</author>
        <proceeding status="complete" source="scholar.google.com">ic.unicamp.br</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract. Typical biodiversity information systems can only solve a small part of user concerns. Available query mechanisms are based on traditional textual database manipulations, combining them with spatial correlations. However, experts need more ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jq9DcJ1wrZsJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:jq9DcJ1wrZsJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.springerlink.com/index/Y6783Q8LU3T55X24.pdf</url>
        <title status="complete" source="scholar.google.com">SinimbuâMultimodal Queries to Support Biodiversity Studies</title>
        <author status="complete" source="scholar.google.com">G de S. Fedel, C Medeiros, J dos Santos</author>
        <proceeding status="partial" source="scholar.google.com">Computational Science and Its &#8230;</proceeding>
        <year>2012</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Typical biodiversity information systems can only solve a small part of user concerns. Available query mechanisms are based on traditional textual database manipulations, combmining them with spatial correlations. However, experts need more complex ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:6cJzer3um4gJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9843723908118987497&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="43">
    <url>http://dl.acm.org/citation.cfm?id=1979016</url>
    <title status="complete" source="scholar.google.com">MOGCLASS: evaluation of a collaborative system of mobile devices for classroom music education of young children</title>
    <pdf>https://www0.comp.nus.edu.sg/~zhaosd/paper/chi2011_mogclass.pdf</pdf>
    <author status="partial" source="scholar.google.com">Y Zhou, G Percival, X Wang, Y Wang&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 2011 &#8230;</proceeding>
    <year>2011</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Children Yinsheng Zhou, Graham Percival, Xinxi Wang, Ye Wang, Shengdong Zhao Schoolof Computing (SoC), National University of Singapore, Singapore 117417 {yzhou86, wangxinx,wangye, zhaosd}@comp.nus.edu.sg, graham@percival-music.ca ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:jQvxbCYwtW8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>12</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=8049392850589256589&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>5</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=8049392850589256589&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="5">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=2072531</url>
        <title status="complete" source="scholar.google.com">The need for music information retrieval with user-centered and multimodal strategies</title>
        <author status="partial" source="scholar.google.com">C Liem, M MÃ¼ller, D Eck, G Tzanetakis&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 1st &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music is a widely enjoyed content type, existing in many multifaceted representations. With the digital information age, a lot of digitized music information has theoretically become available at the user&#39;s fingertips. However, the abundance of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:U7b3CYt6UGgJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7516642515667629651&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://www.comp.nus.edu.sg/~yzhou86/fp047.pdf</url>
        <title status="complete" source="scholar.google.com">MOGAT: Mobile Games with Auditory Training for Children with Cochlear Implants</title>
        <pdf>http://www.comp.nus.edu.sg/~yzhou86/fp047.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Zhou, KC Sim, P Tan, Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">ACM Multimedia</proceeding>
        <year>2012</year>
        <source>comp.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Cochlear implants have improved the lives of tens of thousands of the hearing impaired by providing sufficient auditory perception for speech, but these devices are far from satisfactory for music perception. Many cochlear implant recipients, especially pre- ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:4KKKkSKeXiAJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:4KKKkSKeXiAJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2332475528332354272&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6103017</url>
        <title status="complete" source="scholar.google.com">An Interactive Music Learning System in Ensemble Performance Class</title>
        <author status="complete" source="scholar.google.com">K Takano, S Sasaki</author>
        <proceeding status="partial" source="scholar.google.com">Broadband and Wireless Computing, &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract It is a significant research area in computer-assisted music learning to develop interactive learning materials to cultivate learner&#39;s performing abilities of musical instruments. Especially in the ensemble lesson at school, it is pointed out that not only the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:z3l8C9DS4_sJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18150582714220968399&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://dl.acm.org/citation.cfm?id=2030223</url>
        <title status="complete" source="scholar.google.com">CoolMag: a tangible interaction tool to customize instruments for children in music education</title>
        <author status="complete" source="scholar.google.com">C Zhang, L Shen, D Wang, F Tian, H Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 13th &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we describe CoolMag, a tangible interaction tool to enable children to create different instruments collaboratively in music education. With CoolMag, children could learn the basic playing methods of different instruments. It also has the potential to inspire ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WlK_XZR32YcJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9788986744178168410&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6391739</url>
        <title status="complete" source="scholar.google.com">An Overview of Evaluation Methods for Collaborative Systems</title>
        <pdf>http://sws2012.ime.usp.br/sbsc/SBSC2012/data/4890a127.pdf</pdf>
        <author status="partial" source="scholar.google.com">NS Santos, LS Ferreira&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Systems (SBSC), 2012 &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract What is the state-of-the-art of the research on evaluation methods available for collaborative systems? Based on a Systematic Literature Review, this paper takes an initial step to answer that question and presents an overview of what has been published in the ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="44">
    <url>http://dl.acm.org/citation.cfm?id=1291233.1291411</url>
    <title status="complete" source="scholar.google.com">A compressed domain distortion measure for fast video transcoding</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2007_A_Compressed_Domain_Distortion_Measure_for_Fast_Video_Transcoding.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Huang, AV Tran, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">&#8230; of the 15th international conference on &#8230;</proceeding>
    <year>2007</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. A Compressed Domain Distortion Measure for Fast Video Transcoding Yicheng Huang,Vu An Tran, and Ye Wang School of Computing National University of Singapore 3 Science Drive2, Singapore 117543 {huangyic, tranvuan, wangye}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:gnWK94921ZgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>11</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=11012838824509863298&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=11012838824509863298&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6088487</url>
        <title status="complete" source="scholar.google.com">Compressed video QoE assessment based on codec agnostic semantic video model</title>
        <author status="complete" source="scholar.google.com">C D&#39;Elia, P Mariano, A Marino</author>
        <proceeding status="partial" source="scholar.google.com">Measurements and Networking &#8230;</proceeding>
        <year>2011</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The amount of multimedia information generated in today society is growing exponentially. This makes an important purpose of research to evaluate the quality of service in order to achieve customer satisfaction, ie in order to improve the quality of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:MiaWi4qtO5cJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="45">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1689273</url>
    <title status="complete" source="scholar.google.com">Integrating Articulatory based Features with Auditory Based Features for Robust Stressed Speech Recognition</title>
    <author status="complete" source="scholar.google.com">TL New, H Li, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Information, Communications and &#8230;</proceeding>
    <year>2005</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... into a stress directed speech 0-7803-9282-5/05/$20.00 Å 2005 IEEE 1 33z Ye WangDepartment of Computer Science National University of Singapore Singaporewangye(,comp.nus.edu.sg recognition system [7], employing a ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:DjxX3FCj2xoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numCite>3</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=1935320032575175694&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="3">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4250028</url>
        <title status="complete" source="scholar.google.com">Statistical pattern recognition and built-in reliability test for feature extraction and health monitoring of electronics under shock loads</title>
        <pdf>http://ecadigitallibrary.com/pdf/57thECTC/s26p1bnh.pdf</pdf>
        <author status="partial" source="scholar.google.com">P Lall, P Choudhary, S Gupte, J Suhling&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; , 2007. ECTC&#39;07. &#8230;</proceeding>
        <year>2007</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The built-in stress test (BIST) is extensively used for diagnostics or identification of failure. The current version of BIST approach is focused on reactive failure detection and provides limited insight into reliability and residual life. A new approach has been ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:HYe6AgucY_oJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>17</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18042436103280690973&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>62</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=18042436103280690973&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=3</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5365560</url>
        <title status="complete" source="scholar.google.com">Neural networks and TEO features for an automatic recognition of stress in spontaneous speech</title>
        <author status="partial" source="scholar.google.com">L He, M Lech, NC Maddage&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Computation, 2009. ICNC&#39; &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This study presents automatic stress recognition methods based on acoustic speech analysis. Novel approaches to feature extraction based on the nonlinear Teager energy operator (TEO) calculated within critical bands, discrete wavelet transform bands, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:FqmYQ6Kx3eIJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16347417532901665046&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16347417532901665046&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=3</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://link.aip.org/link/abstract/ASMECP/v2007/i42991/p339/s1</url>
        <title status="complete" source="scholar.google.com">Time-Frequency Analysis and Built-In Reliability Test for Health Monitoring of Electronics Under Shock Loads</title>
        <author status="partial" source="scholar.google.com">P Lall, P Choudhary, S Gupte, P Gupta, J Suhling&#8230;</author>
        <year>2007</year>
        <source>link.aip.org</source>
        <snippet status="partial" source="scholar.google.com">The built-in stress test (BIST) is extensively used for diagnostics or identification of failure. The current version of BIST approach is focused on reactive failure detection and provides limited insight into reliability and residual life. A new approach has been developed to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:rjDlOwxtDrYJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13118542663888744622&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="46">
    <url>http://books.google.com.sg/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA319&amp;dq=%22Wang+Ye%22+NUS&amp;ots=oDOSrGfBf4&amp;sig=_RMEQytGgrDgKjP4h7fWJJ65Qjo</url>
    <title status="complete" source="scholar.google.com">Clustering music recordings by their keys</title>
    <pdf>http://www.cs.fiu.edu/~lli003/Music/clu/5.pdf</pdf>
    <author status="partial" source="scholar.google.com">Y Liu, Y Wang, A Shenoy, WH Tsai&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
    <year>2008</year>
    <source>books.google.com</source>
    <snippet status="partial" source="scholar.google.com">... tsinghua. edu. cn, wangye@ comp. nus. edu. sg, arun@ arunshenoy. com, whtsai@ ntut.edu. tw, clh-dcs@ tsinghua. edu. cn ABSTRACT Music key, a high level feature of musicalaudio, is an effective tool for structural analysis of musical works. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:upv83jIWit4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>24</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=16035653830951345082&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>3</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=16035653830951345082&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="3">
        <result id="0">
        <url>http://www.sciencedirect.com/science/article/pii/S0165168409002898</url>
        <title status="complete" source="scholar.google.com">Pitch-frequency histogram-based music information retrieval for Turkish music</title>
        <author status="complete" source="scholar.google.com">AC Gedik, B Bozkurt</author>
        <proceeding status="complete" source="scholar.google.com">Signal Processing</proceeding>
        <year>2010</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">This study reviews the use of pitch histograms in music information retrieval studies for western and non-western music. The problems in applying the pitch-class histogram-based methods developed for western music to non-western music and specifically to Turkish ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:S7Z0VbGvMT8J:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4553613874426066507&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>27</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4553613874426066507&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=3</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202011/papers/PS3-14.pdf</url>
        <title status="complete" source="scholar.google.com">Potential Relationship Discovery in Tag-Aware Music Style Clustering and Artist Social Networks</title>
        <pdf>http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202011/papers/PS3-14.pdf</pdf>
        <author status="complete" source="scholar.google.com">D Wang, M Ogihara</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 12th International Society for &#8230;</proceeding>
        <year>2011</year>
        <source>mirlab.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT With the rapid growth of music information and data in today&#39;s ever changing world, exploring and analyzing music style has become more and more difficult. Traditional content-based methods for music style analysis and newly emerged tag-based methods ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:XQE5N3AUj_IJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:XQE5N3AUj_IJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17478211151043559773&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17478211151043559773&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=3</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.worldscientific.com/doi/abs/10.1142/S0218213012500169</url>
        <title status="complete" source="scholar.google.com">Automatic Interactive Music Improvisation Based on Data Mining</title>
        <pdf>http://www.researchgate.net/publication/230583148_Automatic_Interactive_Music_Improvisation_based_on_Data_Mining/file/9fcfd50213d9337d9a.pdf</pdf>
        <author status="complete" source="scholar.google.com">C Halkiopoulos, B Boutsinas</author>
        <proceeding status="partial" source="scholar.google.com">International Journal on Artificial &#8230;</proceeding>
        <year>2012</year>
        <source>World Scientific</source>
        <snippet status="partial" source="scholar.google.com">An area of focus in music improvization is interactive improvization between a human and a computer system in real time. In this paper, we present a musical interactive system acting as a melody continuator. For each musical pattern given by the user, a new one is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:zo45jViRjqgJ:scholar.google.com/&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12145805054577315534&amp;hl=en&amp;num=3&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="47">
    <url>http://dl.acm.org/citation.cfm?id=1255135</url>
    <title status="complete" source="scholar.google.com">Interactive Digital Violin Tutor (iDVT): An edutainment system for violin learners</title>
    <author status="complete" source="scholar.google.com">Y Wang, J Zhu</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the international conference on &#8230;</proceeding>
    <year>2007</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. Interactive Digital Violin Tutor (iDVT): An Edutainment System for ViolinLearners Ye Wang and Jia Zhu School of Computing, National University of Singapore,Singapore {wangye, zhujia}@comp.nus.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:DshDr4sSkfYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>2</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=17767002396103526414&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>2</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=17767002396103526414&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="2">
        <result id="0">
        <url>http://proceedings.spiedigitallibrary.org/data/Conferences/SPIEP/64101/82940A_1.pdf</url>
        <title status="complete" source="scholar.google.com">Visualization feedback for musical ensemble practice: A case study on phrase articulation and dynamics</title>
        <pdf>http://www.cim.mcgill.ca/sre/publications/2012-VDA.pdf</pdf>
        <author status="partial" source="scholar.google.com">T Knight, N Bouillot&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of &#8230;</proceeding>
        <year>2012</year>
        <source>proceedings.spiedigitallibrary.org</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We consider the possible advantages of visualization in supporting musical interpretation. Specifically, we investigate the use of visualizations in making a subjective judgement of a student&#39;s performance compared to reference âexpertâ performance for ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:VVWYIlCLw9MJ:scholar.google.com/&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15259193138755425621&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15259193138755425621&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=2</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6266249</url>
        <title status="complete" source="scholar.google.com">Real-Time Pitch Training System for Violin Learners</title>
        <pdf>http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Workshops/data/4729a163.pdf</pdf>
        <author status="partial" source="scholar.google.com">JH Wang, SA Wang, WC Chen&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract This paper specifically targets violin learners who are working on their pitch accuracy. We employ a pitch tracking algorithm to extract the pitch played. Through volume thresholding and region detection, only parts of frames are processed. So our system can ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:gMp1DiddLkkJ:scholar.google.com/&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5273254636025137792&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="48">
    <url>http://onlinelibrary.wiley.com/doi/10.1002/cjce.5450840212/abstract</url>
    <title status="complete" source="scholar.google.com">Enhanced Cometabolic Transformation of 4âChlorophenol in the Presence of Phenol by Granular Activated Carbon Adsorption</title>
    <author status="complete" source="scholar.google.com">KC Loh, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">The Canadian Journal of Chemical &#8230;</proceeding>
    <year>2006</year>
    <source>Wiley Online Library</source>
    <snippet status="partial" source="scholar.google.com">... Keywords: cometabolism, adsorption, phenol, 4-chlorophenol, Pseudomonas putida,granular activated carbon * Author to whom correspondence may be addressed. E-mailaddress: chelohkc@nus.edu.sg Page 2. VOLUME 84, APRIL 2006 ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:hqSdwSxiIFwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>4</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=6638413795110528134&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>2</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=6638413795110528134&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="2">
        <result id="0">
        <url>http://www.springerlink.com/index/82U357P875060726.pdf</url>
        <title status="complete" source="scholar.google.com">Biodegradation of aromatic compounds: current status and opportunities for biomolecular approaches</title>
        <author status="complete" source="scholar.google.com">B Cao, K Nagarajan, KC Loh</author>
        <proceeding status="complete" source="scholar.google.com">Applied microbiology and biotechnology</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Biodegradation can achieve complete and cost-effective elimination of aromatic pollutants through harnessing diverse microbial metabolic processes. Aromatics biodegradation plays an important role in environmental cleanup and has been ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:knC4dLH4U5cJ:scholar.google.com/&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10904332563845247122&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>48</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10904332563845247122&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=2</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://www.scholarbank.nus.edu.sg/handle/10635/13174</url>
        <title status="complete" source="scholar.google.com">Proteomics Analysis of Pseudomonas putida in Biodegradation of Aromatic Compounds</title>
        <pdf>http://www.scholarbank.nus.edu.sg/bitstream/handle/10635/13174/Thesis_Cao%20Bin_Revised.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">CAO BIN</author>
        <year>2008</year>
        <source>scholarbank.nus.edu.sg</source>
        <snippet status="partial" source="scholar.google.com">In this research, proteomics analysis was used to study three previously reported phenomena in biodegradation involving Pseudomonas putida. In the first system, 36 differentially expressed proteins, including 8 catabolic enzymes, were identified. 49 ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8Kzl7wzBt9AJ:scholar.google.com/&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15039701741935635696&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="49">
    <url>http://dl.acm.org/citation.cfm?id=1631491</url>
    <title status="complete" source="scholar.google.com">Mogfun: musical mobile group for fun</title>
    <pdf>http://www.comp.nus.edu.sg/~yzhou86/mmde976-zhou.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Zhou, Z Li, D Tan, G Percival, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 17th ACM &#8230;</proceeding>
    <year>2009</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Yinsheng Zhou, Zhonghua Li, Dillion Tan, Graham Percival, Ye Wang School ofComputing (SoC) National University of Singapore Singapore 117417 {yzhou86, lizhongh,dilliont, wangye}@comp.nus.edu.sg graham@percival-music.ca ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:MspryB1N3owJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>8</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=10150635400451574322&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>2</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=10150635400451574322&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="2">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1979016</url>
        <title status="complete" source="scholar.google.com">MOGCLASS: evaluation of a collaborative system of mobile devices for classroom music education of young children</title>
        <pdf>https://www0.comp.nus.edu.sg/~zhaosd/paper/chi2011_mogclass.pdf</pdf>
        <author status="partial" source="scholar.google.com">Y Zhou, G Percival, X Wang, Y Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 2011 &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Composition, listening, and performance are essential activities in classroom music education, yet conventional music classes impose unnecessary limitations on students&#39; ability to develop these skills. Based on in-depth fieldwork and a user-centered design ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jQvxbCYwtW8J:scholar.google.com/&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8049392850589256589&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8049392850589256589&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=2</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1874048</url>
        <title status="complete" source="scholar.google.com">Mogclass: a collaborative system of mobile devices forclassroom music education</title>
        <pdf>http://www.comp.nus.edu.sg/~yzhou86/mmshc05843-zhou.pdf</pdf>
        <author status="partial" source="scholar.google.com">Y Zhou, G Percival, X Wang, Y Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We introduce MOGCLASS: a system of networked mobile devices to amplify and extend children&#39;s capabilities to perceive, perform and produce music collaboratively in classroom context. MOGCLASS includes various features for students to enhance their ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:IzwNeBaw2pYJ:scholar.google.com/&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>15</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10870194261162015779&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=10870194261162015779&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=2</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="50">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284586</url>
    <title status="complete" source="scholar.google.com">Pop music beat detection in the huffman coded domain</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2007_Pop_Music_Beat_Detection_in_the_Huffman_Coded_Domain.pdf</pdf>
    <author status="complete" source="scholar.google.com">J Zhu, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2007 IEEE International &#8230;</proceeding>
    <year>2007</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. POP MUSIC BEAT DETECTION IN THE HUFFMAN CODED DOMAIN Jia Zhu and YeWang Department of Computer Science, School of Computing National University of Singapore,Singapore 177543 {zhujia, wangye}lcomp.nus.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:4gW-SQhyFW0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>12</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=7860314104567563746&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=7860314104567563746&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6298531</url>
        <title status="complete" source="scholar.google.com">A Synaesthetic Approach for Image Slideshow Generation</title>
        <pdf>http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Conference/data/4711a985.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Xiang, MS Kankanhalli</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo (ICME), 2012 &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we present a novel automatic image slideshow system that explores a new medium between images and music. It can be regarded as a new image selection and slideshow composition criterion. Based on the idea of``hearing colors, seeing sounds&quot; ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:r508TLR6NKgJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12120447411952590255&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="51">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036746</url>
    <title status="complete" source="scholar.google.com">Efficient Partial Spectrum Reconstruction using an Asymmetric PQMF Algorithm for MPEG-Coded Stereo Audio</title>
    <pdf>http://cecs.uci.edu/~papers/icme06/pdfs/0000901.pdf</pdf>
    <author status="complete" source="scholar.google.com">W Huang, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Multimedia and Expo, 2006 IEEE &#8230;</proceeding>
    <year>2006</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... CODED STEREO AUDIO Wendong Huang and Ye Wang School of Computing,National University of Singapore 3 Science Drive, Singapore 117543 {Huangwd,wangye}@comp.nus.edu.sg ABSTRACT This paper presents ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:eybAX40jtswJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>16</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=14751016719602427515&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=14751016719602427515&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="52">
    <url>http://dl.acm.org/citation.cfm?id=1874048</url>
    <title status="complete" source="scholar.google.com">Mogclass: a collaborative system of mobile devices forclassroom music education</title>
    <pdf>http://www.comp.nus.edu.sg/~yzhou86/mmshc05843-zhou.pdf</pdf>
    <author status="partial" source="scholar.google.com">Y Zhou, G Percival, X Wang, Y Wang&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
    <year>2010</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Classroom Music Education Yinsheng Zhou, Graham Percival, Xinxi Wang, Ye Wang, ShengdongZhao School of Computing (SoC) National University of Singapore Singapore 117417 {yzhou86,wangxinx, wangye, zhaosd}@comp.nus.edu.sg graham@percival-music.ca ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:IzwNeBaw2pYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>15</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=10870194261162015779&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=10870194261162015779&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1979016</url>
        <title status="complete" source="scholar.google.com">MOGCLASS: evaluation of a collaborative system of mobile devices for classroom music education of young children</title>
        <pdf>https://www0.comp.nus.edu.sg/~zhaosd/paper/chi2011_mogclass.pdf</pdf>
        <author status="partial" source="scholar.google.com">Y Zhou, G Percival, X Wang, Y Wang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 2011 &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Composition, listening, and performance are essential activities in classroom music education, yet conventional music classes impose unnecessary limitations on students&#39; ability to develop these skills. Based on in-depth fieldwork and a user-centered design ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jQvxbCYwtW8J:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>12</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8049392850589256589&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8049392850589256589&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=1</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="53">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4450581</url>
    <title status="complete" source="scholar.google.com">Watermarking video clips with workload information for DVS</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2008_Watermarking_Video_Clips_with_Workload_Information_for_DVS.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Huang, S Chakraborty, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">VLSI Design, 2008. VLSID &#8230;</proceeding>
    <year>2008</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. Watermarking Video Clips with Workload Information for DVS Yicheng Huang SamarjitChakraborty Ye Wang Department of Computer Science, National University of SingaporeE-mail: {huangyic, samarjit, wangye}@comp.nus.edu.sg Abstract ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:y_Gngp455bUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>10</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=13106945643491357131&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=13106945643491357131&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1459619</url>
        <title status="complete" source="scholar.google.com">Multimedia power management on a platter: from audio to video &amp; games</title>
        <author status="complete" source="scholar.google.com">S Chakraborty, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 16th ACM international &#8230;</proceeding>
        <year>2008</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Today, battery-life is a major design concern for all portable devices ranging from cell phones to PDAs and portable game consoles. The purpose of this tutorial will be to give an overview of power management techniques that are applicable to multimedia ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_MHf5UBV64YJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9721957957832262140&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="54">
    <url>http://dl.acm.org/citation.cfm?id=1496057</url>
    <title status="complete" source="scholar.google.com">Decoding-workload-aware video encoding</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2008_Decoding_workload_aware_Video_Encoding.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Huang, G Hong, VA Tran, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 18th &#8230;</proceeding>
    <year>2008</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Yicheng Huang, Guangming Hong, Vu An Tran and Ye Wang Department of Computer ScienceNational University of Singapore Law Link, Singapore 117590 Republic of Singapore {huangyic,honggm, wangye}@comp.nus.edu.sg; tranvuan82@gmail.com ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:_8f7hM8JCUwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>10</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=5478921208566892543&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=5478921208566892543&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="55">
    <url>http://www.comp.nus.edu.sg/~yzhou86/fp047.pdf</url>
    <title status="complete" source="scholar.google.com">MOGAT: Mobile Games with Auditory Training for Children with Cochlear Implants</title>
    <pdf>http://www.comp.nus.edu.sg/~yzhou86/fp047.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Zhou, KC Sim, P Tan, Y Wang</author>
    <proceeding status="complete" source="scholar.google.com">ACM Multimedia</proceeding>
    <year>2012</year>
    <source>comp.nus.edu.sg</source>
    <snippet status="partial" source="scholar.google.com">... Yinsheng Zhou1, Khe Chai Sim1, Patsy Tan2, Ye Wang1 1School of Computing, NationalUniversity of Singapore, 117417, Singapore 2Singapore General Hospital, 169608, Singapore{yzhou86, simkc, wangye}@comp.nus.edu.sg, patsy.tan.lp@sgh.com.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:4KKKkSKeXiAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:4KKKkSKeXiAJ:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=2332475528332354272&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>http://www.comp.nus.edu.sg/~yzhou86/mtd015.pdf</url>
        <title status="complete" source="scholar.google.com">MOGAT: A Cloud-based Mobile Game System with Auditory Training for Children with Cochlear Implants</title>
        <pdf>http://www.comp.nus.edu.sg/~yzhou86/mtd015.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Zhou, TJKP Monserrat, Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT Musical auditory habilitation is an essential process in adapting cochlear implant recipients to the musical hearing context provided by cochlear implants. However, due to the cost and time limitation, it is impossible for hearing healthcare professionals to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:zzFc9vzuqioJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:zzFc9vzuqioJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="56">
    <url>http://www.springerlink.com/index/B054378M53388853.pdf</url>
    <title status="complete" source="scholar.google.com">A joint encoderâdecoder framework for supporting energy efficient audio decoding</title>
    <pdf>https://www-new.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2009_A_Joint_Encoder-Decoder_Framework_for_Supprting_Energy_Efficient_Audio_Decoding.pdf</pdf>
    <author status="complete" source="scholar.google.com">W Huang, Y Wang</author>
    <proceeding status="complete" source="scholar.google.com">Multimedia systems</proceeding>
    <year>2009</year>
    <source>Springer</source>
    <snippet status="partial" source="scholar.google.com">... Experimental results show the effectiveness of our approach. Communicated by CormacSreenan. W. Huang Â· Y. Wang (B) School of Computing, National University of Singapore,Singapore, Singapore e-mail: wangye@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:S_7a7PXw8iMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>11</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=2590397674722885195&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=2590397674722885195&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
        <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
        <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
        <snippet status="partial" source="scholar.google.com">Smart mobile devices are proliferating, at increasingly smaller sizes. Many of these are capable of capturing and playing a significant quantity of audio and video, as well as uploading and downloading media to social networks. However, there has been relatively ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="57">
    <url>https://dl.comp.nus.edu.sg/dspace/handle/1900.100/3056</url>
    <title status="complete" source="scholar.google.com">Automatic music transcription using audio-visual fusion for violin practice in home environment</title>
    <pdf>https://dl.comp.nus.edu.sg/dspace/bitstream/1900.100/3056/1/TRA7-09.pdf</pdf>
    <author status="complete" source="scholar.google.com">B Zhang, Y Wang</author>
    <year>2009</year>
    <source>dl.comp.nus.edu.sg</source>
    <snippet status="partial" source="scholar.google.com">... for Violin Practice in Home Environment Bingjun Zhang and Ye Wang School ofComputing, National University of Singapore {bingjun, wangye}@comp.nus.edu.sg ... 1http://www.comp.nus.edu.sg/~bingjun/avamt.html Page 7. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:WDqOhljuXd0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>9</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=15951167519198165592&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>2</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=15951167519198165592&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="2">
        <result id="0">
        <url>http://137.132.14.55/handle/10635/20949</url>
        <title status="complete" source="scholar.google.com">Adaptive multimodal fusion based similarity measures in music information retrieval</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/20949/ZhangBJ.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">Z BINGJUN</author>
        <year>2010</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">In the field of music information retrieval (MIR), one fundamental research problem is the measuring of the similarity between music documents. Based on a viable similarity measure, MIR systems can be made more effective to help users retrieve relevant music information. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9FjpHoxTlRIJ:scholar.google.com/&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1339068325491726580&amp;hl=en&amp;num=2&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=2390855</url>
        <title status="complete" source="scholar.google.com">Learning and extraction of violin instrumental controls from audio signal</title>
        <pdf>http://up.stevetjoa.com/p25.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Perez Carrillo, MM Wanderley</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the second &#8230;</proceeding>
        <year>2012</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Acquisition of instrumental gestures in musical performances is an important task used in different fields ranging from acoustics and sound synthesis to motor learning or electroacoustic performances. The most common approach for acquiring gestures is by ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="58">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5659899</url>
    <title status="complete" source="scholar.google.com">Influence of Channel Layer Thickness on the Electrical Performances of Inkjet-Printed In-Ga-Zn Oxide Thin-Film Transistors</title>
    <pdf>http://www.ds.bilkent.edu.tr/makale/makale2011/IEEETrans-Elec-Dev1_2011_HVD.pdf</pdf>
    <author status="partial" source="scholar.google.com">Y Wang, XW Sun, GKL Goh&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Electron Devices, IEEE &#8230;</proceeding>
    <year>2011</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... Hong Yu Yu (SM&#39;10) received the B.Eng. de- gree from Tsinghua University, Beijing, China, in1999, the MASc. degree from Toronto University, Toronto, ON, Canada, in 2001, and the Ph.D.degree from the National University of Singapore (NUS), Singapore, in 2005. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:uBKTKiKsTMYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>6</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=14288984980482888376&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>9</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=14288984980482888376&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="9">
        <result id="0">
        <url>http://iopscience.iop.org/0268-1242/26/8/085012</url>
        <title status="complete" source="scholar.google.com">Improvement in the performance of an InGaZnO thin-film transistor by controlling interface trap densities between the insulator and active layer</title>
        <author status="partial" source="scholar.google.com">TT Trinh, K Ryu, K Jang, W Lee, S Baek&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Semiconductor &#8230;</proceeding>
        <year>2011</year>
        <source>iopscience.iop.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract An amorphous InGaZnO film fabricated by radio frequency magnetron sputtering in only an Ar-reactive gas shows high conductivity, and a thin-film transistors (TFTs)-based IGZO active layer expresses a poor on/off current ratio with a high off current and high ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:aunKHkOdXycJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2837159201871685994&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>6</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2837159201871685994&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://www.sciencedirect.com/science/article/pii/S0026271411002812</url>
        <title status="complete" source="scholar.google.com">Effects of channel thickness variation on bias stress instability of InGaZnO thin-film transistors</title>
        <pdf>http://semicim.yonsei.ac.kr/publication/paper/int/64.cho.pdf</pdf>
        <author status="complete" source="scholar.google.com">EN Cho, JH Kang, I Yun</author>
        <proceeding status="complete" source="scholar.google.com">Microelectronics Reliability</proceeding>
        <year>2011</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">Here, we report on the effects of channel (or active) layer thickness on the bias stress instability of InGaZnO (IGZO) thin-film transistors (TFTs). The investigation on variations of TFT characteristics under the electrical bias stress is very crucial for commercial ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:yb4IhbmeMn0J:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9021447523214606025&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9021447523214606025&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6129495</url>
        <title status="complete" source="scholar.google.com">Debye Length and Active Layer Thickness-Dependent Performance Variations of Amorphous Oxide-Based TFTs</title>
        <author status="complete" source="scholar.google.com">J Jeong, Y Hong</author>
        <proceeding status="complete" source="scholar.google.com">Electron Devices, IEEE Transactions on</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract We analyzed the active layer thickness-dependent performance variations of amorphous oxide-based semiconductor thin-film transistors (AOS TFTs), which are typically operated in depletion mode by using an ATLAS 2-D device simulator. The negative shift of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:-w8wt5emLoYJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=9668848620554686459&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=9668848620554686459&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://pubs.acs.org/doi/abs/10.1021/am201776p</url>
        <title status="complete" source="scholar.google.com">Reduced contact resistance in inkjet printed high-performance amorphous indium gallium zinc oxide transistors</title>
        <author status="partial" source="scholar.google.com">JW Hennek, Y Xia, K Everaerts&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Applied Materials &amp; &#8230;</proceeding>
        <year>2012</year>
        <source>ACS Publications</source>
        <snippet status="partial" source="scholar.google.com">Solution processing of amorphous metal oxide materials to fabricate thin-film transistors (TFTs) has received great recent interest. We demonstrate here an optimized âinkâ and printing process for inkjet patterning of amorphous indium gallium zinc oxide (a-IGZO) ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:AAx3H8Jv--YJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16644019727420034048&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16644019727420034048&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://www.sciencedirect.com/science/article/pii/S0040609011016166</url>
        <title status="complete" source="scholar.google.com">Role of O&lt; sub&gt; 2&lt;/sub&gt;/Ar mixing ratio on the performances of IZO thin film transistors fabricated using a two-step deposition process</title>
        <author status="complete" source="scholar.google.com">W Kim, SH Lee, JH Bang, HS Uhm, JS Park</author>
        <proceeding status="complete" source="scholar.google.com">Thin Solid Films</proceeding>
        <year>2011</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">In this study, a simple method of fabricating a thin-film transistor (TFT) with a double-layered channel using indiumâzinc-oxide (IZO) films was proposed. Two IZO films used as channel layers were consecutively deposited via sputtering without stopping the vacuum and only ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jPA-FOiRFKQJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11823235347712503948&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11823235347712503948&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6207203</url>
        <title status="complete" source="scholar.google.com">High performance solution-deposited amorphous indium gallium zinc oxide thin film transistors by oxygen plasma treatment</title>
        <author status="partial" source="scholar.google.com">PK Nayak, MN Hedhili, D Cha&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Applied Physics &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Solution-deposited amorphous indium gallium zinc oxide (a-IGZO) thin film transistors (TFTs) with high performance were fabricated using O 2-plasma treatment of the films prior to high temperature annealing. The O 2-plasma treatment resulted in a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtv3Fi_unOsJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16977706581296143258&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://pic.sagepub.com/content/early/2013/01/08/0954406212473041.abstract</url>
        <title status="complete" source="scholar.google.com">Web Tension Control of Multispan Roll to Roll System by Artificial Neural Networks for Printed Electronics</title>
        <author status="complete" source="scholar.google.com">KH Choi, M Zubair, G Ponniah</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the Institution of &#8230;</proceeding>
        <year>2013</year>
        <source>pic.sagepub.com</source>
        <snippet status="partial" source="scholar.google.com">Abstract The mass production of printed electronic devices can be achieved by roll-to-roll system that requires highly regulated web tension. This highly regulated tension is required to minimize printing register error and maintain proper roughness and thickness of the ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://pubs.acs.org/doi/abs/10.1021/am3008278</url>
        <title status="complete" source="scholar.google.com">Improved electrical performance of an oxide thin-film transistor having multi-stacked active layers using a solution process</title>
        <author status="partial" source="scholar.google.com">DJ Kim, DL Kim, YS Rim, CH Kim&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; Applied Materials &amp; &#8230;</proceeding>
        <year>2012</year>
        <source>ACS Publications</source>
        <snippet status="partial" source="scholar.google.com">Thin-film transistors (TFTs) with multi-stacked active layers (MSALs) have been studied to improve their electrical performance. The performance enhancement with MSALs has been attributed to higher film density in the effective channel; the density was higher because ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:jdFltCgaTFQJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6074258759543542157&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6407736</url>
        <title status="complete" source="scholar.google.com">Transparent Junctionless Thin-Film Transistors With Tunable Operation Mode</title>
        <author status="complete" source="scholar.google.com">G Zhang, Q Wan, J Sun, G Wu, L Zhu</author>
        <proceeding status="complete" source="scholar.google.com">ieeexplore.ieee.org</proceeding>
        <snippet status="partial" source="scholar.google.com">Abstract Junctionless low-voltage transparent indium-zinc-oxide (IZO) thin-film transistors (TFTs) gated by $ hbox {SiO} _ {2} $-based solid electrolyte films are fabricated on glass substrates by a full room-temperature process. The attractive feature of such TFTs is that ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="59">
    <url>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</url>
    <title status="complete" source="scholar.google.com">Perception-Aware Low-Power Media Processing for Portable Devices</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2010_Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices_.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Wang</author>
    <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
    <snippet status="partial" source="scholar.google.com">Page 1. IEEE COMSOC MMTC E-Letter http://www.comsoc.org/~mmc/ 1/4 Vol.x, No.x, xxxxx 2010Perception-Aware Low-Power Media Processing for Portable Devices Ye Wang (IEEE Member),National University of Singapore, Singapore wangye@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:mtWyFK_ygCYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mtWyFK_ygCYJ:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <numVersions>2</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=2774484204240688538&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="60">
    <url>http://dl.acm.org/citation.cfm?id=2390859</url>
    <title status="complete" source="scholar.google.com">When music, information technology, and medicine meet</title>
    <author status="complete" source="scholar.google.com">Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the second international ACM &#8230;</proceeding>
    <year>2012</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. When Music, Information Technology, and Medicine Meet Ye Wang NationalUniversity of Singapore School of Computing Singapore 117417, Republic ofSingapore wangye@comp.nus.edu.sg ABSTRACT From Napster ...</snippet>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="61">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1540230</url>
    <title status="complete" source="scholar.google.com">A method for separating drum objects from polyphonic musical signals</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_A_Method_for_Separating_Drum_Objects_from_Polyphonic_Musical_Signals.pdf</pdf>
    <author status="complete" source="scholar.google.com">W Huang, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Applications of Signal Processing to Audio &#8230;</proceeding>
    <year>2005</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... Wendong Huang Ye Wang School of Computing National University of SingaporeSingapore 117543 huangwd@comp.nus.edu.sg School of Computing National Universityof Singapore Singapore 117543 wangye@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:kmLNmfRrmJwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>13</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=11283887564673344146&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="62">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4749649</url>
    <title status="complete" source="scholar.google.com">Power Management for Mobile Multimedia: From Audio to Video &amp; Games</title>
    <author status="complete" source="scholar.google.com">S Chakraborty, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">VLSI Design, 2009 22nd International &#8230;</proceeding>
    <year>2009</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. Tutorial T7-B Power Management for Mobile Multimedia: From Audio to Video and GamesSamarjit Chakraborty, National Univ. of Singapore, Singapore, samarjit@comp.nus.edu.sg YeWang, National Univ. of Singapore, Singapore, wangye@comp.nus.edu.sg Abstract ...</snippet>
    <numVersions>4</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=17404510150521027267&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=17404510150521027267&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>http://ijarcce.com/upload/november/22-%20Strategies%20of%20Efficient%20Backlighting.pdf</url>
        <title status="complete" source="scholar.google.com">Strategies of Efficient Backlighting System for Portable Two-Way Radio Design</title>
        <pdf>http://ijarcce.com/upload/november/22-%20Strategies%20of%20Efficient%20Backlighting.pdf</pdf>
        <author status="complete" source="scholar.google.com">ALH Jin, A bin Marzuki, BE Khoo</author>
        <proceeding status="complete" source="scholar.google.com">Strategies</proceeding>
        <year>2012</year>
        <source>ijarcce.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT: In the 2-way radios market, long battery life has becomes an essential &amp; critical customer requirement for communications during critical public safety or rescue missions. Thus in tandem with this need, various researches have been conducted by industry ...</snippet>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:ym490dnXK2MJ:scholar.google.com/&amp;hl=en&amp;num=1&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="63">
    <url>http://dl.acm.org/citation.cfm?id=2071949</url>
    <title status="complete" source="scholar.google.com">Document dependent fusion in multimodal music retrieval</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2011_Document_Dependent_Fusion_in_Multimodal_Music_Retrieval.pdf</pdf>
    <author status="complete" source="scholar.google.com">Z Li, B Zhang, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 19th ACM international &#8230;</proceeding>
    <year>2011</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Retrieval Zhonghua Li, Bingjun Zhang, Ye Wang School of Computing, National University ofSingapore, Singapore {lizhongh, bingjun, wangye}@comp.nus.edu.sg ... Trecvid 2004 search andfeature extraction task by nus pris. In NIST TRECVID Workshop, 2004. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:JyU9BhZI5j4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>3</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=4532389334426133799&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="64">
    <url>http://web.mit.edu/~sgraves/www/PhD_Thesis_WangYexin_.pdf</url>
    <title status="complete" source="scholar.google.com">Logistics Coordination in Vendor-Buyer Systems</title>
    <pdf>http://web.mit.edu/~sgraves/www/PhD_Thesis_WangYexin_.pdf</pdf>
    <author status="complete" source="scholar.google.com">WY Xin</author>
    <year>2007</year>
    <source>mit.edu</source>
    <snippet status="partial" source="scholar.google.com">... By Wang Ye Xin Submitted to the Singapore-MIT Alliance (SMA) Programme ... I gratefullyacknowledge the support and resources made available to me through Singapore-MIT Alliance,a wonderful joint academic programme among NUS, NTU, and MIT. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:256XJ-vbdtEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:256XJ-vbdtEJ:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <numVersions>4</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=15093493004206841563&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="65">
    <url>http://www.comp.nus.edu.sg/~yuy/ism2012_submission_103.pdf</url>
    <title status="complete" source="scholar.google.com">Recognition and Summarization of Chord Progressions and Their Application to Music Information Retrieval</title>
    <pdf>http://www.comp.nus.edu.sg/~yuy/ism2012_submission_103.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Yu, R Zimmermann, Y Wang, V Oria</author>
    <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
    <snippet status="partial" source="scholar.google.com">... Application to Music Information Retrieval Yi Yu, Roger Zimmermann, Ye WangSchool of Computing National University of Singapore Singapore Email:{yuy,rogerz,wangye}@comp.nus.edu.sg Vincent Oria Dept. of Computer ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:1wU5V1EEP6UJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:1wU5V1EEP6UJ:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="66">
    <url>http://dl.acm.org/citation.cfm?id=2396459</url>
    <title status="complete" source="scholar.google.com">A daily, activity-aware, mobile music recommender system</title>
    <pdf>http://www.comp.nus.edu.sg/~wangxinx/papers/cammr_demo.pdf</pdf>
    <author status="complete" source="scholar.google.com">X Wang, Y Wang, D Rosenblum</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 20th ACM &#8230;</proceeding>
    <year>2012</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. A Daily, Activity-Aware, Mobile Music Recommender System Xinxi Wang,David Rosenblum, Ye Wang School of Computing, National University of Singapore{wangxinxi,david,wangye}@comp.nus.edu.sg ABSTRACT ...</snippet>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="67">
    <url>http://dl.acm.org/citation.cfm?id=1874154</url>
    <title status="complete" source="scholar.google.com">Automated sleep quality measurement using EEG signal: first step towards a domain specific music recommendation system</title>
    <pdf>http://staging.comp.nus.edu.sg/~wangye/papers/2.Applications_in_e-Health/2010_Automated_Sleep_Quality_Measurement_using_EEG_Signal.pdf</pdf>
    <author status="complete" source="scholar.google.com">W Zhao, X Wang, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">&#8230; of the international conference on Multimedia</proceeding>
    <year>2010</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Measurement using EEG Signal -First Step Towards a Domain Specific Music RecommendationSystem Wei Zhao, Xinxi Wang and Ye Wang School of Computing, National University ofSingapore {zhaowei,wangxinx,wangye}@comp.nus.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:1pYflbZaLTsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>5</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=4264164162422150870&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="68">
    <url>http://dl.acm.org/citation.cfm?id=2396458</url>
    <title status="complete" source="scholar.google.com">A domain-specific music search engine for gait training</title>
    <author status="complete" source="scholar.google.com">Z Li, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 20th ACM international conference &#8230;</proceeding>
    <year>2012</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. A Domain-Specific Music Search Engine for Gait Training Zhonghua Liand Ye Wang School of Computing, National University of Singapore, Singapore{lizhongh, wangye}@comp.nus.edu.sg ABSTRACT This paper ...</snippet>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="69">
    <url>http://cmsassets.comp.nus.edu.sg/~david/Publications/acmmm2012-fullpaper.pdf</url>
    <title status="complete" source="scholar.google.com">Context-Aware Mobile Music Recommendation for Daily Activities</title>
    <pdf>http://cmsassets.comp.nus.edu.sg/~david/Publications/acmmm2012-fullpaper.pdf</pdf>
    <author status="complete" source="scholar.google.com">X Wang, D Rosenblum, Y Wang</author>
    <year>2012</year>
    <source>cmsassets.comp.nus.edu.sg</source>
    <snippet status="partial" source="scholar.google.com">Page 1. Context-Aware Mobile Music Recommendation for Daily Activities Xinxi Wang,David Rosenblum, Ye Wang School of Computing, National University of Singapore{wangxinxi,david,wangye}@comp.nus.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:KIKGuT9CaWwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:KIKGuT9CaWwJ:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <numVersions>2</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=7811847870113808936&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=7811847870113808936&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=2390859</url>
        <title status="complete" source="scholar.google.com">When music, information technology, and medicine meet</title>
        <author status="complete" source="scholar.google.com">Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the second international ACM &#8230;</proceeding>
        <year>2012</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract From Napster to YouTube and iTunes, music has always been a major driving force of Internet technologies. A huge amount of music content is now accessible to the public. Organizing and categorizing this content to support an effective recommendation system ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="70">
    <url>http://dl.acm.org/citation.cfm?id=1459619</url>
    <title status="complete" source="scholar.google.com">Multimedia power management on a platter: from audio to video &amp; games</title>
    <author status="complete" source="scholar.google.com">S Chakraborty, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 16th ACM international &#8230;</proceeding>
    <year>2008</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. Multimedia Power Management on a Platter: From Audio to Video &amp; Games SamarjitChakraborty Ye Wang Department of Computer Science, National University of SingaporeE-mail: {samarjit, wangye}@comp.nus.edu.sg ABSTRACT ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:_MHf5UBV64YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>2</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=9721957957832262140&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="71">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4517560</url>
    <title status="complete" source="scholar.google.com">Onset detection in pitched non-percussive music using warping-compensated correlation</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2008_Onset_Detection_in_Piteched_Non-Percussive_Music_Using_Warping_Compensated_Correlation.pdf</pdf>
    <author status="complete" source="scholar.google.com">O Schleusing, B Zhang, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Acoustics, Speech and &#8230;</proceeding>
    <year>2008</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... COMPENSATED CORRELATION Olaf Schleusing, Bingjun Zhang, Ye Wang Schoolof Computing, National University of Singapore audio@schleusing.de, {bingjun,wangye}@comp.nus.edu.sg ABSTRACT Automatically extracting ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:Iyh_8psTDngJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>14</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=8650873494734579747&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="72">
    <url>http://www.ruf.rice.edu/~mobile/publications/hong08sensecoding.pdf</url>
    <title status="complete" source="scholar.google.com">Accelerometer-Assisted Motion Estimation for Efficient Video Encoding</title>
    <pdf>http://www.ruf.rice.edu/~mobile/publications/hong08sensecoding.pdf</pdf>
    <author status="complete" source="scholar.google.com">G Hong, A Rahmati, Y Wang, L Zhong</author>
    <proceeding status="complete" source="scholar.google.com">ruf.rice.edu</proceeding>
    <snippet status="partial" source="scholar.google.com">Page 1. 1 Accelerometer-Assisted Motion Estimation for Efficient Video Encoding GuangmingHong1, Ahmad Rahmati2, Ye Wang1, Lin Zhong2 1School of Computing National Universityof Singapore, Singapore, 117590 {honggm, wangye}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:XsUfeEbjDnoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:XsUfeEbjDnoJ:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <numVersions>2</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=8795217014103721310&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="73">
    <url>http://www.comp.nus.edu.sg/~yzhou86/mtd015.pdf</url>
    <title status="complete" source="scholar.google.com">MOGAT: A Cloud-based Mobile Game System with Auditory Training for Children with Cochlear Implants</title>
    <pdf>http://www.comp.nus.edu.sg/~yzhou86/mtd015.pdf</pdf>
    <author status="complete" source="scholar.google.com">Y Zhou, TJKP Monserrat, Y Wang</author>
    <proceeding status="complete" source="scholar.google.com">comp.nus.edu.sg</proceeding>
    <snippet status="partial" source="scholar.google.com">... Auditory Training for Children with Cochlear Implants Yinsheng Zhou, Toni-Jan KeithP. Monserrat, Ye Wang School of Computing, National University of Singapore, 117417,Singapore {yzhou86, tonijank, wangye}@comp.nus.edu.sg ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:zzFc9vzuqioJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:zzFc9vzuqioJ:scholar.google.com/+author:%22Wang+Ye%22+NUS&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</htmlLink>
    <citedBy totalresults="0">
    </citedBy>
  </result>
  <result id="74">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432643</url>
    <title status="complete" source="scholar.google.com">LyricAlly: Automatic synchronization of textual lyrics to acoustic music signals</title>
    <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.119.3200&amp;rep=rep1&amp;type=pdf</pdf>
    <author status="partial" source="scholar.google.com">MY Kan, Y Wang, D Iskandar, TL New&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Audio, Speech, and &#8230;</proceeding>
    <year>2008</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">Page 1. 338 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGEPROCESSING, VOL. 16, NO. 2, FEBRUARY 2008 LyricAlly: AutomaticSynchronization of Textual Lyrics to Acoustic Music Signals Min-Yen Kan, Ye ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:UKV6kRlYEmEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>16</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=6994750038097962320&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>27</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=6994750038097962320&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="27">
        <result id="0">
        <url>http://books.google.com/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA395&amp;ots=oDOSrGiza3&amp;sig=kVRw7nokt6M8mvDFGnkwKEgfrK4</url>
        <title status="complete" source="scholar.google.com">Segmentation-based lyrics-audio alignment using dynamic programming</title>
        <pdf>http://ismir2008.ismir.net/papers/ISMIR2008_126.pdf..</pdf>
        <author status="complete" source="scholar.google.com">K Lee, M Cremer</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the 9th International Conference on Music &#8230;</proceeding>
        <year>2008</year>
        <source>books.google.com</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In this paper, we present a system for automatic alignment of textual lyrics with musical audio. Given an input audio signal, structural segmentation is ï¬rst performed and similar segments are assigned a label by computing the distance between the segment ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:WHVqdJo9770J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13686225526189618520&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>14</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13686225526189618520&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1863626</url>
        <title status="complete" source="scholar.google.com">Automatic recognition of lyrics in singing</title>
        <pdf>http://asmp.eurasipjournals.com/content/pdf/1687-4722-2010-546047.pdf</pdf>
        <author status="complete" source="scholar.google.com">A Mesaros, T Virtanen</author>
        <proceeding status="partial" source="scholar.google.com">EURASIP Journal on Audio, Speech, and Music &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The paper considers the task of recognizing phonemes and words from a singing input by using a phonetic hidden Markov model recognizer. The system is targeted to both monophonic singing and singing in polyphonic music. A vocal separation algorithm is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2C3OvkUft-oJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16913021310027574744&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=16913021310027574744&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://www.aes.org/e-lib/browse.cfm?elib=15970</url>
        <title status="complete" source="scholar.google.com">Music Listening in the Future: Augmented Music-Understanding Interfaces and Crowd Music Listening</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/AES2011goto.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Goto</author>
        <proceeding status="complete" source="scholar.google.com">Watermark</proceeding>
        <year>2012</year>
        <source>aes.org</source>
        <snippet status="partial" source="scholar.google.com">In the future, music listening can be more active, more immersive, richer, and deeper by using automatic music-understanding technologies (semantic audio analysis). In the first half of this invited talk, four Augmented Music-Understanding Interfaces that facilitate deeper ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0mzmCDUpmdYJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15463436103312633042&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15463436103312633042&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://dl.acm.org/citation.cfm?id=2072531</url>
        <title status="complete" source="scholar.google.com">The need for music information retrieval with user-centered and multimodal strategies</title>
        <author status="partial" source="scholar.google.com">C Liem, M MÃ¼ller, D Eck, G Tzanetakis&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 1st &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Music is a widely enjoyed content type, existing in many multifaceted representations. With the digital information age, a lot of digitized music information has theoretically become available at the user&#39;s fingertips. However, the abundance of ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:U7b3CYt6UGgJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numCite>4</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=7516642515667629651&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5495212</url>
        <title status="complete" source="scholar.google.com">Singing information processing based on singing voice modeling</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/ICASSP2010goto.pdf</pdf>
        <author status="partial" source="scholar.google.com">M Goto, T Saitou, T Nakano&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics Speech and &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we propose a novel area of research referred to as singing information processing. To shape the concept of this area, we first introduce singing understanding systems for synchronizing between vocal melody and corresponding lyrics, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:Qq4zaFyHAJkJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11024960718757932610&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11024960718757932610&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5346497</url>
        <title status="complete" source="scholar.google.com">A novel framework for recognizing phonemes of singing voice in polyphonic music</title>
        <pdf>http://staff.aist.go.jp/h.fujihara/pdf/waspaa2009_fujihara.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Fujihara, M Goto, HG Okuno</author>
        <proceeding status="partial" source="scholar.google.com">Applications of Signal &#8230;</proceeding>
        <year>2009</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract A novel method is described that can be used to recognize the phoneme of a singing voice (vocal) in polyphonic music. Though we focus on the voiced phoneme in this paper, this method is design to concurrently recognize other elements of a singing voice ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:n85tdPiJTygJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2904691984875507359&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2904691984875507359&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://www.matthiasmauch.net/_pdf/mauch_laa_2010.pdf</url>
        <title status="complete" source="scholar.google.com">Lyrics-to-audio alignment and phrase-level segmentation using incomplete internet-style chord annotations</title>
        <pdf>http://www.matthiasmauch.net/_pdf/mauch_laa_2010.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Mauch, H Fujihara, M Goto</author>
        <proceeding status="partial" source="scholar.google.com">&#8230; of the 7th Sound and Music &#8230;</proceeding>
        <year>2010</year>
        <source>matthiasmauch.net</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT We propose two novel lyrics-to-audio alignment methods which make use of additional chord information. In the first method we extend an existing hidden Markov model (HMM) for lyrics alignment [1] by adding a chord model based on the chroma features ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2CXce0GZO6YJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:2CXce0GZO6YJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>8</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11978336140451915224&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11978336140451915224&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf</url>
        <title status="complete" source="scholar.google.com">æ­å£°æå ±å¦çã®æè¿ã®ç ç©¶</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å°</author>
        <proceeding status="complete" source="scholar.google.com">æ¥æ¬é³é¿å­¦ä¼èª</proceeding>
        <year>2008</year>
        <source>staff.aist.go.jp</source>
        <snippet status="complete" source="scholar.google.com">â Recent studies on singing information processing. ââ Masataka Goto, Takeshi Saitou, Tomoyasu Nakano and Hiromasa Fujihara (National Institute of Ad- vanced Industrial Science and Technology (AIST), Tsukuba, 305â8568) e-mail: m.goto@aist.go.jp</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3835003938146142193&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>6</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3835003938146142193&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://www.aes.org/e-lib/browse.cfm?elib=15944</url>
        <title status="complete" source="scholar.google.com">New Developments in Music Information Retrieval</title>
        <pdf>https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/0/65cd0224ccb5368ec12579950043afe0/$FILE/2011_Mueller_NewDevelopmentsMIR_AES42-Ilmenau.pdf</pdf>
        <author status="complete" source="scholar.google.com">M MÃ¼ller</author>
        <proceeding status="complete" source="scholar.google.com">Watermark</proceeding>
        <year>2012</year>
        <source>aes.org</source>
        <snippet status="partial" source="scholar.google.com">The digital revolution has brought about a massive increase in the availability and distribution of music-related documents of various modalities comprising textual, audio, as well as visual material. Therefore, the development of techniques and tools for organizing, ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:06IGGF2KSd0J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>11</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15945428088121631443&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15945428088121631443&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5876304</url>
        <title status="complete" source="scholar.google.com">Integrating Additional Chord Information into HMM-Based Lyrics-to-Audio Alignment</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/IEEETASLP201201mauch.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Mauch, H Fujihara, M Goto</author>
        <proceeding status="partial" source="scholar.google.com">Audio, Speech, and Language &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Aligning lyrics to audio has a wide range of applications such as the automatic generation of karaoke scores, song-browsing by lyrics, and the generation of audio thumbnails. Existing methods are restricted to using only lyrics and match them to ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:4wt8-SvvXy0J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3269594826642557923&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=3269594826642557923&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <title status="complete" source="scholar.google.com">Singing Phoneme Class Detection In Polyphonic Music Recordings</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uJWqBAVmuaEJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11653457682537027000&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=11653457682537027000&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="11">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5583863</url>
        <title status="complete" source="scholar.google.com">Muvisync: Realtime music video alignment</title>
        <pdf>http://www.nuriaoliver.com/papers/MuViSync_ICME2010.pdf</pdf>
        <author status="complete" source="scholar.google.com">R Macrae, X Anguera, N Oliver</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia and Expo (ICME), &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In recent years, the popularity of compressed music files and online music downloads has increased dramatically. Today&#39;s users own large digital collections of high quality music on their computers and portable devices to be played in their homes or on ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:p-SRCiGe_fQJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17653439979110655143&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>2</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17653439979110655143&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="12">
        <url>http://www.springerlink.com/index/q321132ru207p862.pdf</url>
        <title status="complete" source="scholar.google.com">Simultaneous synchronization of text and speech for broadcast news subtitling</title>
        <author status="complete" source="scholar.google.com">J Gao, Q Zhao, T Li, Y Yan</author>
        <proceeding status="complete" source="scholar.google.com">Advances in Neural NetworksâISNN 2009</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract. In this paper, we present our initial effort in automatic generation of subtitle for live broadcast news programs, utilizing the fact that nearly perfect transcriptions are available. Instead of using the former error-prone automatic-speech-recognition (ASR)-based ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ye-xR2uPE3cJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=8580359405998043081&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=8580359405998043081&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="13">
        <title status="complete" source="scholar.google.com">Online detecting end times of spoken utterances for synchronization of live speech and its transcripts</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:2VezkPP68hkJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1869832719323846617&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1869832719323846617&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="14">
        <url>http://141.84.8.93/pubdb/publications/pub/baur2010ismir/baur2010ismir.pdf</url>
        <title status="complete" source="scholar.google.com">SongWords: Exploring Music Collections Through Lyrics</title>
        <pdf>http://141.84.8.93/pubdb/publications/pub/baur2010ismir/baur2010ismir.pdf</pdf>
        <author status="complete" source="scholar.google.com">D Baur, B Steinmayr, A Butz</author>
        <year>2010</year>
        <source>141.84.8.93</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT The lyrics of a song are an interesting, yet underused type of symbolic music data. We present SongWords, an application for tabletop computers that allows browsing and exploring a music collection based on its lyrics. Song-Words can present the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:_mxTG_jYneAJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:_mxTG_jYneAJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>11</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16185331195961371902&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="15">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6165370</url>
        <title status="complete" source="scholar.google.com">Towards Cross-Version Harmonic Analysis of Music</title>
        <pdf>http://www.doc.gold.ac.uk/~mas03dm/papers/Ewertetal_HarmonicAnalysis_2012.pdf</pdf>
        <author status="partial" source="scholar.google.com">S Ewert, M Muller, V Konz&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Multimedia, IEEE &#8230;</proceeding>
        <year>2012</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract For a given piece of music, there often exist multiple versions belonging to the symbolic (eg, MIDI representations), acoustic (audio recordings), or visual (sheet music) domain. Each type of information allows for applying specialized, domain-specific ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:s5agRuPqbRsJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1976494073348921011&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=1976494073348921011&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="16">
        <url>http://dl.acm.org/citation.cfm?id=2184559</url>
        <title status="complete" source="scholar.google.com">RRA: An audio format for single-source music and lyrics</title>
        <pdf>http://www.unix.eng.ua.edu/~mrrao/acm-paper.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Rao, JC Lusth</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 50th Annual Southeast Regional &#8230;</proceeding>
        <year>2012</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Karaoke music has world-wide appeal, especially for non-professional singers. However, most karaoke-audio architectures involve separate text and audio data streams which run in different threads. Such an approach suffers from timing synchronization ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:wZrLWAYqsUsJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=5454186830470290113&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="17">
        <url>http://drops.dagstuhl.de/opus/volltexte/2012/3464/</url>
        <title status="complete" source="scholar.google.com">Lyrics-to-Audio Alignment and its Application}}</title>
        <pdf>http://drops.dagstuhl.de/opus/volltexte/2012/3464/pdf/3.pdf</pdf>
        <author status="complete" source="scholar.google.com">H Fujihara, M Goto</author>
        <proceeding status="complete" source="scholar.google.com">Multimodal Music Processing}</proceeding>
        <source>drops.dagstuhl.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract Automatic lyrics-to-audio alignment techniques have been drawing attention in the last years and various studies have been made in this field. The objective of lyrics-to-audio alignment is to estimate a temporal relationship between lyrics and musical audio signals ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:pR0KaNFrpC0J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3288872175025135013&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="18">
        <url>http://www.sciencedirect.com/science/article/pii/S0167639311000021</url>
        <title status="complete" source="scholar.google.com">Towards precise and robust automatic synchronization of live speech and its transcripts</title>
        <author status="complete" source="scholar.google.com">J Gao, Q Zhao, Y Yan</author>
        <proceeding status="complete" source="scholar.google.com">Speech Communication</proceeding>
        <year>2011</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">This paper presents our efforts in automatically synchronizing spoken utterances with their transcripts (textual contents)(ASUT), where the speech is a live stream and its corresponding transcripts are known. This task is first simplified to the problem of online detecting the end ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:flSdhSNB6hwJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2083549398433617022&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="19">
        <url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.173.5164&amp;rep=rep1&amp;type=pdf</url>
        <title status="complete" source="scholar.google.com">Linking music-related information and audio data</title>
        <pdf>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.173.5164&amp;rep=rep1&amp;type=pdf</pdf>
        <author status="complete" source="scholar.google.com">R Macrae</author>
        <year>2008</year>
        <source>Citeseer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Due to recent technological advancement we now have a near endless supply of musical content. There is now a growing interest in new ways of interacting with and filtering this music. Examples are music editing suites that can identify audio and align segments ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:MKyRnl4l-DkJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:MKyRnl4l-DkJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4177129742703635504&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="20">
        <url>http://www.audiolabs-erlangen.de/meinard/students/thesis/2012_EwertSebastian_SignalProcessingMethods_Phd-Thesis.pdf</url>
        <title status="complete" source="scholar.google.com">Signal Processing Methods for Music Synchronization, Audio Matching, and Source Separation</title>
        <pdf>http://www.audiolabs-erlangen.de/meinard/students/thesis/2012_EwertSebastian_SignalProcessingMethods_Phd-Thesis.pdf</pdf>
        <author status="complete" source="scholar.google.com">S Ewert</author>
        <year>2012</year>
        <source>audiolabs-erlangen.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract The field of music information retrieval (MIR) aims at developing techniques and tools for organizing, understanding, and searching multimodal information in large music collections in a robust, efficient and intelligent manner. In this context, this thesis presents ...</snippet>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:Li_TocU8mCEJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="21">
        <url>http://downloads.hindawi.com/journals/asmp/2010/546047.pdf</url>
        <title status="complete" source="scholar.google.com">Automatic Recognition of Lyrics in Singing</title>
        <pdf>http://downloads.hindawi.com/journals/asmp/2010/546047.pdf</pdf>
        <author status="complete" source="scholar.google.com">M Annamaria, V Tuomas</author>
        <proceeding status="partial" source="scholar.google.com">EURASIP Journal on Audio, &#8230;</proceeding>
        <year>2010</year>
        <source>downloads.hindawi.com</source>
        <snippet status="partial" source="scholar.google.com">The paper considers the task of recognizing phonemes and words from a singing input by using a phonetic hidden Markov model recognizer. The system is targeted to both monophonic singing and singing in polyphonic music. A vocal separation algorithm is ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:UoAHx5uLsEAJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:UoAHx5uLsEAJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4661379115503812690&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="22">
        <url>http://drops.dagstuhl.de/opus/volltexte/2012/3463/</url>
        <title status="complete" source="scholar.google.com">Linking Sheet Music and AudioâChallenges and New Approaches</title>
        <pdf>http://drops.dagstuhl.de/opus/volltexte/2012/3463/pdf/2.pdf</pdf>
        <author status="complete" source="scholar.google.com">V Thomas, C Fremerey, M MÃ¼ller, M Clausen</author>
        <year>2012</year>
        <source>drops.dagstuhl.de</source>
        <snippet status="partial" source="scholar.google.com">Abstract Score and audio files are the two most important ways to represent, convey, record, store, and experience music. While score describes a piece of music on an abstract level using symbols such as notes, keys, and measures, audio files allow for reproducing a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:8cjl0nuN9lgJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6410466682567248113&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="23">
        <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5496295</url>
        <title status="complete" source="scholar.google.com">Automatic Synchronization of live speech and its Transcripts based on a frame-synchronous likelihood ratio test</title>
        <author status="complete" source="scholar.google.com">J Gao, Q Zhao, Y Yan</author>
        <proceeding status="partial" source="scholar.google.com">Acoustics Speech and Signal &#8230;</proceeding>
        <year>2010</year>
        <source>ieeexplore.ieee.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we present our initial efforts in the task of Automatically Synchronizing live spoken Utterances with their Transcripts (textual contents)(ASUT) when the texts are known. We treat it as a online speech-text alignment problem. And it is further simplified ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:hb7403l05OcJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=16709608584047541893&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="24">
        <url>http://ismir2012.ismir.net/event/papers/361-ismir-2012.pdf</url>
        <title status="complete" source="scholar.google.com">RANKING LYRICS FOR ONLINE SEARCH</title>
        <pdf>http://ismir2012.ismir.net/event/papers/361-ismir-2012.pdf</pdf>
        <author status="complete" source="scholar.google.com">R Macrae, S Dixon</author>
        <proceeding status="complete" source="scholar.google.com">ismir2012.ismir.net</proceeding>
        <snippet status="partial" source="scholar.google.com">ABSTRACT When someone wishes to find the lyrics for a song they typically go online and use a search engine. There are a large number of lyrics available on the internet as the effort required to transcribe and post lyrics is minimal. These lyrics are promptly returned to the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:882m1oY-pkoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:882m1oY-pkoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="25">
        <url>http://staff.aist.go.jp/m.goto/PAPER/ASJ200909fujihara.pdf</url>
        <title status="complete" source="scholar.google.com">æ¥½æ²ä¸­ã®æ­å£°ã®åºæ¬å¨æ³¢æ°ã¨é³ç´ ãåææ¨å®å¯è½ãªãã¬ã¼ã ã¯ã¼ã¯</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/ASJ200909fujihara.pdf</pdf>
        <author status="complete" source="scholar.google.com">è¤åå¼å°ï¼ å¾è¤çå­ï¼ å¥¥ä¹å(äº¬å¤§</author>
        <proceeding status="complete" source="scholar.google.com">staff.aist.go.jp</proceeding>
        <snippet status="partial" source="scholar.google.com">é³æ¥½ã¯, ç£æ¥­çã«ãæåçã«ãéè¦ãªã³ã³ãã³ãã§ãã, ãã®ä¸­ã§ãæ­å£°ã¯éè¦ãªå½¹å²ãæããã¦ãã. æ¬ç¨¿ã§ã¯, æ··åé³ä¸­ã®æ­å£°ã®æ­è© (é³ç´ ) ã¨åºæ¬å¨æ³¢æ° (F0) ãåæã«èªè­ããããã®ææ³, WPST (Weighted composition of Probabilistic Spectral Template) æ³ãææ¡ã, F0 æ¨å®ã¨ ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uGdvSfHJOboJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:uGdvSfHJOboJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="26">
        <url>http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007goto.pdf</url>
        <title status="complete" source="scholar.google.com">æ­å£°æå ±å¦ç: æ­å£°ãå¯¾è±¡ã¨ããé³æ¥½æå ±å¦ç</title>
        <pdf>http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007goto.pdf</pdf>
        <author status="complete" source="scholar.google.com">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å°</author>
        <year>2010</year>
        <source>staff.aist.go.jp</source>
        <snippet status="partial" source="scholar.google.com">æ¬ç¨¿ã§ã¯,ãæ­å£°æå ±å¦çã ã¨åä»ããæ°ããç ç©¶é åã«ãããæãã®ç ç©¶äºä¾ãç´¹ä»ãã. ããã¯æ­å£°ã«å¯¾ããé³æ¥½æå ±å¦çã§ãã, ãã®ç ç©¶å¯¾è±¡ã¯å¤å²ã«æ¸¡ãã, æ¬ç¨¿ã§ã¯, æ­å£°çè§£ã·ã¹ãã , æ­å£°ã«åºã¥ãé³æ¥½æå ±æ¤ç´¢ã·ã¹ãã , æ­å£°åæã·ã¹ãã ã®ä¸ã¤ã®éè¦ãª ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:uzd8aAf29qoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:uzd8aAf29qoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=12319304342396745659&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="75">
    <url>http://dl.acm.org/citation.cfm?id=1631303</url>
    <title status="complete" source="scholar.google.com">Comprehensive query-dependent fusion using regression-on-folksonomies: a case study of multimodal music search</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2009_Comprehensive_Query-Dependent_Fusion_using_Regression-on-Folksonomies-A_Case_Study_of_Multimodal_Music_Search.pdf</pdf>
    <author status="complete" source="scholar.google.com">B Zhang, Q Xiang, H Lu, J Shen, Y Wang</author>
    <proceeding status="partial" source="scholar.google.com">Proceedings of the 17th ACM &#8230;</proceeding>
    <year>2009</year>
    <source>dl.acm.org</source>
    <snippet status="partial" source="scholar.google.com">... Bingjun Zhang1, Qiaoliang Xiang1, Huanhuan Lu1, Jialie Shen2, Ye Wang1 1School ofComputing, National University of Singapore 2School of Information Systems, SingaporeManagement University 1{bingjun,xiangqiaoliang,luhuan,wangye}@comp.nus.edu.sg, 2jlshen ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:Vz0tAKThYoYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>11</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=9683550243293838679&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>11</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=9683550243293838679&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="11">
        <result id="0">
        <url>http://dl.acm.org/citation.cfm?id=1807216</url>
        <title status="complete" source="scholar.google.com">Multiple feature fusion for social media applications</title>
        <pdf>http://nusdm.comp.nus.edu.sg/papers/SIGMOD2010_MultiFeatureFusion.pdf</pdf>
        <author status="complete" source="scholar.google.com">B Cui, AKH Tung, C Zhang, Z Zhao</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 2010 &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract The emergence of social media as a crucial paradigm has posed new challenges to the research and industry communities, where media are designed to be disseminated through social interaction. Recent literature has noted the generality of multiple features in ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:ah7elKTvte8J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17272975436000665194&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>10</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17272975436000665194&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://dl.acm.org/citation.cfm?id=1874006</url>
        <title status="complete" source="scholar.google.com">Large-scale music tag recommendation with explicit multiple attributes</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2010_Large-scale_Music_Tag_Recommendation_with_Explicit_Multiple_Attributes.pdf</pdf>
        <author status="partial" source="scholar.google.com">Z Zhao, X Wang, Q Xiang, AM Sarroff, Z Li&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the &#8230;</proceeding>
        <year>2010</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Social tagging can provide rich semantic information for large-scale retrieval in music discovery. Such collaborative intelligence, however, also generates a high degree of tags unhelpful to discovery, some of which obfuscate critical information. Towards ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:tjNWBquXKB8J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=2245211175045706678&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>8</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=2245211175045706678&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://dl.acm.org/citation.cfm?id=2072356</url>
        <title status="complete" source="scholar.google.com">Coached active learning for interactive video search</title>
        <pdf>http://www.cs.cityu.edu.hk/~xiaoyong/papers/cal.mm11.pdf</pdf>
        <author status="complete" source="scholar.google.com">XY Wei, ZQ Yang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 19th ACM international &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract Active learning with uncertainty sampling has been popularly employed in implementing interactive video search, due to its promise to reduce labeling efforts. However, since the ultimate goal of interactive search is to find as many relevant shots as ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:0-ZNePfQzVgJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=6399000406857213651&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>3</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=6399000406857213651&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://users.jyu.fi/~ptee/publications/Ferrer_Eerola_2010.pdf</url>
        <title status="complete" source="scholar.google.com">Timbral qualities of semantic structures of music</title>
        <pdf>http://users.jyu.fi/~ptee/publications/Ferrer_Eerola_2010.pdf</pdf>
        <author status="complete" source="scholar.google.com">R Ferrer, T Eerola</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 11th International Society for &#8230;</proceeding>
        <year>2010</year>
        <source>users.jyu.fi</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT The rapid expansion of social media in music has provided the field with impressive datasets that offer insights into the semantic structures underlying everyday uses and classification of music. We hypothesize that the organization of these structures are ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:lURShwbCjzoJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:lURShwbCjzoJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>7</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4219804709165352085&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=4219804709165352085&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://dl.acm.org/citation.cfm?id=2071949</url>
        <title status="complete" source="scholar.google.com">Document dependent fusion in multimodal music retrieval</title>
        <pdf>http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2011_Document_Dependent_Fusion_in_Multimodal_Music_Retrieval.pdf</pdf>
        <author status="complete" source="scholar.google.com">Z Li, B Zhang, Y Wang</author>
        <proceeding status="partial" source="scholar.google.com">Proceedings of the 19th ACM international &#8230;</proceeding>
        <year>2011</year>
        <source>dl.acm.org</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this paper, we propose a novel multimodal fusion framework, document dependent fusion (DDF), which derives the optimal combination strategy for each individual document in the fusion process. For each document, we derive a document weight vector ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:JyU9BhZI5j4J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=4532389334426133799&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://www.springerlink.com/index/077P3728P40161K2.pdf</url>
        <title status="complete" source="scholar.google.com">Probabilistic image tagging with tags expanded by text-based search</title>
        <author status="complete" source="scholar.google.com">X Zhang, Z Huang, H Shen, Z Li</author>
        <proceeding status="partial" source="scholar.google.com">Database Systems for Advanced &#8230;</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Automatic image tagging automatically assigns image with semantic keywords called tags, which significantly facilitates image search and organization. Most of present image tagging approaches assign the query image with the tags derived from the visually similar images ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:3R6rEYVowaQJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=11871885013461704413&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://pc01.lib.ntust.edu.tw/ETD-db/ETD-search/view_etd?URN=etd-0627112-214217</url>
        <title status="complete" source="scholar.google.com">Study on retrieving subjective knowledge from music using adaptive cloud-based structure</title>
        <author status="complete" source="scholar.google.com">KW Su</author>
        <year>2012</year>
        <source>pc01.lib.ntust.edu.tw</source>
        <snippet status="partial" source="scholar.google.com">Abstract In this study, many aspects of Music Information Retrieval techniques and recent year developments are discussed. With extensive research, the assumption that the knowledge of music and its related content and context are more susceptible to subjective ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:NRpcGHrI4i8J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=3450540691239475765&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://137.132.14.55/handle/10635/20949</url>
        <title status="complete" source="scholar.google.com">Adaptive multimodal fusion based similarity measures in music information retrieval</title>
        <pdf>http://137.132.14.55/bitstream/handle/10635/20949/ZhangBJ.pdf?sequence=1</pdf>
        <author status="complete" source="scholar.google.com">Z BINGJUN</author>
        <year>2010</year>
        <source>137.132.14.55</source>
        <snippet status="partial" source="scholar.google.com">In the field of music information retrieval (MIR), one fundamental research problem is the measuring of the similarity between music documents. Based on a viable similarity measure, MIR systems can be made more effective to help users retrieve relevant music information. ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:9FjpHoxTlRIJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1339068325491726580&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <title status="complete" source="scholar.google.com">Extended Self-Organizing Map for Transactional Data</title>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:66tiMJohKlwJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="9">
        <url>http://www.springerlink.com/index/DWJ3P200H7474639.pdf</url>
        <title status="complete" source="scholar.google.com">Semantic structures of timbre emerging from social and acoustic descriptions of music</title>
        <pdf>http://asmp.eurasipjournals.com/content/pdf/1687-4722-2011-11.pdf</pdf>
        <author status="complete" source="scholar.google.com">R Ferrer, T Eerola</author>
        <proceeding status="partial" source="scholar.google.com">EURASIP Journal on Audio, Speech, and Music &#8230;</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract The perceptual attributes of timbre have inspired a considerable amount of multidisciplinary research, but because of the complexity of the phenomena, the approach has traditionally been confined to laboratory conditions, much to the detriment of its ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mp7T8Y5s9fUJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>3</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17723191269784788634&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>1</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17723191269784788634&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="10">
        <url>http://www.sciencedirect.com/science/article/pii/S1568494612002906</url>
        <title status="complete" source="scholar.google.com">A self-organizing map for transactional data and the related categorical domain</title>
        <author status="complete" source="scholar.google.com">WC Liao, CC Hsu</author>
        <proceeding status="complete" source="scholar.google.com">Applied Soft Computing</proceeding>
        <year>2012</year>
        <source>Elsevier</source>
        <snippet status="partial" source="scholar.google.com">After projecting high dimensional data into a two-dimension map via the SOM, users can easily view the inner structure of the data on the 2-D map. In the early stage of data mining, it is useful for any kind of data to inspect their inner structure. However, few studies apply ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:khq3_pWLRQAJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=19575249732835986&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="76">
    <url>http://www.springerlink.com/index/18231153T6838HU7.pdf</url>
    <title status="complete" source="scholar.google.com">Generic forward error correction of short frames for IP streaming applications</title>
    <pdf>http://www.comp.nus.edu.sg/~wangye/papers/5.Error_Robust_Audio_Streaming/2006_Generic_Forward_Error_Correction_of_Short_Frames_for_IP_Streaming_Applications.pdf</pdf>
    <author status="complete" source="scholar.google.com">J Korhonen, Y Huang, Y Wang</author>
    <proceeding status="complete" source="scholar.google.com">Multimedia Tools and Applications</proceeding>
    <year>2006</year>
    <source>Springer</source>
    <snippet status="partial" source="scholar.google.com">... J. Korhonen .Y. Huang .Y. Wang (*) Department of Computer Science, School of Computing,National University of Singapore, 3 Science Drive 2, Singapore 117543, Singapore e-mail:wangye@comp.nus.edu.sg J. Korhonen e-mail: jari.ta.korhonen@nokia.com Y. Huang e-mail ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:h-flwEfZJiIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>13</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=2460893148606752647&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>9</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=2460893148606752647&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="9">
        <result id="0">
        <url>http://www.springerlink.com/index/U7570037WU52Q451.pdf</url>
        <title status="complete" source="scholar.google.com">Sub-packet forward error correction mechanism for video streaming over wireless networks</title>
        <author status="complete" source="scholar.google.com">MF Tsai, CK Shieh, CH Ke, DJ Deng</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Tools and Applications</proceeding>
        <year>2010</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Traditional Forward Error Correction (FEC) mechanisms can be divided into Packet level FEC (PFEC) mechanisms and Byte level FEC (BFEC) mechanisms. The PFEC mechanism of recovering from errors in a source packet requires an entire FEC redundant ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:rYJxmp1Lv_EJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>9</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=17419725023965446829&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>18</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=17419725023965446829&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="1">
        <url>http://infoscience.epfl.ch/record/131249/files/mobimedia2008.pdf</url>
        <title status="complete" source="scholar.google.com">Error control for video streaming with small data units</title>
        <pdf>http://infoscience.epfl.ch/record/131249/files/mobimedia2008.pdf</pdf>
        <author status="complete" source="scholar.google.com">J Korhonen, P Frossard</author>
        <proceeding status="complete" source="scholar.google.com">Proceedings of the MobiMedia</proceeding>
        <year>2004</year>
        <source>infoscience.epfl.ch</source>
        <snippet status="partial" source="scholar.google.com">ABSTRACT In multimedia streaming, small errors are typically easier to mask with common error concealment strategies, but small packet size increases the overhead caused by network header information. To reduce the header overhead, large packets are typically ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:mcAPoyGP5NwJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <htmlLink>http://scholar.googleusercontent.com/scholar?q=cache:mcAPoyGP5NwJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</htmlLink>
        <numVersions>6</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=15917004357666586777&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>5</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=15917004357666586777&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="2">
        <url>http://onlinelibrary.wiley.com/doi/10.1002/dac.1032/abstract</url>
        <title status="complete" source="scholar.google.com">An adaptive multiâhop forward error correction protection scheme for video streaming over wireless mesh networks</title>
        <author status="partial" source="scholar.google.com">MF Tsai, CK Shieh, WS Hwang&#8230;</author>
        <proceeding status="partial" source="scholar.google.com">International Journal of &#8230;</proceeding>
        <year>2009</year>
        <source>Wiley Online Library</source>
        <snippet status="partial" source="scholar.google.com">Abstract Traditional forward error correction (FEC) mechanisms can be divided into end-to-end FEC protection schemes and hop-by-hop FEC protection schemes. The end-to-end FEC protection schemes over-allocate FEC redundancy to each link and may induce a self- ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:DVzp8uz7drwJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13580318721488542733&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <numCite>7</numCite>
        <citeLink>http://scholar.google.com.sg/scholar?cites=13580318721488542733&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9</citeLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="3">
        <url>http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8156235&amp;id=Ly0MAgAAEBAJ&amp;oi=fnd&amp;printsec=abstract</url>
        <title status="complete" source="scholar.google.com">Apparatus and method for determining modes and directing streams in remote communication</title>
        <author status="complete" source="scholar.google.com">DE Barreto, S Kasivajhula, A Kumar</author>
        <proceeding status="complete" source="scholar.google.com">US Patent 8,156,235</proceeding>
        <year>2012</year>
        <source>Google Patents</source>
        <snippet status="partial" source="scholar.google.com">A communication apparatus for remote communication may include a first local module configured to intercept a first stream. The first stream may utilize a first transmission protocol and may be destined to a remote destination over a network. The first local module may ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:-YOzMNrxlf4J:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>5</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=18344834576585819129&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="4">
        <url>http://www.springerlink.com/index/N185401076T7000X.pdf</url>
        <title status="complete" source="scholar.google.com">New Interleaving and Forward Error Correction Strategies for Streaming</title>
        <author status="complete" source="scholar.google.com">M Dadfarnia, M Noorhoseini</author>
        <proceeding status="partial" source="scholar.google.com">Applied Computing, Computer Science, and &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">If the frame size of a multimedia encoder is small, Internet Protocol streaming applications need to pack many encoded media frames in each Real-time Transport Protocol (RTP) packet to avoid unnecessary header overhead. Forward Error Correction (FEC) can be ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:qysZONUwfxcJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1693125677241936811&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="5">
        <url>http://www.springerlink.com/index/NW37027505645156.pdf</url>
        <title status="partial" source="scholar.google.com">Concurrent multipath transmission with forward error correction mechanism to overcome burst packet losses for delay-sensitive video streaming in wireless home &#8230;</title>
        <author status="complete" source="scholar.google.com">N Chilamkurti, JH Park, N Kumar</author>
        <proceeding status="complete" source="scholar.google.com">Multimedia Tools and Applications</proceeding>
        <year>2011</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Abstract Wireless multimedia home servers are the next generation of home entertainment systems. The provision of high quality time-critical video streaming applications in indoor environments is very challenging due to high attenuation and interference caused by the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:lCgSqBwhZ_wJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="6">
        <url>http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8122140&amp;id=qsMEAgAAEBAJ&amp;oi=fnd&amp;printsec=abstract</url>
        <title status="complete" source="scholar.google.com">Apparatus and method for accelerating streams through use of transparent proxy architecture</title>
        <author status="complete" source="scholar.google.com">DE Barreto, S Kasivajhula, A Kumar</author>
        <proceeding status="complete" source="scholar.google.com">US Patent 8,122,140</proceeding>
        <year>2012</year>
        <source>Google Patents</source>
        <snippet status="partial" source="scholar.google.com">A communication apparatus for remote communication may include a local transparent proxy module configured to intercept a first stream destined to a remote destination and configured to make a first determination whether to accelerate communication associated ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:vS_MwFhyFrcJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=13192857883945742269&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="7">
        <url>http://www.springerlink.com/index/m0w10r4140748534.pdf</url>
        <title status="complete" source="scholar.google.com">Adjusting Forward Error Correction for Media Streaming</title>
        <author status="complete" source="scholar.google.com">M Dadfarnia, S Khakbaz</author>
        <proceeding status="partial" source="scholar.google.com">Applied Computing, Computer Science, and &#8230;</proceeding>
        <year>2009</year>
        <source>Springer</source>
        <snippet status="partial" source="scholar.google.com">Considering time constraints in multimedia streaming, multimedia applications use UDP protocol which does not guarantee data arrival. However, UDP flows often have a high data loss rate that needs to be dealt with. Forward Error Correction (FEC) is one of the ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:bfDcwv29Dw4J:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>2</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=1013237338775285869&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
      <result id="8">
        <url>http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8209430&amp;id=nFMfAgAAEBAJ&amp;oi=fnd&amp;printsec=abstract</url>
        <title status="complete" source="scholar.google.com">Apparatus and method for remote communication and bandwidth adjustments</title>
        <author status="complete" source="scholar.google.com">DE Barreto, S Kasivajhula, A Kumar</author>
        <proceeding status="complete" source="scholar.google.com">US Patent 8,209,430</proceeding>
        <year>2012</year>
        <source>Google Patents</source>
        <snippet status="partial" source="scholar.google.com">A communication apparatus for remote communication may include a local communication proxy module configured to receive streams from a local communication application module. The streams may be in a form utilizing a first transmission protocol and destined to a ...</snippet>
        <relatedLink>http://scholar.google.com.sg/scholar?q=related:buin-GDLsJIJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</relatedLink>
        <numVersions>4</numVersions>
        <versionLink>http://scholar.google.com.sg/scholar?cluster=10570171942787606638&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5</versionLink>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
  <result id="77">
    <url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5711656</url>
    <title status="complete" source="scholar.google.com">Sensor-Assisted Video Encoding for Mobile Devices in Real-World Environments</title>
    <pdf>https://www1.comp.nus.edu.sg/~wangye/papers/4.Perception-Aware_Low-Power_Media_Processing_for_Portable_Devices/2011_Sensor-Assisted_Video_Encoding_for_Mobile_Devicesin_Real-World_Environments.pdf</pdf>
    <author status="partial" source="scholar.google.com">X Chen, Z Zhao, A Rahmati, Y Wang&#8230;</author>
    <proceeding status="partial" source="scholar.google.com">Circuits and Systems &#8230;</proceeding>
    <year>2011</year>
    <source>ieeexplore.ieee.org</source>
    <snippet status="partial" source="scholar.google.com">... Manuscript received October 26, 2009; revised May 18, 2010; accepted November 12, 2010.Date of publication February 10, 2011; date of current version March 23, 2011. The work by theNUS team was supported by the Singaporean MOE, under Grant R-252-000-236-112. ...</snippet>
    <relatedLink>http://scholar.google.com.sg/scholar?q=related:gDmiExV2xjsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</relatedLink>
    <numVersions>5</numVersions>
    <versionLink>http://scholar.google.com.sg/scholar?cluster=4307259926522247552&amp;hl=en&amp;num=100&amp;as_sdt=1,5&amp;as_vis=1</versionLink>
    <numCite>1</numCite>
    <citeLink>http://scholar.google.com.sg/scholar?cites=4307259926522247552&amp;as_sdt=2005&amp;sciodt=1,5&amp;hl=en&amp;num=100</citeLink>
    <citedBy totalresults="1">
        <result id="0">
        <url>http://www.cqvip.com/qk/90288x/201215/43336542.html</url>
        <title status="complete" source="scholar.google.com">å¹¶èå¼å­ç»´å éåº¦ä¼ æå¨çåå¯é¡¿å¨åå­¦ç ç©¶</title>
        <author status="complete" source="scholar.google.com">å°¤æ¶æ¶ï¼ ææåï¼ å´æ´ªæ¶</author>
        <proceeding status="complete" source="scholar.google.com">æºæ¢°å·¥ç¨å­¦æ¥</proceeding>
        <year>2012</year>
        <source>cqvip.com</source>
        <snippet status="partial" source="scholar.google.com">éå¯¹å­ç»´å éåº¦ä¼ æå¨å¨è§£è¦é¾åº¦ä¸ç»æå¤æåº¦ä¹é´å­å¨çç¾çç°ç¶, éè¿Legende åæ¢, å¹¶ç¨ååæ°æ¥æè¿°æè½¬è¿å¨, å»ºç«å¹¶èå¼å­ç»´å éåº¦ä¼ æå¨çåå¯é¡¿çº¦ææ­£åæ¹ç¨. ä¾æ®ååæ°çæ§è´¨, å¨ç¸ç©ºé´åæ¨å¯¼åºç³»ç»åå¯é¡¿åéä¹é´éå«çæ­£äº¤å³ç³»å¼, å¹¶æ®æ­¤ç»åºä¸ ...</snippet>
        <citedBy totalresults="0">
        </citedBy>
      </result>
    </citedBy>
  </result>
</query>
</results>
