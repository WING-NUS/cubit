Total results = 14
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://diml.yonsei.ac.kr/~forevertin/journal/2012-TIP-Min.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from yonsei.ac.kr</span><span class="gs_ggsS">yonsei.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5963715" class=yC0>Depth Video Enhancement Based on Weighted Mode Filtering</a></h3><div class="gs_a"><a href="/citations?user=3REUPXYAAAAJ&amp;hl=en&amp;oi=sra">D Min</a>, J Lu, <a href="/citations?user=OX4taSQAAAAJ&amp;hl=en&amp;oi=sra">MN Do</a> - Image Processing, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a novel approach for depth video enhancement. Given a high-<br>resolution color video and its corresponding low-quality depth video, we improve the quality <br>of the depth video by increasing its resolution and suppressing noise. For that, a weighted <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16460224743516695859&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 10</a> <a href="/scholar?q=related:M21ouDF3buQJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16460224743516695859&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'M21ouDF3buQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://visual.cs.ucl.ac.uk/ext/depthSuperRes/eccv2012_draft.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucl.ac.uk</span><span class="gs_ggsS">ucl.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://visual.cs.ucl.ac.uk/ext/depthSuperRes/eccv2012_draft.pdf" class=yC2>Patch Based Synthesis for Single Depth Image Super-Resolution</a></h3><div class="gs_a"><a href="/citations?user=IfZBjkUAAAAJ&amp;hl=en&amp;oi=sra">O Mac Aodha</a>, <a href="/citations?user=dRC_mIAAAAAJ&amp;hl=en&amp;oi=sra">NDF Campbell</a>, A Nair, <a href="/citations?user=CZiTv0gAAAAJ&amp;hl=en&amp;oi=sra">GJ Brostow</a> - 2012 - visual.cs.ucl.ac.uk</div><div class="gs_rs">Abstract. We present an algorithm to synthetically increase the resolution of a solitary depth <br>image using only a generic database of local patches. Modern range sensors measure <br>depths with non-Gaussian noise and at lower starting resolutions than typical visible-light <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15198103974921807561&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 2</a> <a href="/scholar?q=related:yaaqzAqD6tIJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15198103974921807561&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'yaaqzAqD6tIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:yaaqzAqD6tIJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/A303437744P33868.pdf" class=yC4>High Accuracy TOF and Stereo Sensor Fusion at Interactive Rates</a></h3><div class="gs_a">R Nair, <a href="/citations?user=HjGlokkAAAAJ&amp;hl=en&amp;oi=sra">F Lenzen</a>, S Meister, H SchÃ¤fer, C Garbe&hellip; - Computer VisionâECCV  &hellip;, 2012 - Springer</div><div class="gs_rs">We propose two new GPU-based sensor fusion approaches for time of flight (TOF) and <br>stereo depth data. Data fidelity measures are defined to deal with the fundamental <br>limitations of both techniques alone. Our algorithms combine TOF and stereo, yielding <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'bWnfA_QUGkYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6336464" class=yC5>Gesture-dependent depth data extraction for low resolution Time-of-Flight camera</a></h3><div class="gs_a">KM Kyung, K Bae, SH Cho&hellip; - &hellip;  Electronics-Berlin (ICCE- &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a method to extract gesture-dependent depth data from a low <br>resolution depth image of ToF sensor. A depth image is divided into sub-regions for fast <br>movement detection. Divided sub-regions are classified into foreground and background <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'ZKBiG51rsu4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://hal.archives-ouvertes.fr/docs/00/72/56/54/PDF/TOF.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://hal.archives-ouvertes.fr/docs/00/72/56/54/PDF/TOF.pdf" class=yC6>Time-of-Flight Cameras: Principles, Methods and Applications</a></h3><div class="gs_a">M Hansard, <a href="/citations?user=3Pf6C6cAAAAJ&amp;hl=en&amp;oi=sra">S Lee</a>, O Choi, <a href="/citations?user=Lvit5RIAAAAJ&amp;hl=en&amp;oi=sra">R Horaud</a> - 2012 - hal.archives-ouvertes.fr</div><div class="gs_rs">Abstract This chapter introduces the principles and difficulties of time-of-flight depth <br>measurement. The depth-images that are produced by time-of-flight cameras suffer from <br>characteristic problems, which are divided into the following two classes. Firstly there are <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'O9FaHtTp6wIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:O9FaHtTp6wIJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6407114" class=yC8>Refinement of depth maps generated by low-cost depth sensors</a></h3><div class="gs_a">KR Vijayanagar, M Loghman&hellip; - SoC Design Conference ( &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the introduction of the Microsoft Kinect into the gaming industry and release of <br>Kinect-based application development kits, a whole new industry has evolved around the <br>Kinect with applications for gaming, gesture recognition for controlling devices, 3-D <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'RMHaT_5LbiYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/C02V166047Q62372.pdf" class=yC9>Depth recovery using an adaptive color-guided auto-regressive model</a></h3><div class="gs_a">J Yang, X Ye, K Li, C Hou - Computer VisionâECCV 2012, 2012 - Springer</div><div class="gs_rs">This paper proposes an adaptive color-guided auto-regressive (AR) model for high quality <br>depth recovery from low quality measurements captured by depth cameras. We formulate <br>the depth recovery task into a minimization of AR prediction errors subject to <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'ghBwZbd9WJoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://miun.diva-portal.org/smash/get/diva2:561904/FULLTEXT02" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from diva-portal.org</span><span class="gs_ggsS">diva-portal.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://miun.diva-portal.org/smash/record.jsf?pid=diva2:561904" class=yCA>Depth Map Upscaling for Three-Dimensional Television: The Edge-Weighted Optimization Concept</a></h3><div class="gs_a">S Schwarz - 2012 - miun.diva-portal.org</div><div class="gs_rs">Abstract With the recent comeback of three-dimensional (3D) movies to the cinemas, there <br>have been increasing efforts to spread the commercial success of 3D to new markets. The <br>possibility of a 3D experience at home, such as three-dimensional television (3DTV), has <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'Aww3ccnqlGkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/K7M4028567058468.pdf" class=yCC>Fusion of Depth Data with Standard Cameras Data</a></h3><div class="gs_a"><a href="/citations?user=0F_1-xYAAAAJ&amp;hl=en&amp;oi=sra">CD Mutto</a>, <a href="/citations?user=xk2N2wkAAAAJ&amp;hl=en&amp;oi=sra">P Zanuttigh</a>, GM Cortelazzo - Time-of-Flight Cameras and  &hellip;, 2012 - Springer</div><div class="gs_rs">Depth camera possibilities and limitations seen in the previous chapters may natu- rally prompt <br>questions like âIs a depth camera enough for my application?â or âMay one or more standard <br>cameras help it?â. Depth cameras can only provide 3D geometry information about the <b> ...</b> </div><div class="gs_fl"><a href="/scholar?q=related:UROHkYZ2At0J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'UROHkYZ2At0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1077314211002724" class=yCD>Range map superresolution-inpainting, and reconstruction from sparse data</a></h3><div class="gs_a"><a href="/citations?user=iORQG6MAAAAJ&amp;hl=en&amp;oi=sra">AV Bhavsar</a>, AN Rajagopalan - Computer Vision and Image Understanding, 2012 - Elsevier</div><div class="gs_rs">Range images often suffer from issues such as low resolution (LR)(for low-cost scanners) <br>and presence of missing regions due to poor reflectivity, and occlusions. Another common <br>problem (with high quality scanners) is that of long acquisition times. In this work, we <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17807259240251515305&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 1</a> <a href="/scholar?q=related:qeF7nu4XIPcJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17807259240251515305&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'qeF7nu4XIPcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://opticalengineering.spiedigitallibrary.org/article.aspx?articleid=1358733" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from spiedigitallibrary.org</span><span class="gs_ggsS">spiedigitallibrary.org <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[HTML]</span><span class="gs_ct2">[HTML]</span></span> <a href="http://opticalengineering.spiedigitallibrary.org/article.aspx?articleid=1358733" class=yCE>Performance evaluation of time-of-flight and structured light depth sensors in radiometric/geometric variations</a></h3><div class="gs_a"><a href="/citations?user=KB5XZGIAAAAJ&amp;hl=en&amp;oi=sra">H Shim</a>, <a href="/citations?user=3Pf6C6cAAAAJ&amp;hl=en&amp;oi=sra">S Lee</a> - Optical Engineering, 2012 - opticalengineering.spiedigitallibrary. &hellip;</div><div class="gs_rs">Abstract. Time-of-flight (ToF) and structured light depth cameras capture dense three-<br>dimensional (3-D) geometry that is of great benefit for many computer vision problems. For <br>the past couple of years, depth image based gesture recognition, 3-D reconstruction, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Rklzw2TFMZoJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11110878792266238278&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Rklzw2TFMZoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Rklzw2TFMZoJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/M228523167127487.pdf" class=yC10>A Mixed Time-of-Flight and Stereoscopic Camera System</a></h3><div class="gs_a">M Hansard, <a href="/citations?user=3Pf6C6cAAAAJ&amp;hl=en&amp;oi=sra">S Lee</a>, <a href="/citations?user=V3WVKokAAAAJ&amp;hl=en&amp;oi=sra">O Choi</a>, <a href="/citations?user=Lvit5RIAAAAJ&amp;hl=en&amp;oi=sra">R Horaud</a> - Time-of-Flight Cameras, 2013 - Springer</div><div class="gs_rs">Several methods that combine range and color data have been investigated and <br>successfully used in various applications. Most of these systems suffer from the problems of <br>noise in the range data and resolution mismatch between the range sensor and the color <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'RK557xlhrMwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/R6781QP574560637.pdf" class=yC11>Characterization of Time-of-Flight Data</a></h3><div class="gs_a">M Hansard, <a href="/citations?user=3Pf6C6cAAAAJ&amp;hl=en&amp;oi=sra">S Lee</a>, <a href="/citations?user=V3WVKokAAAAJ&amp;hl=en&amp;oi=sra">O Choi</a>, <a href="/citations?user=Lvit5RIAAAAJ&amp;hl=en&amp;oi=sra">R Horaud</a> - Time-of-Flight Cameras, 2013 - Springer</div><div class="gs_rs">This chapter introduces the principles and difficulties of time-of-flight depth measurement. <br>The depth images that are produced by time-of-flight cameras suffer from characteristic <br>problems, which are divided into the following two classes. First, there are systematic <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'NCa0_pINhPgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.dbpia.co.kr/view/ar_view.asp?arid=1874010" class=yC12>ê³ ì£¼í ì±ë¶ì ì´ì©í ê¹ì´ë§µì ë³´ê°</a></h3><div class="gs_a">ê¹ë§ë°° - ë°©ì¡ê³µííë¼ë¬¸ì§, 2012 - dbpia.co.kr</div><div class="gs_rs">&amp;nbsp; &amp;nbsp; ë³¸ ë¼ë¬¸ì ìì ë³´ê°ë²ì ì´ì©íì¬ ì í´ìë ê¹ì´ë§µì ê³ í´ìë ê¹ì´ë§µì¼ë¡ <br>ë³ííë ë°©ë²ì ì ìíë¤. íì¬ì ì¹´ë©ë¼ ì¼ìë ê³ í´ìë ìì ììì ì ê³µíëë° ë°í´, ê¹ì´ <br>ì¸¡ì  ì¥ì¹ë ì í´ìëì ê¹ì´ë§µì ì£¼ë¡ ì ê³µíë¤. ë³¸ ë¼ë¬¸ì ê¸°ì¡´ì ìì í ë³´ê°ë², ê³ ë±ì°¨ì <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:AJCOwhuiz7kJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'AJCOwhuiz7kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
