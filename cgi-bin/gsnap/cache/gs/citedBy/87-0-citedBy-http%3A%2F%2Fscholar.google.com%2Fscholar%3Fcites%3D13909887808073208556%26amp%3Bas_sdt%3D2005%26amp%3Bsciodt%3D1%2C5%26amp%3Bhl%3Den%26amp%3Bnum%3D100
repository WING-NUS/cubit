Total results = 87
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://tazaree.persiangig.com/Article/Image%20Retrieval%20Ideas,%20Influences,%20and%20Trends%20survey.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from persiangig.com</span><span class="gs_ggsS">persiangig.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1348248" class=yC0>Image retrieval: Ideas, influences, and trends of the new age</a></h3><div class="gs_a"><a href="/citations?user=GmJRVxgAAAAJ&amp;hl=en&amp;oi=sra">R Datta</a>, <a href="/citations?user=TYmV4V8AAAAJ&amp;hl=en&amp;oi=sra">D Joshi</a>, <a href="/citations?user=4Nmf18IAAAAJ&amp;hl=en&amp;oi=sra">J Li</a>, <a href="/citations?user=inVzWAcAAAAJ&amp;hl=en&amp;oi=sra">JZ Wang</a> - ACM Computing Surveys (CSUR), 2008 - dl.acm.org</div><div class="gs_rs">Abstract We have witnessed great interest and a wealth of promise in content-based image <br>retrieval as an emerging technology. While the last decade laid foundation to such promise, <br>it also paved the way for a large number of new techniques and systems, got many new <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13044502523917667192&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1596</a> <a href="/scholar?q=related:eIvMye5hB7UJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3F/4C/RN230420334.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=13044502523917667192&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 57 versions</a> <a onclick="return gs_ocit(event,'eIvMye5hB7UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md0', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md0" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:eIvMye5hB7UJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=3521017148730990898&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.ldmc.buaa.edu.cn/download/data_mining/A%20survey%20of%20content-based%20image%20retrieval%20with%20high-level%20semantics%20.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from buaa.edu.cn</span><span class="gs_ggsS">buaa.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0031320306002184" class=yC3>A survey of content-based image retrieval with high-level semantics</a></h3><div class="gs_a">Y Liu, D Zhang, G Lu, WY Ma - Pattern Recognition, 2007 - Elsevier</div><div class="gs_rs">In order to improve the retrieval accuracy of content-based image retrieval systems, research <br>focus has been shifted from designing sophisticated low-level feature extraction algorithms <br>to reducing the &#39;semantic gap&#39;between the visual features and the richness of human <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7879404000356825275&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 576</a> <a href="/scholar?q=related:u3gPAzFEWW0J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7879404000356825275&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'u3gPAzFEWW0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.cs.cmu.edu/~htong/pdf/MM05-Tong.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101337" class=yC5>Graph based multi-modality learning</a></h3><div class="gs_a"><a href="/citations?user=RaINcuUAAAAJ&amp;hl=en&amp;oi=sra">H Tong</a>, <a href="/citations?user=hXpZynkAAAAJ&amp;hl=en&amp;oi=sra">J He</a>, M Li, C Zhang, WY Ma - Proceedings of the 13th annual  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract To better understand the content of multimedia, a lot of research efforts have been <br>made on how to learn from multi-modal feature. In this paper, it is studied from a graph point <br>of view: each kind of feature from one modality is represented as one independent graph; <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7255274507010759980&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 61</a> <a href="/scholar?q=related:LN1SWrnpr2QJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7255274507010759980&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'LN1SWrnpr2QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://users.monash.edu.au/~dengs/resource/papers/pr08.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from monash.edu.au</span><span class="gs_ggsS">monash.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0031320307005316" class=yC7>Region-based image retrieval with high-level semantics using decision tree learning</a></h3><div class="gs_a">Y Liu, D Zhang, G Lu - Pattern Recognition, 2008 - Elsevier</div><div class="gs_rs">Semantic-based image retrieval has attracted great interest in recent years. This paper <br>proposes a region-based image retrieval system with high-level semantic learning. The key <br>features of the system are:(1) it supports both query by keyword and query by region of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3721089038204051338&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 53</a> <a href="/scholar?q=related:ip_A28P2ozMJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3721089038204051338&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'ip_A28P2ozMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://img.cs.uec.ac.jp/doc/mir2005.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uec.ac.jp</span><span class="gs_ggsS">uec.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101838" class=yC9>Probabilistic web image gathering</a></h3><div class="gs_a">K Yanai, K Barnard - Proceedings of the 7th ACM SIGMM international  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract We propose a new method for automated large scale gathering of Web images <br>relevant to specified concepts. Our main goal is to build a knowledge base associated with <br>as many concepts as possible for large scale object recognition studies. A second goal is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8505405246487668606&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 49</a> <a href="/scholar?q=related:fiekEwJFCXYJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8505405246487668606&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'fiekEwJFCXYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4527249" class=yCB>Automatic semantic annotation of real-world web images</a></h3><div class="gs_a">RCF Wong, CHC Leung - Pattern Analysis and Machine  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract As the number of Web images is increasing at a rapid rate, searching them <br>semantically presents a significant challenge. Many raw images are constantly uploaded <br>with little meaningful direct annotations of semantic content, limiting their search and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5731435541475080149&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 36</a> <a href="/scholar?q=related:1berLkImik8J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5731435541475080149&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'1berLkImik8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.61.3054&amp;rep=rep1&amp;type=pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1164127.1164234" class=yCC>Paragrab: A comprehensive architecture for web image management and multimodal querying</a></h3><div class="gs_a"><a href="/citations?user=TYmV4V8AAAAJ&amp;hl=en&amp;oi=sra">D Joshi</a>, <a href="/citations?user=GmJRVxgAAAAJ&amp;hl=en&amp;oi=sra">R Datta</a>, Z Zhuang, WP Weiss&hellip; - Proceedings of the  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract We demonstrate PARAgrab-a scalable Web image archival, retrieval, and <br>annotation system that supports multiple querying modalities. The underlying architecture of <br>our large-scale Web image database is described. Querying and visualization techniques <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13143759149474164549&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 17</a> <a href="/scholar?q=related:Ra-yzU4DaLYJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4A/44/RN204148921.html?source=googlescholar" class="gs_nph" class=yCE>BL Direct</a> <a href="/scholar?cluster=13143759149474164549&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'Ra-yzU4DaLYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.mirlab.org/conference_papers/International_Conference/ACM%202005/docs/mm618.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101290" class=yCF>SEVA: sensor-enhanced video annotation</a></h3><div class="gs_a">X Liu, M Corner, P Shenoy - Proceedings of the 13th annual ACM  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we study how a sensor-rich world can be exploited by digital <br>recording devices such as cameras and camcorders to improve a user&#39;s ability to search <br>through a large repository of image and video files. We design and implement a digital <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9421681930799950689&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 17</a> <a href="/scholar?q=related:YSsMQMyJwIIJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9421681930799950689&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 27 versions</a> <a onclick="return gs_ocit(event,'YSsMQMyJwIIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1244233" class=yC11>Web image annotation by fusing visual features and textual information</a></h3><div class="gs_a">VS Tseng, JH Su, BW Wang, YM Lin - &hellip;  of the 2007 ACM symposium on  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose a novel web image annotation method, namely FMD <br>(Fused annotation by Mixed model graph and Decision tree), which combines visual <br>features and textual information to conceptualize the web images. The FMD approach <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12055600016279526145&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 17</a> <a href="/scholar?q=related:AduIbFcYTqcJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12055600016279526145&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'AduIbFcYTqcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm07-shiruiOFF.PDF" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291307" class=yC12>Enhancing image annotation by integrating concept ontology and text-based bayesian learning model</a></h3><div class="gs_a">R Shi, CH Lee, TS Chua - &hellip;  of the 15th international conference on  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Automatic image annotation (AIA) has been a hot research topic in recent years <br>since it can be used to support concept-based image retrieval. However, most existing AIA <br>models depend heavily on the availability of a large number of labeled training samples, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6590943054411936780&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 16</a> <a href="/scholar?q=related:DDQNpcm7d1sJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6590943054411936780&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'DDQNpcm7d1sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://research.google.com/pubs/archive/35638.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5360514" class=yC14>Video2text: Learning to annotate video content</a></h3><div class="gs_a"><a href="/citations?user=2EaTkYEAAAAJ&amp;hl=en&amp;oi=sra">H Aradhye</a>, <a href="/citations?user=xfRtWTIAAAAJ&amp;hl=en&amp;oi=sra">G Toderici</a>, J Yagnik - Data Mining Workshops,  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper discusses a new method for automatic discovery and organization of <br>descriptive concepts (labels) within large real-world corpora of user-uploaded multimedia, <br>such as YouTube. com. Conversely, it also provides validation of existing labels, if any. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15352347863666148021&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 15</a> <a href="/scholar?q=related:tUIMsgp_DtUJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15352347863666148021&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'tUIMsgp_DtUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Webpage segmentation for extracting images and their surrounding contextual information</h3><div class="gs_a">F Fauzi, JL Hong, M Belkhatir - Proceedings of the seventeen ACM international  &hellip;, 2009</div><div class="gs_fl"><a href="/scholar?cites=11553044132921546637&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 16</a> <a href="/scholar?q=related:jRczoW2oVKAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11553044132921546637&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'jRczoW2oVKAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.140.6936&amp;rep=rep1&amp;type=pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4378772" class=yC16>Web image classification based on the fusion of image and text classifiers</a></h3><div class="gs_a">PR Kalva, <a href="/citations?user=LjsK1HgAAAAJ&amp;hl=en&amp;oi=sra">F Enembreck</a>&hellip; - Document Analysis and  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a novel method for the classification of images that combines <br>information extracted from the images and contextual information. The main hypothesis is <br>that contextual information related to an image can contribute in the image classification <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4663943865473694084&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 11</a> <a href="/scholar?q=related:hPnrKTyouUAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4663943865473694084&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'hPnrKTyouUAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://homepage.fudan.edu.cn/xdzhou/files/2011/10/civr09-2-10.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fudan.edu.cn</span><span class="gs_ggsS">fudan.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1646450" class=yC18>Exploring Flickr&#39;s related tags for semantic annotation of web images</a></h3><div class="gs_a">H Xu, <a href="/citations?user=QUrLihYAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a>, M Wang, <a href="/citations?user=9cZUlEYAAAAJ&amp;hl=en&amp;oi=sra">Y Xiang</a>, B Shi - Proceedings of the ACM  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Exploring social media resources, such as Flickr and Wikipedia to mitigate the <br>difficulty of semantic gap has attracted much attention from both academia and industry. In <br>this paper, we first propose a novel approach to derive semantic correlation matrix from <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1160469110161904771&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 7</a> <a href="/scholar?q=related:g5QD3IHQGhAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1160469110161904771&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'g5QD3IHQGhAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://research.google.com/pubs/archive/37092.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2010028" class=yC1A>Improved video categorization from text metadata and user comments</a></h3><div class="gs_a"><a href="/citations?user=23xz9QgAAAAJ&amp;hl=en&amp;oi=sra">K Filippova</a>, <a href="/citations?user=yxwoJrcAAAAJ&amp;hl=en&amp;oi=sra">KB Hall</a> - Proceedings of the 34th international ACM SIGIR  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract We consider the task of assigning categories (eg, howto/cooking, sports/basketball, <br>pet/dogs) to YouTube videos from video and text signals. We show that two complementary <br>views on the data--from the video and text perspectives--complement each other and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2619608848904469891&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 8</a> <a href="/scholar?q=related:g71raV64WiQJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2619608848904469891&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'g71raV64WiQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm08-wanggang.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459393" class=yC1C>Exploring knowledge of sub-domain in a multi-resolution bootstrapping framework for concept detection in news video</a></h3><div class="gs_a">G Wang, TS Chua, <a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a> - Proceeding of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we present a model based on a multi-resolution, multi-source and <br>multi-modal (M3) bootstrapping framework that exploits knowledge of sub-domains for <br>concept detection in news video. Because the characteristics and distributions of data in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15442298916331928942&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 5</a> <a href="/scholar?q=related:bj1dLwwRTtYJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15442298916331928942&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'bj1dLwwRTtYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://inderscience.metapress.com/index/N53K4P1709865286.pdf" class=yC1E>Multimodal information fusion for selected multimedia applications</a></h3><div class="gs_a">L Guan, <a href="/citations?user=gIZWoKYAAAAJ&amp;hl=en&amp;oi=sra">Y Wang</a>, R Zhang, Y Tie, A Bulzacki&hellip; - International Journal of &hellip;, 2010 - Inderscience</div><div class="gs_rs">The effective interpretation and integration of multiple information content are important for <br>the efficacious utilisation of multimedia in a wide variety of application context. The major <br>challenge in multimodal information fusion lies in the difficulty of identifying the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1291863126916826854&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 4</a> <a href="/scholar?q=related:5oZNWque7REJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'5oZNWque7REJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://www.mirlab.org/conference_papers/International_Conference/ACM%202005/docs/mir49.pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101837" class=yC1F>Similarity space projection for Web image search and annotation</a></h3><div class="gs_a">Y Liu, <a href="/citations?user=Bl4SRU0AAAAJ&amp;hl=en&amp;oi=sra">T Qin</a>, <a href="/citations?user=Nh832fgAAAAJ&amp;hl=en&amp;oi=sra">TY Liu</a>, <a href="/citations?user=fIlGZToAAAAJ&amp;hl=en&amp;oi=sra">L Zhang</a>, WY Ma - Proceedings of the 7th ACM  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract Web image search has been explored and developed in academic as well as <br>commercial areas for over a decade. To measure the similarity between Web images and <br>user queries, most of the existing Web image search systems try to convert an image to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4619453159643083571&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 6</a> <a href="/scholar?q=related:M_MUfSyYG0AJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4619453159643083571&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'M_MUfSyYG0AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://www.thinkmind.org/download.php?articleid=mmedia_2011_3_40_40101" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from thinkmind.org</span><span class="gs_ggsS">thinkmind.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.thinkmind.org/index.php?view=article&amp;articleid=mmedia_2011_3_40_40101" class=yC21>A clustering-based approach to web image context extraction</a></h3><div class="gs_a">S Alcic, <a href="/citations?user=ZJbwV1IAAAAJ&amp;hl=en&amp;oi=sra">S Conrad</a> - &hellip;  2011, The Third International Conferences on  &hellip;, 2011 - thinkmind.org</div><div class="gs_rs">Abstract: Images on the Web come along with textual descriptions that are valuable for <br>different applications, such as image annotation, clustering of images, image categorization, <br>etc. But usually Web pages are poorly structured and cluttered with contents of different <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13583936647543695466&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 4</a> <a href="/scholar?q=related:aqhAAGnWg7wJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'aqhAAGnWg7wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1743436" class=yC23>Region-based automatic web image selection</a></h3><div class="gs_a">K Yanai, K Barnard - Proceedings of the international conference on  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract We propose a new Web image selection method which employs the region-based <br>bag-of-features representation. The contribution of this work is (1) to introduce the region-<br>based bag-of-features representation into an Web image selection task where training <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13609459826262120953&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 3</a> <a href="/scholar?q=related:-ZlNIZuD3rwJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'-ZlNIZuD3rwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.2576&amp;rep=rep1&amp;type=pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/n6v2362v264k3063.pdf" class=yC24>A broadcast model for Web image annotation</a></h3><div class="gs_a">J Li, T Liu, W Wang, W Gao - Advances in Multimedia Information  &hellip;, 2006 - Springer</div><div class="gs_rs">Automatic annotation of Web image has great potential in improving the performance of web <br>image retrieval. This paper presents a Broadcast Model (BM) for Web image annotation. In <br>this model, pages are divided into blocks and the annotation of image is realized through <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16644022289938997348&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 3</a> <a href="/scholar?q=related:ZGB_wRZy--YJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/51/31/RN198327711.html?source=googlescholar" class="gs_nph" class=yC26>BL Direct</a> <a href="/scholar?cluster=16644022289938997348&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'ZGB_wRZy--YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://none.cs.umass.edu/theses/xiaotao.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from umass.edu</span><span class="gs_ggsS">umass.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://none.cs.umass.edu/theses/xiaotao.pdf" class=yC27>System support for pervasive multimedia systems</a></h3><div class="gs_a">X Liu - 2006 - none.cs.umass.edu</div><div class="gs_rs">Modern mobile devices use energy judiciously by incorporating a number of power <br>management features. For instance, modern processors such as Intel&#39;s XScale and Pentium-<br>M and Transmeta&#39;s Crusoe incorporate dynamic voltage and frequency scaling (DVFS) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18372502986317916967&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 3</a> <a href="/scholar?q=related:J2tI1h8--P4J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18372502986317916967&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'J2tI1h8--P4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md21', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md21" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:J2tI1h8--P4J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:J2tI1h8--P4J:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=14272560739146899737&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/22377q1213012rx7.pdf" class=yC29>Automatic web image annotation via web-scale image semantic space learning</a></h3><div class="gs_a">H Xu, <a href="/citations?user=QUrLihYAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a>, L Lin, <a href="/citations?user=9cZUlEYAAAAJ&amp;hl=en&amp;oi=sra">Y Xiang</a>, B Shi - Advances in Data and Web  &hellip;, 2009 - Springer</div><div class="gs_rs">Abstract. The correlation between keywords has been exploited to improve Automatic Image <br>Annotation (AIA). Differing from the traditional lexicon or training data based keyword <br>correlation estimation, we propose using Web-scale image semantic space learning to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6379174734895318690&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 2</a> <a href="/scholar?q=related:ogaVgZ9hh1gJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6379174734895318690&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ogaVgZ9hh1gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5285061" class=yC2A>Ontology-Based Semantic Web Image Retrieval by Utilizing Textual and Visual Annotations</a></h3><div class="gs_a">JH Su, BW Wang, HH Yeh&hellip; - &hellip; 2009. WI-IAT&#39;09. IEEE/WIC &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The goal of traditional visual or textual-based image retrieval is to satisfy user&#39;s <br>queries by associating the images and semantic concepts effectively. As a result, perceptual <br>structures of images have attracted researchers&#39; attention in recent studies. However, few <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3587927243699350228&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 2</a> <a href="/scholar?q=related:1GqliNHgyjEJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3587927243699350228&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'1GqliNHgyjEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/772_Paper.pdf" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lrec-conf.org</span><span class="gs_ggsS">lrec-conf.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/772_Paper.pdf" class=yC2B>News image annotation on a large parallel text-image corpus</a></h3><div class="gs_a"><a href="/citations?user=CKGKuOAAAAAJ&amp;hl=en&amp;oi=sra">P Tirilly</a>, V Claveau, P Gros - 7th Language Resources and  &hellip;, 2010 - lrec-conf.org</div><div class="gs_rs">Abstract In this paper, we present a multimodal parallel text-image corpus, and propose an <br>image annotation method that exploits the textual information associated with images. Our <br>corpus contains news articles composed of a text, images and image captions, and is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14945507287819752081&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 3</a> <a href="/scholar?q=related:kRKCjrEbac8J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14945507287819752081&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'kRKCjrEbac8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md24', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md24" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:kRKCjrEbac8J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5733423" class=yC2D>Effective semantic annotation by image-to-concept distribution model</a></h3><div class="gs_a">JH Su, CL Chou, <a href="/citations?user=kc6wqugAAAAJ&amp;hl=en&amp;oi=sra">CY Lin</a>&hellip; - &hellip; , IEEE Transactions on, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Image annotation based on visual features has been a difficult problem due to the <br>diverse associations that exist between visual features and human concepts. In this paper, <br>we propose a novel approach called Annotation by Image-to-Concept Distribution Model (<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2462913930046407351&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 4</a> <a href="/scholar?q=related:twLHmCsHLiIJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2462913930046407351&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'twLHmCsHLiIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://www.ijfs.org.tw/ePublication/2010_paper_2/ijfs10-2-r-5-IJFS-final-IJFS20091214-0032.pdf" class=yC2F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ijfs.org.tw</span><span class="gs_ggsS">ijfs.org.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ijfs.org.tw/ePublication/2010_paper_2/ijfs10-2-r-5-IJFS-final-IJFS20091214-0032.pdf" class=yC2E>Multi-modal image retrieval by integrating web image annotation, concept matching and fuzzy ranking techniques</a></h3><div class="gs_a">JH Su, BW Wang, TY Hsu, CL Chou&hellip; - International Journal of  &hellip;, 2010 - ijfs.org.tw</div><div class="gs_rs">Abstract Traditional image retrieval aims at bridging visual images and human concepts <br>through visual or textual descriptions. However, it is still a challenging issue to reduce the <br>gap between the images and user&#39;s intentions. To this end, a considerable number of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5792251035550667392&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 4</a> <a href="/scholar?q=related:gNLiz6A1YlAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5792251035550667392&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'gNLiz6A1YlAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:gNLiz6A1YlAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7698332&amp;id=QkLOAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC30>Projecting queries and images into a similarity space</a></h3><div class="gs_a"><a href="/citations?user=Nh832fgAAAAJ&amp;hl=en&amp;oi=sra">TY Liu</a>, T Qin, WY Ma - US Patent 7,698,332, 2010 - Google Patents</div><div class="gs_rs">A method and system for projecting queries and images into a similarity space where <br>queries are close to their relevant images is provided. A similarity space projection (âSSPâ) <br>system learns a query projection function and an image projection function based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2595305318929654576&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 2</a> <a href="/scholar?q=related:MON74m9gBCQJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2595305318929654576&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'MON74m9gBCQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/94913x/200909/31515938.html" class=yC31>åºäºå¤ä¾å­¦ä¹ ç Web å¾åèç±»</a></h3><div class="gs_a">è·¯æ¶ï¼ é©¬å°å¹³ - è®¡ç®æºç ç©¶ä¸åå±, 2009 - cqvip.com</div><div class="gs_rs">å¨å¾ååç±»åèªå¨æ æ³¨ç³»ç»ä¸­, å¤ä¾å­¦ä¹ (MIL) æ¯ç ç©¶çç­ç¹. ç®åMIL ä¸­çç®æ³å¤ä¸ºçç£å­¦ä¹ <br>æ¹æ³. éå¯¹éçç£å­¦ä¹ , å¨åºäºEM ç®æ³åå¯åå¼è¿­ä»£ä¼åç®æ³çæ¡æ¶ä¸, æåºäº6 <br>ç§å¤ä¾èç±»ç®æ³, å¹¶éè¿å®ä»¬å¯¹æ¥èªäºçå®web ç¯å¢ä¸çå¾åè¿è¡èç±»ä»¥åæç¨æ·çæç´¢å´è¶£<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12707269376191568351&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 4</a> <a href="/scholar?q=related:3xkZ8StKWbAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12707269376191568351&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'3xkZ8StKWbAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://hal.archives-ouvertes.fr/docs/00/46/59/60/PDF/These_ED_0366-370_-_Najlae_IDRISSI_-_2008.pdf" class=yC33><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/tel-00465960/" class=yC32>La navigation dans les bases d&#39;images: prise en compte des attributs de texture</a></h3><div class="gs_a">N Idrissi - 2008 - hal.archives-ouvertes.fr</div><div class="gs_rs">RÃ©sumÃ© Ce travail de recherche entre dans le cadre des systÃ¨mes de recherche d&#39;images <br>par le contenu, en particulier la recherche par la texture. Le but de ce travail est de permettre <br>Ã  l&#39;utilisateur de naviguer dans de grande base de donnÃ©es d&#39;images sans formulation de <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5345067601461757147&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 2</a> <a href="/scholar?q=related:28STBap-LUoJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5345067601461757147&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'28STBap-LUoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://www.ppgia.pucpr.br/~alekoe/Papers/ALEKOE-IEEELAT-2008.pdf" class=yC35><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from pucpr.br</span><span class="gs_ggsS">pucpr.br <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4917439" class=yC34>WEB Image Classification using Classifier Combination</a></h3><div class="gs_a">PR Kalva, <a href="/citations?user=LjsK1HgAAAAJ&amp;hl=en&amp;oi=sra">F Enembreck</a>&hellip; - &hellip; , IEEE (Revista IEEE  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a novel method for the classification of images that combines <br>information extracted from the images and contextual information. The main hypothesis is <br>that contextual information related to an image can contribute in the image classification <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14232320091113431768&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 2</a> <a href="/scholar?q=related:2KJ4gLdbg8UJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14232320091113431768&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'2KJ4gLdbg8UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/13418/Thesis_Marchenko_Yelizaveta_PhDDegree.pdf?sequence=1" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/13418" class=yC36>Ontology-based annotation of paintings with artistic concepts</a></h3><div class="gs_a">M YELIZAVETA - 2007 - scholarbank.nus.edu</div><div class="gs_rs">In this thesis, we focus on the automatic annotation of paintings with various artistic <br>concepts. These concepts originate from the several domain ontologies. In our work we <br>combine such domain knowledge with trasductive inference and demonstrate that the use <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=77623979425578235&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:-7y2F5nGEwEJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=77623979425578235&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'-7y2F5nGEwEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://w.ecologylab.net/research/publications/eng078-koh.pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ecologylab.net</span><span class="gs_ggsS">ecologylab.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1600212" class=yC38>Deriving image-text document surrogates to optimize cognition</a></h3><div class="gs_a">E Koh, A Kerne - Proceedings of the 9th ACM symposium on Document  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract The representation of information collections needs to be optimized for human <br>cognition. While documents often include rich visual components, collections, including <br>personal collections and those generated by search engines, are typically represented by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4336670922270218183&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:x5euJTrzLjwJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4336670922270218183&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'x5euJTrzLjwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/28481R371NL60168.pdf" class=yC3A>The Study on the Semantic Image Retrieval Using the Cognitive Spatial Relationships in the Semantic Web</a></h3><div class="gs_a">H Kong, M Hwang, K Na, P Kim - Industrial Applications of Semantic Web, 2005 - Springer</div><div class="gs_rs">In present day, there are a number of image data in the web because of the development of <br>the image acquisition devices. So, many researchers have been studying about the image <br>retrieval and management. Keyword matching, contents-based and concept-based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11581306725231661228&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:rJBGMBsRuaAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/35/08/RN200061273.html?source=googlescholar" class="gs_nph" class=yC3B>BL Direct</a> <a href="/scholar?cluster=11581306725231661228&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'rJBGMBsRuaAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://lass.cs.umass.edu/papers/pdf/TR05-35.pdf" class=yC3D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from umass.edu</span><span class="gs_ggsS">umass.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://lass.cs.umass.edu/papers/pdf/TR05-35.pdf" class=yC3C>SEVA: Sensor-enhanced video annotation</a></h3><div class="gs_a">X Liu, M Corner, P Shenoy - ACM Trans. Multimedia Comput.  &hellip;, 2009 - lass.cs.umass.edu</div><div class="gs_rs">Abstract Advances in consumer electronics technologies have led to a proliferation of digital <br>cameras and camcorders that record images and video in digital form and has encouraged <br>users to create ever-larger personal libraries of pictures and movies. A concurrent trend is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15735647322105887635&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:k7vyUN4_YNoJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15735647322105887635&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'k7vyUN4_YNoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md34', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md34" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:k7vyUN4_YNoJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://rvc.eng.miami.edu/dtliu/papers/2011_IJMDEM_zhu.pdf" class=yC3F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from miami.edu</span><span class="gs_ggsS">miami.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.igi-global.com/article/utilizing-context-information-enhance-content/58050" class=yC3E>Utilizing context information to enhance content-based image classification</a></h3><div class="gs_a">Q Zhu, <a href="/citations?user=RC0UcqMAAAAJ&amp;hl=en&amp;oi=sra">L Lin</a>, ML Shyu, <a href="/citations?user=5C62G0AAAAAJ&amp;hl=en&amp;oi=sra">D Liu</a> - International Journal of Multimedia  &hellip;, 2011 - igi-global.com</div><div class="gs_rs">Abstract Traditional image classification relies on text information such as tags, which <br>requires a lot of human effort to annotate them. Therefore, recent work focuses more on <br>training the classifiers directly on visual features extracted from image content. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14089593396519206164&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:FH03VYtKiMMJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14089593396519206164&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'FH03VYtKiMMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5582564" class=yC40>Effective image semantic annotation by discovering visual-concept associations from image-concept distribution model</a></h3><div class="gs_a">JH Su, CL Chou, <a href="/citations?user=kc6wqugAAAAJ&amp;hl=en&amp;oi=sra">CY Lin</a>&hellip; - Multimedia and Expo ( &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Up to the present, the contemporary studies are not really successful in image <br>annotation due to some critical problems like diverse regularities between visual features <br>and human concepts. Such diverse regularities make it hard to annotate the image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6046299186690953507&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:I5FyXBPF6FMJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6046299186690953507&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'I5FyXBPF6FMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/q1t7q1552t2p3658.pdf" class=yC41>Boosting cross-media retrieval by learning with positive and negative examples</a></h3><div class="gs_a">Y Zhuang, Y Yang - Advances in Multimedia Modeling, 2006 - Springer</div><div class="gs_rs">Content-based cross-media retrieval is a new category of retrieval methods by which the <br>modality of query examples and the returned results need not to be the same, for example, <br>users may query images by an example of audio and vice versa. Multimedia Document (<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11262276374973132216&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:uC2MZLGkS5wJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0C/03/RN201122684.html?source=googlescholar" class="gs_nph" class=yC42>BL Direct</a> <a href="/scholar?cluster=11262276374973132216&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'uC2MZLGkS5wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://cs5235.userapi.com/u133638729/docs/cc090e9c4cd3/Stphane_MarchandMaillet_Adaptive_Multimedia_Ret.pdf#page=10" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from userapi.com</span><span class="gs_ggsS">userapi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/81772u531k7607gn.pdf" class=yC43>A method for processing the natural language query in ontology-based image retrieval system</a></h3><div class="gs_a"><a href="/citations?user=4tn6Ym8AAAAJ&amp;hl=en&amp;oi=sra">M Hwang</a>, H Kong, S Baek, P Kim - Adaptive Multimedia Retrieval: User,  &hellip;, 2007 - Springer</div><div class="gs_rs">There is a large amount of image data on the web because of the development of many <br>image acquisition devices nowadays. Hence, many researchers have been focusing on the <br>study how to manage and retrieve these huge images efficiently. In this paper, we use two <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4021830819661796725&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:ddWoaNJp0DcJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0F/09/RN206775876.html?source=googlescholar" class="gs_nph" class=yC45>BL Direct</a> <a href="/scholar?cluster=4021830819661796725&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ddWoaNJp0DcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB39" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW39"><a href="http://nlpr-web.ia.ac.cn/2009papers/gjhy/gh120.pdf" class=yC47><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ia.ac.cn</span><span class="gs_ggsS">ia.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5202534" class=yC46>Web image mining using concept sensitive Markov stationary features</a></h3><div class="gs_a">C Zhang, J Liu, H Lu, S Ma - &hellip;  and Expo, 2009. ICME 2009. IEEE &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the explosive growth of Web resources, how to mine semantically relevant <br>images efficiently becomes a challenging and necessary task. In this paper, we propose a <br>concept sensitive Markov stationary feature (C-MSF) to represent images and also present <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1793570227556476345&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:uWU4hZ4K5BgJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1793570227556476345&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'uWU4hZ4K5BgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB40" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW40"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.6418&amp;rep=rep1&amp;type=pdf" class=yC49><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.6418&amp;rep=rep1&amp;type=pdf" class=yC48>Relevance Feedback in Content-Based Image Retrieval</a></h3><div class="gs_a">VN Gudivada - International Journal of Computer Science  &hellip;, 2010 - Citeseer</div><div class="gs_rs">Abstract Content-Based Image Retrieval (CBIR) systems are required to effectively harness <br>information from ubiquitous image collections. Despite intense research efforts by the <br>multidisciplinary CBIR community since early 1990s, apparently there is a mismatch <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9299656211777907456&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:AOtGjA0ED4EJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9299656211777907456&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'AOtGjA0ED4EJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md40', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md40" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:AOtGjA0ED4EJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB41" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW41"><a href="http://www.cs.bilkent.edu.tr/~duygulu/papers/MUSCLEBookChapter.pdf" class=yC4B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bilkent.edu.tr</span><span class="gs_ggsS">bilkent.edu.tr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cs.bilkent.edu.tr/~duygulu/papers/MUSCLEBookChapter.pdf" class=yC4A>Linking image and text for semantic labeling of images and videos</a></h3><div class="gs_a"><a href="/citations?user=1KEMrHkAAAAJ&amp;hl=en&amp;oi=sra">P Duygulu</a>, M Bastan, D Ozkan - Machine Learning Techniques for  &hellip;, 2008 - cs.bilkent.edu.tr</div><div class="gs_rs">Early work on image retrieval systems were based on text input, in which the images are <br>annotated by text and then text based methods are used for retrieval [5]. However, two major <br>difficulties are encountered with text based approaches: First, manual annotation, which is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17501428341882837686&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:tlrhs1mQ4fIJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'tlrhs1mQ4fIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md41', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md41" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:tlrhs1mQ4fIJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5628875" class=yC4C>High Level Semantic Retrieval of Thangka Image Based on CK Relation Net</a></h3><div class="gs_a">W Wang, J Qian, L Yin - Computing in the Global Information  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Image retrieval is one of the hottest fields of computer vision and pattern <br>recognition. In recent years, many researchers addressed the challenging problem of <br>interpreting the semantics of images. This paper presented a novel approach based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18236162500607931036&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:nPIIyyzdE_0J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18236162500607931036&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'nPIIyyzdE_0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB43" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW43"><a href="http://www.jos.org.cn/ch/reader/create_pdf.aspx?file_no=3658&amp;year_id=2010&amp;quarter_id=9&amp;falg=1" class=yC4E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jos.org.cn</span><span class="gs_ggsS">jos.org.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.jos.org.cn/ch/reader/create_pdf.aspx?file_no=3658&amp;year_id=2010&amp;quarter_id=9&amp;falg=1" class=yC4D>ä¸ç§èªéåºç Web å¾åè¯­ä¹èªå¨æ æ³¨æ¹æ³</a></h3><div class="gs_a">è®¸çº¢æ¶ï¼ å¨åä¸ï¼ åå®ï¼ æ½ä¼¯ä¹ - è½¯ä»¶å­¦æ¥, 2010 - jos.org.cn</div><div class="gs_rs">æè¦: æåºäºä¸ç§èªéåºçWeb å¾åè¯­ä¹èªå¨æ æ³¨æ¹æ³: é¦åå©ç¨Web æ ç­¾èµæºèªå¨è·åè®­ç»<br>æ°æ®; ç¶åéè¿å¸¦çº¦æçåæ®µæ©ç½å æåå½æ¨¡åå°å³èææ¬æéåå¸èªéåºå­¦ä¹ ååéªç¥è¯çº¦æ<br>ææºå°ç»åå¨ä¸èµ·, å®ç°Web å¾åè¯­ä¹çèªå¨æ æ³¨. å¨4 000 å¹ä»Web è·å¾çå¾åæ°æ®éä¸ç<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16395954804637883021&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 3</a> <a href="/scholar?q=related:jdrMewYiiuMJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16395954804637883021&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'jdrMewYiiuMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md43', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md43" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jdrMewYiiuMJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://hal.inria.fr/docs/00/56/17/96/PDF/Tirilly-et-al-RFIA2010.pdf" class=yC50><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from inria.fr</span><span class="gs_ggsS">inria.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.inria.fr/inria-00561796/fr/" class=yC4F>DÃ©tection de logos pour l&#39;annotation d&#39;images de presse</a></h3><div class="gs_a"><a href="/citations?user=CKGKuOAAAAAJ&amp;hl=en&amp;oi=sra">P Tirilly</a>, V Claveau, P Gros - 2010 - hal.inria.fr</div><div class="gs_rs">RÃ©sumÃ©: In this paper, we propose a new method to annotate news images. To avoid the <br>semantic gap problem due to the use of low-level visual features, we associate high-level <br>visual features (presence of logos and panels) and high-level textual features (a subset of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1285527584484075697&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:sagj7oYc1xEJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1285527584484075697&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'sagj7oYc1xEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/Q4676GX66P16765G.pdf" class=yC51>A Novel Image Semantic Annotation Method Based on Image-Concept Distribution Model</a></h3><div class="gs_a">M Ying, Z Laomo, G Jixun - Affective Computing and Intelligent Interaction, 2012 - Springer</div><div class="gs_rs">The optimal solution to support a great annotator is very difficult to derive. Up to the present, <br>little work can precisely tag the images since the so-called semantic gap is not easy to <br>reduce. This paper constitutes the approach of approximating the solution to discover the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:hUcEDFBDUmsJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7733317521226614661&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'hUcEDFBDUmsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB46" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW46"><a href="http://airccse.org/journal/ijcsea/papers/1011ijcsea12.pdf" class=yC53><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from airccse.org</span><span class="gs_ggsS">airccse.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://airccse.org/journal/ijcsea/papers/1011ijcsea12.pdf" class=yC52>HIGH-LEVEL SEMANTICS OF IMAGES IN WEB DOCUMENTS USING WEIGHTED TAGS AND STRENGTH MATRIX</a></h3><div class="gs_a">P Shanmugavadivu, P Sumathy, A Vadivel - airccse.org</div><div class="gs_rs">ABSTRACT The multimedia information retrieval from World Wide Web is a challenging <br>issue. Describing multimedia object in general, images in particular with low-level features <br>increases the semantic gap. From WWW, information present in a HTML document as <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:yZIUG0tWVoUJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'yZIUG0tWVoUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md46', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md46" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:yZIUG0tWVoUJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4740823" class=yC54>Web Image Clustering Based on Multi-instance</a></h3><div class="gs_a">J Lu, SP Ma, M Zhang - &hellip;  Agent Technology, 2008. WI-IAT&#39;08.  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In image retrieval and annotation, multi-instance learning has been studied actively. <br>Most of the methods solve the MIL problem in a supervised way. In this paper, we proposed <br>two unsupervised frameworks for clustering multi-instance objects based on expectation <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:iHrFHfsg6FQJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6118176356691180168&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'iHrFHfsg6FQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Towards fully-automated image retrieval systems featuring multi-facetted logic-based representations</h3><div class="gs_a">M Belkhatir</div><div class="gs_fl"><a href="/scholar?q=related:Lg1tHjp0Ux4J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Lg1tHjp0Ux4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://www.benjysbrain.com/ComicsMiner/cline2012bb.pdf" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from benjysbrain.com</span><span class="gs_ggsS">benjysbrain.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.benjysbrain.com/ComicsMiner/cline2012bb.pdf" class=yC55>Automatic Recognition and Extraction of Web Comics</a></h3><div class="gs_a">BE Cline - 2012 - benjysbrain.com</div><div class="gs_rs">Abstract The ability to data mine comic strips from the web is useful for creating customized <br>daily comics pages; however, automatically selecting the primary comic from a comic strip <br>web page is difficult because banners, advertisements, and decorative images can be of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:xa_bKWmXT-gJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'xa_bKWmXT-gJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md49', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md49" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:xa_bKWmXT-gJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6120716" class=yC57>Semantic Video Retrieval by Integrating Concept-and Content-Aware Mining</a></h3><div class="gs_a">BW Wang, JH Su, CL Chou&hellip; - &hellip;  and Applications of  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video retrieval has been a hot topic due to the prevalence of video capturing <br>devices and media-sharing services such as YouTube. Until now, few past studies has <br>focused on querying the videos by images due to the semantic gap between images and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ZNP4NSyyllkJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6455543018835989348&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ZNP4NSyyllkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://wang.ist.psu.edu/~djoshi/joshi_ieeecomp.pdf" class=yC59><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://wang.ist.psu.edu/~djoshi/joshi_ieeecomp.pdf" class=yC58>PARAgrab: A Picture Archival, Retrieval and Annotation System</a></h3><div class="gs_a"><a href="/citations?user=TYmV4V8AAAAJ&amp;hl=en&amp;oi=sra">D Joshi</a>, <a href="/citations?user=GmJRVxgAAAAJ&amp;hl=en&amp;oi=sra">R Datta</a>, WP Weiss, <a href="/citations?user=4Nmf18IAAAAJ&amp;hl=en&amp;oi=sra">J Li</a>, <a href="/citations?user=inVzWAcAAAAJ&amp;hl=en&amp;oi=sra">JZ Wang</a> - wang.ist.psu.edu</div><div class="gs_rs">Abstract We propose a design of next generation digital image management system. The <br>proposed design is prototyped with the implementation of PARAgrab-a scalable Web image <br>library which supports multimodal querying for a comprehensive image searching and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:JM4XOw9HmcAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'JM4XOw9HmcAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md51', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md51" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:JM4XOw9HmcAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB52" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW52"><a href="http://wang.ist.psu.edu/~datta/proposal_datta.pdf" class=yC5B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://wang.ist.psu.edu/~datta/proposal_datta.pdf" class=yC5A>Semantics and Beyond: Statistical Modeling for Multimedia Search, Annotation, and Aesthetics</a></h3><div class="gs_a"><a href="/citations?user=sAkg9T8AAAAJ&amp;hl=en&amp;oi=sra">CL Giles</a> - 2007 - wang.ist.psu.edu</div><div class="gs_rs">Abstract The problem of automatically inferring the generally accepted semantics of media <br>objects such as text, images, or video is considered highly challenging, and continues to be <br>a core issue in artificial intelligence research. Yet, a solution to this problem can lead to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Q8a5Q1R6gR0J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Q8a5Q1R6gR0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md52', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md52" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Q8a5Q1R6gR0J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/15829/final_thesis_wanggang_nus_phdx.pdf?sequence=1" class=yC5D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/15829" class=yC5C>A multi-resolution multi-source and multi-modal (M3) transductive framework for concept detection in news video</a></h3><div class="gs_a">W Gang - 2009 - scholarbank.nus.sg</div><div class="gs_rs">We study the problem of detecting concepts in news video. Most existing algorithms for news <br>video concept detection are based on single-resolution (shot), single source (training data), <br>and multi-modal fusion methods under a supervised inductive inference framework. In this <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:O24GTu3y5YoJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10008672847931010619&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'O24GTu3y5YoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB54" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW54"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ZhaZJ_Chapter_Text%20Mining%20in%20Multimedia.pdf" class=yC5F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/L65T405M346055V8.pdf" class=yC5E>Text Mining in Multimedia</a></h3><div class="gs_a">ZJ Zha, M Wang, <a href="/citations?user=d3h-zScAAAAJ&amp;hl=en&amp;oi=sra">J Shen</a>, TS Chua - Mining Text Data, 2012 - Springer</div><div class="gs_rs">A large amount of multimedia data (eg, image and video) is now available on the Web. A <br>multimedia entity does not appear in isolation, but is accompanied by various forms of <br>metadata, such as surrounding text, user tags, ratings, and comments etc. Mining these <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:VB61TlfctuwJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17057062906253090388&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'VB61TlfctuwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> A SEMANTIC ANNOTATION METHOD FOR NETWORK IMAGE</h3><div class="gs_a">Z ZEQING - American Journal of Engineering and Technology  &hellip;, 2011</div><div class="gs_fl"><a href="/scholar?q=related:WiMAVKYt1n0J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9067485092154188634&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'WiMAVKYt1n0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2110379" class=yC60>Semi-supervised image classification for automatic construction of a health image library</a></h3><div class="gs_a">Y Chen, G Zhang, R Xu - Proceedings of the 2nd ACM SIGHIT  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Images represent an important aspect of the entire medical knowledge. Health <br>illustrations and photos are essential for patient education and self-care. Currently there is <br>few effort in automatically building comprehensive image bases for general health <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5703815986268973004&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:zKfBXWsGKE8J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'zKfBXWsGKE8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6406819" class=yC61>Web Image Context Extraction Based on Semantic Representation of Web Page Visual Segments</a></h3><div class="gs_a">G Tryfou, Z Theodosiou&hellip; - Semantic and Social  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Among the most challenging scientific interests of the past years, special attention <br>has been given to the task of web image information mining. Web images exist in huge <br>amounts on the web and several methods for their efficient description and representation <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'XTHnXe-qGkkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> The Study on the Efficient Browsing Methodology in the Ontology-based Image Retrieval System</h3><div class="gs_a"><a href="/citations?user=4tn6Ym8AAAAJ&amp;hl=en&amp;oi=sra">M Hwang</a>, H Kong, P Kim</div><div class="gs_fl"><a href="/scholar?q=related:dj9dZ8waXb0J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'dj9dZ8waXb0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/M5W648781W851037.pdf" class=yC62>Medical Images Annotation</a></h3><div class="gs_a">L Stanescu, DD Burdescu, M Brezovan&hellip; - Creating New Medical  &hellip;, 2012 - Springer</div><div class="gs_rs">Image classification and automatic annotation could be treated as effective solutions to <br>enable keyword-based semantic image retrieval. The importance of automatic image <br>annotation has increased with the growth of the digital images collections being of great <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:LYqQagZoDVgJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'LYqQagZoDVgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB60" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW60"><a href="http://wang.ist.psu.edu/IMAGE/Datta_thesis.pdf" class=yC64><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://wang.ist.psu.edu/IMAGE/Datta_thesis.pdf" class=yC63>The Graduate School College of Engineering</a></h3><div class="gs_a">R Datta - 2009 - wang.ist.psu.edu</div><div class="gs_rs">Abstract The automatic inference of image semantics is an important but highly challenging <br>research problem whose solutions can greatly benefit content-based image search and <br>automatic image annotation. In this thesis, I present algorithms and statistical models for <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:cgD8b4nSDLsJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13478379272544190578&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'cgD8b4nSDLsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md60', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md60" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:cgD8b4nSDLsJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB61" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW61"><a href="http://cda.ornl.gov/publications_2012/Publication_34204.pdf" class=yC66><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ornl.gov</span><span class="gs_ggsS">ornl.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cda.ornl.gov/publications_2012/Publication_34204.pdf" class=yC65>Retrieving biomedical images through content-based learning from examples using fine granularity</a></h3><div class="gs_a">H Jiang, S Xu, FCM Lau - Society of Photo-Optical Instrumentation  &hellip;, 2012 - cda.ornl.gov</div><div class="gs_rs">ABSTRACT Traditional content-based image retrieval methods based on learning from <br>examples analyze and attempt to understand high-level semantics of an image as a whole. <br>They typically apply certain case-based reasoning technique to interpret and retrieve <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:qQ3PfKvttBQJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1492078697355611561&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'qQ3PfKvttBQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md61', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md61" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:qQ3PfKvttBQJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1071581910000029" class=yC67>A user study to investigate semantically relevant contextual information of WWW images</a></h3><div class="gs_a">F Fauzi, M Belkhatir - International journal of human-computer studies, 2010 - Elsevier</div><div class="gs_rs">The contextual information of Web images is investigated to address the issue of enriching <br>their index characterizations with semantic descriptors and therefore bridge the semantic <br>gap (ie the gap between the low-level content-based description of images and their <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4457672221360151697&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:kVxMf0TV3D0J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4457672221360151697&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'kVxMf0TV3D0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB63" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW63"><a href="http://www.wseas.us/e-library/conferences/2012/Vouliagmeni/ACA/ACA-13.pdf" class=yC69><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from wseas.us</span><span class="gs_ggsS">wseas.us <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.wseas.us/e-library/conferences/2012/Vouliagmeni/ACA/ACA-13.pdf" class=yC68>Image indexing based on web page segmentation and clustering</a></h3><div class="gs_a">G Tryfou, N Tsapatsoulis - &hellip;  of the 11th international conference on  &hellip;, 2012 - wseas.us</div><div class="gs_rs">Abstract: Thousands of images are nowadays available on the web. These images are <br>accompanied by a wide range of textual descriptors, such as image file names, anchor texts <br>and, of course, surrounding text. Existing systems that attempt to mine information for <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:GEzAgVTo7vAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17361069063211600920&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'GEzAgVTo7vAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md63', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md63" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:GEzAgVTo7vAJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Web Image Annotation Using Word Co-occurrence and Association Analysis</h3><div class="gs_a">HC Liao - 2008</div><div class="gs_fl"><a href="/scholar?q=related:Qod4hLXzb-cJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Qod4hLXzb-cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB65" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW65"><a href="http://researchbank.rmit.edu.au/eserv/rmit:6607/Awgiskandar.pdf" class=yC6B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from rmit.edu.au</span><span class="gs_ggsS">rmit.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://researchbank.rmit.edu.au/view/rmit:6607" class=yC6A>Image retrieval using automatic region tagging</a></h3><div class="gs_a">D Awg Iskandar - 2008 - researchbank.rmit.edu.au</div><div class="gs_rs">Abstract The task of tagging, annotating or labelling image content automatically with <br>semantic keywords is a challenging problem. To automatically tag images semantically <br>based on the objects that they contain is essential for image retrieval. In addressing these <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ceiBkQS09s4J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14913305147798055025&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'ceiBkQS09s4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/R514315117224V79.pdf" class=yC6C>FCBIR: A Fuzzy Matching Technique for Content-Based Image Retrieval</a></h3><div class="gs_a">V Tseng, JH Su, WJ Huang - &hellip;  Advances and Applications of Fuzzy Logic  &hellip;, 2007 - Springer</div><div class="gs_rs">Semantic image retrieval basically can be viewed as a pattern recognition problem. For <br>human, pattern recognition is inherent in herself/himself by the inference rules through a <br>long time experience. However, for computer, on the one hand, the simulated human <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3vIVmnb1SZwJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11261802232956711646&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'3vIVmnb1SZwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:333"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/mjq6j6k5824077pq.pdf" class=yC6D>Combining Textual and Visual Information for Semantic Labeling of Images and Videos</a></h3><div class="gs_a"><a href="/citations?user=1KEMrHkAAAAJ&amp;hl=en&amp;oi=sra">P Duygulu</a>, M BaÅtan, D Ozkan - Machine Learning Techniques for  &hellip;, 2008 - Springer</div><div class="gs_rs">Semantic labeling of large volumes of image and video archives is difficult, if not impossible, <br>with the traditional methods due to the huge amount of human effort required for manual <br>labeling used in a supervised setting. Recently, semi-supervised techniques which make <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:BhmzoSdcvCMJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2575034412235430150&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'BhmzoSdcvCMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:332"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB68" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW68"><a href="https://scholarbank.nus.edu.sg/bitstream/handle/10635/15994/SHIRUI_PHDThesis_BayesianLearningofConceptOntologyforAutomaticImageAnnotation.pdf?sequence=1" class=yC6F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://scholarbank.nus.edu.sg/handle/10635/15994" class=yC6E>Bayesian learning of concept ontology for automatic image annotation</a></h3><div class="gs_a">RUI SHI - 2007 - scholarbank.nus.edu.sg</div><div class="gs_rs">Automatic image annotation (AIA) has been a hot research topic in recent years since it can <br>be used to support concept-based image retrieval. In the field of AIA, characterizing image <br>concepts by mixture models is one of the most effective techniques. However, mixture <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:hevhtMNxNoEJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9310754365002345349&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'hevhtMNxNoEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:331"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB69" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW69"><a href="http://137.132.14.55/bitstream/handle/10635/23028/PhD%20Thesis%20-%20Pradeep%20Kumar%20Atrey%20-%202006.pdf?sequence=1" class=yC71><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.14.55</span><span class="gs_ggsS">137.132.14.55 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://137.132.14.55/handle/10635/23028" class=yC70>Information assimilation in Multimedia surveillance systems</a></h3><div class="gs_a"><a href="/citations?user=2s3sftgAAAAJ&amp;hl=en&amp;oi=sra">PK ATREY</a> - 2007 - 137.132.14.55</div><div class="gs_rs">Most multimedia surveillance systems nowadays utilize multiple types of sensors to detect <br>events of interest as and when they occur in the environment. However, due to the <br>asynchrony among and diversity of sensors, information assimilation, ie how to combine <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:qAMSwNgggb4J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13727289254509413288&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'qAMSwNgggb4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:330"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/c44q543j1306246k.pdf" class=yC72>Web image gathering with a part-based object recognition method</a></h3><div class="gs_a">K Yanai - Advances in Multimedia Modeling, 2008 - Springer</div><div class="gs_rs">We propose a new Web image gathering system which employs a part-based object <br>recognition method. The novelty of our work is introducing the bag-of-keypoints <br>representation into an Web image gathering task instead of color histogram or segmented <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:4tGFw_oA82IJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/25/3F/RN221721547.html?source=googlescholar" class="gs_nph" class=yC73>BL Direct</a> <a href="/scholar?cluster=7130043712079778274&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'4tGFw_oA82IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:329"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB71" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW71"><a href="http://upcommons.upc.edu/pfc/bitstream/2099.1/7724/1/MasterThesisMehdi.pdf" class=yC75><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from upc.edu</span><span class="gs_ggsS">upc.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://upcommons.upc.edu/handle/2099.1/7724" class=yC74>Contextual Bag-Of-Visual-Words and ECOC-Rank for Retrieval and Multi-class Object Recognition</a></h3><div class="gs_a"><a href="/citations?user=c646VbAAAAAJ&amp;hl=en&amp;oi=sra">M Mirza-Mohammadi</a> - 2009 - upcommons.upc.edu</div><div class="gs_rs">Multi-class object categorization is an important line of research in Computer Vision and <br>Pattern Recognition fields. An artificial intelligent system is able to interact with its <br>environment if it is able to distinguish among a set of cases, instances, situations, objects, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:n9rbxLvb19AJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15048738279389911711&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'n9rbxLvb19AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:328"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB72" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW72"><a href="http://homepage.fudan.edu.cn/xdzhou/files/2011/10/mmm09.pdf" class=yC77><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fudan.edu.cn</span><span class="gs_ggsS">fudan.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/f21111343n52x740.pdf" class=yC76>Adaptive Model for Integrating Different Types of Associated Texts for Automated Annotation of Web Images</a></h3><div class="gs_a">H Xu, <a href="/citations?user=QUrLihYAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a>, L Lin, M Wang, TS Chua - Advances in Multimedia Modeling, 2009 - Springer</div><div class="gs_rs">A lot of texts are associated with Web images, such as image file name, ALT texts, <br>surrounding texts etc on the corresponding Web pages. It is well known that the semantics of <br>Web images are well correlated with these associated texts, and thus they can be used to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:zJ7fxpKwXfgJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17896654638688476876&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'zJ7fxpKwXfgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:327"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/N18WM170313173N4.pdf" class=yC78>Capturing High-Level Semantics of Images in Web Documents Using Strength Matrix</a></h3><div class="gs_a">P Shanmugavadivu, P Sumathy, A Vadivel - Trends in Computer Science,  &hellip;, 2011 - Springer</div><div class="gs_rs">The multimedia information retrieval from distributed environment specifically from World <br>Wide Web is considered as one of the challenging issues to be addressed by the <br>researchers. It has been perceived that the description of images using low-level features <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:iIJAz9m2f4EJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9331377999536226952&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'iIJAz9m2f4EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:326"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/563G25659T51T577.pdf" class=yC79>Extraction of Web Image Information: Semantic or Visual Cues?</a></h3><div class="gs_a">G Tryfou, N Tsapatsoulis - Artificial Intelligence Applications and  &hellip;, 2012 - Springer</div><div class="gs_rs">Text based approaches for web image information retrieval have been exploited for many <br>years, however the noisy textual content of the web pages makes their task challenging. <br>Moreover, text based systems that retrieve information from textual sources such as image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14289929734324168151&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=87">Cited by 1</a> <a href="/scholar?q=related:1yWvz2EHUMYJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'1yWvz2EHUMYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:325"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB75" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW75"><a href="http://docserv.uni-duesseldorf.de/servlets/DerivateServlet/Derivate-21390" class=yC7B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-duesseldorf.de</span><span class="gs_ggsS">uni-duesseldorf.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://docserv.uni-duesseldorf.de/servlets/DerivateServlet/Derivate-21390" class=yC7A>Web Image Context Extraction: Methods and Evaluation</a></h3><div class="gs_a">S Alcic - 2011 - docserv.uni-duesseldorf.de</div><div class="gs_rs">Abstract Images on the Web come in hand with valuable textual content on hosting web <br>pages that can be exploited to generate image annotations. However, web documents are <br>usually composed of contents to multiple topics and the context of an image makes only a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:D9w4592ZEZYJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10813593358639356943&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'D9w4592ZEZYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md75', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md75" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:D9w4592ZEZYJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:324"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S2212017312006068" class=yC7C>Image Retrieval from WWW using Attributes in HTML TAGs</a></h3><div class="gs_a">PS Vadivu, P Sumathy, A Vadivel - Procedia Technology, 2012 - Elsevier</div><div class="gs_rs">Abstract Web pages contain large number of images and it is difficult to map the semantics <br>of the images using text present in the HTML documents. Retrieval system is designed for <br>ranking and retrieving images using ranking mechanism. These ranking mechanism use <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'LM4E8O4xDwsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:323"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB77" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW77"><a href="http://mcg.ict.ac.cn/download/pdf/paper/2006/16.ImageSaker%20A%20Semantic-based%20Image%20Retrieval%20System%20Refining%20with%20Concept%20Model.pdf" class=yC7E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mcg.ict.ac.cn/download/pdf/paper/2006/16.ImageSaker%20A%20Semantic-based%20Image%20Retrieval%20System%20Refining%20with%20Concept%20Model.pdf" class=yC7D>ImageSaker: A Semantic-based Image Retrieval System Refining with Concept Model</a></h3><div class="gs_a">K Gao, <a href="/citations?user=kwp-QzEAAAAJ&amp;hl=en&amp;oi=sra">J Zhou</a>, S Lin, Y Zhang, S Tang - mcg.ict.ac.cn</div><div class="gs_rs">Abstract In this demonstration, a two-level system for semantic-based image retrieval is <br>proposed. To overcome the shortcoming of the traditional retrieval system, we present a <br>novel method which can provide effective retrieval result in a short time. Firstly, it uses <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'G0cj8Ke5es8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md77', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md77" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:G0cj8Ke5es8J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:322"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB78" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW78"><a href="http://www.computer.org/csdl/trans/tk/preprint/ttk2012990122.pdf" class=yC80><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from computer.org</span><span class="gs_ggsS">computer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6378368" class=yC7F>iLike: Bridging the semantic gap in vertical image search by integrating text and visual features</a></h3><div class="gs_a"><a href="/citations?user=-k1N7HAAAAAJ&amp;hl=en&amp;oi=sra">Y Chen</a>, H Sampathkumar, B Luo, X Chen - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the development of Internet and Web 2.0, large volume of multimedia content <br>have been made online. It is highly desired to provide easy accessibility to such contents. <br>Towards this goal, content-based image retrieval has been intensively studied in the <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'TJ5c-cj9NvYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:321"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB79" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW79"><a href="http://aser.ornl.gov/publications_2011/Publication%2031809_S.%20Xu.pdf" class=yC82><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ornl.gov</span><span class="gs_ggsS">ornl.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2063650" class=yC81>Retrieving and ranking unannotated images through collaboratively mining online search results</a></h3><div class="gs_a">S Xu, H Jiang, FCM Lau - Proceedings of the 20th ACM international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract We present a new image search and ranking algorithm for retrieving unannotated <br>images by collaboratively mining online search results which consist of online image and <br>text search results. The online image search results are leveraged as reference examples <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Y8GF-VoiZvoJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18043146731235164515&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'Y8GF-VoiZvoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:320"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB80" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW80"><a href="http://www.paper.edu.cn/download/downPaper/201108-91" class=yC84><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from paper.edu.cn</span><span class="gs_ggsS">paper.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.paper.edu.cn/download/downPaper/201108-91" class=yC83>Extracting Image Context Based on Visual Consistency</a></h3><div class="gs_a">Z Qi, YAN Rongru, H Xuanjing - 2011 - paper.edu.cn</div><div class="gs_rs">Abstract: With the explosion of Internet, images have become widely available on theWeb. <br>Most of 5 the images on theWeb appear with textual contents, which provide valuable <br>information to the semantics of these images. Various methods (such as image retrieval, <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'pDZBGBNtoUcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:319"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB81" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW81"><a href="http://www-kasm.nii.ac.jp/jsai2005/schedule/pdf/000230.pdf" class=yC86><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nii.ac.jp</span><span class="gs_ggsS">nii.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-kasm.nii.ac.jp/jsai2005/schedule/pdf/000230.pdf" class=yC85>Semi-Supervised Learning ãç¨ãã Web ç»ååéã·ã¹ãã </a></h3><div class="gs_a">æ³äºåå¸ - www-kasm.nii.ac.jp</div><div class="gs_rs">We propose introducing a probabilistic method for image selection into the Web image <br>gathering system. The existing system consists of two step processing:(1) gathers HTML files <br>of Web pages related to keywords, analyzes them and fetches only Web images expected <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:6xJyLvc6m6QJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'6xJyLvc6m6QJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md81', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md81" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:6xJyLvc6m6QJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:318"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB82" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW82"><a href="http://www.nbuv.gov.ua/Portal/natural/Sunz/2007_3/Kinosh.pdf" class=yC88><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nbuv.gov.ua</span><span class="gs_ggsS">nbuv.gov.ua <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.nbuv.gov.ua/Portal/natural/Sunz/2007_3/Kinosh.pdf" class=yC87>ÐÐÐÐÐÐ ÐÐÐ¢ÐÐÐÐ ÐÐÐÐ¡ÐÐ Ð ÐÐÐÐÐÐÐ¦ÐÐ¯Ð¥ ÐÐÐÐÐ ÐÐÐÐÐÐ</a></h3><div class="gs_a">ÐÐ ÐÐ¸Ð½Ð¾ÑÐµÐ½ÐºÐ¾, ÐÐ ÐÑÑÑÑÐ¸Ð½ - nbuv.gov.ua</div><div class="gs_rs">Ð¡ÐµÐ¹ÑÐ°Ñ ÑÐ¶Ðµ Ð½ÐµÐ²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿ÐµÑÐµÐ¾ÑÐµÐ½Ð¸ÑÑ Ð·Ð½Ð°ÑÐ¸Ð¼Ð¾ÑÑÑ Ð¸ Ð² ÑÐ¾ Ð¶Ðµ Ð²ÑÐµÐ¼Ñ Ð¿Ð¾Ð²ÑÐµÐ´Ð½ÐµÐ²Ð½Ð¾ÑÑÑ <br>Ð¸ÑÐ¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¼ÑÐ»ÑÑÐ¸Ð¼ÐµÐ´Ð¸Ð° Ð´Ð°Ð½Ð½ÑÑ. Ð¡ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ½Ð¾Ð²ÐµÐ½Ð¸ÐµÐ¼ ÑÐ¸ÑÑÐ¾Ð²ÑÑ ÐºÐ°Ð¼ÐµÑ Ð¸ <br>ÑÐ¾ÑÐ¾Ð°Ð¿Ð¿Ð°ÑÐ°ÑÐ¾Ð², ÑÐ°Ð·Ð²Ð¸ÑÐ¸ÐµÐ¼ ÐºÐ¾Ð¼Ð¿ÑÑÑÐµÑÐ¾Ð² ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑÐ°Ð»ÑÐ½ÑÑ ÐºÐ¾Ð»Ð»ÐµÐºÑÐ¸Ð¹ Ð²Ð¸Ð´ÐµÐ¾ <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:OrHVLiyWV78J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'OrHVLiyWV78J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md82', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md82" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:OrHVLiyWV78J:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:317"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB83" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW83"><a href="http://gxbwk.njournal.sdu.edu.cn/EN/article/downloadArticleFile.do?attachType=PDF&amp;id=2357" class=yC8A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sdu.edu.cn</span><span class="gs_ggsS">sdu.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://gxbwk.njournal.sdu.edu.cn/EN/article/downloadArticleFile.do?attachType=PDF&amp;id=2357" class=yC89>ä¸ç§åºäºä¸»å¨å­¦ä¹ çéæååè®­ç»ç®æ³</a></h3><div class="gs_a">è°¢ä¼çï¼ åæ - å±±ä¸å¤§å­¦å­¦æ¥ (å·¥å­¦ç), 2012 - gxbwk.njournal.sdu.edu.cn</div><div class="gs_rs">æè¦: ä¸ºäºæ´å¥½å°åæ¥ä¸»å¨å­¦ä¹ , åçç£å­¦ä¹ åéæå­¦ä¹ è¿3 ç§æºå¨å­¦ä¹ æ¹æ³çä¼å¿, ç ç©¶äº1 <br>ä¸ªä¸éè¦2 ä¸ªåååä½è§å¾, æ³åè½åå¼ºçé«æå­¦ä¹ ç®æ³. ä»èç±»åè®¾åºå, <br>ç»åºæ¯è½®ååè®­ç»è¿ç¨ä¸­æ·»å èªå¨æ è®°æ ·æ¬çç½®ä¿¡åº¦åº¦éæ¹æ³, éä½è¯¯æ è®°ç; æåºä½ä¸ºä¸»å¨<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:uvHDUMGVfrUJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13078055025446744506&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'uvHDUMGVfrUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md83', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md83" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:uvHDUMGVfrUJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:316"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB84" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW84"><a href="http://www.ppgia.pucpr.br/~alekoe/Papers/ALEKOE-IEEELAT-Junho2008-PrePrintedVersion.pdf" class=yC8C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from pucpr.br</span><span class="gs_ggsS">pucpr.br <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ppgia.pucpr.br/~alekoe/Papers/ALEKOE-IEEELAT-Junho2008-PrePrintedVersion.pdf" class=yC8B>ClassificaÃ§Ã£o de Imagens WEB Utilizando CombinaÃ§Ã£o de Classificadores</a></h3><div class="gs_a">PR Kalva, <a href="/citations?user=LjsK1HgAAAAJ&amp;hl=en&amp;oi=sra">F Enembreck</a>, <a href="/citations?user=kaUy9GEAAAAJ&amp;hl=en&amp;oi=sra">AL Koerich</a> - ppgia.pucpr.br</div><div class="gs_rs">ResumoâEste artigo apresenta um novo mÃ©todo para a classificaÃ§Ã£o de imagens que <br>combina informaÃ§Ãµes extraÃ­das das prÃ³prias imagens e informaÃ§Ã£o contextual. A principal <br>hipÃ³tese Ã© que informaÃ§Ã£o contextual relacionada a uma imagem pode contribuir de <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:T3NoW9qxt0UJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'T3NoW9qxt0UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md84', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md84" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:T3NoW9qxt0UJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:315"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/97941x/201207/42504267.html" class=yC8D>å¾åèªå¨è¯­ä¹æ æ³¨ææ¯ç»¼è¿°</a></h3><div class="gs_a">å­åé¡¶ï¼ æå¨ - è®¡ç®æºç³»ç»åºç¨, 2012 - cqvip.com</div><div class="gs_rs">è¿å¹´æ¥, éçå¯¹åºäºåå®¹å¾åæ£ç´¢ææ¯ç ç©¶çæ·±å¥, å¾åèªå¨è¯­ä¹æ æ³¨å·²æä¸ºäºè¯¥é¢åçç ç©¶<br>ç­ç¹. éå¯¹ç®åå¹¿æ³ç ç©¶çå¾åè¯­ä¹æ æ³¨ææ¯, ä»å¶åç±», å³é®ææ¯, å­å¨é®é¢ååå±æ¹åè¿è¡äº<br>è¿è¡äºè®ºè¿°, ä»¥æä¸ºä»äºè¯¥æ¹åç ç©¶çäººåæä¾ä¸å®çåé´æä¹ååèä»·å¼.</div><div class="gs_fl"><a href="/scholar?q=related:nUNCrdGjhJoJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11134204299717002141&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'nUNCrdGjhJoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:314"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB86" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW86"><a href="http://www.dca.fee.unicamp.br/~leopini/private/teses-pdf/Tese-Andre_Tavares_da_Silva.pdf" class=yC8F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unicamp.br</span><span class="gs_ggsS">unicamp.br <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.dca.fee.unicamp.br/~leopini/private/teses-pdf/Tese-Andre_Tavares_da_Silva.pdf" class=yC8E>RecuperaÃ§Ã£o de imagens por conteÃºdo baseada em realimentaÃ§Ã£o de relevÃ¢ncia e classificador por floresta de caminhos Ã³timos</a></h3><div class="gs_a"><a href="/citations?user=9NtADMMAAAAJ&amp;hl=en&amp;oi=sra">AT da Silva</a> - 2011 - dca.fee.unicamp.br</div><div class="gs_rs">Resumo Com o crescente aumento de coleÃ§Ãµes de imagens resultantes da popularizaÃ§Ã£o <br>da Internet e das cÃ¢meras digitais, mÃ©todos eficientes de busca tornam-se cada vez mais <br>necessÃ¡rios. Neste contexto, esta tese propÃµe novos mÃ©todos de recuperaÃ§Ã£o de <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:nsmD7c2pTkgJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5210288520832469406&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'nsmD7c2pTkgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md86', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md86" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:nsmD7c2pTkgJ:scholar.google.com/&amp;hl=en&amp;num=87&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
