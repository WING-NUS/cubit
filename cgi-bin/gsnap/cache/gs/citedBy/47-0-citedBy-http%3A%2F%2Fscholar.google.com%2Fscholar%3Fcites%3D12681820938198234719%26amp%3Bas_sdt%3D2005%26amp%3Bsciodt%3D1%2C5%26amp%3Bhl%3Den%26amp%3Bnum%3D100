Total results = 47
<div class="gs_r" style="z-index:400"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4276711" class=yC0>Can high-level concepts fill the semantic gap in video retrieval? A case study with broadcast news</a></h3><div class="gs_a"><a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">A Hauptmann</a>, <a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=PlBXfHEAAAAJ&amp;hl=en&amp;oi=sra">WH Lin</a>&hellip; - &hellip;  IEEE Transactions on, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A number of researchers have been building high-level semantic concept detectors <br>such as outdoors, face, building, to help with semantic video retrieval. Our goal is to examine <br>how many concepts would be needed, and how they should be selected and used. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17182694289177070523&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 96</a> <a href="/scholar?q=related:u0uIDmsxde4J:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/09/4B/RN213262178.html?source=googlescholar" class="gs_nph" class=yC1>BL Direct</a> <a href="/scholar?cluster=17182694289177070523&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'u0uIDmsxde4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=1944&amp;context=compsci" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://repository.cmu.edu/compsci/944/" class=yC2>How many highlevel concepts will fill the semantic gap in video retrieval?</a></h3><div class="gs_a"><a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">A Hauptmann</a>, <a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=PlBXfHEAAAAJ&amp;hl=en&amp;oi=sra">WH Lin</a> - 2007 - repository.cmu.edu</div><div class="gs_rs">ABSTRACT A number of researchers have been building high-level semantic concept <br>detectors such as outdoors, face, building, etc., to help with semantic video retrieval. Using <br>the TRECVID video collection and LSCOM truth annotations from 300 concepts, we <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12622802598057090543&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 88</a> <a href="/scholar?q=related:7x0SnxU0La8J:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12622802598057090543&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'7x0SnxU0La8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.comp.nus.edu.sg/~mohan/papers/fusion_survey.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/E31M71152774R630.pdf" class=yC4>Multimodal fusion for multimedia analysis: a survey</a></h3><div class="gs_a"><a href="/citations?user=2s3sftgAAAAJ&amp;hl=en&amp;oi=sra">PK Atrey</a>, <a href="/citations?user=Qq4AAT4AAAAJ&amp;hl=en&amp;oi=sra">MA Hossain</a>, <a href="/citations?user=VcOjgngAAAAJ&amp;hl=en&amp;oi=sra">A El Saddik</a>, MS Kankanhalli - Multimedia Systems, 2010 - Springer</div><div class="gs_rs">Abstract This survey aims at providing multimedia researchers with a state-of-the-art <br>overview of fusion strategies, which are used for combining multiple modalities in order to <br>accomplish various multimedia analysis tasks. The existing literature on multimodal fusion <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2303959858949282248&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 65</a> <a href="/scholar?q=related:yFlu6UhP-R8J:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2303959858949282248&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'yFlu6UhP-R8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://csce.uark.edu/~jgauch/library/Video-Retrieval/Hauptmann.2008.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uark.edu</span><span class="gs_ggsS">uark.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4472084" class=yC6>Video retrieval based on semantic concepts</a></h3><div class="gs_a"><a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a>, MG Christel, <a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a> - Proceedings of the IEEE, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract An approach using many intermediate semantic concepts is proposed with the <br>potential to bridge the semantic gap between what a color, shape, and texture-based <br>ldquolow-levelrdquo image analysis can extract from video and what users really want to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2534619788303193523&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 46</a> <a href="/scholar?q=related:s9V8ukTHLCMJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/00/0E/RN226708430.html?source=googlescholar" class="gs_nph" class=yC8>BL Direct</a> <a href="/scholar?cluster=2534619788303193523&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'s9V8ukTHLCMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv5.papers/nus.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv5.papers/nus.pdf" class=yC9>Trecvid 2005 by nus pris</a></h3><div class="gs_a">TS Chua, SY Neo, HK Goh, <a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a>, Y Xiao&hellip; - NIST TRECVID- &hellip;, 2005 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT We participated in the high-level feature extraction and search task for <br>TRECVID 2005. For the high-level feature extraction task, we make use of the available <br>collaborative annotation results for training, and develop 2 methods to perform automated <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6139068261712169763&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 39</a> <a href="/scholar?q=related:I4s6zw5aMlUJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6139068261712169763&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'I4s6zw5aMlUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:I4s6zw5aMlUJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/r742245481q23631.pdf" class=yCB>A review of text and image retrieval approaches for broadcast news video</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a> - Information Retrieval, 2007 - Springer</div><div class="gs_rs">Abstract The effectiveness of a video retrieval system largely depends on the choice of <br>underlying text and image retrieval components. The unique properties of video collections <br>(eg, multiple sources, noisy features and temporal relations) suggest we examine the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9278500574809399146&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 35</a> <a href="/scholar?q=related:ap_F-Rzbw4AJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0D/2C/RN216038550.html?source=googlescholar" class="gs_nph" class=yCC>BL Direct</a> <a href="/scholar?cluster=9278500574809399146&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'ap_F-Rzbw4AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.lti.cs.cmu.edu/research/thesis/2006/rong_yan.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.lti.cs.cmu.edu/research/thesis/2006/rong_yan.pdf" class=yCD>Probabilistic models for combining diverse knowledge sources in multimedia retrieval</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a> - 2006 - lti.cs.cmu.edu</div><div class="gs_rs">Abstract In recent years, the multimedia retrieval community is gradually shifting its <br>emphasis from analyzing one media source at a time to exploring the opportunities of <br>combining diverse knowledge sources from correlated media types and context. In order <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1282854589144669096&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 32</a> <a href="/scholar?q=related:qJtytHOdzREJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1282854589144669096&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'qJtytHOdzREJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md6', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md6" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:qJtytHOdzREJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:qJtytHOdzREJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=9607173453927529710&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.cs.cmu.edu/~juny/Prof/papers/acmmm05-jyang.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101155" class=yCF>Multiple instance learning for labeling faces in broadcasting news video</a></h3><div class="gs_a">J Yang, <a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a> - Proceedings of the 13th annual ACM  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract Labeling faces in news video with their names is an interesting research problem <br>which was previously solved using supervised methods that demand significant user efforts <br>on labeling training data. In this paper, we investigate a more challenging setting of the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11636004294810457646&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 30</a> <a href="/scholar?q=related:LtYYhUFke6EJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11636004294810457646&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'LtYYhUFke6EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1621450" class=yC11>Automatic multimedia indexing: combining audio, speech, and visual information to index broadcast news</a></h3><div class="gs_a">K Ohtsuki, K Bessho, Y Matsuo&hellip; - Signal Processing  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes an indexing system that automatically creates metadata for <br>multimedia broadcast news content by integrating audio, speech, and visual information. <br>The automatic multimedia content indexing system includes acoustic segmentation (AS), <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1114393203351910337&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 18</a> <a href="/scholar?q=related:wXfm0bYedw8J:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4C/2A/RN186343576.html?source=googlescholar" class="gs_nph" class=yC12>BL Direct</a> <a href="/scholar?cluster=1114393203351910337&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'wXfm0bYedw8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=1377&amp;context=compsci" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4338335" class=yC13>A hybrid approach to improving semantic extraction of news video</a></h3><div class="gs_a"><a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a>, MY Chen, M Christel&hellip; - &hellip; , 2007. ICSC 2007.  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper we describe a hybrid approach to improving semantic extraction from <br>news video. Experiments show the value of careful parameter tuning, exploiting multiple <br>feature sets and multilingual linguistic resources, applying text retrieval approaches for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=71962150809329296&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 12</a> <a href="/scholar?q=related:kF5D5DGp_wAJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=71962150809329296&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'kF5D5DGp_wAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a444237.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dtic.mil</span><span class="gs_ggsS">dtic.mil <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1416462" class=yC15>Human language technology: Opportunities and challenges</a></h3><div class="gs_a">M Ostendorf, E Shriberg&hellip; - Acoustics, Speech, and  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In recent years, there has been dramatic progress in both speech and language <br>processing, in many cases leveraging some of the same underlying methods. This progress <br>and the growing technical ties motivate efforts to combine speech and language <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5312325759262784794&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 12</a> <a href="/scholar?q=related:Gv0D1CEsuUkJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5312325759262784794&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 23 versions</a> <a onclick="return gs_ocit(event,'Gv0D1CEsuUkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:Gv0D1CEsuUkJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=17560169713894925593&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://dblab.mgt.ncu.edu.tw/%E6%95%99%E6%9D%90/2009%20DM/5.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ncu.edu.tw</span><span class="gs_ggsS">ncu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4585378" class=yC17>Learning image-text associations</a></h3><div class="gs_a">T Jiang, <a href="/citations?user=G0hdDqYAAAAJ&amp;hl=en&amp;oi=sra">AH Tan</a> - &hellip; and Data Engineering, IEEE Transactions on, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Web information fusion can be defined as the problem of collating and tracking <br>information related to specific topics on the World Wide Web. Whereas most existing work on <br>Web information fusion has focused on text-based multidocument summarization, this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12038781483221067965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 13</a> <a href="/scholar?q=related:vSjxi_lXEqcJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12038781483221067965&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'vSjxi_lXEqcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://avss2012.org/2009papers/gjkw/gk30.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from avss2012.org</span><span class="gs_ggsS">avss2012.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4757429" class=yC19>Effective annotation and search for video blogs with integration of context and content analysis</a></h3><div class="gs_a">X Zhang, C Xu, <a href="/citations?user=o8PT69EAAAAJ&amp;hl=en&amp;oi=sra">J Cheng</a>, H Lu&hellip; - &hellip; , IEEE Transactions on, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In recent years, weblogs (or blogs) have received great popularity worldwide, <br>among which video blogs (or vlogs) are playing an increasingly important role. However, <br>research on vlog analysis is still in the early stage, and how to manage vlogs effectively so <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17778052580753115442&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 12</a> <a href="/scholar?q=related:MokBSqFUuPYJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17778052580753115442&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'MokBSqFUuPYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://www.dcs.gla.ac.uk/~hemant/NewsStorySegmentation_MMM09.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gla.ac.uk</span><span class="gs_ggsS">gla.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/f26r1411ql135143.pdf" class=yC1B>Tv news story segmentation based on semantic coherence and content similarity</a></h3><div class="gs_a">H Misra, <a href="/citations?user=tiASHnwAAAAJ&amp;hl=en&amp;oi=sra">F Hopfgartner</a>, A Goyal, P Punitha&hellip; - Advances in Multimedia  &hellip;, 2010 - Springer</div><div class="gs_rs">Abstract. In this paper, we introduce and evaluate two novel approaches, one using video <br>stream and the other using close-caption text stream, for segmenting TV news into stories. <br>The segmentation of the video stream into stories is achieved by detecting anchor person <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12915148785805740031&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 10</a> <a href="/scholar?q=related:_8dEr2TTO7MJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12915148785805740031&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'_8dEr2TTO7MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://cs.smith.edu/classwiki/images/6/6a/DiscoveringImageTextAssociation.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from smith.edu</span><span class="gs_ggsS">smith.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/912672W787643042.pdf" class=yC1D>Discovering image-text associations for cross-media web information fusion</a></h3><div class="gs_a">T Jiang, <a href="/citations?user=G0hdDqYAAAAJ&amp;hl=en&amp;oi=sra">AH Tan</a> - Knowledge Discovery in Databases: PKDD 2006, 2006 - Springer</div><div class="gs_rs">The diverse and distributed nature of the information published on the World Wide Web has <br>made it difficult to collate and track information related to specific topics. Whereas most <br>existing work on web information fusion has focused on multiple document summarization<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1549320144370673614&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 8</a> <a href="/scholar?q=related:zieScHZKgBUJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3B/50/RN196504915.html?source=googlescholar" class="gs_nph" class=yC1F>BL Direct</a> <a href="/scholar?cluster=1549320144370673614&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'zieScHZKgBUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm08-wanggang.pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459393" class=yC20>Exploring knowledge of sub-domain in a multi-resolution bootstrapping framework for concept detection in news video</a></h3><div class="gs_a">G Wang, TS Chua, <a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a> - Proceeding of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we present a model based on a multi-resolution, multi-source and <br>multi-modal (M3) bootstrapping framework that exploits knowledge of sub-domains for <br>concept detection in news video. Because the characteristics and distributions of data in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15442298916331928942&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 5</a> <a href="/scholar?q=related:bj1dLwwRTtYJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15442298916331928942&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'bj1dLwwRTtYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://inderscience.metapress.com/index/N53K4P1709865286.pdf" class=yC22>Multimodal information fusion for selected multimedia applications</a></h3><div class="gs_a">L Guan, <a href="/citations?user=gIZWoKYAAAAJ&amp;hl=en&amp;oi=sra">Y Wang</a>, R Zhang, Y Tie, A Bulzacki&hellip; - International Journal of &hellip;, 2010 - Inderscience</div><div class="gs_rs">The effective interpretation and integration of multiple information content are important for <br>the efficacious utilisation of multimedia in a wide variety of application context. The major <br>challenge in multimodal information fusion lies in the difficulty of identifying the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1291863126916826854&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 4</a> <a href="/scholar?q=related:5oZNWque7REJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'5oZNWque7REJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1329665" class=yC23>Concept-based large-scale video database browsing and retrieval via visualization</a></h3><div class="gs_a">H Luo, J Adviser-Fan - 2007 - dl.acm.org</div><div class="gs_rs">Abstract Motivated by Google&#39;s great success on text document retrieval and recent progress <br>in semantic video understanding, researchers began to build a new generation of video <br>retrieval systems that are able to support semantic video retrieval via keywords. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7552823667137218565&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 5</a> <a href="/scholar?q=related:BYjg0hsF0WgJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7552823667137218565&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'BYjg0hsF0WgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://www.ecpe.nu.ac.th/paisarn/Puplications/JSPS.pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nu.ac.th</span><span class="gs_ggsS">nu.ac.th <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/0111866626871327.pdf" class=yC24>A New Learning Algorithm for the Fusion of Adaptive AudioâVisual Features for the Retrieval and Classification of Movie Clips</a></h3><div class="gs_a">P Muneesawang, L Guan, T Amin - Journal of Signal Processing Systems, 2010 - Springer</div><div class="gs_rs">Abstract This paper presents a new learning algorithm for audiovisual fusion and <br>demonstrates its application to video classification for film database. The proposed system <br>utilized perceptual features for content characterization of movie clips. These features are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=616218160244406377&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 4</a> <a href="/scholar?q=related:aTiD4SY_jQgJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=616218160244406377&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'aTiD4SY_jQgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://www.hindawi.com/journals/ijdmb/aip/486487/" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[HTML]</span><span class="gs_ct2">[HTML]</span></span> <a href="http://www.hindawi.com/journals/ijdmb/aip/486487/" class=yC26>Multimodal Indexing of Multilingual News Video</a></h3><div class="gs_a"><a href="/citations?user=XUwiadkAAAAJ&amp;hl=en&amp;oi=sra">H Ghosh</a>, <a href="/citations?user=2OsvtvgAAAAJ&amp;hl=en&amp;oi=sra">SK Kopparapu</a>, <a href="/citations?user=ugI69q0AAAAJ&amp;hl=en&amp;oi=sra">T Chattopadhyay</a>&hellip; - International Journal of &hellip;, 2010 - hindawi.com</div><div class="gs_rs">The problems associated with automatic analysis of news telecasts are more severe in a <br>country like India, where there are many national and regional language channels, besides <br>English. In this paper, we present a framework for multimodal analysis of multilingual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6322536354863597187&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 3</a> <a href="/scholar?q=related:g0YjB1EpvlcJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6322536354863597187&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'g0YjB1EpvlcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:g0YjB1EpvlcJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://www.hindawi.com/journals/am/aip/310762/" class=yC29><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[HTML]</span><span class="gs_ct2">[HTML]</span></span> <a href="http://www.hindawi.com/journals/am/aip/310762/" class=yC28>Utilizing Implicit User Feedback to Improve Interactive Video Retrieval</a></h3><div class="gs_a">S Vrochidis, <a href="/citations?user=Nr7smP8AAAAJ&amp;hl=en&amp;oi=sra">I Kompatsiaris</a>, <a href="/citations?user=OBYLxRkAAAAJ&amp;hl=en&amp;oi=sra">I Patras</a> - Advances in Multimedia, 2011 - hindawi.com</div><div class="gs_rs">This paper describes an approach to exploit the implicit user feedback gathered during <br>interactive video retrieval tasks. We propose a framework, where the video is first indexed <br>according to temporal, textual, and visual features and then implicit user feedback <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11085344600213873774&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 5</a> <a href="/scholar?q=related:buSKZC4O15kJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11085344600213873774&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'buSKZC4O15kJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md20', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md20" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:buSKZC4O15kJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/vt737521865008m3.pdf" class=yC2A>Filling the semantic gap in video retrieval: An exploration</a></h3><div class="gs_a"><a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">A Hauptmann</a>, <a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=PlBXfHEAAAAJ&amp;hl=en&amp;oi=sra">WH Lin</a>, M Christel&hellip; - Semantic multimedia and &hellip;, 2008 - Springer</div><div class="gs_rs">Digital images and motion video have proliferated in the past few years, ranging from ever-<br>growing personal photo and video collections to professional news and documentary <br>archives. In searching through these archives, digital imagery indexing based on low-level <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7245539427021096793&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 3</a> <a href="/scholar?q=related:WQeuabhTjWQJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7245539427021096793&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'WQeuabhTjWQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://www.cs.cmu.edu/~jypan/publications/magic_bookchap_for_review.pdf" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=-9SU65qKgR8C&amp;oi=fnd&amp;pg=PA49&amp;ots=ayOErBAgMp&amp;sig=JqdonNeWbTDil_WWewfbgCUKX8Q" class=yC2B>Cross-Modal Correlation Mining Using Graph Algorithms</a></h3><div class="gs_a"><a href="/citations?user=bEn7ySYAAAAJ&amp;hl=en&amp;oi=sra">JY Pan</a>, HJ Yang, C Faloutsos&hellip; - &hellip;  Discovery and Data  &hellip;, 2007 - books.google.com</div><div class="gs_rs">ABSTRACT Multimedia objects like video clips or captioned images contain data of various <br>modalities such as image, audio, and transcript text. Correlations across different modalities <br>provide information about the multimedia content, and are useful in applications ranging <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16724419436548799416&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 3</a> <a href="/scholar?q=related:uDshJt4SGegJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16724419436548799416&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'uDshJt4SGegJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6002093" class=yC2D>An audio classification and speech recognition system for video content analysis</a></h3><div class="gs_a">H Feng, C Jiang, X Yang - Multimedia Technology (ICMT),  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Audio can provide useful information for video content analysis. Audio classification <br>and speech recognition for video content analysis is proposed in this paper. Firstly, audio <br>data from video stream is extracted. Secondly, the audio frames are classified into silence, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10530534293850152806&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 2</a> <a href="/scholar?q=related:Zt8BqCX5I5IJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Zt8BqCX5I5IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Toward an Adaptive Video Retrieval System</h3><div class="gs_a"><a href="/citations?user=tiASHnwAAAAJ&amp;hl=en&amp;oi=sra">F Hopfgartner</a>, <a href="/citations?user=ERvFJGkAAAAJ&amp;hl=en&amp;oi=sra">JM Jose</a> - Advances in Semantic Media  &hellip;, 2009 - Auerbach Publications</div><div class="gs_fl"><a href="/scholar?cites=8810603978931300827&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 1</a> <a href="/scholar?q=related:2-nXZKONRXoJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8810603978931300827&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'2-nXZKONRXoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/135212766680332r.pdf" class=yC2E>Semantic entity-relationship model for large-scale multimedia news exploration and recommendation</a></h3><div class="gs_a">H Luo, P Cai, W Gong, J Fan - Advances in Multimedia Modeling, 2010 - Springer</div><div class="gs_rs">Abstract. Even though current news websites use large amount of multimedia materials <br>including image, video and audio, the multimedia materials are used as supplementary to <br>the traditional text-based framework. As users always prefer multimedia, the traditional text<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2518495006653190062&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 1</a> <a href="/scholar?q=related:ro8ocN198yIJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2518495006653190062&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'ro8ocN198yIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://www.cs.bilkent.edu.tr/~duygulu/papers/MUSCLEBookChapter.pdf" class=yC30><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bilkent.edu.tr</span><span class="gs_ggsS">bilkent.edu.tr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cs.bilkent.edu.tr/~duygulu/papers/MUSCLEBookChapter.pdf" class=yC2F>Linking image and text for semantic labeling of images and videos</a></h3><div class="gs_a"><a href="/citations?user=1KEMrHkAAAAJ&amp;hl=en&amp;oi=sra">P Duygulu</a>, M Bastan, D Ozkan - Machine Learning Techniques for  &hellip;, 2008 - cs.bilkent.edu.tr</div><div class="gs_rs">Early work on image retrieval systems were based on text input, in which the images are <br>annotated by text and then text based methods are used for retrieval [5]. However, two major <br>difficulties are encountered with text based approaches: First, manual annotation, which is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17501428341882837686&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 1</a> <a href="/scholar?q=related:tlrhs1mQ4fIJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'tlrhs1mQ4fIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:tlrhs1mQ4fIJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Multimodal Indexing of Multilingual News Video</h3><div class="gs_a">G Hiranmay, <a href="/citations?user=2OsvtvgAAAAJ&amp;hl=en&amp;oi=sra">K Sunil Kumar</a>&hellip; - International  &hellip;, 2010 - Hindawi Publishing Corporation</div><div class="gs_fl"><a href="/scholar?cites=3748672000631884928&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 1</a> <a href="/scholar?q=related:gMwdwVL1BTQJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3748672000631884928&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'gMwdwVL1BTQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://ir.ii.uam.es/pubs/sivp08.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uam.es</span><span class="gs_ggsS">uam.es <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/E46444784J773Q42.pdf" class=yC31>Community based feedback techniques to improve video search</a></h3><div class="gs_a"><a href="/citations?user=HI5HwzoAAAAJ&amp;hl=en&amp;oi=sra">D Vallet</a>, <a href="/citations?user=tiASHnwAAAAJ&amp;hl=en&amp;oi=sra">F Hopfgartner</a>, <a href="/citations?user=ZXIo4JAAAAAJ&amp;hl=en&amp;oi=sra">M Halvey</a>, <a href="/citations?user=ERvFJGkAAAAJ&amp;hl=en&amp;oi=sra">JM Jose</a> - Signal, image and video  &hellip;, 2008 - Springer</div><div class="gs_rs">Abstract In this paper, we present a novel approach to aid users in the difficult task of video <br>search. We use a graph based model based on implicit feedback mined from the <br>interactions of previous users of our video search system to provide recommendations to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10402118629533626336&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 1</a> <a href="/scholar?q=related:4C_T4Me_W5AJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10402118629533626336&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'4C_T4Me_W5AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://www.ejournal.aessangli.in/ASEEJournals/IT17.doc" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[DOC]</span> from aessangli.in</span><span class="gs_ggsS">aessangli.in <span class=gs_ctg2>[DOC]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[DOC]</span><span class="gs_ct2">[DOC]</span></span> <a href="http://www.ejournal.aessangli.in/ASEEJournals/IT17.doc" class=yC33>CONTEXT BASED APPROACHES TO LEARN TEXT AND IMAGE ASSOCIATION AND PROCESSING SEMANTICS</a></h3><div class="gs_a">HK SAWANT, D KADAM - ejournal.aessangli.in</div><div class="gs_rs">ABSTRACT: The image comprises of the text-and content-based features. Images can be <br>represented using both text-and content-based features. Web information fusion can be <br>defined as the problem of collating and tracking information related to specific topics on <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7r0ZAASBijMJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'7r0ZAASBijMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md29', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md29" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:7r0ZAASBijMJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://www-2.cs.cmu.edu/~mychen/publication/AlexChenJSPS2009.pdf" class=yC36><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/30082684257p56q0.pdf" class=yC35>A Multi-Pronged Approach to Improving Semantic Extraction of News Video</a></h3><div class="gs_a"><a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a>, MY Chen, M Christel, <a href="/citations?user=PlBXfHEAAAAJ&amp;hl=en&amp;oi=sra">WH Lin</a>&hellip; - Journal of Signal  &hellip;, 2010 - Springer</div><div class="gs_rs">Abstract In this paper we describe a multi-strategy approach to improving semantic <br>extraction from news video. Experiments show the value of careful parameter tuning, <br>exploiting multiple feature sets and multilingual linguistic resources, applying text retrieval <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11726885581627137344&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=47">Cited by 1</a> <a href="/scholar?q=related:QB2JBk5EvqIJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11726885581627137344&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'QB2JBk5EvqIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://www.cs.bilkent.edu.tr/~duygulu/papers/ACM_TOIS.pdf" class=yC38><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bilkent.edu.tr</span><span class="gs_ggsS">bilkent.edu.tr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cs.bilkent.edu.tr/~duygulu/papers/ACM_TOIS.pdf" class=yC37>MAGIC: Graph-based Cross Media Correlation Detection</a></h3><div class="gs_a"><a href="/citations?user=bEn7ySYAAAAJ&amp;hl=en&amp;oi=sra">JY Pan</a>, H Yang, C Faloutsos, <a href="/citations?user=1KEMrHkAAAAJ&amp;hl=en&amp;oi=sra">P Duygulu</a> - cs.bilkent.edu.tr</div><div class="gs_rs">Abstract Given a collection of multimedia objects consisting of items of different modalities, <br>such as image, video, text, or audio, how can we find correlations between any two or more <br>modalities. We propose a novel, cross-media correlation detection method called MAGIC. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ZWNBFX5XUzIJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'ZWNBFX5XUzIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md31', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md31" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:ZWNBFX5XUzIJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://www.ee.ryerson.ca/~rzhang/icme09_mmm.pdf" class=yC3A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ryerson.ca</span><span class="gs_ggsS">ryerson.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5202824" class=yC39>Multimedia multimodal methodologies</a></h3><div class="gs_a">L Guan, P Muneesawang, <a href="/citations?user=gIZWoKYAAAAJ&amp;hl=en&amp;oi=sra">Y Wang</a>&hellip; - Multimedia and Expo &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper outlines several multimedia systems that utilize a multimodal approach. <br>These systems include audiovisual based emotion recognition, image and video retrieval, <br>and face and head tracking. Data collected from diverse sources/sensors are employed to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:oZNUJH0q4CAJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2368940120965682081&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'oZNUJH0q4CAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/15829/final_thesis_wanggang_nus_phdx.pdf?sequence=1" class=yC3C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/15829" class=yC3B>A multi-resolution multi-source and multi-modal (M3) transductive framework for concept detection in news video</a></h3><div class="gs_a">W Gang - 2009 - scholarbank.nus.sg</div><div class="gs_rs">We study the problem of detecting concepts in news video. Most existing algorithms for news <br>video concept detection are based on single-resolution (shot), single source (training data), <br>and multi-modal fusion methods under a supervised inductive inference framework. In this <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:O24GTu3y5YoJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10008672847931010619&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'O24GTu3y5YoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.stormingmedia.us/73/7324/A732444.html" class=yC3D>Human Language Technology: Opportunities and Challenges</a></h3><div class="gs_a">E Shriberg, <a href="/citations?user=NK36Tw0AAAAJ&amp;hl=en&amp;oi=sra">A Stolcke</a>, M Ostendorf - 2005 - stormingmedia.us</div><div class="gs_rs">Abstract: In recent years, there has been dramatic progress in both speech and language <br>processing, in many cases leveraging some of the same underlying methods. This progress <br>and the growing technical ties motivate efforts to combine speech and language <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:04aMUidkq38J:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'04aMUidkq38J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5714637" class=yC3E>An adaptive learning approach for tracking data using visual and textual features</a></h3><div class="gs_a">P Saravanan, <a href="/citations?user=yFPbrB4AAAAJ&amp;hl=en&amp;oi=sra">S Nagarajan</a> - Trendz in Information Sciences &amp;  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper proposes the concept of image and text association, a cornerstone of <br>cross media web information fusion. Two learning methods for discovering the underlying <br>associations between images and texts based on small training data sets are proposed. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ajvLVQRMc8UJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'ajvLVQRMc8UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/2jm3g040wx707339.pdf" class=yC3F>Multimedia translation for linking visual data to semantics in videos</a></h3><div class="gs_a"><a href="/citations?user=1KEMrHkAAAAJ&amp;hl=en&amp;oi=sra">P Duygulu</a>, M BaÅtan - Machine Vision and Applications, 2011 - Springer</div><div class="gs_rs">Abstract The semantic gap problem, which can be referred to as the disconnection between <br>low-level multimedia data and high-level semantics, is an important obstacle to build real-<br>world multimedia systems. The recently developed methods that can use large volumes of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:YJtDViN5NxoJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1889111761383299936&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'YJtDViN5NxoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://www.linkedtv.eu/wp/wp-content/uploads/2012/11/LinkedTV_D5.1_Platform_and_Architecture.pdf" class=yC41><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from linkedtv.eu</span><span class="gs_ggsS">linkedtv.eu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.linkedtv.eu/wp/wp-content/uploads/2012/11/LinkedTV_D5.1_Platform_and_Architecture.pdf" class=yC40>Work Package 5: LinkedTV platform</a></h3><div class="gs_a">R Fricke, J Thomsen - 2012 - linkedtv.eu</div><div class="gs_rs">Abstract (for dissemination) The objective of Linked TV is the integration of hyperlinks in <br>videos to open up new possibilities for an interactive, seamless usage of video on the Web. <br>LinkedTV provides a platform for the automatic identification of media fragments, their <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'_8XUfLDZBzQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md37', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md37" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_8XUfLDZBzQJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://www.ecpe.nu.ac.th/paisarn/Puplications/ICICS07.pdf" class=yC43><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nu.ac.th</span><span class="gs_ggsS">nu.ac.th <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4449866" class=yC42>SVM-Based decision fusion model for detecting concepts in films</a></h3><div class="gs_a">P Muneesawang, L Guan - Information, Communications &amp;  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper studies a support vector machine (SVM) to obtain a decision fusion <br>algorithm for detection of semantic concepts in videos, and its application to films database. <br>Given a movie clip, its spatio-temporal information is captured by audiovisual features. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:qfxOfTOZu8gJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14464323074655190185&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'qfxOfTOZu8gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB39" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW39"><a href="http://www.ijctee.org/files/Issuetwo/IJCTEE_0910_34.pdf" class=yC45><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ijctee.org</span><span class="gs_ggsS">ijctee.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ijctee.org/files/Issuetwo/IJCTEE_0910_34.pdf" class=yC44>An Effective Image-Text Association Algorithm to Retrieve Data from Multimedia Web Documents</a></h3><div class="gs_a">HK Sawant, D Kadam - International Journal - ijctee.org</div><div class="gs_rs">AbstractâThe image comprises of the text-and content-based features. Images can be <br>represented using both text-and content-based features. Web information fusion can be <br>defined as the problem of collating and tracking information related to specific topics on <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:wWMgnBf0fNUJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15383438809431565249&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'wWMgnBf0fNUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md39', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md39" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:wWMgnBf0fNUJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6130353" class=yC46>Video scene classification based on natural language description</a></h3><div class="gs_a"><a href="/citations?user=Zj32-qAAAAAJ&amp;hl=en&amp;oi=sra">L Zhang</a>, MUG Khan, Y Gotoh - Computer Vision Workshops ( &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper addresses the problem of video scene classification based on the small <br>amount of natural language description created for the video stream. The approach <br>incorporates a conventional tfÂ· idf term-document matrix with scene class specific <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jRE4rBni4lUJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'jRE4rBni4lUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB41" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW41"><a href="http://www.cs.cmu.edu/~jypan/publications/magic_bookchap_finals/final_MAGIC_bookchap.pdf" class=yC48><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cs.cmu.edu/~jypan/publications/magic_bookchap_finals/final_MAGIC_bookchap.pdf" class=yC47>Cross-Modal Correlation Mining using Graph Algorithms Jia-Yu Pan Carnegie Mellon University Pittsburgh, USA Phone: 412-268-1845</a></h3><div class="gs_a">HJ Yang, C Faloutsos, P Duygulu - cs.cmu.edu</div><div class="gs_rs">ABSTRACT Multimedia objects like video clips or captioned images contain data of various <br>modalities such as image, audio, and transcript text. Correlations across different modalities <br>provide information about the multimedia content, and are useful in applications ranging <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:R1xAyL-_-K0J:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12535980393205095495&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'R1xAyL-_-K0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md41', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md41" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:R1xAyL-_-K0J:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB42" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW42"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.3082&amp;rep=rep1&amp;type=pdf" class=yC4A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.3082&amp;rep=rep1&amp;type=pdf" class=yC49>Advanced tools for video and multimedia mining</a></h3><div class="gs_a"><a href="/citations?user=bEn7ySYAAAAJ&amp;hl=en&amp;oi=sra">JY Pan</a> - 2006 - Citeseer</div><div class="gs_rs">Abstract How do we automatically find patterns and mine data in large multimedia <br>databases, to make these databases useful and accessible? We focus on two problems:(1) <br>mining âuni-modal patternsâ that summarize the characteristics of a data modality, and (2) <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:bkU0371-3bQJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13032712250638746990&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'bkU0371-3bQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md42', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md42" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:bkU0371-3bQJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB43" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW43"><a href="http://repo.lib.nitech.ac.jp/bitstream/123456789/7566/1/E84-A_1244.pdf" class=yC4C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nitech.ac.jp</span><span class="gs_ggsS">nitech.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://search.ieice.org/bin/summary.php?id=e84-a_5_1244" class=yC4B>Polynomially Fast Parallel Algorithms for Some&lt; I&gt; P&lt;/I&gt;-Complete Problems</a></h3><div class="gs_a">CD Castanho, C Wei, A FUJIWARA - IEICE TRANSACTIONS on  &hellip;, 2001 - search.ieice.org</div><div class="gs_rs">&lt; I&gt; P&lt;/I&gt;-complete problems seem to have no parallel algorithm which runs in <br>polylogarithmic time using a polynomial number of processors. A&lt; I&gt; P&lt;/I&gt;-complete <br>problem is in the class&lt; I&gt; EP&lt;/I&gt;(Efficient and Polynomially fast) if and only if there exists <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:94l553P3o2MJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/55/0A/RN095855295.html?source=googlescholar" class="gs_nph" class=yC4D>BL Direct</a> <a href="/scholar?cluster=7179854308135504375&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'94l553P3o2MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://www.scholarbank.nus.edu.sg/bitstream/handle/10635/13136/Thesis_XU_Huaxin_HT016894E.pdf?sequence=1" class=yC4F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.scholarbank.nus.edu.sg/handle/10635/13136" class=yC4E>Integrated analysis of audiovisual signals and external information sources for event detection in team sports video</a></h3><div class="gs_a">H Xu - 2007 - scholarbank.nus.edu.sg</div><div class="gs_rs">Audiovisual signals and external information sources (news reports, live commentaries, Web <br>casts, etc.) are found to have complementary strengths for detecting events in sports video. <br>This thesis reports research on integrated analysis of them, focusing on tackling the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:M-BY-RAX2tkJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15697884812823552051&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'M-BY-RAX2tkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB45" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW45"><a href="http://stephane.ayache.perso.esil.univmed.fr/these.pdf" class=yC51><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from univmed.fr</span><span class="gs_ggsS">univmed.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://stephane.ayache.perso.esil.univmed.fr/these.pdf" class=yC50>StÃ©phane Ayache</a></h3><div class="gs_a">A Lux, <a href="/citations?user=rFaxB20AAAAJ&amp;hl=en&amp;oi=sra">P Gallinari</a>, P Joly, G QuÃ©not - stephane.ayache.perso.esil.univmed &hellip;</div><div class="gs_rs">This work deals with information retrieval and aims to reach semantic indexing of multimedia <br>documents. The state of the art approach tackle this problem by bridging of the semantic gap <br>between low-level features, from each modality, and high-level features (concepts), which <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:W_wKljMSU3IJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8237948156160703579&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'W_wKljMSU3IJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md45', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md45" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:W_wKljMSU3IJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> 1. Date personale ale titularului de program: 1.1. Nume: PIRNOG</h3><div class="gs_a">FDINB DE STAT</div><div class="gs_fl"><a href="/scholar?q=related:AF_9wjFLMcQJ:scholar.google.com/&amp;hl=en&amp;num=47&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'AF_9wjFLMcQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
