Total results = 37
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm09-richang.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631154" class=yC0>Event driven summarization for web videos</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, HK Tan, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">C Ngo</a>&hellip; - Proceedings of the first  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract The explosive growth of web videos brings out the challenge of how to efficiently <br>browse hundreds or even thousands of videos at a glance. Given an event-driven query, <br>social media web sites can easily return a ranked list of large but diverse and somewhat <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8154543978622150517&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 15</a> <a href="/scholar?q=related:decSP4vCKnEJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8154543978622150517&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'decSP4vCKnEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5466199" class=yC2>Practical online near-duplicate subsequence detection for continuous video streams</a></h3><div class="gs_a">Z Huang, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, J Shao, <a href="/citations?user=IJAU8KoAAAAJ&amp;hl=en&amp;oi=sra">B Cui</a>&hellip; - &hellip; , IEEE Transactions on, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Online video content is surging to an unprecedented level. Massive video <br>publishing and sharing impose heavy demands on online near-duplicate detection for many <br>novel video applications. This paper presents an accurate and practical system for online <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15966519223836801183&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 9</a> <a href="/scholar?q=related:n_g8zKN4lN0J:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15966519223836801183&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'n_g8zKN4lN0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.micc.unifi.it/serra/wp-content/uploads/2010/08/serra-mm10_tag_suggestion.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unifi.it</span><span class="gs_ggsS">unifi.it <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1878155" class=yC3>Tag suggestion and localization in user-generated videos based on social knowledge</a></h3><div class="gs_a"><a href="/citations?user=JUcVsxQAAAAJ&amp;hl=en&amp;oi=sra">L Ballan</a>, <a href="/citations?user=SBm9ZpYAAAAJ&amp;hl=en&amp;oi=sra">M Bertini</a>, <a href="/citations?user=bf2ZrFcAAAAJ&amp;hl=en&amp;oi=sra">A Del Bimbo</a>, <a href="/citations?user=kyZ3excAAAAJ&amp;hl=en&amp;oi=sra">M Meoni</a>&hellip; - Proceedings of second  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Nowadays, almost any web site that provides means for sharing user-generated <br>multimedia content, like Flickr, Facebook, YouTube and Vimeo, has tagging functionalities to <br>let users annotate the material that they want to share. The tags are then used to retrieve <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10382078671655340341&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 8</a> <a href="/scholar?q=related:NQktlouNFJAJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10382078671655340341&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'NQktlouNFJAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://vireo.cs.cityu.edu.hk/papers/mm10-songtan.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874159" class=yC5>Topical summarization of web videos by visual-text time-dependent alignment</a></h3><div class="gs_a">S Tan, HK Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Proceedings of the international conference on &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Search engines are used to return a long list of hundreds or even thousands of <br>videos in response to a query topic. Efficient navigation of videos becomes difficult and <br>users often need to painstakingly explore the search list for a gist of the search result. This <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7721399328803957791&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 7</a> <a href="/scholar?q=related:H6xMosfrJ2sJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7721399328803957791&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'H6xMosfrJ2sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://users.eecs.northwestern.edu/~ganghua/publication/CVPR10.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from northwestern.edu</span><span class="gs_ggsS">northwestern.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5540042" class=yC7>Interest seam image</a></h3><div class="gs_a"><a href="/citations?user=joB-O-8AAAAJ&amp;hl=en&amp;oi=sra">X Zhang</a>, <a href="/citations?user=7SgUlggAAAAJ&amp;hl=en&amp;oi=sra">G Hua</a>, <a href="/citations?user=fIlGZToAAAAJ&amp;hl=en&amp;oi=sra">L Zhang</a>&hellip; - Computer Vision and  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We propose interest seam image, an efficient visual synopsis for video. To extract <br>an interest seam image, a spatiotemporal energy map is constructed for the target video <br>shot. Then an optimal seam which encompasses the highest energy is identified by an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5892378613522960008&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 5</a> <a href="/scholar?q=related:iA4lPSHvxVEJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5892378613522960008&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'iA4lPSHvxVEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.iis.sinica.edu.tw/papers/whm/11144-F.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sinica.edu.tw</span><span class="gs_ggsS">sinica.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5604280" class=yC9>Time-series linear search for video copies based on compact signature manipulation and containment relation modeling</a></h3><div class="gs_a">CY Chiu, <a href="/citations?user=trzbZ3AAAAAJ&amp;hl=en&amp;oi=sra">HM Wang</a> - &hellip;  and Systems for Video Technology, IEEE  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a novel time-series linear search (TLS) method for detecting <br>video copies. The method utilizes a sliding window to locate window sequences that are <br>near-duplicates of a given query sequence. We address two issues of the conventional <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1177252347148297184&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 5</a> <a href="/scholar?q=related:4OPEusVwVhAJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1177252347148297184&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'4OPEusVwVhAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5583216" class=yCB>Design and evaluation of an effective and efficient video copy detection system</a></h3><div class="gs_a"><a href="/citations?user=Ade_7YoAAAAJ&amp;hl=en&amp;oi=sra">A Natsev</a>, M Hill, <a href="/citations?user=-HejxzYAAAAJ&amp;hl=en&amp;oi=sra">JR Smith</a> - Multimedia and Expo (ICME), 2010  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We consider the end-to-end system design and evaluation of an efficient and <br>effective system for video copy detection that bridges the gap between computationally <br>expensive methods and practical applications. We use a compact SIFT-based bag-of-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1966111856568781176&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 5</a> <a href="/scholar?q=related:eEl3D1EISRsJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1966111856568781176&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'eEl3D1EISRsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://vireo.cs.cityu.edu.hk/papers/csvt-hktan_10.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5580060" class=yCC>Efficient mining of multiple partial near-duplicate alignments by temporal network</a></h3><div class="gs_a">HK Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, TS Chua - Circuits and Systems for Video  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper considers the mining and localization of near-duplicate segments at <br>arbitrary positions of partial near-duplicate videos in a corpus. Temporal network is <br>proposed to model the visual-temporal consistency between video sequence by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11479758725924382753&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 4</a> <a href="/scholar?q=related:IejpJb1LUJ8J:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11479758725924382753&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'IejpJb1LUJ8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1852108" class=yCE>Mining near-duplicate graph for cluster-based reranking of web video search results</a></h3><div class="gs_a">Z Huang, B Hu, H Cheng, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, H Liu&hellip; - ACM Transactions on  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Recently, video search reranking has been an effective mechanism to improve the <br>initial text-based ranking list by incorporating visual consistency among the result videos. <br>While existing methods attempt to rerank all the individual result videos, they suffer from <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8737611604199989000&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 5</a> <a href="/scholar?q=related:CN-rzXU7QnkJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8737611604199989000&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'CN-rzXU7QnkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://homepages.dcc.ufmg.br/~tiagorm/publications/journals/jbcs10.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ufmg.br</span><span class="gs_ggsS">ufmg.br <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/C7U1301P583X1378.pdf" class=yCF>Equal but different: A contextual analysis of duplicated videos on youtube</a></h3><div class="gs_a"><a href="/citations?user=1Ee6KSoAAAAJ&amp;hl=en&amp;oi=sra">T Rodrigues</a>, <a href="/citations?user=iOnt0iMAAAAJ&amp;hl=en&amp;oi=sra">F Benevenuto</a>, <a href="/citations?user=Wbi6RfAAAAAJ&amp;hl=en&amp;oi=sra">V Almeida</a>&hellip; - Journal of the Brazilian  &hellip;, 2010 - Springer</div><div class="gs_rs">Abstract Videos have become a predominant part of users&#39; daily lives on the Web, especially <br>with the emergence of online video sharing systems such as YouTube. Since users can <br>independently share videos in these systems, some videos can be duplicates (ie, identical <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13598931613700905938&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 4</a> <a href="/scholar?q=related:0tt4n0AcubwJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13598931613700905938&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'0tt4n0AcubwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.idiap.ch/~gatica/publications/NegoescuGatica-book10.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from idiap.ch</span><span class="gs_ggsS">idiap.ch <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.idiap.ch/~gatica/publications/NegoescuGatica-book10.pdf" class=yC11>Internet multimedia search and mining</a></h3><div class="gs_a"><a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>, TS Chua - Internet Multimedia Search and Mining, 2010 - idiap.ch</div><div class="gs_rs">Abstract: We present in this chapter a review of current work that leverages on large online <br>social networks&#39; meta-information, in particular Flickr Groups. We briefly present this hugely <br>successful feature in Flickr and discuss the various ways in which metadata stemming <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5011184445285179392&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 3</a> <a href="/scholar?q=related:AKAv3bNNi0UJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5011184445285179392&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'AKAv3bNNi0UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:AKAv3bNNi0UJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2037666" class=yC13>Correlation-based retrieval for heavily changed near-duplicate videos</a></h3><div class="gs_a">J Liu, Z Huang, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, <a href="/citations?user=IJAU8KoAAAAJ&amp;hl=en&amp;oi=sra">B Cui</a> - ACM Transactions on Information  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract The unprecedented and ever-growing number of Web videos nowadays leads to <br>the massive existence of near-duplicate videos. Very often, some near-duplicate videos <br>exhibit great content changes, while the user perceives little information change, for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3670245332682604582&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 2</a> <a href="/scholar?q=related:JgAVLa9U7zIJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3670245332682604582&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'JgAVLa9U7zIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://vireo.cs.cityu.edu.hk/papers/mm10_cao.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874309" class=yC14>Trajectory-based visualization of web video topics</a></h3><div class="gs_a">J Cao, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, YD Zhang, DM Zhang&hellip; - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract While there have been research efforts in organizing large scale web videos into <br>clusters or topics, efficient browsing of web video topics remains a challenging problem not <br>yet addressed. The related issues include how to efficiently browse and track the evolution <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17341219531384698690&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 2</a> <a href="/scholar?q=related:QjdAPEljqPAJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17341219531384698690&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'QjdAPEljqPAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://www.jdl.ac.cn/doc/2010/11-Fast%20Copy%20Detection%20Based%20on%20Slice%20Entropy%20Scattergraph.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jdl.ac.cn</span><span class="gs_ggsS">jdl.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5583815" class=yC16>Fast copy detection based on Slice Entropy Scattergraph</a></h3><div class="gs_a">P Cui, Z Wu, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, Q Huang - Multimedia and Expo (ICME),  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the exponential growth of digital video resources, huge amount of videos are <br>uploaded onto the Internet. Therefore, the Content Based Copy Detection (CBCD) issue <br>becomes a hot research topic and has been extensively studied recently. However, most <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10557314353816583250&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 2</a> <a href="/scholar?q=related:UrxeQXgdg5IJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10557314353816583250&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'UrxeQXgdg5IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5871572" class=yC18>Near-duplicate video retrieval: Current research and future trends</a></h3><div class="gs_a"><a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, J Liu, Z Huang, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=wLtu3FYAAAAJ&amp;hl=en&amp;oi=sra">W Wang</a> - 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The exponential growth of online videos, along with the increasing user <br>involvements to video-related activities, has been observed as a constant phenomenon <br>during last decade. User&#39;s time spent on video capturing, editing, uploading, searching <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8897475292677729710&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 2</a> <a href="/scholar?q=related:ru3eSaMuensJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8897475292677729710&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ru3eSaMuensJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://vireo.cs.cityu.edu.hk/papers/mm11-pang.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072467" class=yC19>Galaxy browser: exploratory search of web videos</a></h3><div class="gs_a">L Pang, S Tan, HK Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Most search engines return a ranked list of items in response to a query. The list <br>however tells very little about the relationship among items. For videos especially, users <br>often read to spend significant amount of time to navigate the search result. Exploratory <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4299940045316114280&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 2</a> <a href="/scholar?q=related:aEeLbLB0rDsJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4299940045316114280&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'aEeLbLB0rDsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.cmlab.csie.ntu.edu.tw/~garywgl/project/pdf/mm11_wu.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072409" class=yC1B>Scalable mobile video question-answering system with locally aggregated descriptors and random projection</a></h3><div class="gs_a">GL Wu, YC Su, TH Chiu, LC Hsieh&hellip; - Proceedings of the 19th  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract We present a scalable mobile video Question-Answering system with locally <br>aggregated descriptors and random projection using user-generated videos all around the <br>world for ACM Multimedia 2011 Technicolor challenge:&quot; precise event recognition and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9078613266699432060&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 1</a> <a href="/scholar?q=related:fLDEXqq2_X0J:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9078613266699432060&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'fLDEXqq2_X0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/173T18603138278K.pdf" class=yC1D>News shot cloud: ranking tv news shots by cross tv-channel filtering for efficient browsing of large-scale news video archives</a></h3><div class="gs_a">N Katayama, H Mo, <a href="/citations?user=7aEF5cQAAAAJ&amp;hl=en&amp;oi=sra">S Satoh</a> - Advances in Multimedia Modeling, 2011 - Springer</div><div class="gs_rs">TV news programs are important target of multimedia content analysis since they are one of <br>major information sources for ordinary daily lives. Since the computer storage cost has <br>reduced significantly, today we can digitally archive a huge amount of TV news programs. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17351894892534216947&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 1</a> <a href="/scholar?q=related:87hqcXhQzvAJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17351894892534216947&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'87hqcXhQzvAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://vireo.cs.cityu.edu.hk/papers/hktan_icmr11.pdf" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://vireo.cs.cityu.edu.hk/papers/hktan_icmr11.pdf" class=yC1E>Fusing heterogeneous modalities for video and image re-ranking</a></h3><div class="gs_a">H Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">C Ngo</a> - Proc. ICMR, 2011 - vireo.cs.cityu.edu.hk</div><div class="gs_rs">ABSTRACT Multimedia documents in popular image and video sharing websites such as <br>Flickr and Youtube are heterogeneous documents with diverse ways of representations and <br>rich user-supplied information. In this paper, we investigate how the agreement among <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10174193331251927437&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 3</a> <a href="/scholar?q=related:jVlr-e3-MY0J:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10174193331251927437&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'jVlr-e3-MY0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md18', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md18" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jVlr-e3-MY0J:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://pike.psu.edu/publications/hungsik-dissertation.pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://pike.psu.edu/publications/hungsik-dissertation.pdf" class=yC20>High performance record linkage</a></h3><div class="gs_a">H Kim - 2010 - pike.psu.edu</div><div class="gs_rs">Abstract In current world, the immense size of a data set makes problems in finding <br>similar/identitcal data. In addition, the dirtiness of data, ie typos, missing/tilting information, <br>and additional noises usually occurred by careless editing or entry mistakes, makes <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ZEfjFmqTo3kJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8765011383652927332&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ZEfjFmqTo3kJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:ZEfjFmqTo3kJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:ZEfjFmqTo3kJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=7367825452523938012&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://videoalps.net/publications/MMM2011_SchoeBoe_seqidtv.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from videoalps.net</span><span class="gs_ggsS">videoalps.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/AG73581M34777P54.pdf" class=yC22>Video sequence identification in TV broadcasts</a></h3><div class="gs_a"><a href="/citations?user=GOIf9NIAAAAJ&amp;hl=en&amp;oi=sra">K Schoeffmann</a>, <a href="/citations?user=-LFUanwAAAAJ&amp;hl=en&amp;oi=sra">L Boeszoermenyi</a> - Advances in Multimedia Modeling, 2011 - Springer</div><div class="gs_rs">We present a video sequence identification approach that can reliably and quickly detect <br>equal or similar recurrences of a given video sequence in long video streams, eg such as TV <br>broadcasts. The method relies on motion-based video signatures and has low run-time <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:XTL8HjbtnYQJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9556054801053921885&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'XTL8HjbtnYQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6182586" class=yC24>Structure Tensor Series-Based Large Scale Near-Duplicate Video Retrieval</a></h3><div class="gs_a">X Zhou, L Chen, <a href="/citations?user=y6m820wAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a> - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the huge amount of video data and its exponential growth in recent years, <br>many new challenges, like storage, search and navigation, have arisen. Among these <br>challenges, near-duplicate video retrieval aims to find clips that are identical or nearly <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12819870539125636602&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 1</a> <a href="/scholar?q=related:-j0UL1VU6bEJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12819870539125636602&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'-j0UL1VU6bEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6232456" class=yC25>Video Query Reformulation for Near-Duplicate Detection</a></h3><div class="gs_a">C Chiu, S Li, C Hsieh - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present a novel near-duplicate video detection approach based on <br>video query reformulation to expedite the video subsequence search process. The proposed <br>video query reformulation method addresses two key issues:(1) how to efficiently skip <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:HuEJOa985xEJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'HuEJOa985xEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Conference/data/4711a979.pdf" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6298530" class=yC26>Evaluating Gaussian Like Image Representations over Local Features</a></h3><div class="gs_a">YC Su, GL Wu, TH Chiu, <a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a>&hellip; - Multimedia and Expo ( &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Recently, several gaussian like image representations are proposed as an <br>alternative of the bag-of-word representation over local features. These representations are <br>proposed to overcome the quantization error problem faced in bag-of-word representation<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:yZUOUoaTCUMJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4830554280443155913&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'yZUOUoaTCUMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://libque.cityu.edu.hk/bitstream/2031/6211/1/abstract.html" class=yC29><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://libque.cityu.edu.hk/handle/2031/6211" class=yC28>Video hyperlinking for multimedia search</a></h3><div class="gs_a">HK Tan - 2010 - libque.cityu.edu.hk</div><div class="gs_rs">ï»¿ With the spread of Web 2.0, web videos have become prevalent online. There is a growing <br>need for effective modeling and organization of video data to facilitate browsing or retrieval. <br>Meanwhile, the modeling of web pages through hyperlink graph has seen tremendous <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gk0oVGtrZrQJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12999195483169115522&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'gk0oVGtrZrQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md24', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md24" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:gk0oVGtrZrQJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=16418393280767177700&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://vipl.ict.ac.cn/sites/default/files/papers/files/2011_pcm_sqjiang_A%20Rotation%20Invariant%20Descriptor%20for%20Robust%20Video%20Copy%20Detection.pdf" class=yC2B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://vipl.ict.ac.cn/sites/default/files/papers/files/2011_pcm_sqjiang_A%20Rotation%20Invariant%20Descriptor%20for%20Robust%20Video%20Copy%20Detection.pdf" class=yC2A>A Rotation Invariant Descriptor for Robust Video Copy Detection</a></h3><div class="gs_a"><a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, L Su, Q Huang, P Cui, Z Wu - vipl.ict.ac.cn</div><div class="gs_rs">Abstract. A large amount of videos on the Internet are generated from authorized sources by <br>various kinds of transformations. Many works are proposed for robust description of video, <br>which lead to satisfying matching qualities on Content Based Copy Detection (CBCD) <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:pLElac1f7uAJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16207997444790006180&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'pLElac1f7uAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md25', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md25" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pLElac1f7uAJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://vireo.cs.cityu.edu.hk/papers/mm11-tan.pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072331" class=yC2C>Cross media hyperlinking for search topic browsing</a></h3><div class="gs_a">S Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, HK Tan, L Pang - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract With the rapid growth of social media, there are plenty of information sources freely <br>available online for use. Nevertheless, how to synchronize and leverage these diverse forms <br>of information for multimedia applications remains a problem yet to be seriously studied. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:uHuya3a24vIJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17501751721644424120&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'uHuya3a24vIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://vireo.cs.cityu.edu.hk/VIREO-VH/manual.pdf" class=yC2F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://vireo.cs.cityu.edu.hk/VIREO-VH/manual.pdf" class=yC2E>VIREO-VH: Video Hyperlinking</a></h3><div class="gs_a">L Pang, W Zhang, HK Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - vireo.cs.cityu.edu.hk</div><div class="gs_rs">VIREO-VH is an open source software developed by VIREO, which supports threading and <br>visualizing collections of videos. It contains several components including near-duplicate <br>keyframe retrieval, partial near-duplicate localization with time alignment, and galaxy <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:97wldeYQeagJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'97wldeYQeagJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md27', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md27" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:97wldeYQeagJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6069572" class=yC30>Gradient Ordinal Signature and Fixed-Point Embedding for Efficient Near-Duplicate Video Detection</a></h3><div class="gs_a"><a href="/citations?user=JbkbGvEAAAAJ&amp;hl=en&amp;oi=sra">H Liu</a>, H Lu, Z Wen, X Xue - Circuits and Systems for Video  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In order to meet the requirement of large scale real-time near-duplicate video <br>detection, this paper has achieved two goals. First, this paper proposes a more compact <br>local image descriptor which is termed as gradient ordinal signature (GOS). GOS not only <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11152851840314516117&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 1</a> <a href="/scholar?q=related:lSKCjqjjxpoJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11152851840314516117&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'lSKCjqjjxpoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://vipl.ict.ac.cn/sites/default/files/papers/files/2011_ACMMM_tlchen_Detection%20and%20location%20of%20near-duplicate%20video%20sub-clips%20by%20finding%20dense%20subgraphs.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2071967" class=yC31>Detection and location of near-duplicate video sub-clips by finding dense subgraphs</a></h3><div class="gs_a">T Chen, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, L Chu, Q Huang - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Robust and fast near-duplicate video detection is an important task with many <br>potential applications. Most existing systems focus on the comparison between full copy <br>videos or partial near-duplicate videos. While it is more challenging to find similar content <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16854713116784980505&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=37">Cited by 1</a> <a href="/scholar?q=related:GfIVlUf45-kJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16854713116784980505&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'GfIVlUf45-kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6099932" class=yC33>A novel signature for fast video retrieval</a></h3><div class="gs_a">G Haolin, L Bicheng - &hellip;  and Signal Processing (CISP), 2011 4th  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Many video signatures have been used in video copy detection and retrieval, but <br>most of them are extracted from frames of video. So the time cost would become intolerable <br>when the video database become large enough. To implement the fast retrieval or copy <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Ny9fmisO9CMJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Ny9fmisO9CMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://vireo.cs.cityu.edu.hk/papers/mm12-pang.pdf" class=yC35><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://vireo.cs.cityu.edu.hk/papers/mm12-pang.pdf" class=yC34>Video Hyperlinking: Libraries and Tools for Threading and Visualizing Large Video Collection</a></h3><div class="gs_a">L Pang, W Zhang, HK Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - 2012 - vireo.cs.cityu.edu.hk</div><div class="gs_rs">ABSTRACT While HTML documents could be effortlessly hyperlinked by markup tags, <br>creation of the hyperlinks for multimedia objects is by no means easy due to the involvement <br>of various visual processing units and intensive computational overhead. This paper <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:sXGxplSITFYJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'sXGxplSITFYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md31', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md31" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:sXGxplSITFYJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S092523121200687X" class=yC36>Efficient video segment matching for detecting temporal-based video copies</a></h3><div class="gs_a">CY Chiu, TH Tsai, CY Hsieh - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Abstract Content-based video copy detection has grabbed an increasing attention in the <br>video search community due to the rapid proliferation of video copies over the Internet. Most <br>existing techniques of video copy detection focus on spatial-based video transformations <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'i3iqpz7QhmgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/R50713R824145477.pdf" class=yC37>Scene Signatures for Unconstrained News Video Stories</a></h3><div class="gs_a">E Younessian, D Rajan - Advances in Multimedia Modeling, 2012 - Springer</div><div class="gs_rs">We propose a novel video signature called scene signature which is defined as a collection <br>of SIFT descriptors. A scene signature represents the visual cues from a video scene in a <br>compact and comprehensive manner. We detect Near Duplicate Keyframe clusters within <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:-GhismTdaIsJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10045522393409153272&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'-GhismTdaIsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Near-duplicate Video Retrieval: A Survey</h3><div class="gs_a"><a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, J Liu, Z Huang, W Wang</div><div class="gs_fl"><a href="/scholar?q=related:WG5ryY3pef0J:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'WG5ryY3pef0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://200.131.208.43/jspui/bitstream/123456789/2023/1/ARTIGO_EqualDifferentContextual.pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 200.131.208.43</span><span class="gs_ggsS">200.131.208.43 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://200.131.208.43/handle/123456789/2023" class=yC38>Equal but different: a contextual analysis of duplicated videos on YouTube</a></h3><div class="gs_a">TR MagalhÃ£es, <a href="/citations?user=iOnt0iMAAAAJ&amp;hl=en&amp;oi=sra">F Benevenuto</a>, <a href="/citations?user=Wbi6RfAAAAAJ&amp;hl=en&amp;oi=sra">VAF Almeida</a>&hellip; - 2010 - 200.131.208.43</div><div class="gs_rs">Videos have become a predominant part of users&#39; daily lives on theWeb, especially with the <br>emergence of online video sharing systems such as YouTube. Since users can <br>independently share videos in these systems, some videos can be duplicates (ie, identical <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'Nz9B84N0JbUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/93758x/201203/42328806.html" class=yC3A>åºäºå¸§æ°æ®éæ³¢å¨ç¹æ§çåç¼©åè§é¢å¿«éæ£ç´¢æ¹æ³</a></h3><div class="gs_a">é«æ¯«æï¼ æå¼¼ç¨ï¼ å¼ ç½æ - æ¨¡å¼è¯å«ä¸äººå·¥æºè½, 2012 - cqvip.com</div><div class="gs_rs">ä¸ºå®ç°åç¼©åè§é¢å¿«éæ£ç´¢, æåºåºäºå¸§æ°æ®éæ³¢å¨ç¹æ§çæ£ç´¢æ¹æ³. è¯¥æ¹æ³é¦åè®¡ç®åç¼©åå<br>å¾åå¸§çæ°æ®é, å¾åºæ¥è¯¢çæ®µåç®æ è§é¢ç­é¿åçæ°æ®éæ²çº¿, ç¶åå¨, å¸§å¯¹é½çåºç¡ä¸å°æ¥è¯¢<br>çæ®µå¨ç®æ è§é¢ä¸è¿è¡æ»å¨, æ»å¨çªé¿ä¸ºåä¸ªå¾ç»é¿åº¦. åå¨æ¯æ¬¡æ»å¨åè®¡ç®æ¥è¯¢çæ®µä¸ç®æ <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:AVVWbn9XoYgJ:scholar.google.com/&amp;hl=en&amp;num=37&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'AVVWbn9XoYgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
