Total results = 32
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://doc.utwente.nl/36548/1/00938869.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utwente.nl</span><span class="gs_ggsS">utwente.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=938869" class=yC0>Content-based video retrieval by integrating spatio-temporal and stochastic recognition of events</a></h3><div class="gs_a">M Petkovic, W Jonker - Detection and Recognition of Events in  &hellip;, 2001 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract As amounts of publicly available video data grow the need to query this data <br>efficiently becomes significant. Consequently content-based retrieval of video data turns out <br>to be a challenging and important problem. We address the specific aspect of inferring <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10295241744903452261&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 56</a> <a href="/scholar?q=related:ZTa-CdIL4I4J:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10295241744903452261&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 37 versions</a> <a onclick="return gs_ocit(event,'ZTa-CdIL4I4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=997657" class=yC2>Sports video processing for description, summarization and search</a></h3><div class="gs_a">A Ekin - 2004 - dl.acm.org</div><div class="gs_rs">Abstract This thesis proposes solutions for structural and semantic video modeling, <br>automatic video analysis, and expressive video search and retrieval. We present a structural-<br>semantic video model for effective representation of high-and low-level video information, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2942524955742973202&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 35</a> <a href="/scholar?q=related:Ei3Iadvy1SgJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2942524955742973202&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Ei3Iadvy1SgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:Ei3Iadvy1SgJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=5751229820378947480&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.2865&amp;rep=rep1&amp;type=pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1221587" class=yC3>Documenting life: Videography and common sense</a></h3><div class="gs_a">B Barry, G Davenport - Multimedia and Expo, 2003. ICME&#39;03.  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper introduces a model for producing common sense metadata during video <br>capture and describes how this technique can have a positive impact on content capture, <br>representation, and presentation. Metadata entered into the system at the moment of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2202664861777247816&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 25</a> <a href="/scholar?q=related:SL53hQVwkR4J:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2202664861777247816&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'SL53hQVwkR4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://dspace.mit.edu/bitstream/handle/1721.1/32497/61896480.pdf?sequence=1" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mit.edu</span><span class="gs_ggsS">mit.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dspace.mit.edu/handle/1721.1/32497" class=yC5>Mindful documentary</a></h3><div class="gs_a">BA Barry - 2005 - dspace.mit.edu</div><div class="gs_rs">In the practice of documentary creation, a videographer performs an elaborate balancing act <br>between observing the world, deciding what to record, and understanding the implications of <br>the recorded material, all with respect to her primary goal of story construction. This thesis <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7212886524419805303&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 13</a> <a href="/scholar?q=related:d5A-FRRSGWQJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7212886524419805303&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'d5A-FRRSGWQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:d5A-FRRSGWQJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=2174165569323631215&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="ftp://www.irit.fr/pub/IRIT/SIG/IEEEMM09-Laborie-Manzat-Sedes.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from irit.fr</span><span class="gs_ggsS">irit.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5255216" class=yC7>Managing and querying efficiently distributed semantic multimedia metadata collections</a></h3><div class="gs_a"><a href="/citations?user=jnVxp2AAAAAJ&amp;hl=en&amp;oi=sra">S Laborie</a>, A Manzat, F Sedes - 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Currently, many multimedia contents are acquired and stored in real time and on <br>different locations. In order to retrieve efficiently the desired information and to avoid <br>centralizing all metadata, we propose to compute a centralized metadata resume, ie, a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=178630461050860268&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 9</a> <a href="/scholar?q=related:7DIAOXWfegIJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=178630461050860268&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 26 versions</a> <a onclick="return gs_ocit(event,'7DIAOXWfegIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.3.8164&amp;rep=rep1&amp;type=pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=957152" class=yC9>The mindful camera: common sense for documentary videography</a></h3><div class="gs_a">B Barry - Proceedings of the eleventh ACM international  &hellip;, 2003 - dl.acm.org</div><div class="gs_rs">Abstract Cameras with story understanding can help videographers reflect on their process <br>of content capture during documentary construction. This paper describes a set of tools that <br>use common sense knowledge to support documentary videography.</div><div class="gs_fl"><a href="/scholar?cites=4663289047428159892&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 10</a> <a href="/scholar?q=related:lNnsdK5Ut0AJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4663289047428159892&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'lNnsdK5Ut0AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.9383&amp;rep=rep1&amp;type=pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.9383&amp;rep=rep1&amp;type=pdf" class=yCB>Adaptive Video Summarization</a></h3><div class="gs_a">P Mulhem, J Gensel, <a href="/citations?user=QwWiE6AAAAAJ&amp;hl=en&amp;oi=sra">H Martin</a> - The Handbook of Video Databases  &hellip;, 2003 - Citeseer</div><div class="gs_rs">One of the specific characteristics of the video medium is to be a temporal medium: it has an <br>inherent duration and the time spent to find information present in a video depends <br>somehow on its duration. Without any knowledge about the video, it is necessary to use <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11973387459082960438&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 7</a> <a href="/scholar?q=related:Nr7B0HQEKqYJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11973387459082960438&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'Nr7B0HQEKqYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md6', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md6" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Nr7B0HQEKqYJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.2794&amp;rep=rep1&amp;type=pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=GcO4HGbMi7UC&amp;oi=fnd&amp;pg=PA114&amp;ots=0oeM1OKGae&amp;sig=LvOcRv0u_U-_FS2jEhMqtf45lKY" class=yCD>Video content-based retrieval techniques</a></h3><div class="gs_a">WE Farag, H Abdel-Wahab - Multimedia systems and content- &hellip;, 2004 - books.google.com</div><div class="gs_rs">ABSTRACT The increasing use of multimedia streams nowadays necessitates the <br>development of efficient and effective methodologies and systems for manipulating <br>databases storing these streams. These systems have various areas of application such <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=412812563533045368&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 7</a> <a href="/scholar?q=related:eJ404NaaugUJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=412812563533045368&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'eJ404NaaugUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://paper.ijcsns.org/07_book/200602/200602A02.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ijcsns.org</span><span class="gs_ggsS">ijcsns.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://paper.ijcsns.org/07_book/200602/200602A02.pdf" class=yCF>Video semantic models: survey and evaluation</a></h3><div class="gs_a"><a href="/citations?user=xzGT3fUAAAAJ&amp;hl=en&amp;oi=sra">Y Wang</a>, CX Xing, L Zhou - Int. J. Comput. Sci. Netw. Security, 2006 - paper.ijcsns.org</div><div class="gs_rs">Summary With the development of video technology and appearance of new video-related <br>applications, the amount of video data has increased dramatically which demands support <br>in semantic models to facilitate information representation and query. The video semantic <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8555520997504261476&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 6</a> <a href="/scholar?q=related:ZB2jpARRu3YJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8555520997504261476&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ZB2jpARRu3YJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:ZB2jpARRu3YJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://cjc.ict.ac.cn/quanwenjiansuo/2007-03/wy.zip" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cjc.ict.ac.cn/quanwenjiansuo/2007-03/wy.zip" class=yC11>è§é¢è¯­ä¹æ¨¡ååè¯ä»·åå</a></h3><div class="gs_a">ççï¼ å¨ç«æ±ï¼ é¢æ¥æ - è®¡ç®æºå­¦æ¥, 2007 - cjc.ict.ac.cn</div><div class="gs_rs">æè¦è§é¢è¯­ä¹æ¨¡åçç®çæ¯è¡¨ç¤ºåç®¡çè§é¢ä¸­åå«çå¯¹è±¡, äºä»¶åå³ç³»ç­è¯­ä¹ä¿¡æ¯, <br>å¹¶æä¾å®ç°è¯­ä¹æ¥è¯¢çåºç¡. éçè§é¢ææ¯åä¸è§é¢ç¸å³çåºç¨çåå±, å¯¹ææçè§é¢è¯­ä¹æ¨¡å<br>çè¦æ±è¶æ¥è¶è¿«å. æç« å¯¹ç°æçè§é¢è¯­ä¹æ¨¡åè¿è¡äºå¨é¢çç»¼è¿°, å±åå«16 ç§è§é¢è¯­ä¹æ¨¡å: <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6908404918117395941&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 10</a> <a href="/scholar?q=related:5e3ytKuV318J:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6908404918117395941&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'5e3ytKuV318J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:5e3ytKuV318J:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/PHT5L5K3423M6V8X.pdf" class=yC13>An evaluation method for video semantic models</a></h3><div class="gs_a"><a href="/citations?user=xzGT3fUAAAAJ&amp;hl=en&amp;oi=sra">Y Wang</a>, L Zhou, C Xing - Advances in Multimedia Information Systems, 2005 - Springer</div><div class="gs_rs">The development of video technology and video-related applications demands strong <br>support in semantic data models. To meet such a requirement, many video semantic data <br>models have been proposed. The semantic model plays a key role in providing query <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7772410393317848203&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 4</a> <a href="/scholar?q=related:i7RDnhIm3WsJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3F/63/RN174753100.html?source=googlescholar" class="gs_nph" class=yC14>BL Direct</a> <a href="/scholar?cluster=7772410393317848203&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'i7RDnhIm3WsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://lms.comp.nus.edu.sg/papers/media/2003/cgi03-zhaoyl.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1214462" class=yC15>Automatic tracking of face sequences in MPEG video</a></h3><div class="gs_a">Y Zhao, TS Chua - Computer Graphics International, 2003.  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Human faces are commonly found in video streams and provide useful information <br>for video content analysis. We present a robust face tracking system to extract multiple face <br>sequences from MPEG video without human intervention. Specifically, a view-based DCT-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5933369710207850926&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 4</a> <a href="/scholar?q=related:rlkAqlCQV1IJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5933369710207850926&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'rlkAqlCQV1IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.csa.com/partners/viewrecord.php?requester=gs&amp;collection=TRD&amp;recid=20070990111070CI" class=yC17>Video semantic models and their evaluation criteria.</a></h3><div class="gs_a"><a href="/citations?user=xzGT3fUAAAAJ&amp;hl=en&amp;oi=sra">Y Wang</a>, LZ Zhou, CX Xing - Jisuanji Xuebao/Chinese Journal of  &hellip;, 2007 - csa.com</div><div class="gs_rs">The development of video technology and video-related applications demands strong <br>support in semantic data models. To meet such a requirement, many video semantic data <br>models have been proposed. The semantic model plays a key role in providing query <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11251956675400602086&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 3</a> <a href="/scholar?q=related:5pFWGfv6JpwJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/18/14/RN208591147.html?source=googlescholar" class="gs_nph" class=yC18>BL Direct</a> <a onclick="return gs_ocit(event,'5pFWGfv6JpwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://www-mrim.imag.fr/publications/2003/PM001/video-annot.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from imag.fr</span><span class="gs_ggsS">imag.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-mrim.imag.fr/publications/2003/PM001/video-annot.pdf" class=yC19>Semantic video annotation and vague query</a></h3><div class="gs_a">Q Zhang, MS Kankanhalli, P Mulhem - Proc. 9th International  &hellip;, 2003 - www-mrim.imag.fr</div><div class="gs_rs">The Digital Video Album (DVA) system described here integrates various cooperating <br>subsystems to index and query video documents according to their semantic content and <br>other metadata. A simple structured model is proposed to represent the video content. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9538509705575055652&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 3</a> <a href="/scholar?q=related:JMFqPwqYX4QJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9538509705575055652&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'JMFqPwqYX4QJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md13', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md13" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:JMFqPwqYX4QJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://mrim.imag.fr/publications/2003/PM001/mulhem03bISI.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from imag.fr</span><span class="gs_ggsS">imag.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mrim.imag.fr/publications/2003/PM001/mulhem03bISI.pdf" class=yC1B>ModÃ¨les pour rÃ©sumÃ©s adaptatifs de vidÃ©os</a></h3><div class="gs_a">P Mulhem, J Gensel, <a href="/citations?user=QwWiE6AAAAAJ&amp;hl=en&amp;oi=sra">H Martin</a> - INGENIERIE DES SYSTEMS D  &hellip;, 2002 - mrim.imag.fr</div><div class="gs_rs">RÃSUMÃ. La vidÃ©o est un mÃ©dia qui pose des problÃ¨mes complexes en raison du volume <br>important de donnÃ©es Ã  traiter et de la difficultÃ© de reprÃ©senter et d&#39;extraire des informations <br>de son contenu. Nous proposons d&#39;annoter le contenu d&#39;une vidÃ©o Ã  l&#39;aide de Graphes <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1550349735689949545&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 5</a> <a href="/scholar?q=related:aZFr2d7ygxUJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/22/0B/RN130434150.html?source=googlescholar" class="gs_nph" class=yC1D>BL Direct</a> <a href="/scholar?cluster=1550349735689949545&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'aZFr2d7ygxUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md14', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md14" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:aZFr2d7ygxUJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://nsm1.nsm.iup.edu/farag/Research/Publications/technical-reports/indexing-tr.pdf" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iup.edu</span><span class="gs_ggsS">iup.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://nsm1.nsm.iup.edu/farag/Research/Publications/technical-reports/indexing-tr.pdf" class=yC1E>Techniques for indexing MPEG compressed videos</a></h3><div class="gs_a">W Farag, H Abdel-Wahab - 2002 - nsm1.nsm.iup.edu</div><div class="gs_rs">Abstract The increasing use of multimedia streams nowadays calls for the development of <br>efficient and effective methodologies for manipulating databases storing these streams. <br>Moreover, content-based access to multimedia databases requires effective indexing <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1454356135361892042&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 2</a> <a href="/scholar?q=related:ys4nGTPpLhQJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1454356135361892042&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ys4nGTPpLhQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md15', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md15" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:ys4nGTPpLhQJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://pubs.media.mit.edu/pubs/papers/CSVideoBarryGid.pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mit.edu</span><span class="gs_ggsS">mit.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://pubs.media.mit.edu/pubs/papers/CSVideoBarryGid.pdf" class=yC20>Why Common Sense For Video Production?</a></h3><div class="gs_a">B Barry, G Davenport - 2002 - pubs.media.mit.edu</div><div class="gs_rs">ABSTRACT Video cameras are becoming cheap, small and ubiquitous. With advances in <br>memory, cameras will increasingly be designed to be always ready, always recording. When <br>cameras are always ready, how will videographersâprofessional and/or amateur--decide <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6358217485050053031&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 2</a> <a href="/scholar?q=related:pwEWiR3tPFgJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6358217485050053031&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'pwEWiR3tPFgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md16', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md16" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pwEWiR3tPFgJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://hal.archives-ouvertes.fr/docs/00/38/99/05/PDF/INFORSID09-Laborie-Manzat-Sedes.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/hal-00389905/" class=yC22>CrÃ©ation et utilisation d&#39;un rÃ©sumÃ© de mÃ©tadonnÃ©es pour interroger efficacement des collections multimÃ©dias distribuÃ©es</a></h3><div class="gs_a"><a href="/citations?user=jnVxp2AAAAAJ&amp;hl=en&amp;oi=sra">S Laborie</a>, AM Manzat, F SÃ¨des - Actes du 27Ã¨me CongrÃ¨s &hellip;, 2009 - hal.archives-ouvertes.fr</div><div class="gs_rs">RÃSUMÃ. Actuellement, de nombreux contenus multimÃ©dias sont crÃ©Ã©s Ã  partir de plusieurs <br>sources et stockÃ©s dans des environnements distribuÃ©s. Pour Ã©viter de centraliser <br>l&#39;ensemble des mÃ©tadonnÃ©es d&#39;un systÃ¨me et rÃ©pondre efficacement Ã  une requÃªte d&#39;un <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6362590563727655156&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 3</a> <a href="/scholar?q=related:9EA9Qmh2TFgJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6362590563727655156&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 23 versions</a> <a onclick="return gs_ocit(event,'9EA9Qmh2TFgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://www.ccf.org.cn/resources/1190201776262/2010/04/19/2005/2005%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%BA%8C%E5%B1%8A%E4%B8%AD%E5%9B%BD%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/h050084055.pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ccf.org.cn</span><span class="gs_ggsS">ccf.org.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ccf.org.cn/resources/1190201776262/2010/04/19/2005/2005%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%BA%8C%E5%B1%8A%E4%B8%AD%E5%9B%BD%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/h050084055.pdf" class=yC24>åºäº DHT å¯¹ç­ç½ç»çè½¯ä»¶æä»¶å±äº«ç³»ç»ç ç©¶</a></h3><div class="gs_a">éå¾·åï¼ é»æèï¼ ä¹åé¦ï¼ èå­å¡ - è®¡ç®æºç§å­¦, 2005 - ccf.org.cn</div><div class="gs_rs">Abstract This paper proposes an original reusable component sharing system based on <br>peer-to-peer (P2P) networks. It is unnecessary to design a dedicated server for storing, <br>manipulating and retrieving reusable components in our component sharing system <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8785008424474463566&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 2</a> <a href="/scholar?q=related:Tk2d-Z2e6nkJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8785008424474463566&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'Tk2d-Z2e6nkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://www.computing.edu.au/~phung/wiki_new/uploads/Main/phung_phd05.pdf" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from computing.edu.au</span><span class="gs_ggsS">computing.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.computing.edu.au/~phung/wiki_new/uploads/Main/phung_phd05.pdf" class=yC26>Probabilistic and Film Grammar Based Methods for Video Content Understanding</a></h3><div class="gs_a"><a href="/citations?user=OtA9SwIAAAAJ&amp;hl=en&amp;oi=sra">DQ Phung</a> - 2005 - computing.edu.au</div><div class="gs_rs">The fast growing advances in electronics, computer, and communication technologies have <br>greatly transformed our lives in many ways. Possibly, one of the most significant outcomes is <br>the emergence of the field of &#39;multimedia&#39;âwhich, perhaps an unknown term 50 years ago, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1908865813674696631&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 1</a> <a href="/scholar?q=related:t5_3C1infRoJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'t5_3C1infRoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:t5_3C1infRoJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:t5_3C1infRoJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=1721789611133110712&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://www.irit.fr/GDR-I3/fichiers/assises2002/papers/11-RechercheDInformationMultimedia.pdf" class=yC29><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from irit.fr</span><span class="gs_ggsS">irit.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.irit.fr/GDR-I3/fichiers/assises2002/papers/11-RechercheDInformationMultimedia.pdf" class=yC28>Recherche d&#39;information multimÃ©dia</a></h3><div class="gs_a">F SÃ¨des, <a href="/citations?user=QwWiE6AAAAAJ&amp;hl=en&amp;oi=sra">H Martin</a> - 2002 - irit.fr</div><div class="gs_rs">Le multimÃ©dia permet de combiner des donnÃ©es de diffÃ©rents types (texte, image, audio, <br>vidÃ©o) Ã  l&#39;intÃ©rieur d&#39;un mÃªme document numÃ©rique. De nombreux logiciels permettent de <br>rÃ©aliser de tels documents ou hyperdocuments de faÃ§on ad hoc. NÃ©anmoins, pour <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3087534796947009578&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=32">Cited by 1</a> <a href="/scholar?q=related:KjCvE4og2SoJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3087534796947009578&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'KjCvE4og2SoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md20', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md20" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:KjCvE4og2SoJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/XW6NKF6CECWYY3WD.pdf" class=yC2A>A multimedia database system using dependence weight values for a mobile environment</a></h3><div class="gs_a">K Lee, H Kim, K Lee - Computational Science and Its ApplicationsâICCSA  &hellip;, 2005 - Springer</div><div class="gs_rs">This paper proposes a semantic-based video retrieval system that supports semantic-based <br>retrieval of large-capacity video data in a mobile environment. The proposed system <br>automatically extracts content information from video data and retrieves video data using <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:CzA-QPTVyiMJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2579108981649584139&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'CzA-QPTVyiMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5555826" class=yC2B>A novel commercial break detection and automatic annotation of TV programs for content based retrieval</a></h3><div class="gs_a">N Venkatesh, M Girish Chandra&hellip; - &hellip;  Systems (ICSPS), 2010  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present a novel approach for automatic annotation and content <br>based video retrieval by making use of the features extracted during the process of detecting <br>commercial boundaries in a recorded Television (TV) program. In our approach, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:djPtVfktHBsJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'djPtVfktHBsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="ftp://ftp.math.utah.edu/public_html/pub/tex/bib/ieeemultimedia.pdf.gz" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utah.edu</span><span class="gs_ggsS">utah.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="ftp://ftp.math.utah.edu/public_html/pub/tex/bib/ieeemultimedia.pdf.gz" class=yC2C>A Complete Bibliography of Publications in IEEE MultiMedia</a></h3><div class="gs_a">NHF Beebe - 2004 - math.utah.edu</div><div class="gs_rs">[ABCB02] JÃ¼rgen Assfalg, Marco Bertini, Carlo Colombo, and Alberto Del Bimbo. Semantic <br>annotation of sports videos. IEEE MultiMedia, 9 (2): 52â60, April/June 2002. CODEN <br>IEMUE4. ISSN 1070-986X (print), 1941-0166 (electronic). URL http://csdl. computer. org/<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:F7shjunZApkJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11025614434891578135&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'F7shjunZApkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md23', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md23" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:F7shjunZApkJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/Y67410168VJ44648.pdf" class=yC2E>Educational information system using two phase feature extract in u-learning environment</a></h3><div class="gs_a">K Lee, KH Lee, JC Park - Advances in Multimedia Modeling, 2006 - Springer</div><div class="gs_rs">This paper proposes an educational information system based on u-learning that supports <br>two phase feature extract. A key frame selected by user in annotation-based retrieval takes <br>query image of feature-based retrieval and by agent searches and displays the most <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:O8cPudTCAq0J:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/58/56/RN201123148.html?source=googlescholar" class="gs_nph" class=yC2F>BL Direct</a> <a href="/scholar?cluster=12466740937408628539&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'O8cPudTCAq0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/M545703732422270.pdf" class=yC30>Commercial Break Detection and Content Based Video Retrieval</a></h3><div class="gs_a">N Venkatesh, MG Chandra - Machine Learning and Systems Engineering, 2010 - Springer</div><div class="gs_rs">This chapter presents a novel approach for automatic annotation and content based video <br>retrieval by making use of the features extracted during the process of detecting commercial <br>boundaries in a recorded Television (TV) program. In this approach, commercial <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Tv2Q1r9Ex1EJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Tv2Q1r9Ex1EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/13633/Zhao_Yunlong_PhD_thesis.pdf?sequence=1" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/13633" class=yC31>Automatic extraction and tracking of face sequences in MPEG video</a></h3><div class="gs_a">Z Yunlong - 2004 - scholarbank.nus.edu</div><div class="gs_rs">This PhD work focuses on the problem of extracting multiple face sequences from MPEG <br>video based on face detection and tracking. It aims to facilitate the strata-based digital video <br>modelling to achieve efficient video retrieval and browsing. The research includes the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:LWH47rNLYawJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12421292483445023021&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'LWH47rNLYawJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://articles.ircam.fr/textes/Delezoide06c/index.pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ircam.fr</span><span class="gs_ggsS">ircam.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://articles.ircam.fr/textes/Delezoide06c/index.pdf" class=yC33>ModÃ¨les d&#39;indexation multimedia pour la description automatique de films de cinÃ©ma</a></h3><div class="gs_a">ÃD EDITE - 2006 - articles.ircam.fr</div><div class="gs_rs">Depuis une quinzaine d&#39;annÃ©e, l&#39;analyse automatique des films de cinÃ©ma se focalise sur la <br>description de l&#39;Â«histoireÂ» transmise par le support cinÃ©matographique. La recherche et <br>l&#39;estimation de cette information constituent un travail d&#39;indexation de donnÃ©es. Il existe <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:t3i4JTD3HWEJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'t3i4JTD3HWEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md27', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md27" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:t3i4JTD3HWEJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> UNIVERSITE JOSEPH FOURIERâGRENOBLE</h3><div class="gs_a">J GENSEL</div><div class="gs_fl"><a href="/scholar?q=related:ds06M-isvBkJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1854547259861683574&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'ds06M-isvBkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.dbpia.co.kr/view/ar_view.asp?arid=951211" class=yC35>PVR ìì¤íìì ë©íë°ì´í° ê¸°ë°ì í¨ê³¼ì ì¸ ë¸ë¼ì°ì§ ë°©ë²</a></h3><div class="gs_a">ê¹ì¬ëª© - íêµ­ì ë³´ê³¼íí 2005 ê°ì íì ë°í ë¬¸ì§ (â¡) ì  32, 2005 - dbpia.co.kr</div><div class="gs_rs">ëì§í¸ ê¸°ì ì´ ëë ì´ ë°ì í¨ì ë°ë¼ ì ì§ìì ìì¶ ê¸°ì ì ëíì ì´ë¼ í  ì ìë JPEG ì´ <br>íìíë©´ì ëì§í¸ ììì ê¸ê²©í ë°ì ì ê±°ë­í´ ìë¤. ì´ì ëìì ìì¶ ê¸°ì  MPEG ì´ <br>íìíìê³ , MPEG ì 1, 2, 4 ë¥¼ ë¹ë¡¯íì¬ 7, 21 ë¡ ë°ì ì ê±°ë­íê³  ìë¤. PVR ì ë°©ì¡ì <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:yWkVWcaYwwkJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'yWkVWcaYwwkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://stephane.ayache.perso.esil.univmed.fr/these.pdf" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from univmed.fr</span><span class="gs_ggsS">univmed.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://stephane.ayache.perso.esil.univmed.fr/these.pdf" class=yC36>StÃ©phane Ayache</a></h3><div class="gs_a">A Lux, <a href="/citations?user=rFaxB20AAAAJ&amp;hl=en&amp;oi=sra">P Gallinari</a>, P Joly, G QuÃ©not - stephane.ayache.perso.esil.univmed &hellip;</div><div class="gs_rs">This work deals with information retrieval and aims to reach semantic indexing of multimedia <br>documents. The state of the art approach tackle this problem by bridging of the semantic gap <br>between low-level features, from each modality, and high-level features (concepts), which <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:W_wKljMSU3IJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8237948156160703579&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'W_wKljMSU3IJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md30', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md30" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:W_wKljMSU3IJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://algo.informatik.uni-freiburg.de/mitarbeiter/trahasch/projects/docs/diplomRechert.pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-freiburg.de</span><span class="gs_ggsS">uni-freiburg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://algo.informatik.uni-freiburg.de/mitarbeiter/trahasch/projects/docs/diplomRechert.pdf" class=yC38>Eine inhaltsbasierte Dokumentstruktur f ur Vorlesungsaufzeichnungen</a></h3><div class="gs_a">K Rechert - 2004 - algo.informatik.uni-freiburg.de</div><div class="gs_rs">Zuletzt besteht noch die MÃ¶glichkeit, dass Gruppen von atomaren Elementen in Containern <br>zusammengefasst worden sind. Existiert ein Container, der ausschlieÃlich Beispiele enthÃ¤lt, <br>so kann dieser direkt angewÃ¤hlt werden (Abb. 2.4).</div><div class="gs_fl"><a href="/scholar?q=related:Uuli6hUeq8UJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14243511325964822866&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'Uuli6hUeq8UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md31', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md31" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Uuli6hUeq8UJ:scholar.google.com/&amp;hl=en&amp;num=32&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
