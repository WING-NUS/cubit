Total results = 11
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://hp-thesis.googlecode.com/svn/trunk/IR/A%20Lattice-Based%20Approach%20to%20Query-by-Example%20Spoken%20Document%20Retrieval.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from googlecode.com</span><span class="gs_ggsS">googlecode.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1390397" class=yC0>A lattice-based approach to query-by-example spoken document retrieval</a></h3><div class="gs_a">TK Chia, <a href="/citations?user=jnU62sUAAAAJ&amp;hl=en&amp;oi=sra">KC Sim</a>, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, <a href="/citations?user=FABZCeAAAAAJ&amp;hl=en&amp;oi=sra">HT Ng</a> - &hellip;  of the 31st annual international ACM  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Recent efforts on the task of spoken document retrieval (SDR) have made use of <br>speech lattices: speech lattices contain information about alternative speech transcription <br>hypotheses other than the 1-best transcripts, and this information can improve retrieval <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11206543562001310196&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 28</a> <a href="/scholar?q=related:9GEHrf6jhZsJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11206543562001310196&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'9GEHrf6jhZsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.hlt.utdallas.edu/~feiliu/papers/ICASSP_2010.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utdallas.edu</span><span class="gs_ggsS">utdallas.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5494972" class=yC2>Using n-best recognition output for extractive summarization and keyword extraction in meeting speech</a></h3><div class="gs_a"><a href="/citations?user=g4uO4CgAAAAJ&amp;hl=en&amp;oi=sra">Y Liu</a>, <a href="/citations?user=jl89ugsAAAAJ&amp;hl=en&amp;oi=sra">S Xie</a>, <a href="/citations?user=22ohn6AAAAAJ&amp;hl=en&amp;oi=sra">F Liu</a> - Acoustics Speech and Signal Processing ( &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract There has been increasing interest recently in meeting understanding, such as <br>summarization, browsing, action item detection, and topic segmentation. However, there is <br>very limited effort on using rich recognition output (eg, recognition confidence measure or <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16298536225538162824&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 11</a> <a href="/scholar?q=related:iMAJAlgIMOIJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16298536225538162824&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'iMAJAlgIMOIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://cmsassets.comp.nus.edu.sg/~nght/pubs/tois10.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1658379" class=yC4>Statistical lattice-based spoken document retrieval</a></h3><div class="gs_a">TK Chia, <a href="/citations?user=jnU62sUAAAAJ&amp;hl=en&amp;oi=sra">KC Sim</a>, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, <a href="/citations?user=FABZCeAAAAAJ&amp;hl=en&amp;oi=sra">HT Ng</a> - ACM Transactions on Information  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Recent research efforts on spoken document retrieval have tried to overcome the <br>low quality of 1-best automatic speech recognition transcripts, especially in the case of <br>conversational speech, by using statistics derived from speech lattices containing multiple <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16713900219984546253&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 9</a> <a href="/scholar?q=related:zYmAL7Kz8-cJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16713900219984546253&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'zYmAL7Kz8-cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.1037&amp;rep=rep1&amp;type=pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.1037&amp;rep=rep1&amp;type=pdf" class=yC6>Automatic sentence structure annotation for spoken language processing</a></h3><div class="gs_a"><a href="/citations?user=4YEhqioAAAAJ&amp;hl=en&amp;oi=sra">DL Hillard</a> - 2008 - Citeseer</div><div class="gs_rs">Increasing Internet connectivity across the world is influencing the way that people and <br>businesses accomplish their goals. Simple tasks of daily life, such as finding a movie time, <br>looking up a weather forecast, or getting directions to a location, can now be easily <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6449469591157113665&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 7</a> <a href="/scholar?q=related:QYOaImwegVkJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6449469591157113665&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'QYOaImwegVkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:QYOaImwegVkJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:QYOaImwegVkJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=18065660521103860974&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://cl.naist.jp/~junta-m/slt2008_thesis.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from naist.jp</span><span class="gs_ggsS">naist.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4777899" class=yC8>A similar content retrieval method for podcast episodes</a></h3><div class="gs_a">J Mizuno, J Ogata, <a href="/citations?user=rgYcNmoAAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Spoken Language Technology  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Given podcasts (audio blogs) that are sets of speech files called episodes, this <br>paper describes a method for retrieving episodes that have similar content. Although most <br>previous retrieval methods were based on bibliographic information, tags, or users&#39; <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9046081681620002579&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 4</a> <a href="/scholar?q=related:E-cgelwjin0J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9046081681620002579&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'E-cgelwjin0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://sites.google.com/site/shajithikbal2/icme08hmmBasedEventDetection.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607730" class=yCA>HMM based event detection in audio conversation</a></h3><div class="gs_a">S Ikbal, T Faruquie - Multimedia and Expo, 2008 IEEE  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we address the problem of detecting sensitive events in speech signal <br>such as exchange of credit card information. Although close in nature to the word spotting <br>problem, variability in the linguistic content constituting an event and their composition <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15292799948373488144&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 2</a> <a href="/scholar?q=related:EPLpP4fwOtQJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15292799948373488144&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'EPLpP4fwOtQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5371545" class=yCC>A new syllable-lattice based approach for Mandarin spoken document retrieval</a></h3><div class="gs_a"><a href="/citations?user=Zj32-qAAAAAJ&amp;hl=en&amp;oi=sra">L Zhang</a>, Y Gao, X Xiang, D Lu - Wireless Communications &amp;  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In our Mandarin spoken document retrieval system, the effects of both retrieval <br>source and retrieval model are considered. For the retrieval source, the syllable-lattice is <br>adopted which can ameliorate the effect of speech recognition error on document retrieval<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7853090661145219544&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 2</a> <a href="/scholar?q=related:2JF_TlnI-2wJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'2JF_TlnI-2wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://eprints2008.lib.hokudai.ac.jp/dspace/bitstream/2115/39702/1/TA-SS1-1.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hokudai.ac.jp</span><span class="gs_ggsS">hokudai.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://eprints2008.lib.hokudai.ac.jp/dspace/handle/2115/39702" class=yCD>Query-by-Example Spoken Document Retrieval: The Star Challenge 2008</a></h3><div class="gs_a"><a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, <a href="/citations?user=jnU62sUAAAAJ&amp;hl=en&amp;oi=sra">KC Sim</a>, V Singh, KM Lye - &hellip; : APSIPA ASC 2009 &hellip;, 2009 - eprints2008.lib.hokudai.ac.jp</div><div class="gs_rs">In this paper, we give an update of recent research activities in HLT department of I2R in <br>query-by-example spoken document retrieval (SDR) and report an evaluation campaign, the <br>Star Challenge 2008, which was organized by A* STAR, Singapore. It is suggested that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16360638948118022001&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 1</a> <a href="/scholar?q=related:ccf-4HCqDOMJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16360638948118022001&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ccf-4HCqDOMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.spcom.ecei.tohoku.ac.jp/papers/interspeech2011/masumura2.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tohoku.ac.jp</span><span class="gs_ggsS">tohoku.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.spcom.ecei.tohoku.ac.jp/papers/interspeech2011/masumura2.pdf" class=yCF>Language model expansion using webdata for spoken document retrieval</a></h3><div class="gs_a">R Masumura, <a href="/citations?user=nmwBoMIAAAAJ&amp;hl=en&amp;oi=sra">S Hahm</a>, <a href="/citations?user=VzgIGGgAAAAJ&amp;hl=en&amp;oi=sra">A Ito</a> - Twelfth Annual Conference  &hellip;, 2011 - spcom.ecei.tohoku.ac.jp</div><div class="gs_rs">Abstract In recent years, there has been increasing demand for ad hoc retrieval of spoken <br>documents. We can use existing text retrieval methods by transcribing spoken documents <br>into text data using a Large Vocabulary Continuous Speech Recognizer (LVCSR). <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:SQ0aFg75BV4J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6775095053344443721&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'SQ0aFg75BV4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5284020" class=yC11>Protecting Sensitive Customer Information in Call Center Recordings</a></h3><div class="gs_a">TA Faruquie, S Negi&hellip; - &hellip;  Computing, 2009. SCC&#39; &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Protecting sensitive information while preserving the share ability and usability of <br>data is becoming increasingly important in the outsourced business process industry. <br>Particularly in the context of call-centers a lot of customer related sensitive information is <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:5jAZJzt7GNMJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15211043235433099494&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'5jAZJzt7GNMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.searchingspeech.org/sscs2008/sscs08_proceedings.pdf#page=6" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from searchingspeech.org</span><span class="gs_ggsS">searchingspeech.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.searchingspeech.org/sscs2008/sscs08_proceedings.pdf#page=6" class=yC12>Query-by-Example Spoken Document Retrieval</a></h3><div class="gs_a"><a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - CIP GEGEVENS KONINKLIJKE BIBLIOTHEEK, DEN  &hellip;, 2008 - searchingspeech.org</div><div class="gs_rs">Query-by-Example Spoken Document Retrieval Haizhou Li Institute for Infocomm Research <br>(I2R) Agency for Science, Technology and Research (A* STAR), Singapore. hli@ i2r. a-star. <br>edu. sg 1. INTRODUCTION In this presentation, we gave an overview of ongoing <b> ...</b> </div><div class="gs_fl"><a href="/scholar?q=related:kfYvZLN7uOgJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16769289222924269201&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'kfYvZLN7uOgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:kfYvZLN7uOgJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
