Total results = 15
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://eprints.pascal-network.org/archive/00006338/01/lampert-cvpr2009.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from pascal-network.org</span><span class="gs_ggsS">pascal-network.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5206594" class=yC0>Learning to detect unseen object classes by between-class attribute transfer</a></h3><div class="gs_a"><a href="/citations?user=iCf3SwgAAAAJ&amp;hl=en&amp;oi=sra">CH Lampert</a>, <a href="/citations?user=AQ_I-NkAAAAJ&amp;hl=en&amp;oi=sra">H Nickisch</a>&hellip; - Computer Vision and  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We study the problem of object classification when training and test classes are <br>disjoint, ie no training examples of the target classes are available. This setup has hardly <br>been studied in computer vision research, but it is the rule rather than the exception, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2013890885131954434&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 220</a> <a href="/scholar?q=related:AumvCBfH8hsJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2013890885131954434&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 29 versions</a> <a onclick="return gs_ocit(event,'AumvCBfH8hsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://dbm.neuro.uni-jena.de/pdf-files/Franke-NI10.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-jena.de</span><span class="gs_ggsS">uni-jena.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1053811910000108" class=yC2>Estimating the age of healthy subjects from T&lt; sub&gt; 1&lt;/sub&gt;-weighted MRI scans using kernel methods: Exploring the influence of various parameters</a></h3><div class="gs_a">K Franke, G Ziegler, S KlÃ¶ppel, <a href="/citations?user=wHcxxX0AAAAJ&amp;hl=en&amp;oi=sra">C Gaser</a> - Neuroimage, 2010 - Elsevier</div><div class="gs_rs">The early identification of brain anatomy deviating from the normal pattern of growth and <br>atrophy, such as in Alzheimer&#39;s disease (AD), has the potential to improve clinical outcomes <br>through early intervention. Recently, Davatzikos et al.(2009) supported the hypothesis that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6256411223653046953&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 30</a> <a href="/scholar?q=related:qVqbKtw801YJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6256411223653046953&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'qVqbKtw801YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://mmir.doc.ic.ac.uk/www-pub/j.magalhaes-phd.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ic.ac.uk</span><span class="gs_ggsS">ic.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mmir.doc.ic.ac.uk/www-pub/j.magalhaes-phd.pdf" class=yC4>Statistical models for semantic-multimedia information retrieval</a></h3><div class="gs_a"><a href="/citations?user=P6fBAXMAAAAJ&amp;hl=en&amp;oi=sra">J MagalhÃ£es</a> - PhD PhD Thesis, Department of Computing,  &hellip;, 2008 - mmir.doc.ic.ac.uk</div><div class="gs_rs">Abstract This thesis addresses the problem of improving multimedia information retrieval by <br>exploring semantic-multimedia analysis. For this goal we researched two complementary <br>search paradigms:(1) search-by-keyword and (2) search-by-semantic-example. Search-by<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6383288989930033682&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 5</a> <a href="/scholar?q=related:EhJxJ4T_lVgJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6383288989930033682&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'EhJxJ4T_lVgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md2', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md2" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:EhJxJ4T_lVgJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874062" class=yC6>Portfolio theory of multimedia fusion</a></h3><div class="gs_a">X Wang, M Kankanhalli - Proceedings of the international conference on &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract The number of multimedia applications has been increasing over the past two <br>decades. Multimedia information fusion has therefore attracted significant attention with <br>many techniques having been proposed. However, the uncertainty and correlation among <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5054339921941813865&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 5</a> <a href="/scholar?q=related:adpqRWGfJEYJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5054339921941813865&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'adpqRWGfJEYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC7>TRECVID 2010 Known-item Search by NUS</a></h3><div class="gs_a">XY Chen, J Yuan, L Nie, ZJ Zha, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - TRECVID  &hellip;, 2010 - www-nlpir.nist.gov</div><div class="gs_rs">Abstract. This paper describes our system for auto search and interactive search in the <br>known-item search (KIS) task in TRECVID 2010. KIS task aims to find an unique video <br>answer for each text query. The shift from traditional video search has prompted a series <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12664714192218118309&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 3</a> <a href="/scholar?q=related:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'pVgCEXUawq8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.186.1012&amp;rep=rep1&amp;type=pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.186.1012&amp;rep=rep1&amp;type=pdf" class=yC9>TRECVID 2010 known-item search (KIS) task by I2R</a></h3><div class="gs_a">L Chaisorn, KW Wan, YT Zheng, Y Zhu, TS Kok&hellip; - Proc. TRECVID, 2010 - Citeseer</div><div class="gs_rs">ABSTRACT The KIS task can be regarded as an extreme case of target-specific video <br>search, in which the query aims to uniquely locate a single true answer. Locating the unique <br>video for a query, however, poses new challenges over existing information retrieval <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18222087945561469709&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 4</a> <a href="/scholar?q=related:DXMeFXHc4fwJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18222087945561469709&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'DXMeFXHc4fwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md5', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md5" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:DXMeFXHc4fwJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.worldscientific.com/doi/abs/10.1142/S1793351X09000860" class=yCB>EFFECTIVE AND EFFICIENT VIDEO HIGH-LEVEL SEMANTIC RETRIEVAL USING ASSOCIATIONS AND CORRELATIONS</a></h3><div class="gs_a"><a href="/citations?user=RC0UcqMAAAAJ&amp;hl=en&amp;oi=sra">LIN LIN</a>, MEIL SHYU - International Journal of Semantic Computing, 2009 - World Scientific</div><div class="gs_rs">Two important approaches in multimedia information retrieval are classification and the <br>ranking of the retrieved results. The technique of performing classification using Association <br>Rule Mining (ARM) has been utilized to detect the high-level features from the video, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3528697637644344191&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 3</a> <a href="/scholar?q=related:fxuNO85z-DAJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3528697637644344191&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'fxuNO85z-DAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.isif.org/fusion/proceedings/fusion09CD/data/papers/0340.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from isif.org</span><span class="gs_ggsS">isif.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5203772" class=yCC>Animated movie activity characterization by image and text information fusion</a></h3><div class="gs_a">G PaÃ¯s, F Deloule, D BeauchÃªne&hellip; - &hellip;  Fusion, 2009. FUSION&#39; &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In the context of animated movie characterization, we present an information fusion <br>approach mixing very different types of data related to the activity within a movie. These data <br>are the features extracted from images, words extracted from the synopses and expert <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6245640661764461166&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 1</a> <a href="/scholar?q=related:btI0Txf5rFYJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6245640661764461166&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'btI0Txf5rFYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://crcv-web.eecs.ucf.edu/papers/theses/kishore_thesis_2012_06_25_turnitin_small.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucf.edu</span><span class="gs_ggsS">ucf.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://crcv-web.eecs.ucf.edu/papers/theses/kishore_thesis_2012_06_25_turnitin_small.pdf" class=yCE>ACTION RECOGNITION USING PARTICLE FLOW FIELDS</a></h3><div class="gs_a">KK REDDY - 2012 - crcv-web.eecs.ucf.edu</div><div class="gs_rs">ABSTRACT Research in human action recognition has advanced along multiple fronts in <br>recent years to address various types of actions including simple, isolated actions in staged <br>data (eg, KTH dataset), complex actions (eg, Hollywood dataset) and naturally occurring <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:u_miUMN1AN0J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'u_miUMN1AN0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:u_miUMN1AN0J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://cdn.intechopen.com/pdfs/31068/InTech-Ensemble_learning_with_lda_topic_models_for_visual_concept_detection.pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from intechopen.com</span><span class="gs_ggsS">intechopen.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cdn.intechopen.com/pdfs/31068/InTech-Ensemble_learning_with_lda_topic_models_for_visual_concept_detection.pdf" class=yC10>Ensemble Learning with LDA Topic Models for Visual Concept Detection</a></h3><div class="gs_a">S Tang, YT Zheng, G Cao, YD Zhang, JT Li - 2012 - cdn.intechopen.com</div><div class="gs_rs">With the rapid growth of multimedia application technologies and network technologies, <br>especially the proliferation of Web 2.0 and digital cameras, there has been an explosion of <br>images and videos in the Internet. For example, the volume of videos uploaded to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:TNlYX2oreWYJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7383980799916824908&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'TNlYX2oreWYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TNlYX2oreWYJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/SEL-TMM1401-TangSheng.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6020805" class=yC12>Sparse Ensemble Learning for Concept Detection</a></h3><div class="gs_a">S Tang, YT Zheng, Y Wang&hellip; - &hellip; , IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This work presents a novel sparse ensemble learning scheme for concept detection <br>in videos. The proposed ensemble first exploits a sparse non-negative matrix factorization <br>(NMF) process to represent data instances in parts and partition the data space into <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12874425550055345673&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 1</a> <a href="/scholar?q=related:CQ5DftMlq7IJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12874425550055345673&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'CQ5DftMlq7IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S105381191200794X" class=yC14>Brain maturation: Predicting individual&lt; i&gt; BrainAGE&lt;/i&gt; in children and adolescents using structural MRI</a></h3><div class="gs_a">K Franke, E Luders, A May, M Wilke, <a href="/citations?user=wHcxxX0AAAAJ&amp;hl=en&amp;oi=sra">C Gaser</a> - NeuroImage, 2012 - Elsevier</div><div class="gs_rs">Background Neural development during human childhood and adolescence involves highly <br>coordinated and sequenced events, characterized by both progressive and regressive <br>processes. Despite a multitude of results demonstrating the age-dependent development <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3930413360768123308&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 1</a> <a href="/scholar?q=related:rIEInSCiizYJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3930413360768123308&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'rIEInSCiizYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://crcv.ucf.edu/data/UCF50_files/MVAP_UCF50.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucf.edu</span><span class="gs_ggsS">ucf.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/d811620820675160.pdf" class=yC15>Recognizing 50 human action categories of web videos</a></h3><div class="gs_a">KK Reddy, <a href="/citations?user=p8gsO3gAAAAJ&amp;hl=en&amp;oi=sra">M Shah</a> - Machine Vision and Applications, 2012 - Springer</div><div class="gs_rs">Abstract Action recognition on large categories of unconstrained videos taken from the web <br>is a very challenging problem compared to datasets like KTH (6 actions), IXMAS (13 <br>actions), and Weizmann (10 actions). Challenges like camera motion, different viewpoints, <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'OZJ36nWx4s0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/33358/WangXY.pdf?sequence=1" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/33358" class=yC17>Multimedia Decision Fusion</a></h3><div class="gs_a">X WANG - 2012 - scholarbank.nus.sg</div><div class="gs_rs">The amount of multimedia data available on the Internet has increased exponentially in the <br>past few decades and is likely to keep on increasing. Given multimedia&#39;s nature of having <br>multiple information sources, fusion methods are critical for its data analysis and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:IcHG9VDtYpsJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11196772555573084449&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'IcHG9VDtYpsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://www.micc.unifi.it/ballan/tesi-elaborati/franchi-apr10.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unifi.it</span><span class="gs_ggsS">unifi.it <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.micc.unifi.it/ballan/tesi-elaborati/franchi-apr10.pdf" class=yC19>Annotazione automatica di video basata su co-occorrenza spazio temporale</a></h3><div class="gs_a">G Franchi, <a href="/citations?user=bf2ZrFcAAAAJ&amp;hl=en&amp;oi=sra">A Del Bimbo</a>, <a href="/citations?user=SBm9ZpYAAAAJ&amp;hl=en&amp;oi=sra">M Bertini</a>, A Gandolfi, L Ballan&hellip; - micc.unifi.it</div><div class="gs_rs">Il problema del riconoscimento automatico di contenuti video, sia eventi che oggetti (nel <br>seguito indicati genericamente come concetti), ha ricevuto una grande attenzione <br>nell&#39;ambito della comunita scientifica. La capacita da parte di un calcolatore di <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7292uwvxJ7cJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'7292uwvxJ7cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md14', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md14" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:7292uwvxJ7cJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
