Total results = 124
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://staff.science.uva.nl/~cgmsnoek/CS294/papers/brunelli-review-techreport.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320397904041" class=yC0>A Survey on the Automatic Indexing of Video Data&lt; sup&gt;,&lt;/sup&gt;</a></h3><div class="gs_a"><a href="/citations?user=Y70_PxYAAAAJ&amp;hl=en&amp;oi=sra">R Brunelli</a>, <a href="/citations?user=x1380ogAAAAJ&amp;hl=en&amp;oi=sra">O Mich</a>, <a href="/citations?user=DmhT1_AAAAAJ&amp;hl=en&amp;oi=sra">CM Modena</a> - Journal of visual communication and  &hellip;, 1999 - Elsevier</div><div class="gs_rs">Today a considerable amount of video data in multimedia databases requires sophisticated <br>indices for its effective use. Manual indexing is the most effective method to do this, but it is <br>also the slowest and the most expensive. Automated methods have then to be developed. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2094471277809026848&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 252</a> <a href="/scholar?q=related:IMcQuocOER0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0E/3A/RN062409708.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=2094471277809026848&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'IMcQuocOER0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md0', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md0" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:IMcQuocOER0J:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=4547594448425574545&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT5983237&amp;id=WckWAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC3>Visual dictionary</a></h3><div class="gs_a">R Jain, A Gupta, A Hampapur, B Horowitz - US Patent 5,983,237, 1999 - Google Patents</div><div class="gs_rs">United States Patent [w] Jain et al. US005983237A [ii] Patent Number: [45] Date of Patent: 5,983,237 <br>Nov. 9,1999 [54] VISUAL DICTIONARY [75] Inventors: Ramesh Jain, San Diego; Amarnath <br>Gupta, Redwood City, both of Calif.; Arun Hampapur, White Plains, NY; Bradley Horowitz, <b> ...</b> </div><div class="gs_fl"><a href="/scholar?cites=14134875827826159915&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 107</a> <a href="/scholar?q=related:K3HV9awqKcQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14134875827826159915&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'K3HV9awqKcQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.3656&amp;rep=rep1&amp;type=pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.3656&amp;rep=rep1&amp;type=pdf" class=yC4>Tools for compressed-domain video indexing and editing</a></h3><div class="gs_a">J Meng, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - SPIE Conference on Storage and Retrieval for Image  &hellip;, 1996 - Citeseer</div><div class="gs_rs">Abstract Indexing and editing digital video directly in the compressed domain offer many <br>advantages in terms of storage efficiency and processing speed. We have designed <br>automatic tools in the compressed domain for extracting key visual features such as scene <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7656208358671378566&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 84</a> <a href="/scholar?q=related:huj7GPBQQGoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7656208358671378566&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'huj7GPBQQGoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md2', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md2" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:huj7GPBQQGoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://poseidon.csd.auth.gr/papers/SUBMITTED/JOURNAL/Tsekeridou_CASVT_b/Tsekeridou_CASVT_b.ps.gz" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PS]</span> from auth.gr</span><span class="gs_ggsS">auth.gr <span class=gs_ctg2>[PS]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=915358" class=yC6>Content-based video parsing and indexing based on audio-visual interaction</a></h3><div class="gs_a">S Tsekeridou, <a href="/citations?user=lWmGADwAAAAJ&amp;hl=en&amp;oi=sra">I Pitas</a> - &hellip;  and Systems for Video Technology, IEEE &hellip;, 2001 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A content-based video parsing and indexing method is presented in this paper, <br>which analyzes both information sources (auditory and visual) and accounts for their inter-<br>relations and synergy to extract high-level semantic information. Both frame-and object-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4290338782511925029&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 78</a> <a href="/scholar?q=related:JYe0OWRYijsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/15/37/RN093809130.html?source=googlescholar" class="gs_nph" class=yC8>BL Direct</a> <a href="/scholar?cluster=4290338782511925029&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'JYe0OWRYijsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www.lri.fr/~mbl/DIVA/chi98-paper/introduction.html" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from lri.fr</span><span class="gs_ggsS">lri.fr <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=274701" class=yC9>DIVA: exploratory data analysis with multimedia streams</a></h3><div class="gs_a">WE Mackay, <a href="/citations?user=OucrYFkAAAAJ&amp;hl=en&amp;oi=sra">M Beaudouin-Lafon</a> - &hellip;  of the SIGCHI conference on Human  &hellip;, 1998 - dl.acm.org</div><div class="gs_rs">ABSTRACT DIVA supports exploratory data analysis of multimedia streams, enabling users <br>to visualize, explore and evaluate patterns in data that change over time. The underlying <br>stream algebra provides the mathematical basis for operating on diverse kinds of streams. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15024605212732280580&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 64</a> <a href="/scholar?q=related:BM9jU9YegtAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5A/02/RN045198030.html?source=googlescholar" class="gs_nph" class=yCB>BL Direct</a> <a href="/scholar?cluster=15024605212732280580&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 21 versions</a> <a onclick="return gs_ocit(event,'BM9jU9YegtAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=877505" class=yCC>A database approach for modeling and querying video data</a></h3><div class="gs_a">MS Hacid, C Decleir&hellip; - Knowledge and Data  &hellip;, 2000 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Indexing video data is essential for providing content-based access. In this paper, <br>we consider how database technology can offer an integrated framework for modeling and <br>querying video data. As many concerns in video (eg, modeling and querying) are also <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3120782608790783731&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 63</a> <a href="/scholar?q=related:8wYxgj8_TysJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/41/01/RN085348127.html?source=googlescholar" class="gs_nph" class=yCD>BL Direct</a> <a href="/scholar?cluster=3120782608790783731&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'8wYxgj8_TysJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.3481&amp;rep=rep1&amp;type=pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=244231" class=yCE>Disk striping strategies for large video-on-demand servers</a></h3><div class="gs_a">TS Chua, J Li, BC Ooi, KL Tan - Proceedings of the fourth ACM  &hellip;, 1997 - dl.acm.org</div><div class="gs_rs">ABSTRACT The storage structure of videos on disks affects the number of concurrent users <br>a video- on-demand system can support and hence the average waiting time. In this paper, <br>we propose a phase-based striping method which has the desirable characteristics that it <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13841183847356133748&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 56</a> <a href="/scholar?q=related:dHGJHGDDFcAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13841183847356133748&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'dHGJHGDDFcAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2342&amp;context=cstech" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from purdue.edu</span><span class="gs_ggsS">purdue.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=609629" class=yC10>VideoText database systems</a></h3><div class="gs_a">H Jiang, D Montesi&hellip; - &hellip;  Computing and Systems&#39;  &hellip;, 1997 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper introduces a new approach to realize video databases. It consists of a <br>VideoText data model based on free text annotations associated with logical video <br>segments and a corresponding query language. Traditional database techniques are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2982305081020593657&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 51</a> <a href="/scholar?q=related:-TUTlqtGYykJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2982305081020593657&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'-TUTlqtGYykJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/mmm05-young.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385976" class=yC12>Retrieval of news video using video sequence matching</a></h3><div class="gs_a">Y Kim, TS Chua - &hellip; , 2005. MMM 2005. Proceedings of the 11th  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose a new algorithm to find video clips with different temporal <br>durations and some spatial variations. We adopt a longest common sub-sequence (LCS) <br>matching technique for measuring the temporal similarity between video clips. Based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3349308960069176840&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 43</a> <a href="/scholar?q=related:CM7098Aiey4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3349308960069176840&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'CM7098Aiey4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=1221&amp;context=hcii" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=778737" class=yC14>Simplifying video editing using metadata</a></h3><div class="gs_a">J Casares, AC Long, <a href="/citations?user=E1cp_aYAAAAJ&amp;hl=en&amp;oi=sra">BA Myers</a>, R Bhatnagar&hellip; - Proceedings of the 4th  &hellip;, 2002 - dl.acm.org</div><div class="gs_rs">Abstract Digital video is becoming increasingly ubiquitous. However, editing video remains <br>difficult for several reasons: it is a time-based medium, it has dual tracks of audio and video, <br>and current tools force users to work at the smallest level of detail. Based on interviews <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4547883675772135633&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 41</a> <a href="/scholar?q=related:0SgyfhtUHT8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4547883675772135633&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'0SgyfhtUHT8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://csce.uark.edu/~jgauch/library/Video/Decleir.1999.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uark.edu</span><span class="gs_ggsS">uark.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=754892" class=yC16>A database approach for modeling and querying video data</a></h3><div class="gs_a">C Decleir, MS Hacid&hellip; - Data Engineering, 1999.  &hellip;, 1999 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Indexing video data is essential for providing content based access. We consider <br>how database technology can offer an integrated framework for modeling and querying <br>video data. We develop a data model and a rule-based query language for video content <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2653472592330378730&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 36</a> <a href="/scholar?q=related:6k0Wl0MH0yQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/23/2C/RN062106440.html?source=googlescholar" class="gs_nph" class=yC18>BL Direct</a> <a href="/scholar?cluster=2653472592330378730&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 23 versions</a> <a onclick="return gs_ocit(event,'6k0Wl0MH0yQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://www.cs.cmu.edu/~silver/dl2001paper.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=379461" class=yC19>A multi-view intelligent editor for digital video libraries</a></h3><div class="gs_a"><a href="/citations?user=E1cp_aYAAAAJ&amp;hl=en&amp;oi=sra">BA Myers</a>, JP Casares, S Stevens, <a href="/citations?user=DdNAgNwAAAAJ&amp;hl=en&amp;oi=sra">L Dabbish</a>&hellip; - &hellip; of the 1st ACM/IEEE-CS &hellip;, 2001 - dl.acm.org</div><div class="gs_rs">Abstract Silver is an authoring tool that aims to allow novice users to edit di gital video. The <br>goal is to make editing of digital video as easy as text editing. Silver provides multiple <br>coordinated views, including project, source, outline, subject, storyboard, textual transcript <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17918715790274804276&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 37</a> <a href="/scholar?q=related:NDqD5hIRrPgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17918715790274804276&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 21 versions</a> <a onclick="return gs_ocit(event,'NDqD5hIRrPgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.6810&amp;rep=rep1&amp;type=pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1045926X03000351" class=yC1B>VizIRâa framework for visual information retrieval</a></h3><div class="gs_a">H Eidenberger, <a href="/citations?user=AvNBy7MAAAAJ&amp;hl=en&amp;oi=sra">C Breiteneder</a> - Journal of Visual Languages &amp; Computing, 2003 - Elsevier</div><div class="gs_rs">In this paper the visual information retrieval project VizIR is presented. The goal of the <br>project is the implementation of an open visual information retrieval (VIR) prototype as basis <br>for further research on major problems of VIR. The motivation behind VizIR is the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=792872309486070331&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 32</a> <a href="/scholar?q=related:OyopJyrZAAsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=792872309486070331&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'OyopJyrZAAsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://www.csie.ntnu.edu.tw/~jlkoh/publications/icmcs99.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntnu.edu.tw</span><span class="gs_ggsS">ntnu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=778508" class=yC1D>Semantic video model for content-based retrieval</a></h3><div class="gs_a">JL Koh, CS Lee, ALP Chen - Multimedia Computing and  &hellip;, 1999 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Traditional research on video data retrieval follows two general approaches. One is <br>based on text annotation and the other on content-based comparison. However these <br>approaches do not fully make use of the meaning implied in a video stream. To improve <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11288166466386533430&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 28</a> <a href="/scholar?q=related:NhxyCpifp5wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11288166466386533430&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'NhxyCpifp5wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7068309&amp;id=RuR3AAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC1F>Image exchange with image annotation</a></h3><div class="gs_a">K Toyama, D Vronay, P Anandan - US Patent 7,068,309, 2006 - Google Patents</div><div class="gs_rs">A system and method for providing a peer-to-peer photo-sharing environment. The system <br>includes: manual and automatic photo annotation at the client; periodic client-server <br>synchronization; an index of client photos on a central server or a photo database that is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10127553734171666985&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 27</a> <a href="/scholar?q=related:Kar7kHZMjIwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10127553734171666985&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Kar7kHZMjIwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.151.4511&amp;rep=rep1&amp;type=pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.151.4511&amp;rep=rep1&amp;type=pdf" class=yC20>Temporal multi-resolution analysis for video segmentation</a></h3><div class="gs_a">Y Lin, MS Kankanhalli, TS Chua - Proc. SPIE Conf. Storage and Retrieval  &hellip;, 2000 - Citeseer</div><div class="gs_rs">ABSTRACT Video segmentation is an important step in many of the video applications. We <br>observe that the video shot boundary is a multi-resolution edge phenomenon in the feature <br>space. Based on this observation, we have developed a novel temporal multiresolution <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16519319399723706276&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 26</a> <a href="/scholar?q=related:pCfAj3hpQOUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/29/3A/RN073068022.html?source=googlescholar" class="gs_nph" class=yC22>BL Direct</a> <a href="/scholar?cluster=16519319399723706276&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'pCfAj3hpQOUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md15', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md15" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pCfAj3hpQOUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.ischool.utexas.edu/~geisler/publications/Jnca0112.pdf" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utexas.edu</span><span class="gs_ggsS">utexas.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1084804500901127" class=yC23>Open video: A framework for a test collection</a></h3><div class="gs_a">L Slaughter, G Marchionini, G Geisler - Journal of Network and Computer  &hellip;, 2000 - Elsevier</div><div class="gs_rs">The future will bring widespread access to large digital libraries of video. Consequently, a <br>great deal of research is focused on methods of browsing and retrieving digital video. This <br>type of work requires that investigators acquire and digitize video for their studies since <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17459829773840803147&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 25</a> <a href="/scholar?q=related:S-WRVKzGTfIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/03/0B/RN085960600.html?source=googlescholar" class="gs_nph" class=yC25>BL Direct</a> <a href="/scholar?cluster=17459829773840803147&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 20 versions</a> <a onclick="return gs_ocit(event,'S-WRVKzGTfIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://pdf.aminer.org/001/120/929/imce_integrated_media_creation_environment.pdf" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aminer.org</span><span class="gs_ggsS">aminer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1083315" class=yC26>IMCE: Integrated media creation environment</a></h3><div class="gs_a"><a href="/citations?user=kbcVlyAAAAAJ&amp;hl=en&amp;oi=sra">B Adams</a>, <a href="/citations?user=AEkRUQcAAAAJ&amp;hl=en&amp;oi=sra">S Venkatesh</a>, R Jain - ACM Transactions on Multimedia  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract We discuss the design goals for an integrated media creation environment (IMCE) <br>aimed at enabling the average user to create media artifacts with professional qualities. The <br>resulting requirements are implemented and we demonstrate the efficacy of the resulting <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14502165345814675455&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 25</a> <a href="/scholar?q=related:_39D14sKQskJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14502165345814675455&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'_39D14sKQskJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://www.computer.org/comp/mags/mu/2003/03/u3088.pdf" class=yC29><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from computer.org</span><span class="gs_ggsS">computer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1218260" class=yC28>Viper: A framework for responsive television</a></h3><div class="gs_a">S Agamanolis, VM Bove Jr - Multimedia, IEEE, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Web content commonly incorporates user profile and tracking data to personalize <br>information to clients. Video post-production and delivery systems, however, generally <br>promote a one-size-fits-all authoring approach. Viper lets producers create complex video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8227096988381925198&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 23</a> <a href="/scholar?q=related:Ts8wNR-FLHIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/23/19/RN135921730.html?source=googlescholar" class="gs_nph" class=yC2A>BL Direct</a> <a href="/scholar?cluster=8227096988381925198&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'Ts8wNR-FLHIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://lms.comp.nus.edu.sg/papers/media/2002/MTAP02~1.PDF" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/m2911q0732955g43.pdf" class=yC2B>Stratification approach to modeling video</a></h3><div class="gs_a">TS Chua, L Chen, J Wang - Multimedia Tools and Applications, 2002 - Springer</div><div class="gs_rs">The explosive growth of audiovisual information in the last few years has made the <br>development of advanced video modeling and management tools an urgent task. In this <br>research, we investigate the use of stratification approach to model the contextual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13358721049472749966&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 23</a> <a href="/scholar?q=related:jlFbVwS2Y7kJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3A/61/RN108177950.html?source=googlescholar" class="gs_nph" class=yC2D>BL Direct</a> <a href="/scholar?cluster=13358721049472749966&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'jlFbVwS2Y7kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://www.cs.umb.edu/~duc/publications/papers/dexa00.pdf" class=yC2F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from umb.edu</span><span class="gs_ggsS">umb.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/3QYHTVFQH2D27X7H.pdf" class=yC2E>Semantic reasoning based video database systems</a></h3><div class="gs_a">D Tran, K Hua, K Vu - Database and Expert Systems Applications, 2000 - Springer</div><div class="gs_rs">A constraint of existing content-based video data models is that each modeled semantic <br>description must be associated with time intervals exactly within which it happens and <br>semantics not related to any time interval are not considered. Consequently, users are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=48696660373144659&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 23</a> <a href="/scholar?q=related:UwRKx1oBrQAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/45/0D/RN083768769.html?source=googlescholar" class="gs_nph" class=yC30>BL Direct</a> <a href="/scholar?cluster=48696660373144659&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'UwRKx1oBrQAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://m-fit.googlecode.com/files/Movie%20scene%20segmentation%20using%20background%20information.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from googlecode.com</span><span class="gs_ggsS">googlecode.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S003132030700369X" class=yC31>Movie scene segmentation using background information</a></h3><div class="gs_a">LH Chen, YC Lai, <a href="/citations?user=_IXt8boAAAAJ&amp;hl=en&amp;oi=sra">HY Mark Liao</a> - Pattern Recognition, 2008 - Elsevier</div><div class="gs_rs">Scene extraction is the first step toward semantic understanding of a video. It also provides <br>improved browsing and retrieval facilities to users of video database. This paper presents an <br>effective approach to movie scene extraction based on the analysis of background images<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5975751473744489827&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 26</a> <a href="/scholar?q=related:Y62O8k0i7lIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5975751473744489827&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'Y62O8k0i7lIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.30.785&amp;rep=rep1&amp;type=pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1015143" class=yC33>Experimental video database management system based on advanced object-oriented techniques</a></h3><div class="gs_a">L Huang, JC Lee, <a href="/citations?user=D1LEg-YAAAAJ&amp;hl=en&amp;oi=sra">Q Li</a>, W Xiong - Electronic  &hellip;, 1996 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract Video data management is fast becoming one of the most important topics in <br>multimedia databases. Most of the recent work on video databases has so far focused on <br>video classification, feature extraction, spatial reasoning and image retrieval (video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1981168059690040749&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 22</a> <a href="/scholar?q=related:rb0shtqFfhsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/51/15/RN006787691.html?source=googlescholar" class="gs_nph" class=yC35>BL Direct</a> <a href="/scholar?cluster=1981168059690040749&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'rb0shtqFfhsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://www.cs.uic.edu/~sistla/papers/db/video_icde_1997.ps" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PS]</span> from uic.edu</span><span class="gs_ggsS">uic.edu <span class=gs_ctg2>[PS]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=581751" class=yC36>Similarity based retrieval of videos</a></h3><div class="gs_a">AP Sistla, C Yu&hellip; - Data Engineering, 1997. &hellip;, 1997 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The authors propose a language, called Hierarchical Temporal Logic (HTL), for <br>specifying queries on video databases. The language is based on the hierarchical as well <br>as temporal nature of video data. They give similarity based semantics for the logic, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15351708115771287472&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 22</a> <a href="/scholar?q=related:sGcHyDE5DNUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0A/52/RN028263430.html?source=googlescholar" class="gs_nph" class=yC38>BL Direct</a> <a href="/scholar?cluster=15351708115771287472&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'sGcHyDE5DNUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://poseidon.csd.auth.gr/papers/SUBMITTED/CONFERENCE/Tsekeridou_ICMCS99/Tsekeridou_ICMCS99.ps.gz" class=yC3A><span class="gs_ggsL"><span class=gs_ctg2>[PS]</span> from auth.gr</span><span class="gs_ggsS">auth.gr <span class=gs_ctg2>[PS]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=779279" class=yC39>Audio-visual content analysis for content-based video indexing</a></h3><div class="gs_a">S Tsekeridou, <a href="/citations?user=lWmGADwAAAAJ&amp;hl=en&amp;oi=sra">I Pitas</a> - Multimedia Computing and Systems,  &hellip;, 1999 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract An audio-visual content analysis method is presented, which analyzes both <br>auditory and visual information sources and accounts for their inter-relations and <br>coincidence to extract high-level semantic information. Both shot-based and object-based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13017609808866706719&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 23</a> <a href="/scholar?q=related:HwUkoibXp7QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13017609808866706719&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'HwUkoibXp7QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1017730" class=yC3B>Accommodating hybrid retrieval in a comprehensive video database management system</a></h3><div class="gs_a">SSM Chan, <a href="/citations?user=D1LEg-YAAAAJ&amp;hl=en&amp;oi=sra">Q Li</a>, Y Wu, Y Zhuang - &hellip; , IEEE Transactions on, 2002 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A comprehensive video retrieval system should be able to accommodate and utilize <br>various (complementary) description data in facilitating effective retrieval. We advocate a <br>hybrid retrieval approach by integrating a query-based (database) mechanism with <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4658687062672065742&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 19</a> <a href="/scholar?q=related:zsRGYjP7pkAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/23/45/RN116443697.html?source=googlescholar" class="gs_nph" class=yC3C>BL Direct</a> <a href="/scholar?cluster=4658687062672065742&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'zsRGYjP7pkAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://www.cosc.canterbury.ac.nz/andrew.cockburn/papers/group97.pdf" class=yC3E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from canterbury.ac.nz</span><span class="gs_ggsS">canterbury.ac.nz <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=266857" class=yC3D>CEVA: a tool for collaborative video analysis</a></h3><div class="gs_a"><a href="/citations?user=eBdrkd0AAAAJ&amp;hl=en&amp;oi=sra">A Cockburn</a>, T Dale - Proceedings of the international ACM SIGGROUP  &hellip;, 1997 - dl.acm.org</div><div class="gs_rs">Video protocol analysis is a popular and effective technique for understanding the <br>interactions between people, and between people and machines. Analysing and <br>transcribing video recordings is, however, notoriously time consuming. Neal [14], for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16377342360504039363&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 18</a> <a href="/scholar?q=related:w_fMIhsCSOMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16377342360504039363&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'w_fMIhsCSOMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.9790&amp;rep=rep1&amp;type=pdf" class=yC40><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.9790&amp;rep=rep1&amp;type=pdf" class=yC3F>Cinematic-based model for scene boundary detection</a></h3><div class="gs_a">J Wang, TS Chua, L Chen - The Eight Conference on Multimedia Modeling, 2001 - Citeseer</div><div class="gs_rs">Most current video retrieval systems use shot as the basic unit for information organization <br>and access. A shot, however, models only a visually contiguous sequence of video frames <br>with no coherent semantic meanings. On the other hand, viewers tend to âviewâ video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7876418633163114793&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 17</a> <a href="/scholar?q=related:KYn0CwSpTm0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7876418633163114793&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'KYn0CwSpTm0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md27', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md27" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:KYn0CwSpTm0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/hvygfyar4ydnfbju.pdf" class=yC41>AI-STRATA: A User-centered Model for Content-based description and Retrieval of Audiovisual Sequences</a></h3><div class="gs_a"><a href="/citations?user=oZkePtQAAAAJ&amp;hl=en&amp;oi=sra">Y PriÃ©</a>, A Mille, JM Pinon - Advanced Multimedia Content Processing, 1999 - Springer</div><div class="gs_rs">We first insist on the need for conceptual and knowledge-based audiovisual (AV) models in <br>AV and multimedia information retrieval systems. We then propose several criteria for <br>characterizing audiovisual representation approaches, and present a new approach for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16188185107048924087&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 17</a> <a href="/scholar?q=related:t4PrD5b8p-AJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/09/34/RN059874391.html?source=googlescholar" class="gs_nph" class=yC42>BL Direct</a> <a href="/scholar?cluster=16188185107048924087&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'t4PrD5b8p-AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.7345&amp;rep=rep1&amp;type=pdf" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.7345&amp;rep=rep1&amp;type=pdf" class=yC43>A General Frame Work for Video Segmentation Based on Temporal Multi-resolution Analysis</a></h3><div class="gs_a">TS Chua, M Kankanhalli, Y Lin - Int&#39;l Workshop on Advanced Image  &hellip;, 2000 - Citeseer</div><div class="gs_rs">ABSTRACT Video segmentation is an important step in many video processing applications. <br>By observing that the video shot boundary is a multi-(temporal)-resolution edge <br>phenomenon in the feature space, we develop a general framework to handle all types of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6319097897849522820&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 17</a> <a href="/scholar?q=related:hAo93g7ysVcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6319097897849522820&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'hAo93g7ysVcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md29', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md29" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:hAo93g7ysVcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=1801&amp;context=isr" class=yC46><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=506539" class=yC45>Simplifying video editing with SILVER</a></h3><div class="gs_a">J Casares, AC Long, <a href="/citations?user=E1cp_aYAAAAJ&amp;hl=en&amp;oi=sra">B Myers</a>, S Stevens&hellip; - CHI&#39;02 extended  &hellip;, 2002 - dl.acm.org</div><div class="gs_rs">Abstract Digital video is becoming more ubiquitous. Unfortunately, editing videos remains <br>difficult for several reasons. It has dual tracks of audio and video and may require working at <br>the smallest level of detail. Silver is an authoring tool that uses video metadata to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1170449120986355808&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:YFgPNEZFPhAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/34/09/RN140301848.html?source=googlescholar" class="gs_nph" class=yC47>BL Direct</a> <a href="/scholar?cluster=1170449120986355808&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'YFgPNEZFPhAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://lat.inf.tu-dresden.de/research/papers/1999/ArdizzoneHacid-ICMCS-99.ps.gz" class=yC49><span class="gs_ggsL"><span class=gs_ctg2>[PS]</span> from tu-dresden.de</span><span class="gs_ggsS">tu-dresden.de <span class=gs_ctg2>[PS]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=778213" class=yC48>A semantic modeling approach for video retrieval by content</a></h3><div class="gs_a">E Ardizzone, MS Hacid - Multimedia Computing and Systems,  &hellip;, 1999 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A knowledge-based approach to model and retrieve video data by content is <br>developed. Selected objects of interest in a video sequence are described and stored in a <br>database. This database forms the object layer. On top of this layer we define the schema <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2551023846752897063&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:J8hBN6wOZyMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2551023846752897063&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'J8hBN6wOZyMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S002002559900033X" class=yC4A>A new content-based access method for video databases</a></h3><div class="gs_a">PJ Cheng, WP Yang - Information Sciences, 1999 - Elsevier</div><div class="gs_rs">This paper presents a novel video data model and a nested annotation language for <br>describing complex information of video data. In contrast to conventional approaches, the <br>proposed model classifies different video materials of interest to users into different <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13691458261174350285&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 17</a> <a href="/scholar?q=related:zXFigr_UAb4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/2E/20/RN069926654.html?source=googlescholar" class="gs_nph" class=yC4B>BL Direct</a> <a href="/scholar?cluster=13691458261174350285&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'zXFigr_UAb4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><a href="http://spie.org/x648.html?product_id=257293" class=yC4C>Video segmentation using 3D hints contained in 2D images</a></h3><div class="gs_a">MA Fard, X Tu, <a href="/citations?user=VOPW5YYAAAAJ&amp;hl=en&amp;oi=sra">L Chen</a>, P Faudemay - Proceedings of SPIE, 1996 - spie.org</div><div class="gs_rs">Int his paper, we present a new method for video sequence segmentation which can be <br>used in video indexation applications. Our approach uses the image content as indices of <br>segmentation. As for most video sequences, the images contain 3D hints. In order to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5974802946247790186&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:ao4Orp_D6lIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/2C/5F/RN017558900.html?source=googlescholar" class="gs_nph" class=yC4D>BL Direct</a> <a href="/scholar?cluster=5974802946247790186&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ao4Orp_D6lIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md33', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md33" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:ao4Orp_D6lIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm02-wangjh.pdf" class=yC4F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=641055" class=yC4E>A framework for video scene boundary detection</a></h3><div class="gs_a">J Wang, TS Chua - Proceedings of the tenth ACM international  &hellip;, 2002 - dl.acm.org</div><div class="gs_rs">Abstract Most current video retrieval systems use shot as the basis for information <br>organization and access. In cinematography, scene is the basic story unit that the directors <br>use to convey their ideas. This paper proposes a framework based on the concept of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2323743190817835309&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 17</a> <a href="/scholar?q=related:LQnuzR6YPyAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2323743190817835309&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'LQnuzR6YPyAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://homepages.cwi.nl/~media/publications/mmm01b.pdf" class=yC51><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cwi.nl</span><span class="gs_ggsS">cwi.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://homepages.cwi.nl/~media/publications/mmm01b.pdf" class=yC50>Improving media fragment integration in emerging Web formats</a></h3><div class="gs_a"><a href="/citations?user=lpwWjroAAAAJ&amp;hl=en&amp;oi=sra">L Rutledge</a>, <a href="/citations?user=cj4WKYIAAAAJ&amp;hl=en&amp;oi=sra">P Schmitz</a> - Proc. of Multimedia Modeling, 2001 - homepages.cwi.nl</div><div class="gs_rs">The media components integrated into multimedia presentations are typically entire files. At <br>times the media component desired for integration, either as a navigation destination or as <br>coordinate presentation, is a part of a file, or what we call a fragment. Basic media <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9664510773762403916&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 15</a> <a href="/scholar?q=related:TC4LDlg9H4YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9664510773762403916&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'TC4LDlg9H4YJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md35', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md35" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TC4LDlg9H4YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.26.3108&amp;rep=rep1&amp;type=pdf" class=yC53><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/41QAQ3BKT3DTB1W0.pdf" class=yC52>VideoGraph: a graphical object-based model for representing and querying video data</a></h3><div class="gs_a">D Tran, K Hua, K Vu - Conceptual ModelingâER 2000, 2000 - Springer</div><div class="gs_rs">Modeling video data poses a great challenge since they do not have as clear an underlying <br>structure as traditional databases do. We propose a graphical object-based model, called <br>VideoGraph, in this paper. This scheme has the following advantages:(1) In addition to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9759660839472480737&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 15</a> <a href="/scholar?q=related:4UEMGdNHcYcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4D/27/RN085688212.html?source=googlescholar" class="gs_nph" class=yC54>BL Direct</a> <a href="/scholar?cluster=9759660839472480737&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'4UEMGdNHcYcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/R224V07678373636.pdf" class=yC55>Integrated video and text for content-based access to video databases</a></h3><div class="gs_a">H Jiang, D Montesi, AK Elmagarmid - Multimedia Tools and Applications, 1999 - Springer</div><div class="gs_rs">This paper introduces a new approach to realize video databases. The approach consists of <br>a VideoText data model based on free text annotations associated with logical video <br>segments and a corresponding query language. Traditional database techniques are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5073313485606238691&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 15</a> <a href="/scholar?q=related:4znCT7wHaEYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5D/1F/RN067811112.html?source=googlescholar" class="gs_nph" class=yC56>BL Direct</a> <a href="/scholar?cluster=5073313485606238691&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'4znCT7wHaEYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://dspace.mit.edu/bitstream/handle/1721.1/32497/61896480.pdf?sequence=1" class=yC58><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mit.edu</span><span class="gs_ggsS">mit.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dspace.mit.edu/handle/1721.1/32497" class=yC57>Mindful documentary</a></h3><div class="gs_a">BA Barry - 2005 - dspace.mit.edu</div><div class="gs_rs">In the practice of documentary creation, a videographer performs an elaborate balancing act <br>between observing the world, deciding what to record, and understanding the implications of <br>the recorded material, all with respect to her primary goal of story construction. This thesis <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7212886524419805303&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 13</a> <a href="/scholar?q=related:d5A-FRRSGWQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7212886524419805303&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'d5A-FRRSGWQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md38', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md38" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:d5A-FRRSGWQJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=2174165569323631215&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB39" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW39"><a href="http://nguyendangbinh.org/Proceedings/ECCV/2002/papers/2353/23530403.pdf" class=yC5A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nguyendangbinh.org</span><span class="gs_ggsS">nguyendangbinh.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/aflmlqluyuy43mrx.pdf" class=yC59>Optimization algorithms for the selection of key frame sequences of variable length</a></h3><div class="gs_a">T Liu, J Kender - Computer VisionâECCV 2002, 2006 - Springer</div><div class="gs_rs">This paper presents a novel optimization-based approach for video key frame selection. We <br>define key frames to be a temporally ordered subsequence of the original video sequence, <br>and the optimal k key frames are the subsequence of length k that optimizes an energy <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=399250581035675053&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 14</a> <a href="/scholar?q=related:re3_wklsigUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/49/13/RN115347986.html?source=googlescholar" class="gs_nph" class=yC5B>BL Direct</a> <a href="/scholar?cluster=399250581035675053&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'re3_wklsigUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB40" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW40"><a href="http://www.ee.columbia.edu/ln/dvmm/publications/PhD_theses/dzhong-thesis.pdf" class=yC5D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ee.columbia.edu/ln/dvmm/publications/PhD_theses/dzhong-thesis.pdf" class=yC5C>Segmentation, index and summarization of digital video content</a></h3><div class="gs_a">D Zhong - 2001 - ee.columbia.edu</div><div class="gs_rs">In this thesis, we propose and develop unique frameworks and methods for temporal and <br>spatial video segmentation as well as object based video representation, indexing and <br>retrieval at both the syntactic and the semantic level. First we demonstrate a robust and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10456834001708152427&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:a3YKMx8jHpEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10456834001708152427&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'a3YKMx8jHpEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md40', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md40" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:a3YKMx8jHpEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:a3YKMx8jHpEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=15313832517664976311&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB41" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW41"><a href="http://eprints.eemcs.utwente.nl/6364/01/admire.pdf" class=yC5F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utwente.nl</span><span class="gs_ggsS">utwente.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://eprints.eemcs.utwente.nl/6364/01/admire.pdf" class=yC5E>Cost-effective network-based multimedia information retrieval</a></h3><div class="gs_a">DD Velthausz - 1998 - eprints.eemcs.utwente.nl</div><div class="gs_rs">Encouraged by the developments in multimedia technology and network oriented information <br>services, the available and accessible digital information has grown at an incredible rate. This <br>book addresses the question of how to search for multimedia information in a <b> ...</b> </div><div class="gs_fl"><a href="/scholar?cites=10481047919493947354&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:2p9IQ40pdJEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10481047919493947354&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'2p9IQ40pdJEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md41', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md41" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:2p9IQ40pdJEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:2p9IQ40pdJEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=4994661407363806236&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320300904607" class=yC60>Spatial similarity retrieval in video databases</a></h3><div class="gs_a">YK Chan, CC Chang - Journal of Visual Communication and Image  &hellip;, 2001 - Elsevier</div><div class="gs_rs">A nine-direction lower-triangular (9DLT) matrix describes the relative spatial relationships <br>among the objects in a symbolic image. In this paper, the 9DLT matrix will be transformed <br>into a linear string, called 9DLT string. Based on the 9DLT string, two metrics of similarity <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16160607488638543895&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:F2jRauMCRuAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/34/61/RN098951088.html?source=googlescholar" class="gs_nph" class=yC61>BL Direct</a> <a href="/scholar?cluster=16160607488638543895&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'F2jRauMCRuAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB43" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW43"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.9887&amp;rep=rep1&amp;type=pdf" class=yC63><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1230812.1230813" class=yC62>Computational approaches to temporal sampling of video sequences</a></h3><div class="gs_a">T Liu, JR Kender - ACM Transactions on Multimedia Computing,  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Video key frame extraction is one of the most important research problems for video <br>summarization, indexing, and retrieval. For a variety of applications such as ubiquitous <br>media access and video streaming, the temporal boundaries between video key frames <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3874024195260707398&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:RnLI6XpMwzUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3874024195260707398&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'RnLI6XpMwzUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.683&amp;rep=rep1&amp;type=pdf" class=yC65><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/2wpw49heud29krmb.pdf" class=yC64>A framework for visual information retrieval</a></h3><div class="gs_a">H Eidenberger, <a href="/citations?user=AvNBy7MAAAAJ&amp;hl=en&amp;oi=sra">C Breiteneder</a>, <a href="/citations?user=_Q8-XXAAAAAJ&amp;hl=en&amp;oi=sra">M Hitz</a> - Recent Advances in Visual  &hellip;, 2002 - Springer</div><div class="gs_rs">In this paper a visual information retrieval project (VizIR) is presented. The goal of the project <br>is the implementation of an open Contentbased Visual Retrieval (CBVR) prototype as basis <br>for further research on the major problems of CBVR. The motivation behind VizIR is: an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5299966388168341805&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 10</a> <a href="/scholar?q=related:LblIgFlDjUkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0D/61/RN110086180.html?source=googlescholar" class="gs_nph" class=yC66>BL Direct</a> <a href="/scholar?cluster=5299966388168341805&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'LblIgFlDjUkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.tandfonline.com/doi/abs/10.1080/088395197118190" class=yC67>Film sequence generation strategies for automatic intelligent video editing</a></h3><div class="gs_a">S Butler, A Parkes - Applied Artificial Intelligence, 1997 - Taylor &amp; Francis</div><div class="gs_rs">In this article, we describe an approach to generic automatic intelligent video editing, which <br>was used in the implementation of the LIVE system. We discuss cinema theory and related <br>systems that recombine film fragments into sequences. We introduce LIVE and describe <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10398908312543561977&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 10</a> <a href="/scholar?q=related:-QgEtQNYUJAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0F/18/RN027011354.html?source=googlescholar" class="gs_nph" class=yC68>BL Direct</a> <a href="/scholar?cluster=10398908312543561977&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'-QgEtQNYUJAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB46" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW46"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.1059&amp;rep=rep1&amp;type=pdf" class=yC6A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=927006" class=yC69>VideoMAP: a generic framework for video management and application processing</a></h3><div class="gs_a">RWH Lau, <a href="/citations?user=D1LEg-YAAAAJ&amp;hl=en&amp;oi=sra">Q Li</a>, A Si - &hellip;  Sciences, 2000. Proceedings of the 33rd &hellip;, 2000 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract To support video applications efficiently, video objects must be modeled and <br>structured in secondary storage effectively, allowing flexible, easy, and efficient retrieval as <br>well as support for dynamic video application development. The authors describe a video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12312024531067859714&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 9</a> <a href="/scholar?q=related:AvMxQBQZ3aoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12312024531067859714&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'AvMxQBQZ3aoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB47" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW47"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.2954&amp;rep=rep1&amp;type=pdf" class=yC6C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=u3li2AHwQeIC&amp;oi=fnd&amp;pg=PA362&amp;ots=J0laUgrzN7&amp;sig=N9UZusZj8Tq_q3A8RTuzjSGerBI" class=yC6B>Spatial-temporal semantic grouping of instructional video content</a></h3><div class="gs_a">T Liu, JR Kender - Lecture notes in computer science, 2003 - books.google.com</div><div class="gs_rs">Abstract. This paper presents a new approach for content analysis and semantic <br>summarization of instructional videos of blackboard presentations. We first use low-level <br>image processing techniques to segment frames into board content regions, regions <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=731738177685243394&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 9</a> <a href="/scholar?q=related:AkaF5f6nJwoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/22/37/RN134925030.html?source=googlescholar" class="gs_nph" class=yC6D>BL Direct</a> <a href="/scholar?cluster=731738177685243394&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'AkaF5f6nJwoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://ir.kaist.ac.kr/papers/2003/A%20Multimedia%20Digital%20Library%20System%20Based%20on%20MPEG-7.pdf" class=yC6F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/ME93HD7RG0N2C4FQ.pdf" class=yC6E>A multimedia digital library system based on MPEG-7 and XQuery</a></h3><div class="gs_a">MH Lee, JH Kang, S Myaeng, S Hyun, JM Yoo&hellip; - &hellip;  and Management of  &hellip;, 2003 - Springer</div><div class="gs_rs">We designed and implemented a digital library system that supports content-based retrieval <br>of multimedia objects based on MPEG-7 and XQuery. MPEG-7, a metadata standard for <br>multimedia objects, can describe various features such as color histograms of images and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13059399626975086244&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 9</a> <a href="/scholar?q=related:pAa44cROPLUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/33/5B/RN141863159.html?source=googlescholar" class="gs_nph" class=yC70>BL Direct</a> <a href="/scholar?cluster=13059399626975086244&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'pAa44cROPLUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://pagesperso.lina.univ-nantes.fr/~prie-y/download/prie-context99.pdf" class=yC72><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from univ-nantes.fr</span><span class="gs_ggsS">univ-nantes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/FERQ2FCURLVM2EK6.pdf" class=yC71>A context-based audiovisual representation model for audiovisual information systems</a></h3><div class="gs_a"><a href="/citations?user=oZkePtQAAAAJ&amp;hl=en&amp;oi=sra">Y PriÃ©</a>, A Mille, JM Pinon - Modeling and Using Context, 1999 - Springer</div><div class="gs_rs">In this paper we present a contextual representation model of audiovisual (AV) documents <br>for AV information systems. In the first part, we study AV medium, and show that AV intra-<br>document context is always related to a user task seen as a general description task. We <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6768597888348119958&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:luNy-urj7l0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/31/07/RN067013529.html?source=googlescholar" class="gs_nph" class=yC73>BL Direct</a> <a href="/scholar?cluster=6768597888348119958&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'luNy-urj7l0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/x3062k27l75j4122.pdf" class=yC74>VCMF: a framework for video content modeling</a></h3><div class="gs_a"><a href="/citations?user=YK0-nYAAAAAJ&amp;hl=en&amp;oi=sra">N Bryan-Kinns</a> - Multimedia tools and Applications, 2000 - Springer</div><div class="gs_rs">Current approaches to modeling the structure and semantics of video recordings restrict its <br>reuse. This is because these approaches are either too rigidly structured or too generally <br>structured and so do not represent the structural and semantic regularities of classes of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13380112283695623572&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:lK2ELDu1r7kJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/26/0F/RN075427372.html?source=googlescholar" class="gs_nph" class=yC75>BL Direct</a> <a href="/scholar?cluster=13380112283695623572&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'lK2ELDu1r7kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7840691&amp;id=UtHbAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC76>Personal broadcast server system for providing a customized broadcast</a></h3><div class="gs_a">JS De Bonet, <a href="/citations?user=G2-nFaIAAAAJ&amp;hl=en&amp;oi=sra">PA Viola</a> - US Patent 7,840,691, 2010 - Google Patents</div><div class="gs_rs">A personal broadcast server system provides a customized broadcast to one or more users <br>over a transmission media. A data storage device stores a plurality of broadcast elements. A <br>data management system stores a user profile and a user state for each of the one or <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8359234182767524120&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:GDkpOjP3AXQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8359234182767524120&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'GDkpOjP3AXQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=809782" class=yC77>A knowledge representation and reasoning support for modeling and querying video data</a></h3><div class="gs_a">E Ardizzone, MS Hacid - Tools with Artificial Intelligence, 1999.  &hellip;, 1999 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Describes a knowledge-based approach for modelling and retrieving video data. <br>Selected objects of interest in a video sequence are described and stored in a database. <br>This database forms the object layer. On top of this layer, we define the schema layer that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10005355621430458317&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:zffRUu0p2ooJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10005355621430458317&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'zffRUu0p2ooJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=1827&amp;context=isr" class=yC79><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://repository.cmu.edu/isr/811/" class=yC78>Video Editing Using Lenses and Semantic Zooming</a></h3><div class="gs_a">AC Long, <a href="/citations?user=E1cp_aYAAAAJ&amp;hl=en&amp;oi=sra">B Myers</a>, J Casares, S Stevens, A Corbett - 2003 - repository.cmu.edu</div><div class="gs_rs">ABSTRACT Digital video is becoming increasingly prevalent. Unfortunately, editing video <br>remains difficult for several reasons: it is a time-based medium, it has dual tracks of audio <br>and video, and current tools force users to work at the smallest level of detail. Based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1898296530858447429&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:RZIsIKMaWBoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1898296530858447429&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'RZIsIKMaWBoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB54" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW54"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/visual03.pdf" class=yC7B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/empenryggg0alg96.pdf" class=yC7A>A cinematic-based framework for scene boundary detection in video</a></h3><div class="gs_a">J Wang, TS Chua - The Visual Computer, 2003 - Springer</div><div class="gs_rs">Most current video retrieval systems use shots as the basis for information organization and <br>access. In cinematography, scene is the basic story unit that the directors use to compose <br>and convey their ideas. This paper proposes a framework based on the concept of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16541083775778629801&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:qShDPw68jeUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4B/32/RN137115916.html?source=googlescholar" class="gs_nph" class=yC7C>BL Direct</a> <a href="/scholar?cluster=16541083775778629801&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'qShDPw68jeUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB55" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW55"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.2027&amp;rep=rep1&amp;type=pdf" class=yC7E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=655787" class=yC7D>Query processing in a video retrieval system</a></h3><div class="gs_a">KL Liu, P Sistla, C Yu, N Rishe - Data Engineering, 1998.  &hellip;, 1998 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract AP Sistla et al.(1997) designed a similarity-based video retrieval system. Queries <br>were specified in a language called the Hierarchical Temporal Language (HTL). In this <br>paper, we present several extensions of HTL. These extensions include queries that can <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3810969869618975933&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:vRRbsOdI4zQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/29/32/RN041625569.html?source=googlescholar" class="gs_nph" class=yC7F>BL Direct</a> <a href="/scholar?cluster=3810969869618975933&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'vRRbsOdI4zQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=xio_7Dj1TmUC&amp;oi=fnd&amp;pg=PA181&amp;ots=acvUyuNspt&amp;sig=ZaMfZRzE2TGMFUjWIQRBesDOYZk" class=yC80>Context-sensitive and context-free retrieval in a video database system</a></h3><div class="gs_a">QLIKM Lam - Image Databases and Multi-Media Search, 1997 - books.google.com</div><div class="gs_rs">ABSTRACT Video retrieval is an important topic in video database systems. Annotation-<br>based video retrieval is a natural and an effective means for accessing video data when <br>higher level semantics are involved, and is complementary to content-based access. In <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7156482959861760198&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:xkCy11XvUGMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/07/22/RN042650316.html?source=googlescholar" class="gs_nph" class=yC81>BL Direct</a> <a href="/scholar?cluster=7156482959861760198&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'xkCy11XvUGMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0020025505003403" class=yC82>Video model for dynamic objects</a></h3><div class="gs_a">B Acharya, AK Majumdar, J Mukherjee - Information Sciences, 2006 - Elsevier</div><div class="gs_rs">Content based retrieval from a video database depends greatly on how the semantics of <br>video is described. In this paper we propose a video model that tries to capture the <br>semantics of video segments based on the dynamic characteristics of the objects present <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15782318449145949699&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:Ay5w3QIPBtsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15782318449145949699&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Ay5w3QIPBtsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB58" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW58"><a href="http://www.ojs.academypublisher.com/index.php/jmm/article/viewFile/03032633/1238" class=yC84><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from academypublisher.com</span><span class="gs_ggsS">academypublisher.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4412917" class=yC83>A film classifier based on low-level visual features</a></h3><div class="gs_a">HY Huang, WS Shih, WH Hsu - Multimedia Signal Processing,  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose an approach to categorize the film classes by using low-<br>level features and visual features. The goal of this approach is to classify the films into <br>genres. Our current domain of study is using the movie preview. A film preview often <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1647295050612491750&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:5kUE7B5e3BYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1647295050612491750&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'5kUE7B5e3BYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7725812&amp;id=BlTRAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC85>Authoring system for combining temporal and nontemporal digital media</a></h3><div class="gs_a">PA Balkus, G McElhoe, TW Crofton&hellip; - US Patent 7,725,812, 2010 - Google Patents</div><div class="gs_rs">An authoring tool has a graphical user interface enabling interactive authoring of a <br>multimedia presentation including temporal and nontemporal media. The graphical user <br>interface enables specification of the temporal and spatial relationships among the media <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5443441900071750685&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:HQxBSZH9iksJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5443441900071750685&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'HQxBSZH9iksJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4387561" class=yC86>Movie classification using visual effect features</a></h3><div class="gs_a">HY Huang, WS Shih, WH Hsu - Signal Processing Systems,  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose an approach to category the film kinds using low-level <br>features and visual effect features. This approach aims to category the films into genres. Our <br>current domain of study is the movie preview. A film preview often emphasizes the theme <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14205895407235803473&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:UamkWJt6JcUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14205895407235803473&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'UamkWJt6JcUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB61" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW61"><a href="http://pet.ece.iisc.ernet.in/course/E0262/ref/449-465.pdf" class=yC88><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iisc.ernet.in</span><span class="gs_ggsS">iisc.ernet.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://pet.ece.iisc.ernet.in/course/E0262/ref/449-465.pdf" class=yC87>Content-based video data retrieval</a></h3><div class="gs_a">ALP Chen, CC Liu, TCT Kuo - &hellip;  CHINA PART A PHYS SCI ENG, 1999 - pet.ece.iisc.ernet.in</div><div class="gs_rs">ABSTRACT Many multimedia applications, such as the World-Wide-Web (WWW), video-on-<br>demand (VOD), and digital library, require strong support of a video database system. In this <br>paper, we survey various approaches to content-based video data retrieval. Compared <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17246132048757018366&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:_k6kc7mRVu8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/30/3B/RN067255513.html?source=googlescholar" class="gs_nph" class=yC89>BL Direct</a> <a href="/scholar?cluster=17246132048757018366&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'_k6kc7mRVu8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md61', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md61" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_k6kc7mRVu8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB62" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW62"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.44.9892&amp;rep=rep1&amp;type=pdf" class=yC8B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=694505" class=yC8A>Modeling and querying video data: A hybrid approach</a></h3><div class="gs_a">C Decleir, MS Hacid&hellip; - Content-Based Access of  &hellip;, 1998 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper develops a video data model and a rule-based query language for video <br>retrieval. A video sequence is split into a set of fragments. Each fragment can be analyzed to <br>extract the information of interest that can be put into a database. This database can then <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13897413865636223718&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:5trVdUeI3cAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13897413865636223718&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'5trVdUeI3cAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB63" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW63"><a href="http://nguyendangbinh.org/Proceedings/ICPR/2006/DATA/D04_0529.PDF" class=yC8D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nguyendangbinh.org</span><span class="gs_ggsS">nguyendangbinh.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1699943" class=yC8C>Video scene extraction using mosaic technique</a></h3><div class="gs_a">LH Chen, YC Lai, <a href="/citations?user=_IXt8boAAAAJ&amp;hl=en&amp;oi=sra">HY Liao</a> - Pattern Recognition, 2006. ICPR  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Scene extraction is the first step toward semantic understanding of the video. This <br>paper presents an effective approach to video scene extraction based on the analysis of <br>background images. Our approach exploits the fact that shots belonging to one particular <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4835394149193967769&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:mWi1E1zFGkMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4835394149193967769&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'mWi1E1zFGkMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7702014&amp;id=JnvOAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC8E>System and method for video production</a></h3><div class="gs_a">PR Kellock, SK Subramaniam&hellip; - US Patent  &hellip;, 2010 - Google Patents</div><div class="gs_rs">A system for processing video segments is disclosed. The system includes a descriptor <br>instantiator for creating a descriptor and ascribing at least one value thereto for a <br>corresponding video segment. The system also includes a video constructor for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15683918357158980101&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:BWqwHKZ4qNkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15683918357158980101&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'BWqwHKZ4qNkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB65" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW65"><a href="http://dspace.kaist.ac.kr/bitstream/10203/1684/1/01237886.pdf" class=yC90><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dspace.kaist.ac.kr/handle/10203/1684" class=yC8F>Content-based lecture access for distance learning</a></h3><div class="gs_a">GH Cha, CW Chung - Multimedia and Expo, 2001. ICME 2001.  &hellip;, 2001 - dspace.kaist.ac.kr</div><div class="gs_rs">Abstract Education and training are expected to change dramatically due to the combined <br>impact of the Internet, database, and multimedia technologies. However, the distance <br>learning is often impeded by the lack of effective methods to retrieve specific parts of a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2807514697774358992&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:0GGHJr5L9iYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2807514697774358992&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'0GGHJr5L9iYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB66" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW66"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.251&amp;rep=rep1&amp;type=pdf" class=yC92><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1027638" class=yC91>Director in your pocket: holistic help for the hapless home videographer</a></h3><div class="gs_a"><a href="/citations?user=kbcVlyAAAAAJ&amp;hl=en&amp;oi=sra">B Adams</a>, <a href="/citations?user=AEkRUQcAAAAJ&amp;hl=en&amp;oi=sra">S Venkatesh</a> - Proceedings of the 12th annual ACM  &hellip;, 2004 - dl.acm.org</div><div class="gs_rs">Abstract We present a new aspect of our ongoing research aimed at providing technology <br>for the amateur home videographer. We aim to enable the production of quality video <br>presentations that are well structured and use the expressive properties of the medium to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17076514405845048576&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:AB08emD3--wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17076514405845048576&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'AB08emD3--wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:333"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4106487" class=yC93>Content-adaptive video summarization combining queueing and clustering</a></h3><div class="gs_a">T Liu, R Katpelly - Image Processing, 2006 IEEE International  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents an efficient three-step algorithm to compute key frames for <br>summarization and indexing of digital videos. In the preprocessing step, we remove the <br>video frames that constitute the gradual transitions of video shots using an entropy-based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8888932969120253140&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:1MSrmHDVW3sJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8888932969120253140&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'1MSrmHDVW3sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:332"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB68" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW68"><a href="http://csce.uark.edu/~jgauch/library/Video/Si.1998.pdf" class=yC95><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uark.edu</span><span class="gs_ggsS">uark.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=330914" class=yC94>Modeling video objects in 4DIS temporal database system</a></h3><div class="gs_a">A Si, RWJ Lau, <a href="/citations?user=D1LEg-YAAAAJ&amp;hl=en&amp;oi=sra">Q Li</a>, HV Leong - &hellip;  of the 1998 ACM symposium on  &hellip;, 1998 - dl.acm.org</div><div class="gs_rs">ABSTRACT Video objects are temporal in nature~ A video object is composed of a set of <br>video frames which are related in a total time ordering. By imposing additional timing <br>constraints among video frames, various presentation operators on video objects could be <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9299335001453630928&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:0BUA8enfDYEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9299335001453630928&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'0BUA8enfDYEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:331"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/9t6aalmj7x3fw4at.pdf" class=yC96>Modeling dynamic objects in video databases: a logic based approach</a></h3><div class="gs_a">B Acharya, A Majumdar, J Mukherjee - Conceptual ModelingâER 2001, 2001 - Springer</div><div class="gs_rs">In this work we propose a state-based approach to model video data so that the semantics of <br>the video is specified by the properties (static and dynamic) of the objects present in the <br>video. The object dynamics is captured by the states and state transitions. The video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12580580330840458877&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:fQYD-Cczl64J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/14/0E/RN105573807.html?source=googlescholar" class="gs_nph" class=yC97>BL Direct</a> <a href="/scholar?cluster=12580580330840458877&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'fQYD-Cczl64J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:330"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB70" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW70"><a href="http://www.arocmag.com/ch/reader/create_pdf.aspx?file_no=167&amp;flag=1&amp;journal_id=arocmag" class=yC99><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arocmag.com</span><span class="gs_ggsS">arocmag.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.arocmag.com/ch/reader/create_pdf.aspx?file_no=167&amp;flag=1&amp;journal_id=arocmag" class=yC98>è¯­ä¹è§é¢æ£ç´¢çç°ç¶åç ç©¶è¿å±</a></h3><div class="gs_a">ä½å«å®ï¼ è°¢èå©ï¼ ä½è±æï¼ æ½æè - è®¡ç®æºåºç¨ç ç©¶, 2005 - arocmag.com</div><div class="gs_rs">æè¦: æ¦è¿°äºå¾åçå¯è§åç¹å¾å¦é¢è², çº¹ç, å½¢ç¶åè¿å¨ä¿¡æ¯, æ¶ç©ºå³ç³»åæ, <br>ä»¥åå¤ç¹å¾ç®æ æååç¸ä¼¼åº¦éåº¦; åæäºè§é¢è¯­ä¹çæå, è¯­ä¹æ¥è¯¢, æ£ç´¢; <br>æ¢è®¨äºè§é¢è¯­ä¹æ£ç´¢çæ§è½è¯ä¼°, å­å¨çé®é¢ååå±æ¹å. å³é®è¯: ç¹å¾æå; å¾åæ£ç´¢; æ¶ç©º<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4026296467961634335&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:H15DLU5H4DcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4026296467961634335&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'H15DLU5H4DcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md70', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md70" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:H15DLU5H4DcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:329"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB71" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW71"><a href="http://www.arislab.ynu.ac.jp/thesis/outside/Advances_in_Database_Technologies.pdf" class=yC9B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ynu.ac.jp</span><span class="gs_ggsS">ynu.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.arislab.ynu.ac.jp/thesis/outside/Advances_in_Database_Technologies.pdf" class=yC9A>Extracting event semantics from video data based on Real World Database</a></h3><div class="gs_a">K Salev, T Tomii, H Arisawa - Lecture notes in computer science, 1999 - arislab.ynu.ac.jp</div><div class="gs_rs">Abstract. Video is considered the most effective medium for capturing events. There are two <br>major approaches of representing video contents: the structured modeling approach and <br>stratification. They require video to be divided into simple semantic units on top of which <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13931252811657525015&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:FzcCC5_AVcEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/48/53/RN058374970.html?source=googlescholar" class="gs_nph" class=yC9C>BL Direct</a> <a href="/scholar?cluster=13931252811657525015&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'FzcCC5_AVcEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:328"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB72" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW72"><a href="http://researchbank.rmit.edu.au/eserv/rmit:6871/Zhao.pdf" class=yC9E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from rmit.edu.au</span><span class="gs_ggsS">rmit.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://researchbank.rmit.edu.au/view/rmit:6871" class=yC9D>Effective authorship attribution in large document collections</a></h3><div class="gs_a">Y Zhao - 2007 - researchbank.rmit.edu.au</div><div class="gs_rs">Abstract Techniques that can effectively identify authors of texts are of great importance in <br>scenarios such as detecting plagiarism, and identifying a source of information. A range of <br>attribution approaches has been proposed in recent years, but none of these are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3781076701355512834&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:AhTNFDoVeTQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3781076701355512834&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'AhTNFDoVeTQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:327"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB73" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW73"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.547&amp;rep=rep1&amp;type=pdf" class=yCA0><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0020025501001967" class=yC9F>A mask matching approach for video segmentation on compressed data</a></h3><div class="gs_a">TCT Kuo, ALP Chen - Information Sciences, 2002 - Elsevier</div><div class="gs_rs">Video segmentation provides an easy and efficient way for video retrieval and browsing. A <br>frame is detected as a shot change frame if its content is very different from its previous <br>frames. The process of segmenting videos into shots is usually time consuming due to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4990863603053652070&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:ZmhwFgEcQ0UJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/23/21/RN107230889.html?source=googlescholar" class="gs_nph" class=yCA1>BL Direct</a> <a href="/scholar?cluster=4990863603053652070&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'ZmhwFgEcQ0UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:326"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB74" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW74"><a href="http://infoscience.epfl.ch/record/55766/files/TR01_030.pdf" class=yCA3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from epfl.ch</span><span class="gs_ggsS">epfl.ch <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/fvbjvvulrwcw16e9.pdf" class=yCA2>A conceptual model for remote data acquisition systems</a></h3><div class="gs_a">T Nieva, A Wegmann - Conceptual ModelingâER 2000, 2000 - Springer</div><div class="gs_rs">Data acquisition systems (DAS) are the basis for building monitoring tools that enable <br>supervision of local and remote systems. Unfortunately, DASs are commonly based on <br>proprietary technologies. The data format usually depends on the industrial process, the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7078823226948233783&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:N6oIPjgIPWIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/08/4E/RN085688194.html?source=googlescholar" class="gs_nph" class=yCA4>BL Direct</a> <a href="/scholar?cluster=7078823226948233783&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 21 versions</a> <a onclick="return gs_ocit(event,'N6oIPjgIPWIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:325"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB75" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW75"><a href="http://www-mrim.imag.fr/publications/2003/PM001/video-annot.pdf" class=yCA6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from imag.fr</span><span class="gs_ggsS">imag.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-mrim.imag.fr/publications/2003/PM001/video-annot.pdf" class=yCA5>Semantic video annotation and vague query</a></h3><div class="gs_a">Q Zhang, MS Kankanhalli, P Mulhem - Proc. 9th International  &hellip;, 2003 - www-mrim.imag.fr</div><div class="gs_rs">The Digital Video Album (DVA) system described here integrates various cooperating <br>subsystems to index and query video documents according to their semantic content and <br>other metadata. A simple structured model is proposed to represent the video content. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9538509705575055652&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:JMFqPwqYX4QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9538509705575055652&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'JMFqPwqYX4QJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md75', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md75" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:JMFqPwqYX4QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:324"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT6337691&amp;id=tgcKAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yCA7>Image data transfer</a></h3><div class="gs_a">J Trainor - US Patent 6,337,691, 2002 - Google Patents</div><div class="gs_rs">Image data (derived from film or video clips) is transferred from storage to high speed <br>memory. After a transfer has taken place, a prediction is made as to subsequent image <br>frames that will need to be transferred. The predicted images are transferred from storage <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4225492271451518764&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:LLcGUdX2ozoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4225492271451518764&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'LLcGUdX2ozoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:323"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/m723876104t14nug.pdf" class=yCA8>A two-level queueing system for interactive browsing and searching of video content</a></h3><div class="gs_a">T Liu, R Katpelly - Multimedia systems, 2007 - Springer</div><div class="gs_rs">Abstract This paper presents a two-level queueing system for dynamic summarization and <br>interactive searching of video content. Video frames enter the queueing system; some <br>insignificant and redundant frames are removed; the remaining frames are pulled out of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16896522200210254128&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:MKkIY2uBfOoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/51/5E/RN202853456.html?source=googlescholar" class="gs_nph" class=yCA9>BL Direct</a> <a href="/scholar?cluster=16896522200210254128&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'MKkIY2uBfOoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:322"><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/data/Conferences/SPIEP/33213/74_1.pdf" class=yCAA>State-chart-based approach for modeling video of dynamic objects</a></h3><div class="gs_a">B Acharya, AK Majumdar&hellip; - Proceedings of  &hellip;, 2001 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">ABSTRACT In this work we propose a new approach to model video data. To interpret the <br>video semantic, we propose to model the video on the basis of the underlying dynamics <br>contained in the video. Thus, the video is seen as a measurement of properties of objects <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15116007753754860704&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:oNybyPbYxtEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/18/23/RN108656669.html?source=googlescholar" class="gs_nph" class=yCAB>BL Direct</a> <a href="/scholar?cluster=15116007753754860704&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'oNybyPbYxtEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:321"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB79" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW79"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.189.3774&amp;rep=rep1&amp;type=pdf" class=yCAD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.189.3774&amp;rep=rep1&amp;type=pdf" class=yCAC>The Influence of Context and Interactivity on Video Browsing</a></h3><div class="gs_a">BM Wildemuth, <a href="/citations?user=HeL4Y2EAAAAJ&amp;hl=en&amp;oi=sra">T Russell</a>, TJ Ward&hellip; - UNC (SILS) Technical  &hellip;, 2006 - Citeseer</div><div class="gs_rs">Conclusions: The effects of context on browsing search results were negligible, but should <br>be explored further through re-examination of the definition and operationalization of the <br>concept of context. Interactivity, in combination with context, had positive effects on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5075528837103184108&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:7GjJBpbmb0YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5075528837103184108&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'7GjJBpbmb0YJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md79', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md79" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:7GjJBpbmb0YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:320"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7930624&amp;id=n5HLAQAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yCAE>Editing time-based media with enhanced content</a></h3><div class="gs_a">M Phillips, B Cooper, L Fay - US Patent 7,930,624, 2011 - Google Patents</div><div class="gs_rs">Creation of a program with interactive content and time-based media would be improved by <br>having several people working simultaneously on both the interactive content and the time-<br>based media. The range of types of data that can be associated with the time-based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15937806686376210331&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:m69MOL12Lt0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15937806686376210331&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'m69MOL12Lt0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:319"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/77052L7657R13M0V.pdf" class=yCAF>A movie classifier based on visual features</a></h3><div class="gs_a">HY Huang, WS Shih, WH Hsu - Computer Analysis of Images and Patterns, 2007 - Springer</div><div class="gs_rs">In this paper, we propose an approach to classify the film categories by using low-level <br>features and visual features. The goal of this approach is to classify the films into genres. Our <br>current domain of study is the movie preview. A film preview often emphasizes the theme <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=795631040320350934&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:1qqROjemCgsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5E/55/RN215438210.html?source=googlescholar" class="gs_nph" class=yCB0>BL Direct</a> <a href="/scholar?cluster=795631040320350934&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'1qqROjemCgsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:318"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB82" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW82"><a href="http://make.cs.nthu.edu.tw/makeWeb/alp/alp_paper/A%20Semantic%20Model%20for%20Video%20Description%20and%20Retrieval.pdf" class=yCB2><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nthu.edu.tw</span><span class="gs_ggsS">nthu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/91N3L8C228THFQJ7.pdf" class=yCB1>A semantic model for video description and retrieval</a></h3><div class="gs_a">CH Lin, A Lee, A Chen - Advances in Multimedia Information Processingâ &hellip;, 2002 - Springer</div><div class="gs_rs">In this paper, a semantic video retrieval system is proposed based on the stories of the <br>videos. A hierarchical knowledge model is used to express the semantic meanings <br>contained in the videos, and a video query language is also provided. The terms of Object, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18080811157467515774&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:fkvFk_Px6_oJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4C/10/RN124345224.html?source=googlescholar" class="gs_nph" class=yCB3>BL Direct</a> <a href="/scholar?cluster=18080811157467515774&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'fkvFk_Px6_oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:317"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB83" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW83"><a href="https://doc.freeband.nl/dsweb/Get/Document-8268/GigaCE-Moving%20Storyboards_%20a%20novel%20approach%20to%20content-based%20video%20retrieval.pdf" class=yCB5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from freeband.nl</span><span class="gs_ggsS">freeband.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://doc.freeband.nl/dsweb/Get/Document-8268/GigaCE-Moving%20Storyboards_%20a%20novel%20approach%20to%20content-based%20video%20retrieval.pdf" class=yCB4>Moving storyboards: a novel approach to content-based video retrieval</a></h3><div class="gs_a">GC Van den Eijkel, PAP Porskamp&hellip; - Telematica Instituut  &hellip;, 2000 - doc.freeband.nl</div><div class="gs_rs">ABSTRACT Moving storyboards is a novel approach to content-based video retrieval and <br>combines the advantages of video segmentation techniques with intuitive and user-friendly <br>presentation mechanisms. KEYWORDS: Video Segmentation, Storyboards, Content <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7828720648768154333&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:3WqXo_MzpWwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7828720648768154333&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'3WqXo_MzpWwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md83', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md83" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:3WqXo_MzpWwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:316"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB84" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW84"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.8397&amp;rep=rep1&amp;type=pdf" class=yCB7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/w251251128v83207.pdf" class=yCB6>Supporting video applications through 4DIS temporal framework 4DIS temporal framework</a></h3><div class="gs_a">R Lau, H Leong, <a href="/citations?user=D1LEg-YAAAAJ&amp;hl=en&amp;oi=sra">Q Li</a>, A Si - Multimedia Information Analysis and Retrieval, 1998 - Springer</div><div class="gs_rs">Video has become an essential component of multimedia applications nowadays. To <br>support multimedia and video applications efficiently, video objects must be modeled and <br>structured in secondary storage effectively, allowing flexible; easy, and efficient retrieval. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6187150828210028735&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:vwA5t-Us3VUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5A/2E/RN048580191.html?source=googlescholar" class="gs_nph" class=yCB8>BL Direct</a> <a href="/scholar?cluster=6187150828210028735&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'vwA5t-Us3VUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:315"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB85" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW85"><a href="http://www2002.org/CDROM/alternate/683/" class=yCBA><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from www2002.org</span><span class="gs_ggsS">www2002.org <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[HTML]</span><span class="gs_ct2">[HTML]</span></span> <a href="http://www2002.org/CDROM/alternate/683/" class=yCB9>COVA: A System for content-based distance learning</a></h3><div class="gs_a">G Cha - Proceedings International WWW Conference, 2002 - www2002.org</div><div class="gs_rs">ABSTRACT Education and training are expected to change dramatically due to the <br>combined impact of the Internet, database, and multi-media technologies. However, the <br>distance learning is often impeded by the lack of effective tools and system to manage and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10877417654368395785&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:CeIwwrlZ9JYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10877417654368395785&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'CeIwwrlZ9JYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md85', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md85" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:CeIwwrlZ9JYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:314"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4050069" class=yCBB>An interactive system for video content exploration</a></h3><div class="gs_a">T Liu, R Katpelly - Consumer Electronics, IEEE Transactions on, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A user-centered video exploration system is essential for video content browsing <br>and searching. In this paper, we present a system that facilitates video content exploration to <br>any level of detail within any video segment. The proposed system allows semantic <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12785168916724457613&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:jSC6GmQLbrEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/43/1C/RN203493949.html?source=googlescholar" class="gs_nph" class=yCBC>BL Direct</a> <a href="/scholar?cluster=12785168916724457613&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'jSC6GmQLbrEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:313"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB87" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW87"><a href="http://pdf.aminer.org/000/246/175/videomap_a_generic_framework_for_video_management_and_application_processing.pdf" class=yCBE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aminer.org</span><span class="gs_ggsS">aminer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://pdf.aminer.org/000/246/175/videomap_a_generic_framework_for_video_management_and_application_processing.pdf" class=yCBD>Development of a web-based video management and application processing system</a></h3><div class="gs_a">SSM Chan, Y Wu, <a href="/citations?user=D1LEg-YAAAAJ&amp;hl=en&amp;oi=sra">Q Li</a>, Y Zhuang - Proc. SPIE Int&#39;l Symp. on the  &hellip;, 2001 - pdf.aminer.org</div><div class="gs_rs">ABSTRACT How to facilitate efficient video manipulation and access in a web-based <br>environment is becoming a popular trend for video applications. In this paper, we present a <br>web-oriented video management and application processing system, based on our <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9996257664288730070&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:1qs0G2LXuYoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/29/21/RN104428290.html?source=googlescholar" class="gs_nph" class=yCBF>BL Direct</a> <a href="/scholar?cluster=9996257664288730070&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'1qs0G2LXuYoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md87', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md87" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:1qs0G2LXuYoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:312"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB88" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW88"><a href="http://server.cs.ucf.edu/~vision/papers/theses/yun_theses.pdf" class=yCC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucf.edu</span><span class="gs_ggsS">ucf.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://server.cs.ucf.edu/~vision/papers/theses/yun_theses.pdf" class=yCC0>Video content extraction: Scene segmentation, linking and attention detection</a></h3><div class="gs_a"><a href="/citations?user=tQKD7T8AAAAJ&amp;hl=en&amp;oi=sra">Y Zhai</a> - 2006 - server.cs.ucf.edu</div><div class="gs_rs">Abstract In this fast paced digital age, a vast amount of videos are produced every day, such <br>as movies, TV programs, personal home videos, surveillance video, etc. This places a high <br>demand for effective video data analysis and management techniques. In this dissertation, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9386446955899964095&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:v8p2XcZbQ4IJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9386446955899964095&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'v8p2XcZbQ4IJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md88', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md88" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:v8p2XcZbQ4IJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:311"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7555557&amp;id=-pjHAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yCC2>Review and approval system</a></h3><div class="gs_a">J Bradley, PJ Gray, G Lea, ME Phillips&hellip; - US Patent  &hellip;, 2009 - Google Patents</div><div class="gs_rs">The review and Approval system of the present invention advantageously provides <br>computer implemented access for a reviewer to digital content for the purpose of reviewing <br>and approving the digital content. The system is capable of communication with an editing <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17154275136772777001&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:KSSoglk6EO4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17154275136772777001&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'KSSoglk6EO4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:310"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB90" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW90"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8389&amp;rep=rep1&amp;type=pdf" class=yCC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8389&amp;rep=rep1&amp;type=pdf" class=yCC3>Towards Pseudo-object Models for Content-based Visual Information Retrieval</a></h3><div class="gs_a">TS Chua, M Kankanhalli - Intern. Symposium on Multimedia Information  &hellip;, 1998 - Citeseer</div><div class="gs_rs">Abstract Current image/video retrieval systems rely mainly on the visual features and text <br>annotations of images as the basis for retrieval. A typical feature such as the color histogram <br>only attempts to capture the main characteristics of the overall image. Although it is able to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14687254775147322955&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:S95Qzmac08sJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14687254775147322955&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'S95Qzmac08sJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md90', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md90" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:S95Qzmac08sJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:309"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Department of Information Systems and Computing MSc Multimedia Information Systems Academic Year 2001-2002 Title:-Web Based Application to  &hellip;</h3><div class="gs_a">RS Desai - 1895 - Brunel University</div><div class="gs_fl"><a href="/scholar?q=related:-OJeBBWPxJEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'-OJeBBWPxJEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:308"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB92" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW92"><a href="http://www.cs.umb.edu/~duc/publications/papers/jass01.pdf" class=yCC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from umb.edu</span><span class="gs_ggsS">umb.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cs.umb.edu/~duc/publications/papers/jass01.pdf" class=yCC5>A Graphical Object based Approach to Representing and Querying Video Data</a></h3><div class="gs_a">DA Tran, KA Hua, K Vu - cs.umb.edu</div><div class="gs_rs">The above features make VideoGraph very flexible in representing various metadata types <br>extracted from diverse information sources. To facilitate video retrieval, we also introduce a <br>formalism for the query language based on path expressions. Query processing involves <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:yuZHCqG_oUwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'yuZHCqG_oUwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md92', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md92" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:yuZHCqG_oUwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:307"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB93" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW93"><a href="ftp://pubftp.computer.org/MAGS/MULTIMED/mms/webEng/112486.pdf" class=yCC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from computer.org</span><span class="gs_ggsS">computer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="ftp://pubftp.computer.org/MAGS/MULTIMED/mms/webEng/112486.pdf" class=yCC7>Content-Based Lecture Access for Distance Learning on the Web</a></h3><div class="gs_a">GH Cha, CW Chung - pubftp.computer.org</div><div class="gs_rs">ABSTRACT Education and training are expected to change dramatically due to the <br>combined impact of the Internet, database, and multimedia technologies. However, the <br>distance learning is often impeded by the lack of effective tools and system to manage and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:x2OuC932GnsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8870673845303796679&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'x2OuC932GnsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md93', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md93" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:x2OuC932GnsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:306"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Video Modeling and Retrieval</h3><div class="gs_a">Y Zhang, TS Chua - 2001</div><div class="gs_fl"><a href="/scholar?q=related:dxR9EinL_EYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5115186654050325623&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'dxR9EinL_EYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:305"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/3412435683570113.pdf" class=yCC9>A database approach for expressive modeling and efficient querying of visual information</a></h3><div class="gs_a">A Azough, A Delteil, MS Hacid, F DeMarchi - Advances in Multimedia  &hellip;, 2010 - Springer</div><div class="gs_rs">Abstract. With the rapid development of multimedia technologies, the number of available <br>multimedia resources is always increasing and the need for efficient multimedia modeling, <br>indexing and retrieval techniques is growing. In this work, we propose a hierarchical, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:0rqwFag7V9UJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15372821446156925650&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'0rqwFag7V9UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:304"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> A TEMPORAL MULTI-RESOLUTION APPROACH TO VIDEO SHOT SEGMENTATION</h3><div class="gs_a">TS Chua, A Chandrashekhara, HM Feng - Handbook of video databases:  &hellip;, 2003 - CRC</div><div class="gs_fl"><a href="/scholar?q=related:AQM9FnC5kEQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'AQM9FnC5kEQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:303"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB97" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW97"><a href="http://make.cs.nthu.edu.tw/makeWeb/DBPaper/pdf/icmcs99_crf.pdf" class=yCCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nthu.edu.tw</span><span class="gs_ggsS">nthu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://make.cs.nthu.edu.tw/makeWeb/DBPaper/pdf/icmcs99_crf.pdf" class=yCCA>Jia-Ling Koh</a></h3><div class="gs_a">CS Lee, ALP Chen - make.cs.nthu.edu.tw</div><div class="gs_rs">Abstract Traditional research on video data retrieval follows two general approaches. One is <br>based on text annotation and the other on content-based comparison. However, these <br>approaches do not fully make use of the meaning implied in a video stream. To improve <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:d6HbGGK07QIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'d6HbGGK07QIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md97', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md97" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:d6HbGGK07QIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:302"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> A Film Classifier Based on Low-level Visual</h3><div class="gs_a">HY Huang, WS Shih, WH Hsu</div><div class="gs_fl"><a href="/scholar?q=related:F3rTJEcuKXcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'F3rTJEcuKXcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:301"><div class="gs_ri"><h3 class="gs_rt"><a href="http://spiedigitallibrary.org/data/Conferences/SPIEP/60981/252_1.pdf" class=yCCC>4 Place Jussieu Centre de Royallieu 4 Place Jussieu F-75252 Paris cedex 05 F-60200 CompiÃ¨gne F-75252 Paris cedex 05</a></h3><div class="gs_a">P Faudemay, <a href="/citations?user=VOPW5YYAAAAJ&amp;hl=en&amp;oi=sra">L Chen</a>, C MontaciÃ©, MJ Caraty&hellip; - spiedigitallibrary.org</div><div class="gs_rs">ABSTRACT A video is a multimedia document which is structured in scenes and shots. <br>Scenes are lists of consecutive shots characterized by common visual and audio features. <br>Shots are sets of consecutive frames separated by cuts, which can be easily recognized <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:DwviNNNrRZMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'DwviNNNrRZMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
