Total results = 17
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.9403&amp;rep=rep1&amp;type=pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.9403&amp;rep=rep1&amp;type=pdf" class=yC0>Automated annotations of synchronized multimedia presentations</a></h3><div class="gs_a">H Sack, J Waitelonis - In Proceedings of the ESWC 2006 Workshop on  &hellip;, 2006 - Citeseer</div><div class="gs_rs">Abstract. Semantic annotation of multimedia data for improving search engine performance <br>is an important issue. We have focused on the automated annotation of video recordings of <br>university lectures. Most times these lectures are supported by a desktop presentation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1891753662913540739&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 16</a> <a href="/scholar?q=related:g3KB-O7bQBoJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1891753662913540739&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'g3KB-O7bQBoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md0', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md0" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:g3KB-O7bQBoJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www1bpt.bridgeport.edu/~jelee/pubs/VideoAbstraction.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bridgeport.edu</span><span class="gs_ggsS">bridgeport.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=2Z3LcUZvZIwC&amp;oi=fnd&amp;pg=PA321&amp;ots=ApONea4Wbc&amp;sig=eB9UmXm2LDJmEvQa1Y6VMDyD1jw" class=yC2>Video abstraction</a></h3><div class="gs_a">J Oh, Q Wen, <a href="/citations?user=CCOAVjAAAAAJ&amp;hl=en&amp;oi=sra">J Lee</a>, S Hwang - 2004 - books.google.com</div><div class="gs_rs">ABSTRACT This chapter introduces Video Abstraction, which is a short representation of an <br>original video, and widely used in video cataloging, indexing, and retrieving. It provides a <br>general view of video abstraction and presents different methods to produce various video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16877988807179254615&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 13</a> <a href="/scholar?q=related:VzvwjmWpOuoJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16877988807179254615&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'VzvwjmWpOuoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1413922" class=yC4>Scene detection using visual and audio attention</a></h3><div class="gs_a">A Chianese, <a href="/citations?user=0FXky6Cr1RoC&amp;hl=en&amp;oi=sra">V Moscato</a>, A Penta&hellip; - Proceedings of the 2008  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Shot and scene segmentation are basic steps for a variety of applications in video <br>analysis and processing. In this paper, we propose a new method for automatic scene <br>detection which takes visual patterns of movies and audio features into account. In <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11428508247080462899&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 6</a> <a href="/scholar?q=related:M2raK7M3mp4J:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'M2raK7M3mp4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5590486" class=yC5>Synchronization E-learning Model for Harmonizing presentation</a></h3><div class="gs_a"><a href="/citations?user=IOJvECUAAAAJ&amp;hl=en&amp;oi=sra">S Kim</a>, YI Yoon - &hellip;  and Information Science (ICIS), 2010 IEEE/ &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The rapid advances in multimedia and e-learning technologies let us present text, <br>music, video and oral instruction on computers in more vivid ways. The problem is that <br>learning environment has just single information media. That does not include multimedia <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6273719040137851538&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 2</a> <a href="/scholar?q=related:knIFODq6EFcJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6273719040137851538&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'knIFODq6EFcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1623806" class=yC6>Experience of Animate Similarity Concepts in Multimedia Database</a></h3><div class="gs_a"><a href="/citations?user=s6zNFvAAAAAJ&amp;hl=en&amp;oi=sra">M Albanese</a>, A Chianese, <a href="/citations?user=0FXky6Cr1RoC&amp;hl=en&amp;oi=sra">V Moscato</a>&hellip; - Data Engineering  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes and reports an experience related to several applications of <br>the Animate Vision Paradigm to query processing and indexing in multimedia databases. In <br>particular we show as two main problems related to video and image databases may be <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1016919881078350924&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 2</a> <a href="/scholar?q=related:TCS1dD7THA4J:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1016919881078350924&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'TCS1dD7THA4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5591244" class=yC7>Synchronization Enterprise Model for Harmonizing Presentation</a></h3><div class="gs_a">S Kim, YI Yoon - &hellip;  and Information Science (ICIS), 2010 IEEE/ &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Despite the increasing importance gained by enterprise standards in the past few <br>years, and the unquestionable goals reached (mainly regarding interoperability among <br>learning enterprise model) current enterprise standards are yet not sufficiently aware of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10767060941669781829&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 1</a> <a href="/scholar?q=related:RdE1T-FIbJUJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10767060941669781829&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'RdE1T-FIbJUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/n656042765177523.pdf" class=yC8>Scene extraction system for video clips using attached comment interval and pointing region</a></h3><div class="gs_a">S Wakamiya, D Kitayama, K Sumiya - Multimedia Tools and Applications, 2011 - Springer</div><div class="gs_rs">Abstract A method was developed to enable users of video sharing websites to easily <br>retrieve video scenes relevant to their interests. The system analyzes both text and non-text <br>aspects of a user&#39;s comment and then retrieves and displays relevant scenes along with <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3028649131801421512&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 2</a> <a href="/scholar?q=related:yKZRwlbsByoJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3028649131801421512&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'yKZRwlbsByoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1722746" class=yC9>Video shot boundary detection algorithm using LZW compression technique</a></h3><div class="gs_a">SV Basavaraja, S Velusamy&hellip; - Proceedings of the Fifth  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract We have proposed a efficient and robust shot segmentation algorithm that is based <br>on a well known Lempel-Ziv-Welch (LZW) text compression technique. The algorithm works <br>on the fact that, when a frame is compressed using its previous/reference frame <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5406435352072630047&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 1</a> <a href="/scholar?q=related:Hy9n5U6EB0sJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5406435352072630047&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Hy9n5U6EB0sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://18.7.29.232/bitstream/handle/1721.1/36791/79625196.pdf?sequence=1" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 18.7.29.232</span><span class="gs_ggsS">18.7.29.232 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://18.7.29.232/handle/1721.1/36791" class=yCA>System for rapid subtitling</a></h3><div class="gs_a">H Abelson, SJ Leonard - 2005 - 18.7.29.232</div><div class="gs_rs">A system for rapid subtitling of audiovisual sequences was developed, and evaluated. This <br>new system resulted in average time-savings of 50% over the previous work in the field. To <br>subtitle a 27-minute English lecture, users saved 2.2 hours, spending an average of 1.9 <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:2RlZCHib4CwJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'2RlZCHib4CwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320310001446" class=yCC>Travelmedia: An intelligent management system for media captured in travel</a></h3><div class="gs_a"><a href="/citations?user=DcltNjQAAAAJ&amp;hl=en&amp;oi=sra">WT Chu</a>, CJ Li, SC Tseng - Journal of Visual Communication and Image  &hellip;, 2011 - Elsevier</div><div class="gs_rs">A media management system exploiting characteristics of travel media is designed to <br>facilitate efficient management and browsing. According to travel schedules, travel media <br>often have implicit thematic structure. Correlation between different modalities also <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:5vbXUiWT6-kJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16855727818877171430&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'5vbXUiWT6-kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="https://circle.ubc.ca/bitstream/handle/2429/5790/ubc_2009_spring_chan_clarence.pdf?sequence=1" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ubc.ca</span><span class="gs_ggsS">ubc.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://circle.ubc.ca/handle/2429/5790" class=yCD>A framework for the lightweight augmentation of webcast archives</a></h3><div class="gs_a">C Chan - 2008 - circle.ubc.ca</div><div class="gs_rs">We propose a framework for augmenting archives of webcast lectures at alow benefit/cost <br>ratio that finesses the issue of costly video post-production, while still significantly enhancing <br>the quality of the webcast. We argue that lightweight augmentations such as alternate <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:zua5VGPIQ0AJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4630765170815788750&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'zua5VGPIQ0AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://dspace.mit.edu/bitstream/handle/1721.1/36791/79625196.pdf?sequence=1" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mit.edu</span><span class="gs_ggsS">mit.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dspace.mit.edu/handle/1721.1/36791" class=yCF>System for rapid subtitling</a></h3><div class="gs_a">SJ Leonard - 2005 - dspace.mit.edu</div><div class="gs_rs">A system for rapid subtitling of audiovisual sequences was developed, and evaluated. This <br>new system resulted in average time-savings of 50% over the previous work in the field. To <br>subtitle a 27-minute English lecture, users saved 2.2 hours, spending an average of 1.9 <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:mkr-l8FAeNMJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15238000539430963866&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'mkr-l8FAeNMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:mkr-l8FAeNMJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=15487385077936650752&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212001543" class=yC11>Somebody helps me: Travel video scene detection using web-based context</a></h3><div class="gs_a"><a href="/citations?user=DcltNjQAAAAJ&amp;hl=en&amp;oi=sra">WT Chu</a>, CJ Li - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">We conduct video scene detection with the aids of web-based context, especially for travel <br>videos captured by amateur photographers in journeys. Correlations between personal <br>videos and predefined travel schedules, which are used to retrieve related data from <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MGAomLylNFQJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6067656827400118320&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MGAomLylNFQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://www.shse.u-hyogo.ac.jp/sumiya/report/MTAP_2010_wakamiya.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-hyogo.ac.jp</span><span class="gs_ggsS">u-hyogo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.shse.u-hyogo.ac.jp/sumiya/report/MTAP_2010_wakamiya.pdf" class=yC12>Scene Extraction System for Video Clips using Attached Comment Interval and Pointing Region</a></h3><div class="gs_a">SWD Kitayama, K Sumiya - shse.u-hyogo.ac.jp</div><div class="gs_rs">Abstract A method was developed to enable users of video sharing websites to easily <br>retrieve video scenes relevant to their interests. The system analyzes both text and nontext <br>aspects of a user&#39;s comment and then retrieves and displays relevant scenes along with <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'NlRLrq9j5IwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md13', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md13" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:NlRLrq9j5IwJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://www.ai.cs.kobe-u.ac.jp/publications_html/report/horie2.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kobe-u.ac.jp</span><span class="gs_ggsS">kobe-u.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ai.cs.kobe-u.ac.jp/publications_html/report/horie2.pdf" class=yC14>Video Structuring for Mixed Reality Application</a></h3><div class="gs_a">A Horie, K Uehara - Proceedings of the Annual Conference on  &hellip;, 2005 - ai.cs.kobe-u.ac.jp</div><div class="gs_rs">Abstract. In this paper, we propose a video structuring method for a mixed reality (MR) <br>application. MR is a technology in the field of communicating visual information to users. <br>Based on this interactive environment, MR applications are widely used in various fields, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:6muuztX_dgMJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=249668123126295530&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'6muuztX_dgMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md14', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md14" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:6muuztX_dgMJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www-kasm.nii.ac.jp/jsai2005/schedule/pdf/000266.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nii.ac.jp</span><span class="gs_ggsS">nii.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-kasm.nii.ac.jp/jsai2005/schedule/pdf/000266.pdf" class=yC16>è¤åç¾å®æã¢ããªã±ã¼ã·ã§ã³ã®ããã®æ åã®æ§é åã¨ãã®å®è£</a></h3><div class="gs_a">å æ±æ°ï¼ ä¸åé¦æ­ - www-kasm.nii.ac.jp</div><div class="gs_rs">In this paper, we propose a video structuring method for the mixed reality application. As the <br>video data is serial data, we cannot change the order between two scenes. However, some <br>kind of programs like the news program or the cooking program, we may change order <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:2XEyHAxO1HoJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'2XEyHAxO1HoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md15', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md15" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:2XEyHAxO1HoJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.fedoa.unina.it/605/" class=yC18>Indexing Techniques for Image and Video Databases: an approach based on Animate Vision Paradigm</a></h3><div class="gs_rs"><a href="https://support.google.com/websearch/bin/answer.py?answer=45449&hl=en">This site may harm your computer.</a></div><div class="gs_a"><a href="/citations?user=0FXky6Cr1RoC&amp;hl=en&amp;oi=sra">V Moscato</a> - 2006 - fedoa.unina.it</div><div class="gs_rs">[ITALIANO] In questo lavoro di tesi vengono presentate e discusse delle innovative tecniche <br>di indicizzazione per database video e di immagini basate sul paradigma della âAnimate <br>Visionâ(Visione Animata). Da un lato, sarÃ  mostrato come utilizzando, quali algoritmi di <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:iOuvXH-7zJsJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11226554126818077576&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'iOuvXH-7zJsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
