Total results = 38
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://staff.aist.go.jp/h.fujihara/pdf/ism2006_fujihara.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4061176" class=yC0>Automatic synchronization between lyrics and music CD recordings based on Viterbi alignment of segregated vocal signals</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a>, J Ogata, K Komatani&hellip; - &hellip; , 2006. ISM&#39;06.  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes a system that can automatically synchronize between <br>polyphonic musical audio signals and corresponding lyrics. Although there were methods <br>that can synchronize between monophonic speech signals and corresponding text <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13888616118180065235&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 50</a> <a href="/scholar?q=related:0y_jQcZGvsAJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13888616118180065235&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'0y_jQcZGvsAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.4864&amp;rep=rep1&amp;type=pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.4864&amp;rep=rep1&amp;type=pdf" class=yC2>A query-by-example technique for retrieving cover versions of popular songs with similar melodies</a></h3><div class="gs_a">WH Tsai, HM Yu, <a href="/citations?user=trzbZ3AAAAAJ&amp;hl=en&amp;oi=sra">HM Wang</a> - Int. Symp. on Music Information Retrieval ( &hellip;, 2005 - Citeseer</div><div class="gs_rs">ABSTRACT Retrieving audio material based on audio queries is an important and <br>challenging issue in the research field of content-based access to popular music. As part of <br>this research field, we present a preliminary investigation into retrieving cover versions of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16205548299560606019&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 38</a> <a href="/scholar?q=related:Q9m-WVGs5eAJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16205548299560606019&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'Q9m-WVGs5eAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Q9m-WVGs5eAJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.5510&amp;rep=rep1&amp;type=pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.5510&amp;rep=rep1&amp;type=pdf" class=yC4>Separation of vocals from polyphonic audio recordings</a></h3><div class="gs_a"><a href="/citations?user=CSujHJ0AAAAJ&amp;hl=en&amp;oi=sra">S Vembu</a>, S Baumann - Proc. ISMIR, 2005 - Citeseer</div><div class="gs_rs">ABSTRACT Source separation techniques like independent component analysis and the <br>more recent non-negative matrix factorization are gaining widespread use for the monaural <br>separation of individual tracks present in a music sample. The underlying principle behind <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16995744993622094260&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 39</a> <a href="/scholar?q=related:tPH3PQYE3esJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16995744993622094260&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'tPH3PQYE3esJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md2', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md2" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:tPH3PQYE3esJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www1.i2r.a-star.edu.sg/~hli/papers/101109TASL2006876756.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from a-star.edu.sg</span><span class="gs_ggsS">a-star.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4067048" class=yC6>Exploring vibrato-motivated acoustic features for singer identification</a></h3><div class="gs_a">TL Nwe, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - Audio, Speech, and Language Processing, IEEE &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Vibrato is a slightly tremulous effect imparted to vocal or instrumental tone for added <br>warmth and expressiveness through slight variation in pitch. It corresponds to a periodic <br>fluctuation of the fundamental frequency. It is common for a singer to develop a vibrato <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11242109298055456239&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 32</a> <a href="/scholar?q=related:7yW25Nf-A5wJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/03/0A/RN204606041.html?source=googlescholar" class="gs_nph" class=yC8>BL Direct</a> <a href="/scholar?cluster=11242109298055456239&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'7yW25Nf-A5wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://staff.aist.go.jp/m.goto/PAPER/ICASSP2006fujihara.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1661260" class=yC9>F0 estimation method for singing voice in polyphonic audio signal based on statistical vocal model and viterbi search</a></h3><div class="gs_a">H Fujihara, T Kitahara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a>&hellip; - &hellip; , Speech and Signal  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes a method for estimating F0s of vocal from polyphonic audio <br>signals. Because melody is sung by a singer in many musical pieces, the estimation of F0s <br>of the vocal part is useful for many applications. Based on existing multiple-F0 estimation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15662040097833481086&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 32</a> <a href="/scholar?q=related:fvNJ6Xy-WtkJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15662040097833481086&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'fvNJ6Xy-WtkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.csl.sony.fr/downloads/papers/2007/pachet-07a.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sony.fr</span><span class="gs_ggsS">sony.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4275079" class=yCB>Exploring billions of audio features</a></h3><div class="gs_a"><a href="/citations?user=rMvx0LYAAAAJ&amp;hl=en&amp;oi=sra">F Pachet</a>, P Roy - &hellip; -Based Multimedia Indexing, 2007. CBMI&#39;07.  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Many works in audio and image signal analysis are based on the use of&quot; features&quot; <br>to represent characteristics of sounds or images. Features are used in various ways, for <br>instance as inputs to classifiers to categorize automatically objects, eg for audio scene <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6035150020227929976&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 30</a> <a href="/scholar?q=related:eJfgwfcowVMJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6035150020227929976&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'eJfgwfcowVMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://downloads.hindawi.com/journals/asmp/2009/153017.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1592529" class=yCD>Analytical features: a knowledge-based approach to audio feature generation</a></h3><div class="gs_a"><a href="/citations?user=rMvx0LYAAAAJ&amp;hl=en&amp;oi=sra">F Pachet</a>, P Roy - EURASIP Journal on Audio, Speech, and Music  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract We present a feature generation system designed to create audio features for <br>supervised classification tasks. The main contribution to feature generation studies is the <br>notion of analytical features (AFs), a construct designed to support the representation of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9561310525116433615&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 17</a> <a href="/scholar?q=related:z8TEvEOZsIQJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9561310525116433615&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 18 versions</a> <a onclick="return gs_ocit(event,'z8TEvEOZsIQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255" class=yCF>Singing voice detection for karaoke application</a></h3><div class="gs_a">A Shenoy, Y Wu, Y Wang - Visual  &hellip;, 2005 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract We present a framework to detect the regions of singing voice in musical audio <br>signals. This work is oriented towards the development of a robust transcriber of lyrics for <br>karaoke applications. The technique leverages on a combination of low-level audio <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 11</a> <a href="/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3A/63/RN177222664.html?source=googlescholar" class="gs_nph" class=yC11>BL Direct</a> <a href="/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'WkuJAZ0HHakJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4518087" class=yC12>On fusion of timbre-motivated features for singing voice detection and singer identification</a></h3><div class="gs_a">TL Nwe, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - Acoustics, Speech and Signal Processing, 2008. &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Timbre is the quality of sound which allows the ear to distinguish between musical <br>sounds. In this paper, we study timbre effects in identification of singing voice segments in <br>popular songs. Firstly, we identify between singing voice and instrumental segments in a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16962847299029156860&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 8</a> <a href="/scholar?q=related:_MPT1b4jaOsJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16962847299029156860&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'_MPT1b4jaOsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://pdf.aminer.org/000/439/676/language_identification_in_vocal_music.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aminer.org</span><span class="gs_ggsS">aminer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://pdf.aminer.org/000/439/676/language_identification_in_vocal_music.pdf" class=yC13>Language identification in vocal music</a></h3><div class="gs_a">J Schwenninger, R Brueckner, D Willett&hellip; - Proc. of the 7th  &hellip;, 2006 - pdf.aminer.org</div><div class="gs_rs">Abstract Language identification is an important field in spoken language processing. The <br>identification of the language sung or spoken in music, however, has attracted only minor <br>attention so far. This, however, is an important task when it comes to categorizing, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12109642155100542412&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 7</a> <a href="/scholar?q=related:zBXdY2EXDqgJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12109642155100542412&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'zBXdY2EXDqgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:zBXdY2EXDqgJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://staff.aist.go.jp/m.goto/PAPER/IEEEJSTSP201110fujihara.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5876296" class=yC15>Lyricsynchronizer: Automatic synchronization system between musical audio signals and lyrics</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a>, J Ogata&hellip; - Selected Topics in Signal &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes a system that can automatically synchronize polyphonic <br>musical audio signals with their corresponding lyrics. Although methods for synchronizing <br>monophonic speech signals and corresponding text transcriptions by using Viterbi <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2316631857609331925&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 7</a> <a href="/scholar?q=related:1VQohGZUJiAJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2316631857609331925&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'1VQohGZUJiAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.9910&amp;rep=rep1&amp;type=pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.9910&amp;rep=rep1&amp;type=pdf" class=yC17>Automatic Characterization of Music Complexity: a multifaceted approach</a></h3><div class="gs_a">S Streich - &hellip;  Pompeu Fabra, Barcelona, Spain. Retrieved October, 2005 - Citeseer</div><div class="gs_rs">Abstract The aim of this work is to present the concept of a multi-faceted music complexity <br>descriptor set. The complexity of music is one of the less intensively researched areas in <br>music information retrieval. Especially an automated estimation based on the audio <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14908716645100938065&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 5</a> <a href="/scholar?q=related:UeMxiMxm5s4J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14908716645100938065&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'UeMxiMxm5s4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:UeMxiMxm5s4J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.3371&amp;rep=rep1&amp;type=pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA121&amp;ots=oDOSrGgwg1&amp;sig=gmPOPtfoAEYmBBWCaY2CfdhG-fs" class=yC19>Vocal segment classification in popular music</a></h3><div class="gs_a"><a href="/citations?user=9YG5UnQAAAAJ&amp;hl=en&amp;oi=sra">L Feng</a>, AB Nielsen, <a href="/citations?user=gQVuJh8AAAAJ&amp;hl=en&amp;oi=sra">LK Hansen</a> - 9th International Conference  &hellip;, 2008 - books.google.com</div><div class="gs_rs">ABSTRACT This paper explores the vocal and non-vocal music classiï¬cation problem within <br>popular songs. A newly built labeled database covering 147 popular songs is announced. It <br>is designed for classifying signals from 1sec time windows. Features are selected for this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7978986249417191583&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 3</a> <a href="/scholar?q=related:n_A8KLgNu24J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7978986249417191583&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'n_A8KLgNu24J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/Q8205NH13L1RH912.pdf" class=yC1B>Effectiveness of signal segmentation for music content representation</a></h3><div class="gs_a">N Maddage, M Kankanhalli, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - Advances in Multimedia Modeling, 2008 - Springer</div><div class="gs_rs">In this paper we compare the effectiveness of rhythm based signal segmentation technique <br>with the traditional fixed length segmentation for music contents representation. We consider <br>vocal regions, instrumental regions and chords which represent the harmony as different <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10581131093821192891&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 3</a> <a href="/scholar?q=related:u3oNKau615IJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/59/2C/RN221721717.html?source=googlescholar" class="gs_nph" class=yC1C>BL Direct</a> <a href="/scholar?cluster=10581131093821192891&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'u3oNKau615IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf" class=yC1D>æ­å£°æå ±å¦çã®æè¿ã®ç ç©¶</a></h3><div class="gs_a">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å° - æ¥æ¬é³é¿å­¦ä¼èª, 2008 - staff.aist.go.jp</div><div class="gs_rs">â Recent studies on singing information processing. ââ Masataka Goto, Takeshi Saitou, Tomoyasu <br>Nakano and Hiromasa Fujihara (National Institute of Ad- vanced Industrial Science and Technology <br>(AIST), Tsukuba, 305â8568) e-mail: m.goto@aist.go.jp  </div><div class="gs_fl"><a href="/scholar?cites=3835003938146142193&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 6</a> <a href="/scholar?q=related:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3835003938146142193&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'8cdsVcOrODUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md14', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md14" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1661330" class=yC1F>Vibrato-Motivated Acoustic Features for Singger Identification</a></h3><div class="gs_a"><a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, TL Nwe - Acoustics, Speech and Signal Processing, 2006. &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract It is common that a singer develops a vibrato to personalize his/her singing style. In <br>this paper, we explore the acoustic features that reflect vibrato information, to identify singers <br>of popular music. We start with an enhanced vocal detection method that allows us to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6742826986646219163&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 2</a> <a href="/scholar?q=related:myn3WGtVk10J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6742826986646219163&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'myn3WGtVk10J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607654" class=yC20>Using dtw based unsupervised segmentation to improve the vocal part detection in pop music</a></h3><div class="gs_a">L Xiao, J Zhou, T Zhang - Multimedia and Expo, 2008 IEEE  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Vocal part detection, which plays an important role in music information retrieval, is <br>still a tough task so far. Previous works focused on short time features, which cannot capture <br>some essential long term characteristics of singing. In this paper, we propose a Dynamic <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1816904981912120762&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 2</a> <a href="/scholar?q=related:um2CZXTxNhkJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'um2CZXTxNhkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5298607" class=yC21>Pitch oriented automatic singer identification in pop music</a></h3><div class="gs_a">P Chang - Semantic Computing, 2009. ICSC&#39;09. IEEE  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we proposed two novel methods used to distinguish the singer of a <br>pop music. We focused on a single singer and single track case. These two methods are <br>ldquoPitch Extractionrdquo method and ldquo1/12 OFCCrdquo method. The Pitch <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4456692914184476079&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 2</a> <a href="/scholar?q=related:r1X3xpda2T0J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4456692914184476079&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'r1X3xpda2T0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6022500" class=yC22>Automatic singer identification based on auditory features</a></h3><div class="gs_a">W Cai, Q Li, X Guan - Natural Computation (ICNC), 2011  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The paper describes a method of identifying singers&#39; voice from the monophonic <br>music including sounds of various musical instruments based on auditory features. In this <br>system, there are four problems to solve, vocal segment detection, feature extraction, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3824484272703074777&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 2</a> <a href="/scholar?q=related:2fGI2y5MEzUJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3824484272703074777&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'2fGI2y5MEzUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1823753" class=yC23>Word level automatic alignment of music and lyrics using vocal synthesis</a></h3><div class="gs_a">NC Maddage, <a href="/citations?user=jnU62sUAAAAJ&amp;hl=en&amp;oi=sra">KC Sim</a>, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - ACM Transactions on Multimedia  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract We propose a signal-based approach instead of the commonly used model-based <br>approach, to automatically align vocal music with text lyrics at the word level. In this <br>approach, we use a text-to-speech system to synthesize the singing voice according to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16739146216492474140&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 1</a> <a href="/scholar?q=related:HJ9bzctkTegJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16739146216492474140&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'HJ9bzctkTegJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4786093" class=yC24>Semi-Supervised Classification of Speaker&#39;s Psychological Stress</a></h3><div class="gs_a">S Torabi, F AlmasGanj&hellip; - &hellip; , 2008. CIBEC 2008.  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract It is well known that speech signal is affected by speaker&#39;s stress. Some of the <br>recent works have evaluated different acoustic features individually for the detection of <br>stress from speech. In our previous work, a new mixed feature (TEO-Pch-LFPC) was <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=996905120285770065&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 1</a> <a href="/scholar?q=related:UcFm0ey31Q0J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'UcFm0ey31Q0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://staff.aist.go.jp/m.goto/PAPER/SIGMUS200608fujihara.pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://staff.aist.go.jp/m.goto/PAPER/SIGMUS200608fujihara.pdf" class=yC25>é³æ¥½é³é¿ä¿¡å·ã¨æ­è©ã®æéçå¯¾å¿ä»ãææ³: æ­å£°ã®åé¢ã¨æ¯é³ã® Viterbi ã¢ã©ã¤ã³ã¡ã³ã</a></h3><div class="gs_a">è¤åå¼å°ï¼ å¾è¤çå­ï¼ ç·æ¹æ·³ï¼ é§è°·åç¯&hellip; - æå ±å¦çå­¦ä¼ç ç©¶å ±å. &hellip;, 2006 - staff.aist.go.jp</div><div class="gs_rs">æ¬ç¨¿ã§ã¯, ä¼´å¥é³ãå«ãé³æ¥½é³é¿ä¿¡å·ã¨å¯¾å¿ããæ­è©ã®æéçãªå¯¾å¿ä»ãææ³ã«ã¤ãã¦è¿°ã¹ã. <br>ã¯ãªã¼ã³ãªé³å£°ä¿¡å·ã¨ãã®çºè©±åå®¹ã®æéçå¯¾å¿ä»ããæ¨å®ããã Viterbi ã¢ã©ã¤ã³ã¡ã³ãææ³ã¯<br>ããã¾ã§ãå­å¨ããã, æ­å£°ã¨åæã«æ¼å¥ãããä¼´å¥é³ã®æªå½±é¿ã§å¸è²© CD ä¸­ã®æ­å£°ã«ã¯é©ç¨<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11217294609239502640&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=38">Cited by 1</a> <a href="/scholar?q=related:MO-f7APWq5sJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11217294609239502640&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MO-f7APWq5sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://www.cs.utoronto.ca/~dross/ChandrasekharSarginRoss_ICASSP2011.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utoronto.ca</span><span class="gs_ggsS">utoronto.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5947660" class=yC27>Automatic Language Identification in music videos with low level audio and visual features</a></h3><div class="gs_a"><a href="/citations?user=KUt4JCAAAAAJ&amp;hl=en&amp;oi=sra">V Chandrasekhar</a>, <a href="/citations?user=rrc2OZEAAAAJ&amp;hl=en&amp;oi=sra">ME Sargin</a>&hellip; - Acoustics, Speech and  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic Language Identification (LID) in music has received significantly less <br>attention than LID in speech. Here, we study the problem of LID in music videos uploaded <br>on YouTube. We use a&quot; bag-of-words&quot; approach based on state-of-the-art content based <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Gs_9FgBpIfoJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18023802632820084506&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'Gs_9FgBpIfoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-bonn.de</span><span class="gs_ggsS">uni-bonn.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf" class=yC29>Melody Separation from Polyphonic Audio Recordings</a></h3><div class="gs_a">S Vembu - 2005 - www-kd.iai.uni-bonn.de</div><div class="gs_rs">Abstract In the field of music information retrieval, query-by-humming is an interesting <br>application area in which humming sequences or melodies are matched with a database of <br>songs to come up with a list of indexed songs that are similar in melodic content to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16884184643397726041&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Wa_TF3qsUOoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md23', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md23" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Identifying Singers of Popular Songs</h3><div class="gs_a">TL Nwe, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - &hellip;  European Conference on Speech Communication and &hellip;, 2005</div><div class="gs_fl"><a href="/scholar?q=related:nvwHI3QIotoJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15754163741392370846&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'nvwHI3QIotoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/FT671731J0403662.pdf" class=yC2B>A Survey of Music Structure Analysis Techniques for Music Applications</a></h3><div class="gs_a">N Maddage, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, M Kankanhalli - Recent Advances in Multimedia Signal  &hellip;, 2009 - Springer</div><div class="gs_rs">Music carries multilayer information which forms different structures. The information <br>embedded in the music can be categorized into time information, harmony/melody, music <br>regions, music similarities, song structures and music semantics. In this chapter, we first <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3vo8SvZRZuUJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16529989600559299294&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'3vo8SvZRZuUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://drops.dagstuhl.de/opus/volltexte/2012/3464/pdf/3.pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dagstuhl.de</span><span class="gs_ggsS">dagstuhl.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://drops.dagstuhl.de/opus/volltexte/2012/3464/" class=yC2C>Lyrics-to-Audio Alignment and its Application}}</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Multimodal Music Processing} - drops.dagstuhl.de</div><div class="gs_rs">Abstract Automatic lyrics-to-audio alignment techniques have been drawing attention in the <br>last years and various studies have been made in this field. The objective of lyrics-to-audio <br>alignment is to estimate a temporal relationship between lyrics and musical audio signals <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:pR0KaNFrpC0J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3288872175025135013&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'pR0KaNFrpC0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Singing Voice Detection in Western Popular Music</h3><div class="gs_a">W Yuansheng</div><div class="gs_fl"><a href="/scholar?q=related:dx-cdoqIEO4J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'dx-cdoqIEO4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="https://ccrma.stanford.edu/~kglee/pubs/klee-sppra09-final.pdf" class=yC2F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from stanford.edu</span><span class="gs_ggsS">stanford.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.actapress.com/PaperInfo.aspx?PaperID=38587&amp;reason=500" class=yC2E>Automatic Labeling of Training Data for Singing Voice Detection in Musical Audio</a></h3><div class="gs_a">K Lee, M Cremer - Proceedings of the 6th IASTED International  &hellip;, 2009 - actapress.com</div><div class="gs_rs">AUTOMATIC LABELING OF TRAINING DATA FOR SINGING VOICE DETECTION IN MUSICAL<br>AUDIO Kyogu Lee Media Technology Lab, Gracenote 2000 Powell Street, Emeryville, CA94608,<br>USA klee@gracenote.com Markus Cremer Media Technology Lab, Gracenote 2000 <b>...</b> </div><div class="gs_fl"><a href="/scholar?q=related:_npQe5m2DPoJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18017976979517635326&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'_npQe5m2DPoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> VIBRATO-MOTIVATED ACOUSTIC FEATURES FOR SINGER IDENTIFICATION</h3><div class="gs_a"><a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">LI Haizhou</a>, TL NWE</div><div class="gs_fl"><a href="/scholar?q=related:ZYc_l3H1LTQJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3759931132141864805&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ZYc_l3H1LTQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4797191" class=yC30>An Effective Vocal/Non-vocal Segmentation Approach for Embedded Music Retrieve System on Mobile Phone</a></h3><div class="gs_a">H Tuo, H Li, K Lei - &hellip;  and Mobile Computing, 2009. CMC&#39;09. WRI &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the growing bodies of MP3 songs in Internet, content-based analysis plays an <br>important role for its retrieving and management. Due to most useful information is carried by <br>vocal portions, it is necessary to separate the vocal segments from music. This paper <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:44sDuDX6SGMJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7154243116705483747&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'44sDuDX6SGMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://www.iba-suk.edu.pk/ibasuk/faculty/MoviesMining.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iba-suk.edu.pk</span><span class="gs_ggsS">iba-suk.edu.pk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/C0668J24363G3640.pdf" class=yC31>Mining movie archives for song sequences</a></h3><div class="gs_a">SM Doudpota - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract Music and songs are integral parts of Bollywood movies. Every movie of two to <br>three hours, contains three to ten songs, each song is 3â10 min long. Music lovers like to <br>listen music and songs of a movie, however it is time consuming and error prone to search <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:I7AmwXJAY1wJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6657235535794712611&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'I7AmwXJAY1wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631272.1631364" class=yC33>Automatic and instant ring tone generation based on music structure analysis</a></h3><div class="gs_a">T Zhang, CK Fong, L Xiao, J Zhou - Proceedings of the 17th ACM  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Real tones, which are often excerpts from pop songs, have become popular as ring <br>tones. This paper describes how a ring tone can be produced by analyzing the structure of <br>music and selecting the most appropriate portion of the music. With audio feature analysis <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:YQzy0g82Tz8J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4561924389141089377&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'YQzy0g82Tz8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://jj-aucouturier.info/papers/PHD-2006.pdf" class=yC35><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jj-aucouturier.info</span><span class="gs_ggsS">jj-aucouturier.info <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://jj-aucouturier.info/papers/PHD-2006.pdf" class=yC34>Dix Experiences sur la Modelisation du Timbre Polyphonique</a></h3><div class="gs_a">S Bengio - jj-aucouturier.info</div><div class="gs_rs">Resume La grande majoritÃ© des systemes d&#39;extraction de metadonnÃ©es haut-niveaua partir <br>de signaux musicaux repose sur un modele implicite de leur âsonâ ou timbre polyphonique. <br>Ce modele reprÃ©sente le timbre comme la distribution statistique globale d&#39;attributs <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:IxbO610h6PwJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18223852579426539043&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'IxbO610h6PwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md33', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md33" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:IxbO610h6PwJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="https://www.uea.ac.uk/polopoly_fs/1.85551!steward_project2005.pdf" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uea.ac.uk</span><span class="gs_ggsS">uea.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://www.uea.ac.uk/polopoly_fs/1.85551!steward_project2005.pdf" class=yC36>FINAL YEAR PROJECT</a></h3><div class="gs_a">J Steward - 2004 - uea.ac.uk</div><div class="gs_rs">Mobile devices are an area of technology which is making huge progress in terms of <br>capabilities and size. Together with the advances in wireless technology, allowing some <br>devices to almost be constantly in communication with other devices or the internet has <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:6BP-7JpLH-0J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17086458640040072168&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'6BP-7JpLH-0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md34', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md34" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:6BP-7JpLH-0J:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.6223&amp;rep=rep1&amp;type=pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.6223&amp;rep=rep1&amp;type=pdf" class=yC38>Effectiveness of Signal Segmentation for Music Content Representation</a></h3><div class="gs_a">NCMMS Kankanhalli, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - Citeseer</div><div class="gs_rs">Abstract. In this paper we compare the effectiveness of rhythm based signal segmentation <br>technique with the traditional fixed length segmentation for music contents representation. <br>We consider vocal regions, instrumental regions and chords which represent the harmony <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:tis58lxAeiEJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2412311318355323830&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'tis58lxAeiEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md35', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md35" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:tis58lxAeiEJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2043615" class=yC3A>Beat space segmentation and octave scale cepstral feature for sung language recognition in pop music</a></h3><div class="gs_a">NC Maddage, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - ACM Transactions on Multimedia Computing,  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Sung language recognition relies on both effective feature extraction and acoustic <br>modeling. In this paper, we study rhythm based music segmentation with the frame size <br>being the duration of the smallest note in the music, as opposed to fixed length <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:PDNw-UBaogIJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'PDNw-UBaogIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf" class=yC3C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ircam.fr</span><span class="gs_ggsS">ircam.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.atiam.ircam.fr/Archives/Stages0708/Regnier.pdf" class=yC3B>DÃ©tection de la voix chantÃ©e dans un morceau de musique</a></h3><div class="gs_a">L REGNIER - atiam.ircam.fr</div><div class="gs_rs">RÃ©sumÃ© De tous les Ã©lÃ©ments constitutifs d&#39;un morceau de musique, la voix est sans <br>conteste celui qui focalise le plus l&#39;attention de l&#39;auditeur. Ceci provient du fait que la voix est <br>porteuse d&#39;un message (le texte) et d&#39;une identitÃ© (celle du chanteur). Pour ces raisons, de <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15229319373475501458&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'klX7IkhpWdMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md37', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md37" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:klX7IkhpWdMJ:scholar.google.com/&amp;hl=en&amp;num=38&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
