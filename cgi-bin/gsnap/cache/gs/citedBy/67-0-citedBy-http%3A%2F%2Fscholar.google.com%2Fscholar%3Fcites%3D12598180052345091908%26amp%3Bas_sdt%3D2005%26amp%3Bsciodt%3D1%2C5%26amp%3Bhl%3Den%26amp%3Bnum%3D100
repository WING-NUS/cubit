Total results = 67
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.ece.northwestern.edu/~jyu410/index_files/papers/p61-Yuan_Duan_Tian_MIR04.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from northwestern.edu</span><span class="gs_ggsS">northwestern.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1026722" class=yC0>Fast and robust short video clip search using an index structure</a></h3><div class="gs_a">J Yuan, LY Duan, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, C Xu - Proceedings of the 6th ACM SIGMM  &hellip;, 2004 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we present an index structure-based method to fast and robustly <br>search short video clips in large video collections. First we temporally segment a given long <br>video stream into overlapped matching windows, then map extracted features from the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1006236853978561669&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 72</a> <a href="/scholar?q=related:hXxMYRbf9g0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1006236853978561669&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'hXxMYRbf9g0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/trecvid04.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/trecvid04.pdf" class=yC2>TRECVID 2004 search and feature extraction task by NUS PRIS</a></h3><div class="gs_a">TS Chua, SY Neo, KY Li, G Wang, R Shi&hellip; - Proceedings of the  &hellip;, 2004 - 137.132.145.151</div><div class="gs_rs">ABSTRACT This paper describes the details of our systems for feature extraction and search <br>tasks of TRECVID-2004. For feature extraction, we emphasize the use of visual auto-concept <br>annotation technique, with the fusion of text and specialized detectors, to induce concepts <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4598184418179278710&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 60</a> <a href="/scholar?q=related:dudPuV0I0D8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4598184418179278710&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'dudPuV0I0D8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:dudPuV0I0D8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://vireo.cs.cityu.edu.hk/papers/csvt06.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1626302" class=yC4>Clip-based similarity measure for query-dependent clip retrieval and video summarization</a></h3><div class="gs_a">Y Peng, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Circuits and Systems for Video Technology,  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper proposes a new approach and algorithm for the similarity measure of <br>video clips. The similarity is mainly based on two bipartite graph matching algorithms: <br>maximum matching (MM) and optimal matching (OM). MM is able to rapidly filter irrelevant <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17400818532076996750&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 55</a> <a href="/scholar?q=related:jsh77UIgfPEJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/57/0B/RN196071348.html?source=googlescholar" class="gs_nph" class=yC6>BL Direct</a> <a href="/scholar?cluster=17400818532076996750&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'jsh77UIgfPEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/mmm05-young.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385976" class=yC7>Retrieval of news video using video sequence matching</a></h3><div class="gs_a">Y Kim, TS Chua - &hellip; , 2005. MMM 2005. Proceedings of the 11th  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose a new algorithm to find video clips with different temporal <br>durations and some spatial variations. We adopt a longest common sub-sequence (LCS) <br>matching technique for measuring the temporal similarity between video clips. Based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3349308960069176840&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 43</a> <a href="/scholar?q=related:CM7098Aiey4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3349308960069176840&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'CM7098Aiey4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/JWWW03-lekha.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/VL11P38010523613.pdf" class=yC9>A multi-modal approach to story segmentation for news video</a></h3><div class="gs_a">L Chaisorn, TS Chua, CH Lee - World Wide Web, 2003 - Springer</div><div class="gs_rs">This research proposes a two-level, multi-modal framework to perform the segmentation and <br>classification of news video into single-story semantic units. The video is analyzed at the <br>shot and story unit (or scene) levels using a variety of features and techniques. At the shot <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6108549097289369038&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 35</a> <a href="/scholar?q=related:zkX3HArtxVQJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5E/44/RN130983315.html?source=googlescholar" class="gs_nph" class=yCB>BL Direct</a> <a href="/scholar?cluster=6108549097289369038&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'zkX3HArtxVQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.4241&amp;rep=rep1&amp;type=pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/AAEC2F7CG65T7696.pdf" class=yCC>EMD-based video clip retrieval by many-to-many matching</a></h3><div class="gs_a">Y Peng, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Image and Video Retrieval, 2005 - Springer</div><div class="gs_rs">Abstract. This paper presents a new approach for video clip retrieval based on Earth Mover&#39;s <br>Distance (EMD). Instead of imposing one-to-one matching constraint as in [11, 14], our <br>approach allows many-to-many matching methodology and is capable of tolerating errors <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12465029817755646895&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 24</a> <a href="/scholar?q=related:rzv6oZOu_KwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3A/14/RN171992113.html?source=googlescholar" class="gs_nph" class=yCE>BL Direct</a> <a href="/scholar?cluster=12465029817755646895&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'rzv6oZOu_KwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://nthur.lib.nthu.edu.tw/bitstream/987654321/12196/1/2030144010014.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nthu.edu.tw</span><span class="gs_ggsS">nthu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1626305" class=yCF>Fast coarse-to-fine video retrieval using shot-level spatio-temporal statistics</a></h3><div class="gs_a">YH Ho, <a href="/citations?user=fXN3dl0AAAAJ&amp;hl=en&amp;oi=sra">CW Lin</a>, JF Chen&hellip; - Circuits and Systems for  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose a fast coarse-to-fine video retrieval scheme using shot-<br>level spatio-temporal statistics. The scheme consists of a two-step coarse search followed by <br>a fine search. In the coarse search stage, the shot-level motion and color distribution is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9785767271507840528&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 22</a> <a href="/scholar?q=related:EE7eeHwHzocJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3A/07/RN196071373.html?source=googlescholar" class="gs_nph" class=yC11>BL Direct</a> <a href="/scholar?cluster=9785767271507840528&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'EE7eeHwHzocJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://archive.itee.uq.edu.au/~zxf/_papers/TOIS.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uq.edu.au</span><span class="gs_ggsS">uq.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1508855" class=yC12>Bounded coordinate system indexing for real-time video clip search</a></h3><div class="gs_a">Z Huang, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, J Shao, <a href="/citations?user=y6m820wAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a>, <a href="/citations?user=IJAU8KoAAAAJ&amp;hl=en&amp;oi=sra">B Cui</a> - ACM Transactions on  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Recently, video clips have become very popular online. The massive influx of video <br>clips has created an urgent need for video search engines to facilitate retrieving relevant <br>clips. Different from traditional long videos, a video clip is a short video often expressing a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4125117981811267355&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 25</a> <a href="/scholar?q=related:G9dv2vJcPzkJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4125117981811267355&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'G9dv2vJcPzkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1027590" class=yC14>A color fingerprint of video shot for content identification</a></h3><div class="gs_a">X Yang, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, <a href="/citations?user=qZavFBcAAAAJ&amp;hl=en&amp;oi=sra">EC Chang</a> - Proceedings of the 12th annual ACM  &hellip;, 2004 - dl.acm.org</div><div class="gs_rs">Abstract In this paper we propose a novel space-time color feature representation for video <br>shot and apply it to content identification. In this representation the shot is cut into&lt; i&gt; k&lt;/i&gt; <br>equal size segments, and each segment is represented by a blending image formed <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17712528054091731075&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 21</a> <a href="/scholar?q=related:g1BHkmuKz_UJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17712528054091731075&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'g1BHkmuKz_UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://pdf.aminer.org/000/500/471/clip_based_similarity_measure_for_hierarchical_video_retrieval.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aminer.org</span><span class="gs_ggsS">aminer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1026721" class=yC15>Clip-based similarity measure for hierarchical video retrieval</a></h3><div class="gs_a">Y Peng, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Proceedings of the 6th ACM SIGMM international  &hellip;, 2004 - dl.acm.org</div><div class="gs_rs">Abstract This paper proposes a new approach and algorithm for the similarity measure of <br>video clips. The similarity is mainly based on two bipartite graph matching algorithms: <br>maximum matching (MM) and optimal matching (OM). MM is able to rapidly filter irrelevant <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4048920630106475467&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 20</a> <a href="/scholar?q=related:y0s_adynMDgJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4048920630106475467&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'y0s_adynMDgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.ece.northwestern.edu/~jyu410/index_files/papers/Yuan_Duan_Tian_PCM04.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from northwestern.edu</span><span class="gs_ggsS">northwestern.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/NJYDGBB5VD4FGTDA.pdf" class=yC17>Fast and robust short video clip search for copy detection</a></h3><div class="gs_a">J Yuan, LY Duan, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, S Ranganath&hellip; - Advances in Multimedia  &hellip;, 2005 - Springer</div><div class="gs_rs">Query by video clip (QVC) has attracted wide research interests in multimedia information <br>retrieval. In general, QVC may include feature extraction, similarity measure, database <br>organization, and search or query scheme. Towards an effective and efficient solution, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12406138583259222408&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 20</a> <a href="/scholar?q=related:iBFvmU91K6wJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/59/26/RN160206558.html?source=googlescholar" class="gs_nph" class=yC19>BL Direct</a> <a href="/scholar?cluster=12406138583259222408&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'iBFvmU91K6wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm02-wangjh.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=641055" class=yC1A>A framework for video scene boundary detection</a></h3><div class="gs_a">J Wang, TS Chua - Proceedings of the tenth ACM international  &hellip;, 2002 - dl.acm.org</div><div class="gs_rs">Abstract Most current video retrieval systems use shot as the basis for information <br>organization and access. In cinematography, scene is the basic story unit that the directors <br>use to convey their ideas. This paper proposes a framework based on the concept of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2323743190817835309&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 17</a> <a href="/scholar?q=related:LQnuzR6YPyAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2323743190817835309&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'LQnuzR6YPyAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://lbox.itee.uq.edu.au/~zxf/_papers/TKDESubseq.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uq.edu.au</span><span class="gs_ggsS">uq.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4599579" class=yC1C>Effective and efficient query processing for video subsequence identification</a></h3><div class="gs_a"><a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, J Shao, Z Huang&hellip; - Knowledge and Data  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the growing demand for visual information of rich content, effective and <br>efficient manipulations of large video databases are increasingly desired. Many <br>investigations have been made on content-based video retrieval. However, despite the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9728581582622281354&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 20</a> <a href="/scholar?q=related:is6buGfdAocJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9728581582622281354&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 22 versions</a> <a onclick="return gs_ocit(event,'is6buGfdAocJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/8k58781wh5664454.pdf" class=yC1E>A clustering technique for video copy detection</a></h3><div class="gs_a">N Guil, J GonzÃ¡lez-Linares, J CÃ³zar&hellip; - Pattern Recognition and  &hellip;, 2007 - Springer</div><div class="gs_rs">In this work, a new method for detecting copies of a query video in a videos database is <br>proposed. It includes a new clustering technique that groups frames with similar visual <br>content, maintaining their temporal order. Applying this technique, a keyframe is extracted <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11541693176220462540&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 13</a> <a href="/scholar?q=related:zK0rE8tULKAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/1D/5A/RN210335211.html?source=googlescholar" class="gs_nph" class=yC1F>BL Direct</a> <a href="/scholar?cluster=11541693176220462540&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'zK0rE8tULKAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://vireo.cs.cityu.edu.hk/papers/icme03.pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1220918" class=yC20>Video clip retrieval by maximal matching and optimal matching in graph theory</a></h3><div class="gs_a">YX Peng, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, QJ Dong, ZM Guo&hellip; - Multimedia and Expo,  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, a novel approach for automatic matching, ranking and retrieval of <br>video clips is proposed. Motivated by the maximal and optimal matching theories in graph <br>analysis, a new similarity measure of video clips is defined based on the representation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16071851310957993325&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 13</a> <a href="/scholar?q=related:bR2McJ2vCt8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16071851310957993325&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'bR2McJ2vCt8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0957417409010495" class=yC22>Effective content-based video retrieval using pattern-indexing and matching techniques</a></h3><div class="gs_a">JH Su, YT Huang, HH Yeh, VS Tseng - Expert Systems with Applications, 2010 - Elsevier</div><div class="gs_rs">Recently, multimedia data grows rapidly due to the advanced multimedia capturing devices, <br>such as digital video recorder, mobile camera and so on. Since conventional query-by-text <br>retrieval cannot satisfy users&#39; requirements in finding the desired videos effectively, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2320724708510518206&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 9</a> <a href="/scholar?q=related:vss9oNPeNCAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2320724708510518206&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'vss9oNPeNCAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.jos.org.cn/1000-9825/14/1409.pdf" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jos.org.cn</span><span class="gs_ggsS">jos.org.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.jos.org.cn/1000-9825/14/1409.pdf" class=yC23>ä¸ç§éè¿è§é¢çæ®µè¿è¡è§é¢æ£ç´¢çæ¹æ³</a></h3><div class="gs_a">å½­å®æ°, N Chong-Wahï¼ è£åºæ°ï¼ é­å®æï¼ èå»ºå½ - è½¯ä»¶å­¦æ¥, 2003 - jos.org.cn</div><div class="gs_rs">å½­å®æ°1,2+, Ngo Chong-Wah3, è£åºæ°1,2, é­å®æ1,2, èå»ºå½1,2  <b> ...</b> <br>1(åäº¬å¤§å­¦è®¡ç®æºç§å­¦ææ¯ç ç©¶æ,åäº¬100871) <br>2(åäº¬å¤§å­¦æå­ä¿¡æ¯å¤çææ¯å½å®¶éç¹å®éªå®¤,åäº¬100871) <b> ...</b> </div><div class="gs_fl"><a href="/scholar?cites=2187199933795914945&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 21</a> <a href="/scholar?q=related:wdSpYsB-Wh4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2187199933795914945&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'wdSpYsB-Wh4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md16', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md16" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:wdSpYsB-Wh4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://www.cecs.uci.edu/~papers/icme06/pdfs/0001785.pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uci.edu</span><span class="gs_ggsS">uci.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036967" class=yC25>Learning-based interactive video retrieval system</a></h3><div class="gs_a">CJ Wu, HC Zeng, S Huang, <a href="/citations?user=LlybOXQAAAAJ&amp;hl=en&amp;oi=sra">SH Lai</a>&hellip; - Multimedia and Expo,  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents an interactive video event retrieval system based on improved <br>adaboost learning. This system consists of three main steps. Firstly, a long video sequence <br>is partitioned into several video clips by using a distribution-based approach instead of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18120851319311552082&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 6</a> <a href="/scholar?q=related:Us47O0QyevsJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18120851319311552082&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'Us47O0QyevsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/visual03.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/empenryggg0alg96.pdf" class=yC27>A cinematic-based framework for scene boundary detection in video</a></h3><div class="gs_a">J Wang, TS Chua - The Visual Computer, 2003 - Springer</div><div class="gs_rs">Most current video retrieval systems use shots as the basis for information organization and <br>access. In cinematography, scene is the basic story unit that the directors use to compose <br>and convey their ideas. This paper proposes a framework based on the concept of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16541083775778629801&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 6</a> <a href="/scholar?q=related:qShDPw68jeUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4B/32/RN137115916.html?source=googlescholar" class="gs_nph" class=yC29>BL Direct</a> <a href="/scholar?cluster=16541083775778629801&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'qShDPw68jeUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://vireo.cs.cityu.edu.hk/papers/mta07.pdf" class=yC2B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/0207W098018590Q7.pdf" class=yC2A>OM-based video shot retrieval by one-to-one matching</a></h3><div class="gs_a">Y Peng, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, J Xiao - Multimedia Tools and Applications, 2007 - Springer</div><div class="gs_rs">Abstract This paper proposes a new approach for shot-based retrieval by optimal matching <br>(OM), which provides an effective mechanism for the similarity measure and ranking of shots <br>by one-to-one matching. In the proposed approach, a weighted bipartite graph is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13305475919282014105&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 6</a> <a href="/scholar?q=related:mYdRaNqLprgJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3A/20/RN210043401.html?source=googlescholar" class="gs_nph" class=yC2C>BL Direct</a> <a href="/scholar?cluster=13305475919282014105&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'mYdRaNqLprgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://ispl.korea.ac.kr/conference/icassp2005/pdfs/0200445.pdf" class=yC2E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from korea.ac.kr</span><span class="gs_ggsS">korea.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ispl.korea.ac.kr/conference/icassp2005/pdfs/0200445.pdf" class=yC2D>Video stream retrieval based on temporal feature of frame difference</a></h3><div class="gs_a">MTKSP Hartono, S Hashimoto - 2005 - ispl.korea.ac.kr</div><div class="gs_rs">ABSTRACT In recent years, we can easily access an enormous amount of digital video <br>stream in standardized video format such as MPEG and etc. However, it is not so easy to find <br>a desired video stream from video database in reasonable short time. Some efficient <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6049838197335949996&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 7</a> <a href="/scholar?q=related:rILMfMlX9VMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6049838197335949996&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'rILMfMlX9VMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md20', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md20" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:rILMfMlX9VMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=837848" class=yC2F>A fast video clip retrieval algorithm based on VA-File</a></h3><div class="gs_a">F Liu, DG Dong, X Miao, XY Xue - Electronic  &hellip;, 2003 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract Video clip retrieval is a significant research topic of content-base multimedia <br>retrieval. Generally, video clip retrieval process is carried out as following:(1) segment a <br>video clip into shots;(2) extract a key frame from each shot as its representative;(3) denote <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12194421116607987033&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 5</a> <a href="/scholar?q=related:Wf3wkGVJO6kJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/49/30/RN146760477.html?source=googlescholar" class="gs_nph" class=yC30>BL Direct</a> <a href="/scholar?cluster=12194421116607987033&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'Wf3wkGVJO6kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/95200x/200401/9021351.html" class=yC31>åºäºåå®¹çè§é¢æ£ç´¢å³é®ææ¯</a></h3><div class="gs_a">å½­å®æ°ï¼ é­å®æï¼ èå»ºå½ - è®¡ç®æºå·¥ç¨, 2004 - cqvip.com</div><div class="gs_rs">åºäºåå®¹çè§é¢æ£ç´¢ä¸ç´æ¯è®¡ç®æºç§å­¦ç ç©¶çé¾ç¹é®é¢, è¯¥ææåºäºå®çç ç©¶é®é¢, æ£ç´¢ç­ç¥, <br>æ£ç´¢ç§ç±»åè¯ä»·ææ , æåºäºå­å¨çé®é¢åè§£å³çæ¹æ³. è¿äºç­ç¥åæ¹æ³ä½ä¸ºåå¤§æ¹æ­£åªä½èµäº§<br>ç®¡çç³»ç»çæ ¸å¿åè½, è¿ç¨äºçµè§å°å¤§åè§é¢æ°æ®åºçæ£ç´¢, åå¾äºæ¯è¾å¥½çææ.</div><div class="gs_fl"><a href="/scholar?cites=6649462199042724427&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 12</a> <a href="/scholar?q=related:S1JUwaOiR1wJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6649462199042724427&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'S1JUwaOiR1wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://vireo.cs.cityu.edu.hk/papers/civr05_b.pdf" class=yC33><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/BF9F72QFVTTU0UBB.pdf" class=yC32>Hot event detection and summarization by graph modeling and matching</a></h3><div class="gs_a">Y Peng, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Image and Video Retrieval, 2005 - Springer</div><div class="gs_rs">Abstract. This paper proposes a new approach for hot event detection and summarization of <br>news videos. The approach is mainly based on two graph algorithms: optimal matching <br>(OM) and normalized cut (NC). Initially, OM is employed to measure the visual similarity <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4473150745825308660&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 5</a> <a href="/scholar?q=related:9O-wT-fSEz4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/14/26/RN171992290.html?source=googlescholar" class="gs_nph" class=yC34>BL Direct</a> <a href="/scholar?cluster=4473150745825308660&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'9O-wT-fSEz4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1509217" class=yC35>Efficient content-based video retrieval by mining temporal patterns</a></h3><div class="gs_a">JH Su, YT Huang, VS Tseng - &hellip;  of the 9th International Workshop on  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In recent years, multimedia content processing has become a hot topic with the <br>rapid development of information technology and popularity of World Wide Web. Among the <br>emerging research topics, content-based video retrieval is an attractive and challenging <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2220664872367490089&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 5</a> <a href="/scholar?q=related:KQSnku9i0R4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2220664872367490089&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'KQSnku9i0R4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://posgrado.escom.ipn.mx/biblioteca/Personalized%20video%20similarity%20measure.pdf" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ipn.mx</span><span class="gs_ggsS">ipn.mx <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/K6112U27W8235M32.pdf" class=yC36>Personalized video similarity measure</a></h3><div class="gs_a"><a href="/citations?user=d3h-zScAAAAJ&amp;hl=en&amp;oi=sra">J Shen</a>, Z Cheng - Multimedia systems, 2011 - Springer</div><div class="gs_rs">Abstract As an effective technique to manage and explore large scale of video collections, <br>personalized video search has received great attentions in recent years. One of the key <br>problems in the related technique development is how to design and evaluate the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11102164182707503178&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 4</a> <a href="/scholar?q=related:SoBYnIDPEpoJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11102164182707503178&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'SoBYnIDPEpoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=952713" class=yC38>Hierarchical video indexing based on changes of camera and object motions</a></h3><div class="gs_a">JH Oh, M Thenneru, N Jiang - Proceedings of the 2003 ACM symposium &hellip;, 2003 - dl.acm.org</div><div class="gs_rs">Abstract Since an entire video stream is too coarse as a level of abstraction, it is <br>decomposed into a number of shots in general. A shot, which is defined as a collection of <br>frames recorded from a single camera operation, is a basic unit for indexing video data. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3798873777893938588&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 4</a> <a href="/scholar?q=related:nBXq3ZJPuDQJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3798873777893938588&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'nBXq3ZJPuDQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://www.ejournal.org.cn/Jweb_dzxb/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=3915" class=yC3A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ejournal.org.cn</span><span class="gs_ggsS">ejournal.org.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ejournal.org.cn/Jweb_dzxb/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=3915" class=yC39>ä¸ç§åºäºäºåå¾æä¼å¹éçéå¤´æ£ç´¢æ¹æ³</a></h3><div class="gs_a">å½­å®æ°ï¼ èå»ºå½ - çµå­å­¦æ¥, 2004 - ejournal.org.cn</div><div class="gs_rs">æè¦: éå¤´æ£ç´¢æ¯åºäºåå®¹çè§é¢æ£ç´¢çéè¦åå®¹. æ¬æé¦æ¬¡å°è¯å°äºåå¾çæä¼å¹éç¨äºéå¤´<br>æ£ç´¢. ä¸ç°ææ¹æ³ç¸æ¯, æ¬ææåºçæ¹æ³å¼ºè°å¨ä¸ä¸å¯¹åºçåæä¸, å¨é¢å®¢è§å°åº¦éä¸¤ä¸ªéå¤´ç<br>ç¸ä¼¼åº¦. æä¸¤ä¸ªéå¤´çç¸ä¼¼åº¦åº¦éå»ºæ¨¡ä¸ºä¸ä¸ªå¸¦æçäºåå¾: éå¤´ä¸­çæ¯ä¸å¸§çæäºåå¾ç<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13355196840962442545&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 9</a> <a href="/scholar?q=related:MUX8msQwV7kJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13355196840962442545&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'MUX8msQwV7kJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md27', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md27" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:MUX8msQwV7kJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://finishstrike.googlecode.com/svn/arquivo/artigos/nao-lidos/grafo%20por%20objeto3.pdf" class=yC3C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from googlecode.com</span><span class="gs_ggsS">googlecode.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4368177" class=yC3B>Bipartite graph matching for video clip localization</a></h3><div class="gs_a">ZKG do Patrocinio, SJF Guimaraes&hellip; - &hellip;  Graphics and Image  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video clip localization consists in identifying real positions of a specific video clip in <br>a video stream. To cope with this problem, we propose a new approach considering the <br>maximum cardinality matching of a bipartite graph to measure video clip similarity with a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11159585346463023720&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 4</a> <a href="/scholar?q=related:aEKVIL_P3poJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11159585346463023720&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'aEKVIL_P3poJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/notebook_papers/nus.paper.pdf" class=yC3E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/notebook_papers/nus.paper.pdf" class=yC3D>TREC 2003 Video Retrieval and Story Segmentation task at NUS PRIS</a></h3><div class="gs_a">TS Chua, Y Zhao, L Chaisorn, CK Koh&hellip; - TREC (VIDEO)  &hellip;, 2003 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT This paper describes the details of our systems for story segmentation task and <br>search task of the TREC-2003 Video Track. In story segmentation task, we propose a two-<br>level multi-modal framework. First we analyze the video at the shot level using a variety of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7030869519621737973&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:9c0ss5OqkmEJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7030869519621737973&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'9c0ss5OqkmEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md29', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md29" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:9c0ss5OqkmEJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.140.7260&amp;rep=rep1&amp;type=pdf" class=yC40><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1454232" class=yC3F>Challenges and techniques for effective and efficient similarity search in large video databases</a></h3><div class="gs_a">J Shao, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, <a href="/citations?user=y6m820wAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a> - Proceedings of the VLDB Endowment, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Searching relevant visual information based on content features in large databases <br>is an interesting and changeling topic that has drawn lots of attention from both the research <br>community and industry. This paper gives an overview of our investigations on effective <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13888100201100327483&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 5</a> <a href="/scholar?q=related:O7qd84xxvMAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13888100201100327483&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'O7qd84xxvMAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/2081X763420K97G5.pdf" class=yC41>Video copy detection: sequence matching using hypothesis test</a></h3><div class="gs_a">D Dutta, S Saha, <a href="/citations?user=Ku-LgdUAAAAJ&amp;hl=en&amp;oi=sra">B Chanda</a> - Advances in Computer Science and  &hellip;, 2010 - Springer</div><div class="gs_rs">video copy detection is intended for verifying whether a video sequence is copied from <br>another or not. Such techniques can be used for protecting the copyright. A content-based <br>video detection system extracts signature of the video from its visual constituents. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12997274474977934721&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 2</a> <a href="/scholar?q=related:gb1PwUWYX7QJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12997274474977934721&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'gb1PwUWYX7QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://www.hal.t.u-tokyo.ac.jp/paper/2009/5%20A%20Degree-of-Edit%20Ranking%20For%20Consumer%20Generated%20Video%20Retrieval.pdf" class=yC43><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5202726" class=yC42>A degree-of-edit ranking for consumer generated video retrieval</a></h3><div class="gs_a">G Irie, K Hidaka, T Satou, T Yamasaki&hellip; - Multimedia and Expo,  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We introduce degree-of-edit (DoE) ranking to focus on ldquohow much a CGV is <br>editedrdquo as a ranking measure for consumer generated video (CGV) retrieval; a method <br>to estimate DoE ranking is proposed. In the proposed method, the DoE score of a CGV is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4540876184390396855&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:t-uew9RuBD8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4540876184390396855&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'t-uew9RuBD8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1251421" class=yC44>The 3C architecture: an XML topic maps-based framework for integrating content, context and common knowledge about multimedia</a></h3><div class="gs_a">P Boppana, YA Aslandogan - Information Reuse and  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present an XML topic maps (XTM) based framework for the integration of three&quot; <br>Cs&quot; of the media: content, context and common (background/domain) knowledge. The <br>framework incorporates content analysis modules such as object detectors, context <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3212844349732094974&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 2</a> <a href="/scholar?q=related:_gNn6OlQliwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3212844349732094974&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'_gNn6OlQliwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/90287a/200701/25757981.html" class=yC45>å©ç¨ç­ä»·å³ç³»çè®ºè¿è¡è§é¢çæ®µæ£ç´¢çæ¹æ³</a></h3><div class="gs_a">èµµäºç´ï¼ å¨ç®ä¸­ï¼ ä½æ° - ä¸­å½å¾è±¡å¾å½¢å­¦æ¥, 2007 - cqvip.com</div><div class="gs_rs">è§é¢çæ®µæ£ç´¢æ¯åºäºåå®¹çè§é¢æ£ç´¢çä¸»è¦æ¹å¼, å¯æ¯ç°æççæ®µæ£ç´¢æ¹æ³å¤§å¤åªæ¯å¯¹é¢ååå²<br>å¥½ççæ®µè¿è¡æ£ç´¢. ä¸ºäºä»è¿ç»­çè§é¢èç®ä¸­èªå¨åå²åºå¤ä¸ªç¸ä¼¼ççæ®µ, <br>æåºäºä¸ç§æ°çææçè§é¢çæ®µæ£ç´¢æ¹æ³, å¹¶é¦æ¬¡å°è¯å°ç­ä»·å³ç³»çè®ºåºç¨äºè§é¢çæ®µçæ£ç´¢<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2870748972401416715&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:C9pNA_vy1icJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2870748972401416715&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'C9pNA_vy1icJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/94913x/200611/23242943.html" class=yC46>åºäºç´¢å¼ç»æçé«æè¿å¨è§é¢æ£ç´¢</a></h3><div class="gs_a">å¼ éï¼ è·¯çº¢ï¼ èåé³ - è®¡ç®æºç ç©¶ä¸åå±, 2006 - cqvip.com</div><div class="gs_rs">è§é¢çæ­æ£ç´¢æ¯è§é¢é¢åçç ç©¶ç­ç¹, ä¸ºäºæé«æ¥è¯¢æç, å©ç¨é«ç»´ç´¢å¼ç»æVector-<br>Approximation File (VA-File) æ¥ç»ç»è§é¢å­çæ®µ, å¹¶éç¨æ°çç¸ä¼¼åº¦æ¨¡åååºäºéå®æ§æ»å¨çªå£<br>çé«æè§é¢æ£ç´¢ç®æ³è¿è¡è§é¢çæ®µæ£ç´¢. æåºçå­çæ®µçåå²ç®æ³è½å¤è¾å¥½å°åºåè¿å¨çç»è<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=913054996587002084&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:5Lj2i67SqwwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=913054996587002084&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'5Lj2i67SqwwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="http://file.lw23.com/b/be/be4/be450039-39d6-4d95-b1a9-f5ec34574219.pdf" class=yC48><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lw23.com</span><span class="gs_ggsS">lw23.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://file.lw23.com/b/be/be4/be450039-39d6-4d95-b1a9-f5ec34574219.pdf" class=yC47>åºäºå³é®å¸§åºåçè§é¢çæ®µæ£ç´¢</a></h3><div class="gs_a">æ½æºå¹³ï¼ ææ¸åï¼ å²ä¿ï¼ å²å¿ æ¤ - è®¡ç®æºåºç¨, 2005 - file.lw23.com</div><div class="gs_rs">æ½æºå¹³, ææ¸å, å²ä¿, å²å¿ æ¤(1. ä¸­å½ç§å­¦é¢è®¡ç®ææ¯ç ç©¶æ, åäº¬100080: 2. <br>ä¸­å½ç§å­¦é¢ç ç©¶çé¢, åäº¬100039)(shizp@ ics. ict. ac. cn) æè¦: æåºäºä¸ç§åºäºå³é®å¸§èåç<br>è§é¢çæ®µæ£ç´¢æ¹æ³. ä½¿ç¨ç¹å¾èååå¸ç´æ¹å¾å°è§é¢åå²ä¸ºå­éå¤´, å­éå¤´ç¨å³é®å¸§è¡¨ç¤º. æ£ç´¢<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4859068106530665781&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:NbUwzbLgbkMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4859068106530665781&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'NbUwzbLgbkMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md36', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md36" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:NbUwzbLgbkMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.106.4990&amp;rep=rep1&amp;type=pdf" class=yC4A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.106.4990&amp;rep=rep1&amp;type=pdf" class=yC49>Fast Video Segment Identification from Large Video Collection</a></h3><div class="gs_a">J Yuan, LY Duan, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - 2004 Pacific-Rim Conference on Multimedia ( &hellip;, 2004 - Citeseer</div><div class="gs_rs">Abstract. In this paper we design a new global visual feature and use it as signature for âfast <br>and dirtyâ video segment identification among video collection containing a large number of <br>sequences. Different from previous key frame based shot representation, the proposed <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6696231521187968008&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:CLQuIhfL7VwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6696231521187968008&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'CLQuIhfL7VwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md37', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md37" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:CLQuIhfL7VwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://www.lbd.dcc.ufmg.br/colecoes/webmedia/2010/18_webmi_c.pdf" class=yC4C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ufmg.br</span><span class="gs_ggsS">ufmg.br <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.lbd.dcc.ufmg.br/colecoes/webmedia/2010/18_webmi_c.pdf" class=yC4B>Identification and analysis of video subsequence using bipartite graph matching</a></h3><div class="gs_a">SJF GuimarÃ£es, ZKG do PatrocÃ­nio Jr - 16th WebMedia Brazilian  &hellip;, 2010 - lbd.dcc.ufmg.br</div><div class="gs_rs">ABSTRACT Subsequence identification consists in identifying real positions of a specific <br>video clip in a video stream together with the operations that may be used to transform the <br>former into a subsequence from the latter. To cope with this problem, we propose a new <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17493697883867763223&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:F9LwL4oZxvIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17493697883867763223&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'F9LwL4oZxvIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md38', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md38" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:F9LwL4oZxvIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB39" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW39"><a href="http://www.cecs.uci.edu/~papers/icme06/pdfs/0001029.pdf" class=yC4E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uci.edu</span><span class="gs_ggsS">uci.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036778" class=yC4D>Adaptive video news story tracking based on earth mover&#39;s distance</a></h3><div class="gs_a">M Uddenfeldt, <a href="/citations?user=J_NYwJMAAAAJ&amp;hl=en&amp;oi=sra">K Hoashi</a>, K Matsumoto&hellip; - Multimedia and Expo,  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper proposes an adaptive system for video news story tracking based on the <br>earth mover&#39;s distance (EMD). When an interesting story appears in the news, it is flagged <br>manually as a topic for tracking. Our system then tracks the events as they unfold over time <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2333554328555364911&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:LyKeUUxzYiAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2333554328555364911&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'LyKeUUxzYiAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/F741L4U05302L748.pdf" class=yC4F>Real-time monitoring system for TV commercials using video features</a></h3><div class="gs_a">S Lee, W Yoo, YS Yoon - Entertainment Computing-ICEC 2006, 2006 - Springer</div><div class="gs_rs">Abstract. For companies, TV commercial is a very important way to introduce and advertise <br>their products. It is expensive to put an advertisement on TV. So these companies generally <br>charge other companies to monitor that their TV commercials are broadcasted properly as <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13919905785264433994&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:SmuXlY9wLcEJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4A/2A/RN196405433.html?source=googlescholar" class="gs_nph" class=yC50>BL Direct</a> <a href="/scholar?cluster=13919905785264433994&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'SmuXlY9wLcEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=794648" class=yC51>Ordinal-based method for robust image/video signature generation</a></h3><div class="gs_a">DC Chen, L Chaisorn&hellip; - Optical  &hellip;, 2008 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract This paper proposes an algorithm for generating a video signature based on an <br>ordinal measure. Current methods which use a measure of temporal ordinal rank are robust <br>to many transformations but can only detect the entire query video, not a segment of the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1055721939058276134&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:Jrcnl4Ktpg4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1055721939058276134&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Jrcnl4Ktpg4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md41', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md41" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Jrcnl4Ktpg4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB42" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW42"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/15115/Yang_Zixiang_Thesis.pdf?sequence=1" class=yC53><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/15115" class=yC52>Efficient video identification based on locality sensitive hashing and triangle inequality</a></h3><div class="gs_a">Y Zixiang - 2005 - scholarbank.nus.edu</div><div class="gs_rs">Searching for duplicated version video clips in large video database, or video identification, <br>requires fast and robust similarity search in high-dimensional space. Locality sensitive <br>hashing, or LSH, is a well-known indexing method for efficient approximate similarity <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15280622622353213935&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:70kpmFCtD9QJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15280622622353213935&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'70kpmFCtD9QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4627227" class=yC54>Content-Based Video Search: is there a need, and is it possible?</a></h3><div class="gs_a">Z Huang, Y Li, J Shao, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, L Wang&hellip; - &hellip; -Explosion and Next  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract There is a large and rapidly increasing amount of video data on the Internet and in <br>personal or organizational collections. Fast and accurate video search emerges to be an <br>important issue. The need and main technical challenges for video retrieval are similar to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4406023978416164577&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:4U5Z2XZXJT0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4406023978416164577&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'4U5Z2XZXJT0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://www.uddenfeldt.se/mats/files/kddi_report.pdf" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uddenfeldt.se</span><span class="gs_ggsS">uddenfeldt.se <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.uddenfeldt.se/mats/files/kddi_report.pdf" class=yC55>A System for Adaptive Video News Story Tracking Based on the Earth Mover&#39;s Distance</a></h3><div class="gs_a">M Uddenfeldt - 2006 - uddenfeldt.se</div><div class="gs_rs">Abstract Every day there is an abundance of information broadcasted by all the news <br>networks of the world. An automatic news story tracking system could watch all broadcasted <br>news and track the stories in which we are interested in. In this thesis, a novel approach to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2092784898377440585&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:Sd0W68YQCx0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2092784898377440585&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'Sd0W68YQCx0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md44', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md44" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Sd0W68YQCx0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/95659x/200708/25172712.html" class=yC57>åºäºæ¨¡ç³ç´æ¹å¾çä¸¤é¶æ®µç¸ä¼¼è§é¢çèªå¨é´å«åæ£ç´¢</a></h3><div class="gs_a">ççï¼ å¾ç®ï¼ çå¿è¾ï¼ å¼ èå - å°åå¾®åè®¡ç®æºç³»ç», 2007 - cqvip.com</div><div class="gs_rs">éå¯¹ä¸åè§é¢æ·è´çç¸ä¼¼æ§èªå¨é´å«é®é¢èæ¯, ç»åºäºä¸ä¸ªå¼é¡¾æçåææçç±ç²å°ç²¾çä¸¤é¶æ®µ<br>æ¹æ¡. é¦åæ½åè§é¢å¸§åå®¹çæ¨¡ç³é¢è²ç´æ¹å¾ç¹å¾åé, ææç¹å¾ç©ºé´ä¸­åæ è¯¥è§é¢æ§è´¨çç¹å¾<br>ç¹éååç¸åºè½¨è¿¹çº¿, èåä»ç¹å¾ç¹ç©ºé´åå¸çè§åº¦è®¡ç®ä½ç½®åå¸ç´æ¹å¾åé, å¨æ­¤åºç¡ä¸<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8826740656007593727&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:_77UTdzhfnoJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8826740656007593727&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'_77UTdzhfnoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB46" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW46"><a href="http://file.lw23.com/8/88/885/88528403-4dbd-4d7a-9a03-29bd0cc300c0.pdf" class=yC59><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lw23.com</span><span class="gs_ggsS">lw23.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://file.lw23.com/8/88/885/88528403-4dbd-4d7a-9a03-29bd0cc300c0.pdf" class=yC58>ç§åç¼©åä¸­åºäºéå¤´çè§é¢æ£ç´¢æ¹æ³</a></h3><div class="gs_a">æ¹å»ºè£ï¼ è¡å®å¦® - å¾®çµå­å­¦ä¸è®¡ç®æº, 2007 - file.lw23.com</div><div class="gs_rs">æè¦: éå¤´æ¯è§é¢çåºæ¬åå. æç« æåºäºä¸ä¸ªè®¡ç®éå¤´çº¹çç´æ¹å¾åå¨åç¼©åä¸­åºäºéå¤´ç<br>è§é¢æ£ç´¢æ¹æ³. å¯¹éå¤´ä¸­ææI å¸§çDC å¾åå«æåé¢è²åçº¹çç´æ¹å¾. ç¶ååå«å½¢æéå¤´çå¯å<br>é¿å°æ³é¢è²åçº¹çç´æ¹å¾, å¹¶ç¨éå¤´çè¿äºç¹å¾å¨éå¤´å±æ¬¡ä¸ç¨ä¸åçè·ç¦»åº¦éæ¹æ³è¿è¡è§é¢<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1974172072954307689&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:aSTwbQqrZRsJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1974172072954307689&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'aSTwbQqrZRsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md46', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md46" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:aSTwbQqrZRsJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB47" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW47"><a href="http://www.sersc.org/journals/IJFGCN/vol3_no3/5.pdf" class=yC5B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sersc.org</span><span class="gs_ggsS">sersc.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.sersc.org/journals/IJFGCN/vol3_no3/5.pdf" class=yC5A>A Hypothesis Test based Robust Technique for Video Sequence Matching</a></h3><div class="gs_a">IS ECSU - database - sersc.org</div><div class="gs_rs">Abstract Video sequence matching is the most crucial step to verify whether a video <br>sequence has been copied from another or not. The video sequence in question is <br>represented by a set of visual descriptors. Based on such descriptors the sequence has to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:aQDPFMSq84cJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'aQDPFMSq84cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md47', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md47" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:aQDPFMSq84cJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/14908/yuan%20junsong.pdf?sequence=1" class=yC5D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/14908" class=yC5C>Robust short clip representation and fast search through large video collections</a></h3><div class="gs_a">Y JUNSONG - 2005 - scholarbank.nus.sg</div><div class="gs_rs">In this thesis we present a video copy detection method to effectively and efficiently search <br>and locate clip re-occurrences (copies) inside large video collections. Three aspects of <br>video copy detection including (1) feature robustness to coding variations,(2) search <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:hXrmIZF49UMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4896952734569626245&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'hXrmIZF49UMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> An Efficient Technique for Video Shot Indexing using Low Level</h3><div class="gs_a">JH Oh, N Baskaran</div><div class="gs_fl"><a href="/scholar?q=related:d9xgLCF1fMQJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14158320113884912759&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'d9xgLCF1fMQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB50" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW50"><a href="http://144.206.159.178/ft/CONF/16420017/16420091.pdf" class=yC5F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 144.206.159.178</span><span class="gs_ggsS">144.206.159.178 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://144.206.159.178/ft/CONF/16420017/16420091.pdf" class=yC5E>Ordinal-Based Method for Robust Image/Video Signature Generation</a></h3><div class="gs_a">CC Daniel, L Chaisorn, S Rahardja - Proc. of SPIE Vol - 144.206.159.178</div><div class="gs_rs">ABSTRACT This paper proposes an algorithm for generating a video signature based on an <br>ordinal measure. Current methods which use a measure of temporal ordinal rank are robust <br>to many transformations but can only detect the entire query video, not a segment of the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:VZ4rQEtq4xUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'VZ4rQEtq4xUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md50', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md50" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:VZ4rQEtq4xUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/15829/final_thesis_wanggang_nus_phdx.pdf?sequence=1" class=yC61><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/15829" class=yC60>A multi-resolution multi-source and multi-modal (M3) transductive framework for concept detection in news video</a></h3><div class="gs_a">W Gang - 2009 - scholarbank.nus.sg</div><div class="gs_rs">We study the problem of detecting concepts in news video. Most existing algorithms for news <br>video concept detection are based on single-resolution (shot), single source (training data), <br>and multi-modal fusion methods under a supervised inductive inference framework. In this <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:O24GTu3y5YoJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10008672847931010619&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'O24GTu3y5YoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/Y5H035675387U013.pdf" class=yC62>Story-Based Retrieval by Learning and Measuring the Concept-Based and Content-Based Similarity</a></h3><div class="gs_a">Y Peng, J Xiao - Advances in Multimedia Modeling, 2010 - Springer</div><div class="gs_rs">Abstract. This paper proposes a new idea and approach for the story-based news video <br>retrieval, ie clip-based retrieval. Generally speaking, clip-based retrieval can be divided into <br>two phases: feature representation and similarity ranking. The existing methods only <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:YeqWbubDCloJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6488213607645899361&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'YeqWbubDCloJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://www.inf.uni-konstanz.de/gk/pubsys/publishedFiles/ZhLiKe11.pdf" class=yC64><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-konstanz.de</span><span class="gs_ggsS">uni-konstanz.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0747563210003420" class=yC63>Shot retrieval based on fuzzy evolutionary aiNet and hybrid features</a></h3><div class="gs_a">XH Li, YZ Zhan, J Ke, HW Zheng - Computers in Human Behavior, 2011 - Elsevier</div><div class="gs_rs">As the multimedia data increasing exponentially, how to get the video data we need <br>efficiently become so important and urgent. In this paper, a novel method for shot retrieval is <br>proposed, which is based on fuzzy evolutionary aiNet and hybrid features. To begin with, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:5TWgdenMKXcJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8586619467631441381&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'5TWgdenMKXcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ri"><h3 class="gs_rt"><a href="http://inderscience.metapress.com/index/M182488X614Q0765.pdf" class=yC65>A content-based video retrieval system: video retrieval with extensive features</a></h3><div class="gs_a">P Rajendran, TN Shanmugam - International Journal of Multimedia  &hellip;, 2011 - Inderscience</div><div class="gs_rs">The existing video retrieval techniques are inefficient because they utilise a specific feature. <br>In this paper, a proficient system with extensive features is proposed for improving the <br>retrieval efficiency. The system segments a video into shots, and then a few representative <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:oFCpdkeZ8RUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1581213476396290208&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'oFCpdkeZ8RUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/X6851K1227423528.pdf" class=yC66>A multi-type indexing CBVR system constructed with MPEG-7 visual features</a></h3><div class="gs_a">YF Huang, HW Chen - Active Media Technology, 2011 - Springer</div><div class="gs_rs">Since multimedia has played an important role in our daily life, multimedia mining becomes <br>a popular research area. Among the emerging research topics of multimedia mining, content-<br>based video retrieval is a challenging one which attracts researchers&#39; attention. Here, we <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:KUq6dy2KcjkJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4139522935393765929&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'KUq6dy2KcjkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/T7K141K02728174G.pdf" class=yC67>Identification of video subsequence using bipartite graph matching</a></h3><div class="gs_a">SJF GuimarÃ£es, ZKG do PatrocÃ­nio - Journal of the Brazilian Computer  &hellip;, 2011 - Springer</div><div class="gs_rs">Abstract Subsequence identification consists of identifying real positions of a specific video <br>clip in a video stream together with the operations that may be used to transform the former <br>into a subsequence from the latter. To cope with this problem, we propose a new <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1177396498437034925&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:rdMgkeDzVhAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1177396498437034925&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'rdMgkeDzVhAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5941619" class=yC68>Photometric attack invariant video sequence matching</a></h3><div class="gs_a">D Dutta, SK Saha, <a href="/citations?user=Ku-LgdUAAAAJ&amp;hl=en&amp;oi=sra">B Chanda</a> - &hellip;  Computer Technology (ICECT),  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the development of technology, volume of video data has grown heavily. It has <br>become quite easy to capture, edit and copy such data leading to piracy. As a result <br>copyright preservation has become quite crucial. A manual process of verifying a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:VCmynkh_PCUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'VCmynkh_PCUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB58" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW58"><a href="http://www.iitg.ernet.in/iciss2012/pdf/Tutorials/Sanjoy_Saha.pdf" class=yC6A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iitg.ernet.in</span><span class="gs_ggsS">iitg.ernet.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.iitg.ernet.in/iciss2012/pdf/Tutorials/Sanjoy_Saha.pdf" class=yC69>Content Based Video Copy Detection: Issues and Practices</a></h3><div class="gs_a">SK Saha - iitg.ernet.in</div><div class="gs_rs">With the rapid development in the field of multimedia technology, it has become easier to <br>access and store video data of huge volume. It is well reflected in the availability of such <br>data on various sites like video blogs and Web-TV. Sharing and distribution of video over <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'KiTOqt9KgyMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md58', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md58" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:KiTOqt9KgyMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Shot-based Retrieval by Integrating Color and Motion Features</h3><div class="gs_a">Y Peng, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a></div><div class="gs_fl"><a href="/scholar?q=related:eP5Hvy6f2qcJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'eP5Hvy6f2qcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB60" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW60"><a href="http://ir.lib.nthu.edu.tw/bitstream/987654321/18035/1/2030227030019.pdf" class=yC6C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nthu.edu.tw</span><span class="gs_ggsS">nthu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ir.lib.nthu.edu.tw/handle/987654321/18035" class=yC6B>Learning-Based Interactive Video Retrieval System</a></h3><div class="gs_a">CJWHC Zeng, SHHSH Lai, WH Wang - ir.lib.nthu.edu.tw</div><div class="gs_rs">This paper presents an interactive video event retrieval system based on improved adaboost <br>learning. This system consists of three main steps. Firstly, a long video sequence is <br>partitioned into several video clips by using a distribution-based approach instead of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:L8-q0axbJFIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5918956608081809199&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'L8-q0axbJFIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284767" class=yC6D>An Error-Tolerant Video Retrieval Method Based on the Shot Composition Sequence in a Scene</a></h3><div class="gs_a">I Kondo, S Shimada, M Morimoto - Multimedia and Expo, 2007  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents an error-tolerant video retrieval method based on the shot <br>composition sequence in a scene. Conventional video players in the home can not access <br>interesting scenes directly because they offer only play, fast-forward, and rewind. What is <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:26lS5N6HRWgJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7513560944720652763&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'26lS5N6HRWgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB62" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW62"><a href="http://cse2.uta.edu/Research/Publications/Downloads/CSE-2003-19.pdf" class=yC6F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uta.edu</span><span class="gs_ggsS">uta.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cse2.uta.edu/Research/Publications/Downloads/CSE-2003-19.pdf" class=yC6E>A New Paradigm for Multimedia Information Access Through the Integration of Content, Context and Common Knowledge</a></h3><div class="gs_a">P Boppana, YA Aslandogan - cse2.uta.edu</div><div class="gs_rs">ABSTRACT: We present a new paradigm for multimedia information access, named the <br>âHolisticâ information access paradigm, and its underlying architecture, the 3C (Content, <br>context, and common-background) knowledge integration architecture. The holistic <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:biqx8TpK7PoJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18080908221009177198&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'biqx8TpK7PoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md62', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md62" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:biqx8TpK7PoJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/ima.20154/abstract" class=yC70>Efficient video retrieval using index structure</a></h3><div class="gs_a">J Zhang - International Journal of Imaging Systems and  &hellip;, 2008 - Wiley Online Library</div><div class="gs_rs">Abstract Video retrieval remains a challenging problem since most of traditional query <br>algorithms are ineffectual and time-consuming. In this article, we proposed a new video <br>retrieval method, which segments the video stream by visual similarity between <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:YdPC8htmkTUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3859978625856230241&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'YdPC8htmkTUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB64" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW64"><a href="http://www.arocmag.com/ch/reader/create_pdf.aspx?file_no=200901113&amp;flag=1&amp;journal_id=arocmag" class=yC72><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arocmag.com</span><span class="gs_ggsS">arocmag.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.arocmag.com/ch/reader/create_pdf.aspx?file_no=200901113&amp;flag=1&amp;journal_id=arocmag" class=yC71>ä¸ç§æ¯æä¸åæ¶é´å°ºåº¦çè§é¢ç¸ä¼¼æ§å¹éç®æ³å¡</a></h3><div class="gs_a">éæºç­ï¼ è´¾åæ - è®¡ç®æºåºç¨ç ç©¶, 2009 - arocmag.com</div><div class="gs_rs">æè¦: éå¯¹ä¸åæ¶é´å°ºåº¦è§é¢é´çå¹éé®é¢, å¦äººä¸ºåç¼è¾(å¿«è¿, æ¢æ¾ç­) è§é¢ä¸åå§è§é¢é´ç<br>å¹éä»¥åä¸åå¸§çè§é¢é´çæ£ç´¢ç­, æåºäºä¸ç§åºäºå¨ææ¶é´è§åçæä¼å¹éç®æ³. <br>å¨å­çæ®µçåºç¡ä¸è¿è¡è§é¢ç¸ä¼¼æ§å¹é, éè¿æå°åä¸¤æ®µè§é¢çæ´ä½è·ç¦»å»ºç«è§é¢ä¹é´çå­<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:QPAcsWBT6tAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15053936379192733760&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'QPAcsWBT6tAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md64', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md64" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:QPAcsWBT6tAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB65" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW65"><a href="http://www.interaction-ipsj.org/archives/paper2009/oral/0005/0005.pdf" class=yC74><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from interaction-ipsj.org</span><span class="gs_ggsS">interaction-ipsj.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.interaction-ipsj.org/archives/paper2009/oral/0005/0005.pdf" class=yC73>åç»å±æãµã¤ãã«ãããæ åæ¤ç´¢ã®ããã®ç·¨éåº¦é åºã«ããã©ã³ã­ã³ã°æ³ã®ææ¡ã¨è©ä¾¡</a></h3><div class="gs_a">å¥æ±è±ªï¼ æ¥é«æµ©å¤ªï¼ ä½è¤éï¼ å°å³¶æï¼ ç¸æ¾¤æ¸æ´ - ã¤ã³ã¿ã©ã¯ã·ã§ã³, 2009 - interaction-ipsj.org</div><div class="gs_rs">æ¬ç ç©¶ã§ã¯, åç»å±æãµã¤ãã«ãããæ¶è²»èçææ å (Consumer Generated Videos: CGV) <br>æ¤ç´¢ã®æ°ããªã©ã³ã­ã³ã°ææ¨ã¨ãã¦, CGV ã ãç·¨éããã¦ããç¨åº¦ã ã«åºã¥ãç·¨éåº¦é åºãå°å¥ã, <br>ãããæ¨å®ããææ³ã®ææ¡ãè¡ã. ææ¡ææ³ã¯, ç·¨éãããç¨åº¦ã«ãã£ã¦å¤åããã«ããç¹ã®æ°ã, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:_CoB2oeNqWgJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'_CoB2oeNqWgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md65', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md65" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_CoB2oeNqWgJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> A Multi-type Index CBVR System Constructed with MPEG-7 Visual Features</h3><div class="gs_a">H Chen - 2010</div><div class="gs_fl"><a href="/scholar?q=related:ySDj_vXje5oJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'ySDj_vXje5oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
