Total results = 51
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://csce.uark.edu/~jgauch/library/Video/Yang.2003b.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uark.edu</span><span class="gs_ggsS">uark.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=957146" class=yC0>VideoQA: question answering on news video</a></h3><div class="gs_a">H Yang, L Chaisorn, Y Zhao, SY Neo&hellip; - Proceedings of the  &hellip;, 2003 - dl.acm.org</div><div class="gs_rs">Abstract When querying a news video archive, the users are interested in retrieving precise <br>answers in the form of a summary that best answers the query. However, current video <br>retrieval systems, including the search engines on the web, are designed to retrieve <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12408849427244413797&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 80</a> <a href="/scholar?q=related:ZXM2Js8WNawJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12408849427244413797&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 30 versions</a> <a onclick="return gs_ocit(event,'ZXM2Js8WNawJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://vision.eecs.ucf.edu/projects/Markov/01658031.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucf.edu</span><span class="gs_ggsS">ucf.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1658031" class=yC2>Video scene segmentation using Markov chain Monte Carlo</a></h3><div class="gs_a"><a href="/citations?user=tQKD7T8AAAAJ&amp;hl=en&amp;oi=sra">Y Zhai</a>, <a href="/citations?user=p8gsO3gAAAAJ&amp;hl=en&amp;oi=sra">M Shah</a> - Multimedia, IEEE Transactions on, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Videos are composed of many shots that are caused by different camera <br>operations, eg, on/off operations and switching between cameras. One important goal in <br>video analysis is to group the shots into temporal scenes, such that all the shots in a single <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15395518755727310252&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 69</a> <a href="/scholar?q=related:rH39R73ep9UJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/23/57/RN196047201.html?source=googlescholar" class="gs_nph" class=yC4>BL Direct</a> <a href="/scholar?cluster=15395518755727310252&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 20 versions</a> <a onclick="return gs_ocit(event,'rH39R73ep9UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/trecvid04.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/trecvid04.pdf" class=yC5>TRECVID 2004 search and feature extraction task by NUS PRIS</a></h3><div class="gs_a">TS Chua, SY Neo, KY Li, G Wang, R Shi&hellip; - Proceedings of the  &hellip;, 2004 - 137.132.145.151</div><div class="gs_rs">ABSTRACT This paper describes the details of our systems for feature extraction and search <br>tasks of TRECVID-2004. For feature extraction, we emphasize the use of visual auto-concept <br>annotation technique, with the fusion of text and specialized detectors, to induce concepts <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4598184418179278710&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 60</a> <a href="/scholar?q=related:dudPuV0I0D8J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4598184418179278710&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'dudPuV0I0D8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md2', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md2" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:dudPuV0I0D8J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv5.papers/nus.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv5.papers/nus.pdf" class=yC7>Trecvid 2005 by nus pris</a></h3><div class="gs_a">TS Chua, SY Neo, HK Goh, <a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a>, Y Xiao&hellip; - NIST TRECVID- &hellip;, 2005 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT We participated in the high-level feature extraction and search task for <br>TRECVID 2005. For the high-level feature extraction task, we make use of the available <br>collaborative annotation results for training, and develop 2 methods to perform automated <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6139068261712169763&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 39</a> <a href="/scholar?q=related:I4s6zw5aMlUJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6139068261712169763&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'I4s6zw5aMlUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:I4s6zw5aMlUJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/nus.final.paper.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/nus.final.paper.pdf" class=yC9>A two-level multi-modal approach for story segmentation of large news video corpus</a></h3><div class="gs_a">L Chaisorn, TS Chua, CK Koh, Y Zhao, H Xu&hellip; - TRECVID  &hellip;, 2003 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT This paper presents an enhanced work from our previous paper [Chaisorn et al. <br>2002]. The system is enhanced to perform news story segmentation on a large video corpus <br>used in TRECVID 2003 evaluation. We use a combination of features include visual-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6887006697235574202&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 36</a> <a href="/scholar?q=related:um12KhqQk18J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6887006697235574202&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'um12KhqQk18J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:um12KhqQk18J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.ee.columbia.edu/ln/dvmm/publications/03/icme2003pr.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1221641" class=yCB>A statistical framework for fusing mid-level perceptual features in news story segmentation</a></h3><div class="gs_a"><a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">H Winston</a>, HM Hsu, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Multimedia and Expo, 2003.  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract News story segmentation is essential for video indexing, summarization and <br>intelligence exploitation. In this paper, we present a general statistical framework, called <br>exponential model or maximum entropy model that can systematically select the most <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13661619486143107496&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 32</a> <a href="/scholar?q=related:qFEdUIrSl70J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13661619486143107496&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'qFEdUIrSl70J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://adi.ac.ir/uploads/03.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from adi.ac.ir</span><span class="gs_ggsS">adi.ac.ir <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4633637" class=yCD>Modality mixture projections for semantic video event detection</a></h3><div class="gs_a"><a href="/citations?user=d3h-zScAAAAJ&amp;hl=en&amp;oi=sra">J Shen</a>, D Tao, <a href="/citations?user=ahUibskAAAAJ&amp;hl=en&amp;oi=sra">X Li</a> - &hellip;  and Systems for Video Technology, IEEE  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Event detection is one of the most fundamental components for various kinds of <br>domain applications of video information system. In recent years, it has gained a <br>considerable interest of practitioners and academics from different areas. While detecting <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6975820005009692912&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 27</a> <a href="/scholar?q=related:8Jiqx1UXz2AJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6975820005009692912&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'8Jiqx1UXz2AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://nguyendangbinh.org/Proceedings/ICCV/2005/ICCV05/0146-P0314-zhai_scene_segmentation.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nguyendangbinh.org</span><span class="gs_ggsS">nguyendangbinh.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1544845" class=yCF>A general framework for temporal video scene segmentation</a></h3><div class="gs_a"><a href="/citations?user=tQKD7T8AAAAJ&amp;hl=en&amp;oi=sra">Y Zhai</a>, <a href="/citations?user=p8gsO3gAAAAJ&amp;hl=en&amp;oi=sra">M Shah</a> - Computer Vision, 2005. ICCV 2005. Tenth  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Videos are composed of many shots caused by different camera operations, eg, <br>on/off operations and switching between cameras. One important goal in video analysis is to <br>group the shots into temporal scenes, such that all the shots in a single scene are related <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13589958669327533372&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 24</a> <a href="/scholar?q=related:PAWCNWg7mbwJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13589958669327533372&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 20 versions</a> <a onclick="return gs_ocit(event,'PAWCNWg7mbwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://arxiv.org/pdf/cs.MM/0501044" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101158" class=yC11>Augmented segmentation and visualization for presentation videos</a></h3><div class="gs_a">A Haubold, JR Kender - Proceedings of the 13th annual ACM  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract We investigate methods of segmenting, visualizing, and indexing presentation <br>videos by both audio and visual data. The audio track is segmented by speaker, and <br>augmented with key phrases which are extracted using an Automatic Speech Recognizer <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11671770609182811170&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 22</a> <a href="/scholar?q=related:IqQvhId1-qEJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11671770609182811170&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 24 versions</a> <a onclick="return gs_ocit(event,'IqQvhId1-qEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/73218T14Q31VN30X.pdf" class=yC13>Video scene segmentation and semantic representation using a novel scheme</a></h3><div class="gs_a">S Zhu, Y Liu - Multimedia Tools and Applications, 2009 - Springer</div><div class="gs_rs">Abstract Grouping video content into semantic segments and classifying semantic scenes <br>into different types are the crucial processes to content-based video organization, <br>management and retrieval. In this paper, a novel approach to automatically segment <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14918358307748716548&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 21</a> <a href="/scholar?q=related:BCBcDdenCM8J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14918358307748716548&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'BCBcDdenCM8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv6.papers/nus_i2r.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv6.papers/nus_i2r.pdf" class=yC14>TRECVID 2006 by NUS-I 2 R</a></h3><div class="gs_a">TS Chua, SY Neo, Y Zheng, HK Goh&hellip; - NIST TRECVID  &hellip;, 2006 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT NUS and I2R joint participated in the high-level feature extraction and <br>automated search task for TRECVID 2006. In both task, we only make use of the standard <br>TRECVID available annotation results. For HLF task, we develop 2 methods to perform <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13823931184840869238&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 20</a> <a href="/scholar?q=related:doXilyt42L8J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13823931184840869238&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'doXilyt42L8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:doXilyt42L8J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.9338&amp;rep=rep1&amp;type=pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/895JGE89F202WD9A.pdf" class=yC16>Story segmentation in news videos using visual and text cues</a></h3><div class="gs_a"><a href="/citations?user=tQKD7T8AAAAJ&amp;hl=en&amp;oi=sra">Y Zhai</a>, <a href="/citations?user=MeQC1XYAAAAJ&amp;hl=en&amp;oi=sra">A Yilmaz</a>, <a href="/citations?user=p8gsO3gAAAAJ&amp;hl=en&amp;oi=sra">M Shah</a> - Image and Video Retrieval, 2005 - Springer</div><div class="gs_rs">Abstract. In this paper, we present a framework for segmenting the news programs into <br>different story topics. The proposed method utilizes both visual and text information of the <br>video. We represent the news video by a Shot Connectivity Graph (SCG), where the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=386026046199183476&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 18</a> <a href="/scholar?q=related:dDjAzaRwWwUJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/24/59/RN171992136.html?source=googlescholar" class="gs_nph" class=yC18>BL Direct</a> <a href="/scholar?cluster=386026046199183476&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'dDjAzaRwWwUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm07-neosyOFF.PDF" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291278" class=yC19>The use of topic evolution to help users browse and find answers in news video corpus</a></h3><div class="gs_a">SY Neo, Y Ran, HK Goh, Y Zheng, TS Chua&hellip; - Proceedings of the 15th  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Earlier research in news video has been focusing mainly on improving retrieval <br>accuracies given the limited amount of extractable video semantics. In this paper, we <br>propose an enhancement to news video searching by leveraging extractable video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2755699895959884965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 18</a> <a href="/scholar?q=related:pRRFfnQ2PiYJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2755699895959884965&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'pRRFfnQ2PiYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://nguyendangbinh.org/Proceedings/ACCV/2006/papers/3852/38520633.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nguyendangbinh.org</span><span class="gs_ggsS">nguyendangbinh.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/fg326x0154j60374.pdf" class=yC1B>A hierarchical framework for generic sports video classification</a></h3><div class="gs_a">M Kolekar, S Sengupta - Computer VisionâACCV 2006, 2006 - Springer</div><div class="gs_rs">Abstract. A five layered, event driven hierarchical framework for generic sports video <br>classification has been proposed in this paper. The top layer classifications are based on a <br>few popular audio and video content analysis techniques like short-time energy and Zero <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3989295918095161067&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 14</a> <a href="/scholar?q=related:61b6VYDTXDcJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/47/0D/RN181238810.html?source=googlescholar" class="gs_nph" class=yC1D>BL Direct</a> <a href="/scholar?cluster=3989295918095161067&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'61b6VYDTXDcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://mklab.iti.gr/mklab_people/~papad/documents/TCSVT09.pdf" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iti.gr</span><span class="gs_ggsS">iti.gr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5159431" class=yC1E>Statistical motion information extraction and representation for semantic video analysis</a></h3><div class="gs_a"><a href="/citations?user=pJA7rO8AAAAJ&amp;hl=en&amp;oi=sra">GT Papadopoulos</a>, A Briassouli&hellip; - Circuits and Systems &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, an approach to semantic video analysis that is based on the statistical <br>processing and representation of the motion signal is presented. Overall, the examined <br>video is temporally segmented into shots and for every resulting shot appropriate motion <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9297781336407055005&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 12</a> <a href="/scholar?q=related:nZrMG91aCIEJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9297781336407055005&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'nZrMG91aCIEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICME%202004/html/papers/P62580.pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1394401" class=yC20>A hierarchical approach to story segmentation of large broadcast news video corpus</a></h3><div class="gs_a">L Chaisorn, TS Chua, CH Lee&hellip; - Multimedia and Expo,  &hellip;, 2004 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A multi-modal two-level framework for news story segmentation was proposed in <br>Chaisorn et al.(2002). This paper presents our extended work scaled to cope with a large <br>news video corpus used in TRECVID 2003 evaluation. We divided our system into two <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9960521794061186242&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 10</a> <a href="/scholar?q=related:wrSNcMzhOooJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9960521794061186242&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'wrSNcMzhOooJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/nus.partial.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/nus.partial.pdf" class=yC22>Two-level multi-modal framework for news story segmentation of large video corpus</a></h3><div class="gs_a">L Chaisorn, C Koh, Y Zhao, H Xu&hellip; - 12th Text Retrieval  &hellip;, 2003 - www-nlpir.nist.gov</div><div class="gs_rs">To tackle the problem of story segmentation, we proposed a two-level multi-modal <br>framework [Chaisorn et al. 2002]. First we analyze the video at the shot level using a variety <br>of low and high-level features, and classify the shots into pre-defined categories using <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14116296357896250010&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 9</a> <a href="/scholar?q=related:muKLBL8o58MJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14116296357896250010&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'muKLBL8o58MJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md16', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md16" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:muKLBL8o58MJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://www.mingzhao.name/publications/2006_MMM_PersonX.pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mingzhao.name</span><span class="gs_ggsS">mingzhao.name <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1651320" class=yC24>Multi-faceted contextual model for person identification in news video</a></h3><div class="gs_a"><a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a>, SY Neo, HK Goh&hellip; - Multi-Media Modelling  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Person identification is very important in the domain of multimedia news as it is <br>often the focus of events in news stories and interest of searchers. However, this detection is <br>impeded by the imprecise audio/visual analysis tools. In this paper, we describe a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4346048044377015775&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 9</a> <a href="/scholar?q=related:34XfhatDUDwJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4346048044377015775&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'34XfhatDUDwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://www.cecs.uci.edu/~papers/icme06/pdfs/0002101.pdf" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uci.edu</span><span class="gs_ggsS">uci.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4037046" class=yC26>Story boundary detection in news video using global rule induction technique</a></h3><div class="gs_a">L Chaisorn, TS Chua - Multimedia and Expo, 2006 IEEE  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Global rule induction technique has been successfully used in information <br>extraction (IE) from text documents. In this paper, we employ the technique to identify story <br>boundaries in news video. We divide our framework into two levels: shot and story levels. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18351877220226669860&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 7</a> <a href="/scholar?q=related:JJ0xcRn3rv4J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18351877220226669860&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'JJ0xcRn3rv4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://scholarbank.nus.edu.sg/bitstream/handle/10635/14865/ChaisornL.pdf?sequence=1" class=yC29><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu.sg/handle/10635/14865" class=yC28>A Hierarchical Multi-Modal approach to story segmentation in news video</a></h3><div class="gs_a">L Chaisorn - 2005 - scholarbank.nus.edu.sg</div><div class="gs_rs">This research presents a multi-modal two-level framework for news story segmentation. We <br>divide our system into two levels: shot level that classifies the input video shots into one of <br>the predefined categories using a hybrid of heuristic and learning based approaches; and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7879365373768026534&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 7</a> <a href="/scholar?q=related:psn0jg8hWW0J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7879365373768026534&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'psn0jg8hWW0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://lms.comp.nus.edu.sg/papers/media/2006/acmmm06-short-neosy.pdf" class=yC2B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1180687" class=yC2A>News video search with fuzzy event clustering using high-level features</a></h3><div class="gs_a">SY Neo, Y Zheng, TS Chua, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - Proceedings of the 14th annual  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract Precise automated video search is gaining in importance as the amount of <br>multimedia information is increasing at exponential rates. One of the drawbacks that make <br>video retrieval difficult is the lack of available semantics. In this paper, we propose to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1023496221001471478&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 10</a> <a href="/scholar?q=related:9lW_6WMwNA4J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1023496221001471478&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'9lW_6WMwNA4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://hal.archives-ouvertes.fr/docs/00/53/29/44/PDF/these_Ewa_Kijak.pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/tel-00532944/" class=yC2C>Structuration multimodale des vidÃ©os de sport par modÃ¨les stochastiques</a></h3><div class="gs_a">E Kijak - 2003 - hal.archives-ouvertes.fr</div><div class="gs_rs">Ce chapitre propose un panorama des systÃ¨mes spÃ©cifiques dÃ©diÃ©s plus particuliÃ¨rement <br>aux Ã©vÃ©nements sportifs. AprÃ¨s une description gÃ©nÃ©rale du contexte de l&#39;analyse des <br>vidÃ©os de sports, nous dÃ©crirons quelles sont les informations a priori utilisÃ©es par les <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10919920476736050839&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 10</a> <a href="/scholar?q=related:l9ZdP9FZi5cJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10919920476736050839&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'l9ZdP9FZi5cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md21', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md21" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:l9ZdP9FZi5cJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=1547231355426720420&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/83akqywkpr7c3alw.pdf" class=yC2E>Video scene segmentation using sequential change detection</a></h3><div class="gs_a">Z Li, H Lu, <a href="/citations?user=t9EqYQIAAAAJ&amp;hl=en&amp;oi=sra">YP Tan</a> - Advances in Multimedia Information Processing-PCM  &hellip;, 2005 - Springer</div><div class="gs_rs">In content-based video analysis, commonly the first step is to segment a video into <br>independent shots. However, it is rather inefficient to represent video using shot information, <br>as one hour video may contain more than a hundred shots. To address this limitation, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4824405596023399454&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 4</a> <a href="/scholar?q=related:HqRhK1S780IJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4824405596023399454&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'HqRhK1S780IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607678" class=yC2F>A novel scheme for video scenes segmentation and semantic representation</a></h3><div class="gs_a">S Zhu, Y Liu - Multimedia and Expo, 2008 IEEE International  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Grouping video contents into semantic segments is the crucial pass to content-<br>based video summarization and retrieval. In this paper, we present a novel scene <br>segmentation and semantic representation scheme for various video types. We first detect <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=689978619941632697&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 4</a> <a href="/scholar?q=related:uTo8MOZLkwkJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'uTo8MOZLkwkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://vireo.cs.cityu.edu.hk/papers/civr05_b.pdf" class=yC31><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/BF9F72QFVTTU0UBB.pdf" class=yC30>Hot event detection and summarization by graph modeling and matching</a></h3><div class="gs_a">Y Peng, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Image and Video Retrieval, 2005 - Springer</div><div class="gs_rs">Abstract. This paper proposes a new approach for hot event detection and summarization of <br>news videos. The approach is mainly based on two graph algorithms: optimal matching <br>(OM) and normalized cut (NC). Initially, OM is employed to measure the visual similarity <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4473150745825308660&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 5</a> <a href="/scholar?q=related:9O-wT-fSEz4J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/14/26/RN171992290.html?source=googlescholar" class="gs_nph" class=yC32>BL Direct</a> <a href="/scholar?cluster=4473150745825308660&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'9O-wT-fSEz4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://www.mmk.ei.tum.de/publ/pdf/06/06alh2.pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tum.de</span><span class="gs_ggsS">tum.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036586" class=yC33>A two-layer graphical model for combined video shot and scene boundary detection</a></h3><div class="gs_a">M Al-Hames, S Zettl, F Wallhoff, S Reiter&hellip; - Multimedia and Expo &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this work we present a novel two-layer hybrid graphical model for combined shot <br>and scene boundary detection in videos. In the first layer of the model, low-level features are <br>used to detect shot boundaries. The shot layer is connected to a higher layer that detects <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7462625143250025296&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 4</a> <a href="/scholar?q=related:UPtQcweSkGcJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7462625143250025296&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'UPtQcweSkGcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.freepatentsonline.com/EP1524857.html" class=yC35>Inferring information about media stream objects</a></h3><div class="gs_a">CJC Burges, <a href="/citations?user=1FwhEVYAAAAJ&amp;hl=en&amp;oi=sra">CE Herley</a> - EP Patent 1,524,857, 2005 - freepatentsonline.com</div><div class="gs_rs">Abstract: Information about media objects within media streams is inferred based on repeat <br>instances of the media objects within the media streams. A system and methods enable the <br>monitoring of one or more media streams and the identification of repeat instances of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7300601855254667757&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 3</a> <a href="/scholar?q=related:7SEDBbXyUGUJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'7SEDBbXyUGUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:7SEDBbXyUGUJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284772" class=yC36>News video retrieval using implicit event semantics</a></h3><div class="gs_a">SY Neo, Y Zheng, HK Goh, TS Chua&hellip; - Multimedia and Expo,  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Current state-of-the-art news video retrieval systems mainly focus on automated <br>speech recognition (ASR) text to perform retrieval. This paradigm greatly affects retrieval <br>performance as ASR text alone is not sufficient to provide an accurate representation of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12014342483156170817&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 4</a> <a href="/scholar?q=related:QXhobtWEu6YJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'QXhobtWEu6YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/notebook_papers/nus.paper.pdf" class=yC38><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/notebook_papers/nus.paper.pdf" class=yC37>TREC 2003 Video Retrieval and Story Segmentation task at NUS PRIS</a></h3><div class="gs_a">TS Chua, Y Zhao, L Chaisorn, CK Koh&hellip; - TREC (VIDEO)  &hellip;, 2003 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT This paper describes the details of our systems for story segmentation task and <br>search task of the TREC-2003 Video Track. In story segmentation task, we propose a two-<br>level multi-modal framework. First we analyze the video at the shot level using a variety of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7030869519621737973&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 3</a> <a href="/scholar?q=related:9c0ss5OqkmEJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7030869519621737973&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'9c0ss5OqkmEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md28', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md28" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:9c0ss5OqkmEJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4085334" class=yC39>Semantic indexing of news video sequences: a multimodal hierarchical approach based on hidden markov model</a></h3><div class="gs_a">MH Kolekar, S Sengupta - TENCON 2005 2005 IEEE Region  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose a system that semantically classifies news video at <br>different layers of semantic significance, using different elements of visual content. The <br>classification hierarchy generates low-level concepts, and concept hierarchy generates <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15603222728644429396&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 3</a> <a href="/scholar?q=related:VAYn_GbIidgJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'VAYn_GbIidgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5228544" class=yC3A>News story segmentation based on audio-visual features fusion</a></h3><div class="gs_a">Y Song, W Wang, F Guo - &hellip;  &amp; Education, 2009. ICCSE&#39;09. 4th  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a method for news video story segmentation, which fuses multi-<br>feature including audio and visual. At first, this paper detects the anchorperson shot for news <br>video and determines the beginning of news story, and then detects topic caption between <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4620016203566216012&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 2</a> <a href="/scholar?q=related:TPMcXkKYHUAJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'TPMcXkKYHUAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://d-nb.info/988948273/34" class=yC3C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from d-nb.info</span><span class="gs_ggsS">d-nb.info <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://d-nb.info/988948273/34" class=yC3B>Graphische Modelle in der Mustererkennung</a></h3><div class="gs_a">MA Al-Hames - 2008 - d-nb.info</div><div class="gs_rs">Kurzfassung Graphische Modelle verbinden die Wahrscheinlichkeits-und die <br>Graphentheorie: Mit Knoten und Kanten werden statistische AbhÃ¤ngigkeiten zwischen <br>Variablen ausgedrÃ¼ckt. Dadurch kÃ¶nnen Probleme intuitiv erfasst und hÃ¤ufig mit geringer <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8815607294694971217&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 2</a> <a href="/scholar?q=related:UdeinyBUV3oJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8815607294694971217&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'UdeinyBUV3oJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md31', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md31" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:UdeinyBUV3oJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:UdeinyBUV3oJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=8271570539362362324&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://server.cs.ucf.edu/~vision/papers/theses/yun_theses.pdf" class=yC3E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucf.edu</span><span class="gs_ggsS">ucf.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://server.cs.ucf.edu/~vision/papers/theses/yun_theses.pdf" class=yC3D>Video content extraction: Scene segmentation, linking and attention detection</a></h3><div class="gs_a"><a href="/citations?user=tQKD7T8AAAAJ&amp;hl=en&amp;oi=sra">Y Zhai</a> - 2006 - server.cs.ucf.edu</div><div class="gs_rs">Abstract In this fast paced digital age, a vast amount of videos are produced every day, such <br>as movies, TV programs, personal home videos, surveillance video, etc. This places a high <br>demand for effective video data analysis and management techniques. In this dissertation, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9386446955899964095&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 1</a> <a href="/scholar?q=related:v8p2XcZbQ4IJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9386446955899964095&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'v8p2XcZbQ4IJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md32', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md32" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:v8p2XcZbQ4IJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7788696&amp;id=KebUAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC3F>Inferring information about media stream objects</a></h3><div class="gs_a">CJC Burges, <a href="/citations?user=1FwhEVYAAAAJ&amp;hl=en&amp;oi=sra">CE Herley</a> - US Patent 7,788,696, 2010 - Google Patents</div><div class="gs_rs">Information about media objects within media streams is inferred based on repeat instances <br>of the media objects within the media streams. A system and methods enable the monitoring <br>of one or more media streams and the identification of repeat instances of media objects (<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11341340367381696425&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 1</a> <a href="/scholar?q=related:qY-VovmIZJ0J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11341340367381696425&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'qY-VovmIZJ0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://liris.cnrs.fr/Documents/Liris-4218.pdf" class=yC41><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cnrs.fr</span><span class="gs_ggsS">cnrs.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://liris.cnrs.fr/Documents/Liris-4218.pdf" class=yC40>Statistical Framework for Optimal Segmentation of Video</a></h3><div class="gs_a">V Parshyn, L Chen - 2006 - liris.cnrs.fr</div><div class="gs_rs">Abstract. Automatic segmentation of videos is widely used for structuring and is a necessary <br>preliminary step for many applications. In this report we propose an optimal strategy for the <br>temporal segmentation issue on the basis of statistical modeling. The goal is to maximize <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17298377969991282965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 1</a> <a href="/scholar?q=related:FfHF6xwvEPAJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17298377969991282965&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'FfHF6xwvEPAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md34', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md34" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:FfHF6xwvEPAJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://www.ceaj.org/Jweb_gcyyy/EN/article/downloadArticleFile.do?attachType=PDF&amp;id=20715" class=yC43><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ceaj.org</span><span class="gs_ggsS">ceaj.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ceaj.org/Jweb_gcyyy/EN/article/downloadArticleFile.do?attachType=PDF&amp;id=20715" class=yC42>åºäºæ­é³åè¯å«çæ°é»è§é¢æäºåå²æ¹æ³</a></h3><div class="gs_a">å¾æ°æï¼ æå½è¾ï¼ çäºè - Computer Engineering and Applications, 2008 - ceaj.org</div><div class="gs_rs">æè¦: æ°é»è§é¢çè¯­ä¹åååå²æ¯åºäºåå®¹çæ°é»è§é¢æ£ç´¢åææ¥ææçéè¦æ­¥éª¤, <br>åå°ä¼å¤ç ç©¶èçå³æ³¨, æåºäºJ ç§åºäºæ­é³åè¯å«çæ°é»è§é¢æäºååå²çæ°æ¹æ³, <br>é¦åä»æ°é»èç®ä¸­æååæ­é³åçå£°# æç¥ç¹å¾çä½ä¸ºå¶å£°çº¹, è®­ç»åºå¶ç¸åºçæ··åé«æ¯æ¨¡å(<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10703827324000699621&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=51">Cited by 1</a> <a href="/scholar?q=related:5YQuaD2ii5QJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10703827324000699621&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'5YQuaD2ii5QJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md35', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md35" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:5YQuaD2ii5QJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1549614" class=yC44>An efficient news video browsing system for wireless network application</a></h3><div class="gs_a">CY Chen, JC Wang, JF Wang&hellip; - &hellip;  and Mobile Computing,  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, an efficient news video browsing system for wireless network <br>application is presented. To provide a better news video categorization, the news scripts of <br>all the news stories are utilized to perform semantic analysis first. Theses stories are then <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:k1bfBL6Y0F4J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6832128576613996179&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'k1bfBL6Y0F4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://www.ntu.edu.sg/eee/urop/Congress2003/Proceedings/abstract/NUS_SoC/NUS-NeoShiYong-SoC.pdf" class=yC46><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ntu.edu.sg/eee/urop/Congress2003/Proceedings/abstract/NUS_SoC/NUS-NeoShiYong-SoC.pdf" class=yC45>NUROP CONGRESS PAPER SEARCHING FOR MULTIMEDIA NEWS ON THE WEB</a></h3><div class="gs_a">SY NEO, TS CHUA - ntu.edu.sg</div><div class="gs_rs">ABSTRACT Current search engine is too general and often return too much of irrelevant <br>information in respond to user&#39;s free-text queries. Many redundant documents are returned <br>and this often takes users a long time to find the actual piece of news that they want. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:TPQsCV-wtKsJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12372707998515917900&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'TPQsCV-wtKsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md37', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md37" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TPQsCV-wtKsJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/icme06-lekha.pdf" class=yC48><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/icme06-lekha.pdf" class=yC47>Information Extraction from News Video using Global Rule Induction Technique</a></h3><div class="gs_a">L Chaisorn, TS Chua - 137.132.145.151</div><div class="gs_rs">ABSTRACT Global rule induction technique has been successfully used in information <br>extraction (IE) from text documents. In this paper, we employ global rule induction technique <br>to perform information extraction from news video documents. We divide our framework <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:TWgyGOtWTg0J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=958799338399754317&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'TWgyGOtWTg0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md38', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md38" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TWgyGOtWTg0J:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB39" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW39"><a href="http://www.buet.ac.bd/icece/publ2004/P072.pdf" class=yC4A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from buet.ac.bd</span><span class="gs_ggsS">buet.ac.bd <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.buet.ac.bd/icece/publ2004/P072.pdf" class=yC49>HIERARCHICAL NEWS VIDEO CATEGORIZATION BASED ON SEMANTIC ANALYSIS</a></h3><div class="gs_a">JC Wang, CY Chen, JF Wang, CP Chen - buet.ac.bd</div><div class="gs_rs">ABSTRACT This study presents a hierarchical news video categorization architecture based <br>on semantic analysis. First, news video is segmented into different news stories based on <br>anchorperson shot detection. For each news story, we extract its caption information to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:FddUYi6GnWsJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7754501667178206997&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'FddUYi6GnWsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md39', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md39" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:FddUYi6GnWsJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4221881" class=yC4B>Modeling the Dance Video Annotations</a></h3><div class="gs_a">B Ramadoss, K Rajkumar - Digital Information Management,  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a dance video content model (DVCM) to represent the <br>semantics of the dance videos at multiple granularity levels. The DVCM is designed based <br>on the concepts such as video, shot, segment, event and object, which are the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:qNUaaqAaIiEJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'qNUaaqAaIiEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB41" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW41"><a href="http://137.132.14.55/bitstream/handle/10635/14402/thesis.pdf?sequence=1" class=yC4D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.14.55</span><span class="gs_ggsS">137.132.14.55 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://137.132.14.55/handle/10635/14402" class=yC4C>Event detection in soccer video based on audio/visual keywords</a></h3><div class="gs_a">K YULIN - 2004 - 137.132.14.55</div><div class="gs_rs">In this thesis, we propose a multi-modal two-level event detection framework and <br>demonstrate it on soccer videos. We use a mid-level representation called Audio and Visual <br>Keyword (AVK) that can be learned and detected in video segments. AVKs are intended to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:8u8rejdXeoUJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9618095849987633138&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'8u8rejdXeoUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> STORY BOUNDARY DETECTION IN NEWS VIDEO USING GLOBAL RULE INDUCTION</h3><div class="gs_a">L Chaisorn, TS Chua</div><div class="gs_fl"><a href="/scholar?q=related:bp_kg6ncRhQJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'bp_kg6ncRhQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1296910" class=yC4E>Question answering on large news video archive</a></h3><div class="gs_a">TS Chua - Image and Signal Processing and Analysis, 2003.  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Many users are interested in looking for precise answers when querying the news <br>video archives, while current systems are designed to return only video sequences. This <br>research explores the use of question-answering (QA) to support personalized news <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:UA2yh4L1WjwJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'UA2yh4L1WjwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4620874" class=yC4F>A segmentation method of news video stories based on announcer&#39;s voiceprint</a></h3><div class="gs_a">XW Xu, GH Li, J Yuan - Machine Learning and Cybernetics,  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract As an important step of content based news video retrieving and intelligence <br>mining, semantic unit segmentation has attracted many researcherspsila interests. This <br>paper focuses on a new method of news video stories segmentation which is based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3x-6q3Q38NAJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'3x-6q3Q38NAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/01H6050H20380470.pdf" class=yC50>On-line video abstract generation of multimedia news</a></h3><div class="gs_a">V ValdÃ©s, <a href="/citations?user=k_b4Fp4AAAAJ&amp;hl=en&amp;oi=sra">JM MartÃ­nez</a> - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract The amount of video content available nowadays makes video abstraction <br>techniques a necessary tool to ease the access to the already huge and ever growing video <br>databases. Nevertheless, many of the existing video abstraction approaches have high <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:tf59s8dAYxIJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1324973941831106229&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'tf59s8dAYxIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4665043" class=yC51>Effective video event detection via subspace projection</a></h3><div class="gs_a"><a href="/citations?user=d3h-zScAAAAJ&amp;hl=en&amp;oi=sra">J Shen</a>, D Tao, <a href="/citations?user=ahUibskAAAAJ&amp;hl=en&amp;oi=sra">X Li</a> - &hellip;  Processing, 2008 IEEE 10th Workshop on, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes a new video event detection framework based on subspace <br>selection technique. With the approach, feature vectors presenting different kinds of video <br>information can be easily projected from different modalities onto an unified subspace, on <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:rQbezBYLYIkJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9898924173514639021&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'rQbezBYLYIkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=UkXhfJD1maoC&amp;oi=fnd&amp;pg=PA157&amp;ots=5XqFWjE8SU&amp;sig=1PtCmEtdHyTzdiAm00JLC_3ErjE" class=yC52>TV Program Structuring Techniques</a></h3><div class="gs_a">AE Abduraman, SA Berrani&hellip; - TV Content Analysis:  &hellip;, 2012 - books.google.com</div><div class="gs_rs">158â  TV ContentAnalysis: Techniques and Applications 6.4 Conclusion.....................................<br>........................... 174 References....................................................................... 175 6.1 Introduction <br>The objective of this chapter is to present the problem of TV program structuring and its <b> ...</b> </div><div class="gs_fl"><a href="/scholar?q=related:Q_cM1J21ZxMJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Q_cM1J21ZxMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://doras.dcu.ie/17254/1/csaba_czirjek_20120702095211.pdf" class=yC54><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dcu.ie</span><span class="gs_ggsS">dcu.ie <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://doras.dcu.ie/17254/" class=yC53>Main character detection in news and movie content</a></h3><div class="gs_a">C CzirjeÌk - 2005 - doras.dcu.ie</div><div class="gs_rs">Advances in multimedia compression standards, data storage, digital hardware technology <br>and network performance have led to a considerable increase in the amount of digital <br>content being archived and made available online. As a result, data organization, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gEPT4NWEGFQJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'gEPT4NWEGFQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/16051/thesis-sub.pdf?sequence=1" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/16051" class=yC55>combining multimodal external resources for event-based news video retrieval and question answering</a></h3><div class="gs_a">NEOSHI YONG - 2008 - scholarbank.nus.edu</div><div class="gs_rs">The ever-increasing amount of multimedia data available online creates an urgent need on <br>how to index these information and support effective retrieval by users. In recent years, we <br>observe the gradual shift from performing retrieval solely based on analyzing one media <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:h5EV9TPOgHUJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8466994022007017863&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'h5EV9TPOgHUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB50" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW50"><a href="http://www.joces.org.cn/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=12495" class=yC58><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from joces.org.cn</span><span class="gs_ggsS">joces.org.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.joces.org.cn/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=12495" class=yC57>ä¸ç§å¤æ¨¡æèåæ°é»è§é¢æ¡ç®åå²ç®æ³</a></h3><div class="gs_a">çå½è¥ï¼ å¯çº¢å¬ï¼ ææ¶ - è®¡ç®æºå·¥ç¨ä¸ç§å­¦, 2011 - joces.org.cn</div><div class="gs_rs">æè¦æ°é»è§é¢æ¡ç®åå²æ¯æ°é»è§é¢æ£ç´¢åæµè§ä¸­éè¦çåºå±æ¯æææ¯æ¬ææåºäºä¸ç§èåä¸»æ<br>äººæ¨¡æ¿å¹éåä¸»é¢å­å¹å¸§æ£æµçå¤æ¨¡ææ°é»è§é¢æ¡ç®åå²ç®æ³åç¨åºäºä¸»æäººæ¨¡æ¿çç®æ³è¿è¡<br>ç¬¬ä¸æ¬¡åå²åç¨åºäºæ¹è¿çå­å¹æ£æµæ¹æ³è¿è¡ç¬¬äºæ¬¡åå²æåå°ä¸¤æ¬¡åå²çç»æèåå¹¶å»é¤<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:RrDqKLCzS-kJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16810727603272593478&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'RrDqKLCzS-kJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md50', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md50" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:RrDqKLCzS-kJ:scholar.google.com/&amp;hl=en&amp;num=51&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
