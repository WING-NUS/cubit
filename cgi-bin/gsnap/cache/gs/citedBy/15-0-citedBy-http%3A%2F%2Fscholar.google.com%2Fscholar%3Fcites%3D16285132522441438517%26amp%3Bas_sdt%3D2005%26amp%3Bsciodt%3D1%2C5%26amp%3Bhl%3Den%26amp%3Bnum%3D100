Total results = 15
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.eurecom.fr/~schutz/publications/ICIP08%20-%20Eurecom%20-%20A%20Multimodal%20Approach%20to%20Music%20Transcription.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurecom.fr</span><span class="gs_ggsS">eurecom.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4711699" class=yC0>A multimodal approach to music transcription</a></h3><div class="gs_a"><a href="/citations?user=vU_Wzf4AAAAJ&amp;hl=en&amp;oi=sra">M Paleari</a>, <a href="/citations?user=KdPSGmAAAAAJ&amp;hl=en&amp;oi=sra">B Huet</a>, A Schutz&hellip; - Image Processing, 2008.  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Music transcription refers to extraction of a human readable and interpretable <br>description from a recording of a music performance. Automatic music transcription remains, <br>nowadays, a challenging research problem when dealing with polyphonic sounds or <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=49686493817295155&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 13</a> <a href="/scholar?q=related:M1EdVpqFsAAJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=49686493817295155&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'M1EdVpqFsAAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_Application_Specific_Music_Transcription_for_Tutoring.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4623948" class=yC2>Application-specific music transcription for tutoring</a></h3><div class="gs_a">Y Wang, B Zhang - Multimedia, IEEE, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic music transcription (AMT) refers to the ability of computers to write note <br>information, such as the pitch, onset time, duration, and source of each sound, after listening <br>to the music. Our application scenario is computer-assisted, musical-instrument tutoring, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=176626421973561276&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 5</a> <a href="/scholar?q=related:vK-ug8uAcwIJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=176626421973561276&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 18 versions</a> <a onclick="return gs_ocit(event,'vK-ug8uAcwIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://numediart.org/docs/numediart_2009_s07_p1_report.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from numediart.org</span><span class="gs_ggsS">numediart.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://numediart.org/docs/numediart_2009_s07_p1_report.pdf" class=yC4>Multimodal Guitar: Performance Toolbox and Study Workbench</a></h3><div class="gs_a">C Frisson, L ReboursiÃ¨re, WY Chu&hellip; - QPSR of the numediart &hellip;, 2009 - numediart.org</div><div class="gs_rs">ABSTRACT This project aims at studying how recent interactive and interaction technologies <br>would help extend how we play the guitar, thus defining the âmultimodal guitarâ. We <br>investigate two axes, 1)âA gestural/polyphonic sensing/processing toolbox to augment <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3723913816448989750&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 3</a> <a href="/scholar?q=related:Nnqcy-L_rTMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3723913816448989750&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'Nnqcy-L_rTMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md2', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md2" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Nnqcy-L_rTMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="https://dl.comp.nus.edu.sg/dspace/bitstream/1900.100/3056/1/TRA7-09.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://dl.comp.nus.edu.sg/dspace/handle/1900.100/3056" class=yC6>Automatic music transcription using audio-visual fusion for violin practice in home environment</a></h3><div class="gs_a">B Zhang, Y Wang - 2009 - dl.comp.nus.edu.sg</div><div class="gs_rs">Abstract: Violin practice in a home environment, where there is often no teacher available, <br>can benefit from automatic music transcription to provide feedback to the student. This paper <br>describes a high performance violin transcription system with three main contributions. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15951167519198165592&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 2</a> <a href="/scholar?q=related:WDqOhljuXd0J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15951167519198165592&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'WDqOhljuXd0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.tandfonline.com/doi/abs/10.1080/09298215.2012.708048" class=yC8>Temporal Differences in String Bowing of Symphony Orchestra Players</a></h3><div class="gs_a"><a href="/citations?user=IYunNg0AAAAJ&amp;hl=en&amp;oi=sra">J PÃ¤tynen</a>, <a href="/citations?user=NYm9lH8AAAAJ&amp;hl=en&amp;oi=sra">S Tervo</a>, <a href="/citations?user=vz4vC1cAAAAJ&amp;hl=en&amp;oi=sra">T Lokki</a> - Journal of New Music Research, 2012 - Taylor &amp; Francis</div><div class="gs_rs">Abstract A study on the temporal differences between the bowing of string instrument players <br>in an orchestra is presented. A high quality video of a professional symphony orchestra was <br>recorded high above the first rows of the audience area. The movement of the instrument <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3MDuCziL-cIJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14049413585393598684&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'3MDuCziL-cIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Workshops/data/4729a163.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6266249" class=yC9>Real-Time Pitch Training System for Violin Learners</a></h3><div class="gs_a">JH Wang, SA Wang, WC Chen&hellip; - Multimedia and Expo &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper specifically targets violin learners who are working on their pitch <br>accuracy. We employ a pitch tracking algorithm to extract the pitch played. Through volume <br>thresholding and region detection, only parts of frames are processed. So our system can <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gMp1DiddLkkJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5273254636025137792&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'gMp1DiddLkkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://137.132.14.55/bitstream/handle/10635/20949/ZhangBJ.pdf?sequence=1" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.14.55</span><span class="gs_ggsS">137.132.14.55 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://137.132.14.55/handle/10635/20949" class=yCB>Adaptive multimodal fusion based similarity measures in music information retrieval</a></h3><div class="gs_a">Z BINGJUN - 2010 - 137.132.14.55</div><div class="gs_rs">In the field of music information retrieval (MIR), one fundamental research problem is the <br>measuring of the similarity between music documents. Based on a viable similarity measure, <br>MIR systems can be made more effective to help users retrieve relevant music information. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9FjpHoxTlRIJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1339068325491726580&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'9FjpHoxTlRIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.3219&amp;rep=rep1&amp;type=pdf#page=26" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.3219&amp;rep=rep1&amp;type=pdf#page=26" class=yCD>Project# 03 Multimodal Guitar: Performance Toolbox and Study Workbench</a></h3><div class="gs_a">C Frisson, L Reboursiere, WY Chu, O LÃ¤hdeoja&hellip; - on Multimodal Interfaces &hellip;, 2009 - Citeseer</div><div class="gs_rs">AbstractâThis project aims at studying how recent interactive and interaction technologies <br>would help extend how we play the guitar, thus defining the âmultimodal guitarâ. We <br>investigate two axes, 1)âA gestural/polyphonic sensing/processing toolbox to augment <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9IzfhnW3LSQJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'9IzfhnW3LSQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:9IzfhnW3LSQJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.nickgillian.com/papers/Gillian_Digito.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nickgillian.com</span><span class="gs_ggsS">nickgillian.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.nickgillian.com/papers/Gillian_Digito.pdf" class=yCF>Digito: A Fine-Grain Gesturally Controlled Virtual Musical Instrument</a></h3><div class="gs_a">N Gillian, JA Paradiso - nickgillian.com</div><div class="gs_rs">ABSTRACT This paper presents Digito, a gesturally controlled virtual musical instrument. <br>Digito is controlled through a number of intricate hand gestures, providing both discrete and <br>continuous control of Digito&#39;s sound engine; with the finegrain hand gestures captured by <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:e-lUqt60e3wJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8969961951271905659&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'e-lUqt60e3wJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:e-lUqt60e3wJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://130.102.44.246/journals/computer_music_journal/v036/36.3.maezawa.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 130.102.44.246</span><span class="gs_ggsS">130.102.44.246 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.mitpressjournals.org/doi/abs/10.1162/COMJ_a_00129" class=yC11>Automated Violin Fingering Transcription Through Analysis of an Audio Recording</a></h3><div class="gs_a">A Maezawa, K Itoyama, K Komatani, <a href="/citations?user=_SB7ZjQAAAAJ&amp;hl=en&amp;oi=sra">T Ogata</a>&hellip; - Computer Music  &hellip;, 2012 - MIT Press</div><div class="gs_rs">We present a method to recuperate fingerings for a given piece of violin music in order to <br>recreate the timbre of a given audio recording of the piece. This is achieved by first <br>analyzing an audio signal to determine the most likely sequence of two-dimensional <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:toQ_tQ8Bi2oJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7677231156760118454&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'toQ_tQ8Bi2oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.eecs.umich.edu/nime2012/Proceedings/papers/82_Final_Manuscript.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from umich.edu</span><span class="gs_ggsS">umich.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.eecs.umich.edu/nime2012/Proceedings/papers/82_Final_Manuscript.pdf" class=yC13>Drum Stroke Computing: Multimodal Signal Processing for Drum Stroke Identification and Performance Metrics</a></h3><div class="gs_a">J Hochenbaum, A Kapur - eecs.umich.edu</div><div class="gs_rs">ABSTRACT In this paper we present a multimodal system for analyzing drum performance. <br>In the first example we perform automatic drum hand recognition utilizing a technique for <br>automatic labeling of training data using direct sensors, and only indirect sensors (eg a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:nftu-m8sjg0J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'nftu-m8sjg0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:nftu-m8sjg0J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://hal-institut-mines-telecom.archives-ouvertes.fr/docs/00/57/64/71/PDF/thesis_Schutz.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://hal-institut-mines-telecom.archives-ouvertes.fr/docs/00/57/64/71/PDF/thesis_Schutz.pdf" class=yC15>Antony Schutz</a></h3><div class="gs_a">SM eta la SÃ©paration - 2011 - hal-institut-mines-telecom.archives- &hellip;</div><div class="gs_rs">RÃ©sumÃ© Pour les Ãªtres humains, le son n&#39;a d&#39;importance que pour son contenu. La voie est <br>un langage parlÃ©, la musique une intention artistique. Le processus physiologique est <br>hautement dÃ©veloppÃ©, tout comme notre capacitÃ©a comprendre les processus sous-jacent<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:-IK9Kr9m-LcJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13256458474406904568&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'-IK9Kr9m-LcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:-IK9Kr9m-LcJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://hal-institut-mines-telecom.archives-ouvertes.fr/docs/00/55/93/01/PDF/Paleari_-_PhD_Thesis.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://hal-institut-mines-telecom.archives-ouvertes.fr/docs/00/55/93/01/PDF/Paleari_-_PhD_Thesis.pdf" class=yC17>Marco PALEARI</a></h3><div class="gs_a">O des Ãmotions - 2009 - hal-institut-mines-telecom.archives- &hellip;</div><div class="gs_rs">Abstract Affective Computing refers to computing that relates to, arises from, or deliberately <br>influences emotions and has is natural application domain in highly abstracted <br>humancomputer interactions. Affective computing can be divided into three main parts, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:TNQWKyO5Ep8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11462427562309243980&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'TNQWKyO5Ep8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md12', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md12" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TNQWKyO5Ep8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://pastel.archives-ouvertes.fr/docs/00/55/93/01/PDF/Paleari_-_PhD_Thesis.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://pastel.archives-ouvertes.fr/pastel-00005615/" class=yC19>Informatique Affective: Affichage, Reconnaissance, et SynthÃ¨se par Ordinateur des Ãmotions</a></h3><div class="gs_a"><a href="/citations?user=vU_Wzf4AAAAJ&amp;hl=en&amp;oi=sra">M Paleari</a> - 2009 - pastel.archives-ouvertes.fr</div><div class="gs_rs">Abstract Affective Computing refers to computing that relates to, arises from, or deliberately <br>influences emotions and has is natural application domain in highly abstracted <br>humancomputer interactions. Affective computing can be divided into three main parts, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:upMHWFSL_a4J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12609387726077662138&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'upMHWFSL_a4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://pastel.archives-ouvertes.fr/docs/00/57/64/71/PDF/thesis_Schutz.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://pastel.archives-ouvertes.fr/pastel-00576471/" class=yC1B>Quelques Contributions au Traitement de Signal Musical et Ã  la SÃ©paration Aveugle de Source Audio Mono-Microphone</a></h3><div class="gs_a">A Schutz - 2010 - pastel.archives-ouvertes.fr</div><div class="gs_rs">RÃ©sumÃ© Pour les Ãªtres humains, le son n&#39;a d&#39;importance que pour son contenu. La voie est <br>un langage parlÃ©, la musique une intention artistique. Le processus physiologique est <br>hautement dÃ©veloppÃ©, tout comme notre capacitÃ©a comprendre les processus sous-jacent<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Iah5e2Ivl8gJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14454073629093767201&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Iah5e2Ivl8gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
