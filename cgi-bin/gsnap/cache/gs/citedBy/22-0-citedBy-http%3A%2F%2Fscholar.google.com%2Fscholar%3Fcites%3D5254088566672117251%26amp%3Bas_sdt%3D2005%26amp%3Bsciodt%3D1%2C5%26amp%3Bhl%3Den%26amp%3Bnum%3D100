Total results = 22
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://ww.journalofvision.org/content/11/4/14.full" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from journalofvision.org</span><span class="gs_ggsS">journalofvision.org <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ww.journalofvision.org/content/11/4/14.short" class=yC0>Fixations on low-resolution images</a></h3><div class="gs_a">T Judd, <a href="/citations?user=NJ9c4ygAAAAJ&amp;hl=en&amp;oi=sra">F Durand</a>, <a href="/citations?user=8cxDHS4AAAAJ&amp;hl=en&amp;oi=sra">A Torralba</a> - Journal of vision, 2011 - ww.journalofvision.org</div><div class="gs_rs">Abstract When an observer looks at an image, his eyes fixate on a few select points. <br>Fixations from different observers are often consistentâobservers tend to look at the same <br>locations. We investigate how image resolution affects fixation locations and consistency <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6710963233056141464&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22">Cited by 13</a> <a href="/scholar?q=related:mLAEF4EhIl0J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6710963233056141464&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'mLAEF4EhIl0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://research.microsoft.com/en-us/people/yichenw/iccv11_salientobjectdetection.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from microsoft.com</span><span class="gs_ggsS">microsoft.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6126348" class=yC2>Salient object detection by composition</a></h3><div class="gs_a"><a href="/citations?user=X9A-Tp4AAAAJ&amp;hl=en&amp;oi=sra">J Feng</a>, <a href="/citations?user=8qSLKUEAAAAJ&amp;hl=en&amp;oi=sra">Y Wei</a>, L Tao, C Zhang&hellip; - Computer Vision (ICCV),  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Conventional saliency analysis methods measure the saliency of individual pixels. <br>The resulting saliency map inevitably loses information in the original image and finding <br>salient objects in it is difficult. We propose to detect salient objects by directly measuring <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13535332996285042268&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22">Cited by 12</a> <a href="/scholar?q=related:XI4xmKUp17sJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13535332996285042268&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'XI4xmKUp17sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.comp.nus.edu.sg/~harishk/homepage_material/ACM_MM_2010_harish.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874047" class=yC4>Making computers look the way we look: exploiting visual attention for image understanding</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, <a href="/citations?user=mUvcmRsAAAAJ&amp;hl=en&amp;oi=sra">R Subramanian</a>, M Kankanhalli&hellip; - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Human Visual attention (HVA) is an important strategy to focus on specific <br>information while observing and understanding visual stimuli. HVA involves making a series <br>of fixations on select locations while performing tasks such as object recognition, scene <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10386692180968100685&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22">Cited by 4</a> <a href="/scholar?q=related:TVPu7YHxJJAJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10386692180968100685&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'TVPu7YHxJJAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0262885611001235" class=yC6>Saliency from hierarchical adaptation through decorrelation and variance normalization</a></h3><div class="gs_a">A Garcia-Diaz, XRF Vidal, XM Pardo, R Dosil - Image and Vision  &hellip;, 2011 - Elsevier</div><div class="gs_rs">This paper presents a novel approach to visual saliency that relies on a contextually <br>adapted representation produced through adaptive whitening of color and scale features. <br>Unlike previous models, the proposal is grounded on the specific adaptation of the basis <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4376373513755637836&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22">Cited by 3</a> <a href="/scholar?q=related:TDjVE4YAvDwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4376373513755637836&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'TDjVE4YAvDwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6253254" class=yC7>Quantitative Analysis of Human-Model Agreement in Visual Saliency Modeling: A Comparative Study</a></h3><div class="gs_a"><a href="/citations?user=7jTNT1IAAAAJ&amp;hl=en&amp;oi=sra">A Borji</a>, DN Sihite, <a href="/citations?user=xhUvqK8AAAAJ&amp;hl=en&amp;oi=sra">L Itti</a> - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Visual attention is a process that enables biological and machine vision systems to <br>select the most relevant regions from a scene. Relevance is determined by two components: <br>1) top-down factors driven by task and 2) bottom-up factors that highlight image regions <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=891326704412286600&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22">Cited by 2</a> <a href="/scholar?q=related:iF5QS-qgXgwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=891326704412286600&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'iF5QS-qgXgwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.iis.sinica.edu.tw/~liutyng/Publication_files/iccv11.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sinica.edu.tw</span><span class="gs_ggsS">sinica.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6126333" class=yC8>Fusing generic objectness and visual saliency for salient object detection</a></h3><div class="gs_a">KY Chang, <a href="/citations?user=20N2rlkAAAAJ&amp;hl=en&amp;oi=sra">TL Liu</a>, <a href="/citations?user=mwFy9kkAAAAJ&amp;hl=en&amp;oi=sra">HT Chen</a>&hellip; - Computer Vision (ICCV),  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present a novel computational model to explore the relatedness of objectness <br>and saliency, each of which plays an important role in the study of visual attention. The <br>proposed framework conceptually integrates these two concepts via constructing a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9598052572706151903&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22">Cited by 3</a> <a href="/scholar?q=related:352sU_YhM4UJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9598052572706151903&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'352sU_YhM4UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/harish-ISM2011.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6123364" class=yCA>Affective video summarization and story board generation using pupillary dilation and eye gaze</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, K Yadati, M Kankanhalli&hellip; - Multimedia (ISM), 2011  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We propose a semi-automated, eye-gaze based method for affective analysis of <br>videos. Pupillary Dilation (PD) is introduced as a valuable behavioural signal for <br>assessment of subject arousal and engagement. We use PD information for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7986207911324922454&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22">Cited by 1</a> <a href="/scholar?q=related:VrLvWMi11G4J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7986207911324922454&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'VrLvWMi11G4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072305" class=yCC>Can computers learn from humans to see better?: inferring scene semantics from viewers&#39; eye movements</a></h3><div class="gs_a"><a href="/citations?user=mUvcmRsAAAAJ&amp;hl=en&amp;oi=sra">R Subramanian</a>, <a href="/citations?user=vOHjMGcAAAAJ&amp;hl=en&amp;oi=sra">V Yanulevskaya</a>, <a href="/citations?user=fOmROdkAAAAJ&amp;hl=en&amp;oi=sra">N Sebe</a> - Proceedings of the 19th  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract This paper describes an attempt to bridge the semantic gap between computer <br>vision and scene understanding employing eye movements. Even as computer vision <br>algorithms can efficiently detect scene objects, discovering semantic relationships <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1058432919547193662&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22">Cited by 1</a> <a href="/scholar?q=related:Pqkw7CFPsA4J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Pqkw7CFPsA4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.comp.nus.edu.sg/~harishk/homepage_material/research_description.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.comp.nus.edu.sg/~harishk/homepage_material/research_description.pdf" class=yCD>Brief summary of work done during PhD</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a> - comp.nus.edu.sg</div><div class="gs_rs">The focus of my PhD thesis has been to get a better understanding of visual perception and <br>attention as people interact with digital images and video. My first problem was on finding <br>how low level global and local information in images influence category discrimination <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:wlNrPDqYTssJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'wlNrPDqYTssJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:wlNrPDqYTssJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/N04005105X661350.pdf" class=yCF>Bottom-up saliency detection for attention determination</a></h3><div class="gs_a">SS Ge, <a href="/citations?user=-d7xjlMAAAAJ&amp;hl=en&amp;oi=sra">H He</a>, <a href="/citations?user=Vy9dg7sAAAAJ&amp;hl=en&amp;oi=sra">Z Zhang</a> - Machine Vision and Applications, 2011 - Springer</div><div class="gs_rs">Abstract In this paper, the technique of saliency detection is proposed to model people&#39;s <br>biological ability of attending to their interest. There are two phases in the scheme of <br>intelligent saliency searching: saliency filtering and saliency refinement. In saliency <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:E3zER8PLkfwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'E3zER8PLkfwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2168629" class=yC10>Incorporating visual field characteristics into a saliency map</a></h3><div class="gs_a">H Kubota, <a href="/citations?user=WCrRFtkAAAAJ&amp;hl=en&amp;oi=sra">Y Sugano</a>, <a href="/citations?user=P1FZwkAAAAAJ&amp;hl=en&amp;oi=sra">T Okabe</a>, <a href="/citations?user=VEUW-qIAAAAJ&amp;hl=en&amp;oi=sra">Y Sato</a>&hellip; - Proceedings of the  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Characteristics of the human visual field are well known to be different in central <br>(fovea) and peripheral areas. Existing computational models of visual saliency, however, do <br>not take into account this biological evidence. The existing models compute visual <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3zq5TfKzDv0J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'3zq5TfKzDv0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2168573" class=yC11>Let&#39;s look at the cockpit: exploring mobile eye-tracking for observational research on the flight deck</a></h3><div class="gs_a"><a href="/citations?user=vSq5PgUAAAAJ&amp;hl=en&amp;oi=sra">N Weibel</a>, A Fouse, C Emmenegger&hellip; - Proceedings of the  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract As part of our research on multimodal analysis and visualization of activity <br>dynamics, we are exploring the integration of data produced by a variety of sensor <br>technologies within ChronoViz, a tool aimed at supporting the simultaneous visualization <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6147581095304337992&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22">Cited by 1</a> <a href="/scholar?q=related:SJqFVW-YUFUJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'SJqFVW-YUFUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396378" class=yC12>Touch saliency</a></h3><div class="gs_a">M Xu, <a href="/citations?user=V9W87PYAAAAJ&amp;hl=en&amp;oi=sra">B Ni</a>, J Dong, Z Huang, M Wang&hellip; - Proceedings of the 20th  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In this work, we propose a new concept of touch saliency, and attempt to answer the <br>question of whether the underlying image saliency map may be implicitly derived from the <br>accumulative touch behaviors (or more specifically speaking, zoom-in and panning <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'bVcWgIwTJX4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/harish-mm11.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072406" class=yC13>Eye-tracking methodology and applications to images and video</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, M Kankanhalli - Proceedings of the 19th ACM international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Our tutorial introduces eye-tracking as an exciting, non-intrusive method of <br>capturing user attention during human interaction with digital images and videos. We <br>believe eye-gaze can play a valuable role in understanding and processing (a) huge <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9cC-iDePvZoJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11150225721119391989&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'9cC-iDePvZoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/DMR16NL1N8H87460.pdf" class=yC15>Visual Attention Prediction Using Saliency Determination of Scene Understanding for Social Robots</a></h3><div class="gs_a"><a href="/citations?user=-d7xjlMAAAAJ&amp;hl=en&amp;oi=sra">H He</a>, SS Ge, <a href="/citations?user=Vy9dg7sAAAAJ&amp;hl=en&amp;oi=sra">Z Zhang</a> - International Journal of Social Robotics, 2011 - Springer</div><div class="gs_rs">Abstract In this paper, the biological ability of visual attention is modeled for social robots to <br>understand scenes and circumstance. Visual attention is determined by evaluating visual <br>stimuli and prior knowledge in the intelligent saliency searching. Visual stimuli are <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:r0T0kMrqGnwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8942718165833761967&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'r0T0kMrqGnwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://research.microsoft.com/en-us/um/people/jiansun/papers/ECCV12_GeodesicSaliency.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from microsoft.com</span><span class="gs_ggsS">microsoft.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://research.microsoft.com/en-us/um/people/jiansun/papers/ECCV12_GeodesicSaliency.pdf" class=yC16>Geodesic Saliency Using Background Priors</a></h3><div class="gs_a"><a href="/citations?user=8qSLKUEAAAAJ&amp;hl=en&amp;oi=sra">Y Wei</a>, F Wen, W Zhu, J Sun - research.microsoft.com</div><div class="gs_rs">Abstract. Generic object level saliency detection is important for many vision tasks. Previous <br>approaches are mostly built on the prior that âappearance contrast between objects and <br>backgrounds is highâ. Although various computational models have been developed, the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:eKzHn3etUcwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14722739386144238712&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'eKzHn3etUcwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md15', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md15" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:eKzHn3etUcwJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://137.132.14.55/handle/10635/33304" class=yC18>Human Visual Perception, study and applications to understanding Images and Videos</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a> - 2011 - 137.132.14.55</div><div class="gs_rs">Assessing whether a photograph is interesting, or spotting people in conversation or <br>important objects in an images and videos, are visual tasks that we humans do effortlessly <br>and in a robust manner. In this thesis I first explore and quantify how humans distinguish <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jna1s1LyMH0J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9020976490639357582&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'jna1s1LyMH0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md16', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md16" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jna1s1LyMH0J:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://clic.cimec.unitn.it/~elia.bruni/publications/yanulevskaya-etal-acm-2012.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unitn.it</span><span class="gs_ggsS">unitn.it <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://clic.cimec.unitn.it/~elia.bruni/publications/yanulevskaya-etal-acm-2012.pdf" class=yC19>In the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings</a></h3><div class="gs_a"><a href="/citations?user=vOHjMGcAAAAJ&amp;hl=en&amp;oi=sra">V Yanulevskaya</a>, <a href="/citations?user=jInmtEkAAAAJ&amp;hl=en&amp;oi=sra">J Uijlings</a>, <a href="/citations?user=Oyb3NYgAAAAJ&amp;hl=en&amp;oi=sra">E Bruni</a>, A Sartori, F Bacci&hellip; - 2012 - clic.cimec.unitn.it</div><div class="gs_rs">ABSTRACT Most artworks are explicitly created to evoke a strong emotional response. <br>During the centuries there were several art movements which employed different techniques <br>to achieve emotional expressions conveyed by artworks. Yet people were always <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:f9ZMwGNFwXkJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'f9ZMwGNFwXkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md17', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md17" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:f9ZMwGNFwXkJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://ylu.cc/CVPR12.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ylu.cc</span><span class="gs_ggsS">ylu.cc <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6247785" class=yC1B>Learning attention map from images</a></h3><div class="gs_a">Y Lu, W Zhang, C Jin, X Xue - Computer Vision and Pattern  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract While bottom-up and top-down processes have shown effectiveness during <br>predicting attention and eye fixation maps on images, in this paper, inspired by the <br>perceptual organization mechanism before attention selection, we propose to utilize figure<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3155839769888457000&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=22">Cited by 1</a> <a href="/scholar?q=related:KIFA5IzLyysJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3155839769888457000&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'KIFA5IzLyysJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/Q55757264V443037.pdf" class=yC1D>Opponent and Feedback: Visual Attention Captured</a></h3><div class="gs_a">S Wang, M Song, D Tao, L Zhang, J Bu&hellip; - Neural Information  &hellip;, 2011 - Springer</div><div class="gs_rs">Visual attention, as an important issue in computer vision field, has been raised for decades. <br>And many approaches mainly based on the bottom-up or top-down computing models have <br>been put forward to solve this problem. In this paper, we propose a new and effective <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:eYDB5w6h5YMJ:scholar.google.com/&amp;hl=en&amp;num=22&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'eYDB5w6h5YMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/4322418K8555838H.pdf" class=yC1E>Depth Matters: Influence of Depth Cues on Visual Saliency</a></h3><div class="gs_a">C Lang, <a href="/citations?user=qIaGn7YAAAAJ&amp;hl=en&amp;oi=sra">T Nguyen</a>, <a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, K Yadati&hellip; - Computer VisionâECCV  &hellip;, 2012 - Springer</div><div class="gs_rs">Most previous studies on visual saliency have only focused on static or dynamic 2D scenes. <br>Since the human visual system has evolved predominantly in natural three dimensional <br>environments, it is important to study whether and how depth information influences visual <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'T8lz6YwESLwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6400822" class=yC1F>An expected motion information model of salience for active cameras</a></h3><div class="gs_a">WA Talbott, J Movellan - Development and Learning and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Faced with the challenge of learning about its environment, an important first step <br>for a robot is deciding how to direct its sensors to extract meaningful information. <br>Computational models of visual salience have been developed to predict where humans <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'iW539jare4YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
