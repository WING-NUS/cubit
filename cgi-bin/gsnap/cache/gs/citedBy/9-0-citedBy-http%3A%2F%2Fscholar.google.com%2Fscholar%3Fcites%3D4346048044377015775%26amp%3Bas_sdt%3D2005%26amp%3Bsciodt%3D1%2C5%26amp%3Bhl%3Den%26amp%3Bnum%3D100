Total results = 9
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://lms.comp.nus.edu.sg/papers/media/2006/civr06-zhaom.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/f703366568hgp655.pdf" class=yC0>Automatic person annotation of family photo album</a></h3><div class="gs_a"><a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a>, Y Teo, <a href="/citations?user=zwJG6FcAAAAJ&amp;hl=en&amp;oi=sra">S Liu</a>, TS Chua, R Jain - Image and Video Retrieval, 2006 - Springer</div><div class="gs_rs">Abstract. Digital photographs are replacing tradition films in our daily life and the quantity is <br>exploding. This stimulates the strong need for efficient management tools, in which the <br>annotation of âwhoâ in each photo is essential. In this paper, we propose an automated <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5493605683724843011&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9">Cited by 48</a> <a href="/scholar?q=related:A3h8UkM1PUwJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5D/27/RN192552136.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=5493605683724843011&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">All 18 versions</a> <a onclick="return gs_ocit(event,'A3h8UkM1PUwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv5.papers/nus.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv5.papers/nus.pdf" class=yC3>Trecvid 2005 by nus pris</a></h3><div class="gs_a">TS Chua, SY Neo, HK Goh, <a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a>, Y Xiao&hellip; - NIST TRECVID- &hellip;, 2005 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT We participated in the high-level feature extraction and search task for <br>TRECVID 2005. For the high-level feature extraction task, we make use of the available <br>collaborative annotation results for training, and develop 2 methods to perform automated <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6139068261712169763&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9">Cited by 39</a> <a href="/scholar?q=related:I4s6zw5aMlUJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6139068261712169763&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'I4s6zw5aMlUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:I4s6zw5aMlUJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm07-neosyOFF.PDF" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291278" class=yC5>The use of topic evolution to help users browse and find answers in news video corpus</a></h3><div class="gs_a">SY Neo, Y Ran, HK Goh, Y Zheng, TS Chua&hellip; - Proceedings of the 15th  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Earlier research in news video has been focusing mainly on improving retrieval <br>accuracies given the limited amount of extractable video semantics. In this paper, we <br>propose an enhancement to news video searching by leveraging extractable video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2755699895959884965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=9">Cited by 18</a> <a href="/scholar?q=related:pRRFfnQ2PiYJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2755699895959884965&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'pRRFfnQ2PiYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5972553" class=yC7>Text detection and recognition for person identification in videos</a></h3><div class="gs_a"><a href="/citations?user=BxwURbMAAAAJ&amp;hl=en&amp;oi=sra">J Poignant</a>, F Thollard, G QuÃ©not&hellip; - &hellip;  Indexing (CBMI), 2011  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This article presents a demo of person search in audiovisual broadcast using only <br>the text available in a video and in resources external to the video. We also present the <br>different steps used to recognize characters in video for multi-modal person recognition <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:zDA7nO0pT2kJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'zDA7nO0pT2kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="https://scholarbank.nus.edu.sg/bitstream/handle/10635/15715/thesis_final.pdf?sequence=1" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://scholarbank.nus.edu.sg/handle/10635/15715" class=yC8>Integrating and conceptualizing heterogeneous ontologies on the web</a></h3><div class="gs_a">GOHHAI KIAT - 2006 - scholarbank.nus.edu.sg</div><div class="gs_rs">The World Wide Web (WWW) has evolved to be a major source of information. This has <br>brought about an overwhelming feeling of having too much information and being unable to <br>find or interpret data. In addition, since most content in HTML format is designed primarily <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:djrRxgG0BosJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10017892340855814774&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'djrRxgG0BosJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.zhaoming.name/publications/2007_IWAIT_PhotoAlbum.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from zhaoming.name</span><span class="gs_ggsS">zhaoming.name <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.zhaoming.name/publications/2007_IWAIT_PhotoAlbum.pdf" class=yCA>COMBINING METADATA AND CONTEXT INFORMATION IN ANNOTATING PERSONAL MEDIA</a></h3><div class="gs_a"><a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a>, TS Chua, R Jain - zhaoming.name</div><div class="gs_rs">ABSTRACT The proliferation of digital cameras, including those with mobile phones, has <br>resulted in enormous volumes of photographs and video in personal media. To make these <br>resources retrievable, we need to be able to annotate 4W&#39;sâwhen, where, who and what. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:40Qk1HwI05cJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10940097252041180387&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'40Qk1HwI05cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md5', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md5" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:40Qk1HwI05cJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/16051/thesis-sub.pdf?sequence=1" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/16051" class=yCC>combining multimodal external resources for event-based news video retrieval and question answering</a></h3><div class="gs_a">NEOSHI YONG - 2008 - scholarbank.nus.edu</div><div class="gs_rs">The ever-increasing amount of multimedia data available online creates an urgent need on <br>how to index these information and support effective retrieval by users. In recent years, we <br>observe the gradual shift from performing retrieval solely based on analyzing one media <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:h5EV9TPOgHUJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8466994022007017863&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'h5EV9TPOgHUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://hal.archives-ouvertes.fr/docs/00/76/41/43/PDF/Poignant_CORIA2011.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/hal-00764143/" class=yCE>DÃ©tection et reconnaissance de texte dans les documents vidÃ©os-Et leurs apports Ã  la reconnaissance de personnes</a></h3><div class="gs_a"><a href="/citations?user=BxwURbMAAAAJ&amp;hl=en&amp;oi=sra">J Poignant</a> - Actes de la 8e COnfÃ©rence en Recherche d&#39; &hellip;, 2011 - hal.archives-ouvertes.fr</div><div class="gs_rs">RÃSUMÃ. Cet article prÃ©sente les diffÃ©rentes Ã©tapes de reconnaissance des caractÃ¨res <br>dans un systÃ¨me de reconnaissance multimodale de personnes dans des documents <br>audiovisuels (dÃ©fi ANR REPERE). La dÃ©tection du texte est rÃ©alisÃ©e par une technique <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'w6t1ouenFXAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://asso-aria.org/coria/2011/409.pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from asso-aria.org</span><span class="gs_ggsS">asso-aria.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://asso-aria.org/coria/2011/409.pdf" class=yC10>DÃ©tection et reconnaissance de texte dans les documents vidÃ©os</a></h3><div class="gs_a"><a href="/citations?user=BxwURbMAAAAJ&amp;hl=en&amp;oi=sra">J Poignant</a> - asso-aria.org</div><div class="gs_rs">RÃSUMÃ. Cet article prÃ©sente les diffÃ©rentes Ã©tapes de reconnaissance des caractÃ¨res <br>dans un systÃ¨me de reconnaissance multimodale de personnes dans des documents <br>audiovisuels (dÃ©fi ANR REPERE). La dÃ©tection du texte est rÃ©alisÃ©e par une technique <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:l_Z3oxbaJsIJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13990109083239708311&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'l_Z3oxbaJsIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:l_Z3oxbaJsIJ:scholar.google.com/&amp;hl=en&amp;num=9&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
