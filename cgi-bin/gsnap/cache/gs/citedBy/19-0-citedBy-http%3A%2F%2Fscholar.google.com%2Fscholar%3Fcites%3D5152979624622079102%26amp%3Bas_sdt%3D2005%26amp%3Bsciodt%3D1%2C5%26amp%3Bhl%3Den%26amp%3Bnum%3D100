Total results = 19
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://repository.ias.ac.in/7859/1/346.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ias.ac.in</span><span class="gs_ggsS">ias.ac.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0262885604000666" class=yC0>Content based image retrieval using motif cooccurrence matrix</a></h3><div class="gs_a">N Jhanwar, S Chaudhuri, G Seetharaman&hellip; - Image and Vision  &hellip;, 2004 - Elsevier</div><div class="gs_rs">We present a new technique for content based image retrieval using motif cooccurrence <br>matrix (MCM). The MCM is derived from the motif transformed image. The whole image is <br>divided into 2Ã 2 pixel grids. Each grid is replaced by a scan motif that minimizes the local <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11824007731735470073&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 82</a> <a href="/scholar?q=related:-Z-Qv2JQF6QJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11824007731735470073&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'-Z-Qv2JQF6QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/JWWW03-lekha.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/VL11P38010523613.pdf" class=yC2>A multi-modal approach to story segmentation for news video</a></h3><div class="gs_a">L Chaisorn, TS Chua, CH Lee - World Wide Web, 2003 - Springer</div><div class="gs_rs">This research proposes a two-level, multi-modal framework to perform the segmentation and <br>classification of news video into single-story semantic units. The video is analyzed at the <br>shot and story unit (or scene) levels using a variety of features and techniques. At the shot <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6108549097289369038&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 35</a> <a href="/scholar?q=related:zkX3HArtxVQJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5E/44/RN130983315.html?source=googlescholar" class="gs_nph" class=yC4>BL Direct</a> <a href="/scholar?cluster=6108549097289369038&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'zkX3HArtxVQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acm-tomccap06-xuhx.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1126007" class=yC5>Fusion of AV features and external information sources for event detection in team sports video</a></h3><div class="gs_a">H Xu, TS Chua - ACM Transactions on Multimedia Computing,  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract The use of AV features alone is insufficient to induce high-level semantics. This <br>article proposes a framework that utilizes both internal AV features and various types of <br>external information sources for event detection in team sports video. Three schemes are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9085544146769579941&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 27</a> <a href="/scholar?q=related:pY9Uo0NWFn4J:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9085544146769579941&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'pY9Uo0NWFn4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://lms.comp.nus.edu.sg/papers/media/2002/MTAP02~1.PDF" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/m2911q0732955g43.pdf" class=yC7>Stratification approach to modeling video</a></h3><div class="gs_a">TS Chua, L Chen, J Wang - Multimedia Tools and Applications, 2002 - Springer</div><div class="gs_rs">The explosive growth of audiovisual information in the last few years has made the <br>development of advanced video modeling and management tools an urgent task. In this <br>research, we investigate the use of stratification approach to model the contextual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13358721049472749966&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 23</a> <a href="/scholar?q=related:jlFbVwS2Y7kJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3A/61/RN108177950.html?source=googlescholar" class="gs_nph" class=yC9>BL Direct</a> <a href="/scholar?cluster=13358721049472749966&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'jlFbVwS2Y7kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.5713&amp;rep=rep1&amp;type=pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=722971" class=yCA>Relevance feedback techniques for color-based image retrieval</a></h3><div class="gs_a">TS Chua, WC Low, CX Chu - Multimedia Modeling, 1998. MMM &hellip;, 1998 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Color has been widely used in content-based image retrieval systems. The problem <br>with using color is that its representation is low level and hence its retrieval effectiveness is <br>limited. This paper investigates techniques for improving the effectiveness of image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9612148169611825777&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 21</a> <a href="/scholar?q=related:cVIiOtU1ZYUJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9612148169611825777&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'cVIiOtU1ZYUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.199.6634&amp;rep=rep1&amp;type=pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=Pp3Xg-xg2CUC&amp;oi=fnd&amp;pg=PA95&amp;ots=PPpTE1buhn&amp;sig=OdC1IqReg0fGl2BvkA8uwagIS9g" class=yCC>The segmentation and classification of story boundaries in news video</a></h3><div class="gs_a">L Chaisorn, TS Chua - Proceedings of the IFIP TC2/WG2, 2002 - books.google.com</div><div class="gs_rs">Abstract The segmentation and classification of news video into single-story semantic units <br>is a challenging problem. This research proposes a two-level, multi-modal framework to <br>tackle this problem. The video is analyzed at the shot and story unit (or scene) levels using <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14815142300848227144&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 21</a> <a href="/scholar?q=related:SAPsum31mc0J:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14815142300848227144&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'SAPsum31mc0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=3308&amp;context=compsci" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://repository.cmu.edu/compsci/2275/" class=yCE>User-powered&#39;content-free&#39;approach to image retrieval</a></h3><div class="gs_a">T Kanade, S Uchihashi - 2004 - repository.cmu.edu</div><div class="gs_rs">Abstract:&quot; Consider a stereotypical image-retrieval problem; a user submits a set of query <br>images to a system and through repeated interactions during which the system presents its <br>current choices and the user gives his/her preferences to them, the choices are narrowed <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2476112422471879228&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 17</a> <a href="/scholar?q=related:PEpeFSHrXCIJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2476112422471879228&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'PEpeFSHrXCIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=779320" class=yC10>Relevance feedback techniques for image retrieval using multiple attributes</a></h3><div class="gs_a">TS Chua, CX Chu, M Kankanhalli - Multimedia Computing and  &hellip;, 1999 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The paper proposes a relevance feedback (RF) approach to content based image <br>retrieval using multiple attributes. The proposed approach has been applied to images&#39; text <br>and color attributes. In order to ensure that meaningful features are extracted, a pseudo <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7774989363931879705&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 13</a> <a href="/scholar?q=related:GZmZAaJP5msJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7774989363931879705&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'GZmZAaJP5msJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.cacs.louisiana.edu/~guna/reprints/icpr2002-cbir.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from louisiana.edu</span><span class="gs_ggsS">louisiana.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1047416" class=yC11>Content based image retrieval using optimum peano scan</a></h3><div class="gs_a">N Jhanwar, S Chaudhuri&hellip; - Pattern Recognition,  &hellip;, 2002 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present a new technique for content based image retrieval where feature vector <br>to be matched is very much specific to the query image. A particular Peano scan which is <br>optimal in encoding the query image is used to convert the scanning pattern of all <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=415585638494921444&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 11</a> <a href="/scholar?q=related:5GI-tO90xAUJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0D/35/RN126129117.html?source=googlescholar" class="gs_nph" class=yC13>BL Direct</a> <a href="/scholar?cluster=415585638494921444&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'5GI-tO90xAUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/nus.partial.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/nus.partial.pdf" class=yC14>Two-level multi-modal framework for news story segmentation of large video corpus</a></h3><div class="gs_a">L Chaisorn, C Koh, Y Zhao, H Xu&hellip; - 12th Text Retrieval  &hellip;, 2003 - www-nlpir.nist.gov</div><div class="gs_rs">To tackle the problem of story segmentation, we proposed a two-level multi-modal <br>framework [Chaisorn et al. 2002]. First we analyze the video at the shot level using a variety <br>of low and high-level features, and classify the shots into pre-defined categories using <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14116296357896250010&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 9</a> <a href="/scholar?q=related:muKLBL8o58MJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14116296357896250010&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'muKLBL8o58MJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:muKLBL8o58MJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://lms.comp.nus.edu.sg/papers/media/2003/cgi03-zhaoyl.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1214462" class=yC16>Automatic tracking of face sequences in MPEG video</a></h3><div class="gs_a">Y Zhao, TS Chua - Computer Graphics International, 2003.  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Human faces are commonly found in video streams and provide useful information <br>for video content analysis. We present a robust face tracking system to extract multiple face <br>sequences from MPEG video without human intervention. Specifically, a view-based DCT-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5933369710207850926&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 4</a> <a href="/scholar?q=related:rlkAqlCQV1IJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5933369710207850926&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'rlkAqlCQV1IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/notebook_papers/nus.paper.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tvpapers03/notebook_papers/nus.paper.pdf" class=yC18>TREC 2003 Video Retrieval and Story Segmentation task at NUS PRIS</a></h3><div class="gs_a">TS Chua, Y Zhao, L Chaisorn, CK Koh&hellip; - TREC (VIDEO)  &hellip;, 2003 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT This paper describes the details of our systems for story segmentation task and <br>search task of the TREC-2003 Video Track. In story segmentation task, we propose a two-<br>level multi-modal framework. First we analyze the video at the shot level using a variety of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7030869519621737973&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 3</a> <a href="/scholar?q=related:9c0ss5OqkmEJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7030869519621737973&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'9c0ss5OqkmEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:9c0ss5OqkmEJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Extracting Story Units in News Video</h3><div class="gs_a">L Chaisorn, TS Chua, CH Lee - Proceedings of IWAIT, 2003</div><div class="gs_fl"><a href="/scholar?cites=1474656682461943625&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 2</a> <a href="/scholar?q=related:SdfMi3AIdxQJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1474656682461943625&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'SdfMi3AIdxQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8389&amp;rep=rep1&amp;type=pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8389&amp;rep=rep1&amp;type=pdf" class=yC1A>Towards Pseudo-object Models for Content-based Visual Information Retrieval</a></h3><div class="gs_a">TS Chua, M Kankanhalli - Intern. Symposium on Multimedia Information  &hellip;, 1998 - Citeseer</div><div class="gs_rs">Abstract Current image/video retrieval systems rely mainly on the visual features and text <br>annotations of images as the basis for retrieval. A typical feature such as the color histogram <br>only attempts to capture the main characteristics of the overall image. Although it is able to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14687254775147322955&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=19">Cited by 1</a> <a href="/scholar?q=related:S95Qzmac08sJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14687254775147322955&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'S95Qzmac08sJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md13', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md13" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:S95Qzmac08sJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Video Modeling and Retrieval</h3><div class="gs_a">Y Zhang, TS Chua - 2001</div><div class="gs_fl"><a href="/scholar?q=related:dxR9EinL_EYJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5115186654050325623&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'dxR9EinL_EYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="https://scholarbank.nus.edu.sg/bitstream/handle/10635/15994/SHIRUI_PHDThesis_BayesianLearningofConceptOntologyforAutomaticImageAnnotation.pdf?sequence=1" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://scholarbank.nus.edu.sg/handle/10635/15994" class=yC1C>Bayesian learning of concept ontology for automatic image annotation</a></h3><div class="gs_a">RUI SHI - 2007 - scholarbank.nus.edu.sg</div><div class="gs_rs">Automatic image annotation (AIA) has been a hot research topic in recent years since it can <br>be used to support concept-based image retrieval. In the field of AIA, characterizing image <br>concepts by mixture models is one of the most effective techniques. However, mixture <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:hevhtMNxNoEJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9310754365002345349&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'hevhtMNxNoEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/13633/Zhao_Yunlong_PhD_thesis.pdf?sequence=1" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/13633" class=yC1E>Automatic extraction and tracking of face sequences in MPEG video</a></h3><div class="gs_a">Z Yunlong - 2004 - scholarbank.nus.edu</div><div class="gs_rs">This PhD work focuses on the problem of extracting multiple face sequences from MPEG <br>video based on face detection and tracking. It aims to facilitate the strata-based digital video <br>modelling to achieve efficient video retrieval and browsing. The research includes the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:LWH47rNLYawJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12421292483445023021&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'LWH47rNLYawJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://www.scholarbank.nus.edu.sg/bitstream/handle/10635/13136/Thesis_XU_Huaxin_HT016894E.pdf?sequence=1" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.scholarbank.nus.edu.sg/handle/10635/13136" class=yC20>Integrated analysis of audiovisual signals and external information sources for event detection in team sports video</a></h3><div class="gs_a">H Xu - 2007 - scholarbank.nus.edu.sg</div><div class="gs_rs">Audiovisual signals and external information sources (news reports, live commentaries, Web <br>casts, etc.) are found to have complementary strengths for detecting events in sports video. <br>This thesis reports research on integrated analysis of them, focusing on tackling the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:M-BY-RAX2tkJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15697884812823552051&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'M-BY-RAX2tkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ri"><h3 class="gs_rt"><a href="http://japanlinkcenter.org/JST.JSTAGE/iieej/30.540?from=Google" class=yC22>æ åã¢ã¼ã«ã¤ãæè¡ã®åå</a></h3><div class="gs_a">åå³¶æ²»å½¦ - ç»åé»å­å­¦ä¼èª, 2001 - J-STAGE</div><div class="gs_rs">å¤§éæ åã³ã³ãã³ããå¹ççã«æ¤ç´¢Â· é²è¦§Â· ç®¡çããããã®æ åã¢ã¼ã«ã¤ããæ§ç¯ããå§ãã¦ãã. <br>æ¬ç¨¿ã§ã¯, æ åã¢ã¼ã«ã¤ãæ§ç¯ãæ¯ããä¸»è¦æè¡ã¨ãã¦, æ åã¤ã³ãã¯ã·ã³ã°æè¡, <br>æ åè³ç£ç®¡çæè¡, æ åæ¤ç´¢æè¡, ããããã®ææ°ååãç´¹ä»ãã.</div><div class="gs_fl"><a href="/scholar?q=related:7R7N2d4uxJoJ:scholar.google.com/&amp;hl=en&amp;num=19&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'7R7N2d4uxJoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
