Total results = 39
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.comp.nus.edu.sg/~kanmy/dossier/papers/Neo_1568987456.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/78446033686p4420.pdf" class=yC0>Video retrieval using high level features: Exploiting query matching and confidence-based weighting</a></h3><div class="gs_a">SY Neo, J Zhao, <a href="/citations?user=aNVcd3EAAAAJ&amp;hl=en&amp;oi=sra">MY Kan</a>, TS Chua - Image and Video Retrieval, 2006 - Springer</div><div class="gs_rs">Abstract. Recent research in video retrieval has focused on automated, high-level feature <br>indexing on shots or frames. One important application of such indexing is to support precise <br>video retrieval. We report on extensions of this semantic indexing on news video retrieval. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9652872861403202182&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 67</a> <a href="/scholar?q=related:hkLybbnk9YUJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/02/23/RN192552113.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=9652872861403202182&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 27 versions</a> <a onclick="return gs_ocit(event,'hkLybbnk9YUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://124.207.250.90/staff/lingyu/ACM-MM-Ad2006.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 124.207.250.90</span><span class="gs_ggsS">124.207.250.90 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1180697" class=yC3>Segmentation, categorization, and identification of commercial clips from TV streams using multimodal analysis</a></h3><div class="gs_a">LY Duan, <a href="/citations?user=7_BkyxEAAAAJ&amp;hl=en&amp;oi=sra">J Wang</a>, Y Zheng, JS Jin, H Lu&hellip; - Proceedings of the 14th  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract TV advertising is ubiquitous, perseverant, and economically vital. Millions of <br>people&#39;s living and working habits are affected by TV commercials. In this paper, we present <br>a multimodal (&quot; visual+ audio+ text&quot;) commercial video digest scheme to segment <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11446355647284931839&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 59</a> <a href="/scholar?q=related:_2w02dCf2Z4J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11446355647284931839&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'_2w02dCf2Z4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv6.papers/att.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv6.papers/att.pdf" class=yC5>AT&amp;T research at trecvid 2006</a></h3><div class="gs_a">Z Liu, D Gibbon, E Zavesky, <a href="/citations?user=70I8ZIMAAAAJ&amp;hl=en&amp;oi=sra">B Shahraray</a>&hellip; - TREC Video Retrieval &hellip;, 2006 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT TRECVID (TREC Video Retrieval Evaluation) is sponsored by NIST to <br>encourage research in digital video indexing and retrieval. It was initiated in 2001 as a <br>âvideo trackâ of TREC and became an independent evaluation in 2003. AT&amp;T participated <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2245751077733081806&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 41</a> <a href="/scholar?q=related:ziI76rSCKh8J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2245751077733081806&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'ziI76rSCKh8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md2', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md2" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:ziI76rSCKh8J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www.informedia.cs.cmu.edu/documents/CIVR06_yan.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/n37l784r334w489k.pdf" class=yC7>Efficient margin-based rank learning algorithms for information retrieval</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">A Hauptmann</a> - Image and Video Retrieval, 2006 - Springer</div><div class="gs_rs">Abstract. Learning a good ranking function plays a key role for many applications including <br>the task of (multimedia) information retrieval. While there are a few rank learning methods <br>available, most of them need to explicitly model the relations between every pair of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6215728031787772153&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 40</a> <a href="/scholar?q=related:-egla7azQlYJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/16/60/RN192552083.html?source=googlescholar" class="gs_nph" class=yC9>BL Direct</a> <a href="/scholar?cluster=6215728031787772153&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'-egla7azQlYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/r742245481q23631.pdf" class=yCA>A review of text and image retrieval approaches for broadcast news video</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a> - Information Retrieval, 2007 - Springer</div><div class="gs_rs">Abstract The effectiveness of a video retrieval system largely depends on the choice of <br>underlying text and image retrieval components. The unique properties of video collections <br>(eg, multiple sources, noisy features and temporal relations) suggest we examine the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9278500574809399146&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 35</a> <a href="/scholar?q=related:ap_F-Rzbw4AJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0D/2C/RN216038550.html?source=googlescholar" class="gs_nph" class=yCB>BL Direct</a> <a href="/scholar?cluster=9278500574809399146&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'ap_F-Rzbw4AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.lti.cs.cmu.edu/research/thesis/2006/rong_yan.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.lti.cs.cmu.edu/research/thesis/2006/rong_yan.pdf" class=yCC>Probabilistic models for combining diverse knowledge sources in multimedia retrieval</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a> - 2006 - lti.cs.cmu.edu</div><div class="gs_rs">Abstract In recent years, the multimedia retrieval community is gradually shifting its <br>emphasis from analyzing one media source at a time to exploring the opportunities of <br>combining diverse knowledge sources from correlated media types and context. In order <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1282854589144669096&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 32</a> <a href="/scholar?q=related:qJtytHOdzREJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1282854589144669096&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'qJtytHOdzREJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md5', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md5" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:qJtytHOdzREJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:qJtytHOdzREJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=9607173453927529710&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.stat.ucla.edu/~zzsi/doc/Zheng2006CIVR.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucla.edu</span><span class="gs_ggsS">ucla.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/DJ1X764151607565.pdf" class=yCE>Using high-level semantic features in video retrieval</a></h3><div class="gs_a">W Zheng, J Li, <a href="/citations?user=wOfkLfgAAAAJ&amp;hl=en&amp;oi=sra">Z Si</a>, F Lin, B Zhang - Image and video retrieval, 2006 - Springer</div><div class="gs_rs">Abstract. Extraction and utilization of high-level semantic features are critical for more <br>effective video retrieval. However, the performance of video retrieval hasn&#39;t benefited much <br>despite of the advances in high-level feature extraction. To make good use of high-level <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14201347610376584490&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 23</a> <a href="/scholar?q=related:Kr1W4GhSFcUJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/05/0C/RN192552343.html?source=googlescholar" class="gs_nph" class=yC10>BL Direct</a> <a href="/scholar?cluster=14201347610376584490&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'Kr1W4GhSFcUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://doc.utwente.nl/66684/1/04118235.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utwente.nl</span><span class="gs_ggsS">utwente.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4118235" class=yC11>Multimedia search without visual analysis: the value of linguistic and contextual information</a></h3><div class="gs_a">FMG De Jong, T Westerveld&hellip; - Circuits and Systems for  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper addresses the focus of this special issue by analyzing the potential <br>contribution of linguistic content and other nonimage aspects to the processing of <br>audiovisual data. It summarizes the various ways in which linguistic content analysis <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14190645850803413929&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 18</a> <a href="/scholar?q=related:qSOCTjdN78QJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/18/06/RN207335639.html?source=googlescholar" class="gs_nph" class=yC13>BL Direct</a> <a href="/scholar?cluster=14190645850803413929&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'qSOCTjdN78QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.science.uva.nl/research/publications/2007/SnoekICME2007/snoek-concept-icme2007.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4285063" class=yC14>Are concept detector lexicons effective for video search?</a></h3><div class="gs_a"><a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Multimedia and Expo, 2007 IEEE  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Until now, systematic studies on the effectiveness of concept detectors for video <br>search have been carried out using less than 20 detectors, or in combination with other <br>retrieval techniques. We investigate whether video search using just large concept <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4547925203590294318&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 18</a> <a href="/scholar?q=related:LqsLceB5HT8J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4547925203590294318&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'LqsLceB5HT8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm07-neosyOFF.PDF" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291278" class=yC16>The use of topic evolution to help users browse and find answers in news video corpus</a></h3><div class="gs_a">SY Neo, Y Ran, HK Goh, Y Zheng, TS Chua&hellip; - Proceedings of the 15th  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Earlier research in news video has been focusing mainly on improving retrieval <br>accuracies given the limited amount of extractable video semantics. In this paper, we <br>propose an enhancement to news video searching by leveraging extractable video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2755699895959884965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 18</a> <a href="/scholar?q=related:pRRFfnQ2PiYJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2755699895959884965&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'pRRFfnQ2PiYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://lms.comp.nus.edu.sg/papers/media/2007/acmmm07-huanboOFF.PDF" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291295" class=yC18>Segregated feedback with performance-based adaptive sampling for interactive news video retrieval</a></h3><div class="gs_a">HB Luan, SY Neo, HK Goh, YD Zhang, SX Lin&hellip; - Proceedings of the 15th &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Existing video research incorporates the use of relevance feedback based on user-<br>dependent interpretations to improve the retrieval results. In this paper, we segregate the <br>process of relevance feedback into 2 distinct facets:(a) recall-directed feedback; and (b) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18005967219541332703&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 16</a> <a href="/scholar?q=related:39IWVskL4vkJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18005967219541332703&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'39IWVskL4vkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://www.ee.columbia.edu/ln/dvmm/publications/08/mir2008_zavesky.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1460136" class=yC1A>CuZero: embracing the frontier of interactive visual search for informed users</a></h3><div class="gs_a">E Zavesky, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Proceeding of the 1st ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Users of most visual search systems suffer from two primary sources of frustration. <br>Before a search over this data is executed, a query must be formulated. Traditional keyword <br>search systems offer only passive, non-interactive input, which frustrates users that are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17719139335607675463&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 10</a> <a href="/scholar?q=related:R-qLgFgH5_UJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17719139335607675463&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'R-qLgFgH5_UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://www.ee.columbia.edu/~yjiang/publication/tcsvt_cdvs.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5686924" class=yC1C>Concept-driven multi-modality fusion for video search</a></h3><div class="gs_a">XY Wei, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Circuits and Systems for Video  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract As it is true for human perception that we gather information from different sources <br>in natural and multi-modality forms, learning from multi-modalities has become an effective <br>scheme for various information retrieval problems. In this paper, we propose a novel multi-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9058963900236874973&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 14</a> <a href="/scholar?q=related:3cS0uqvnt30J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9058963900236874973&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'3cS0uqvnt30J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://lms.comp.nus.edu.sg/papers/media/2008/civr08-luan.pdf" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1386352.1386411" class=yC1E>Adaptive multiple feedback strategies for interactive video search</a></h3><div class="gs_a">H Luan, Y Zheng, SY Neo, Y Zhang, S Lin&hellip; - Proceedings of the 2008 &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose adaptive multiple feedback strategies for interactive video <br>retrieval. We first segregate interactive feedback into 3 distinct types (recall-driven relevance <br>feedback, precision-driven active learning and locality-driven relevance feedback) so that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12517552171279368847&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 8</a> <a href="/scholar?q=related:j8KGAmFHt60J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12517552171279368847&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'j8KGAmFHt60J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4338344" class=yC20>LDA-based retrieval framework for semantic news video retrieval</a></h3><div class="gs_a">J Caol, J Li, Y Zhang, S Tang - Semantic Computing, 2007.  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Topic-based language model has attracted much attention as the propounding of <br>semantic retrieval in recent years. Especially for the ASR text with errors, the topic <br>representation is more reasonable than the exact term representation. Among these <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6614341188191690107&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 7</a> <a href="/scholar?q=related:e51mMUTcylsJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6614341188191690107&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'e51mMUTcylsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://nlpr-web.ia.ac.cn/2008papers/gjkw/gk11.pdf" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ia.ac.cn</span><span class="gs_ggsS">ia.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4476271" class=yC21>Digesting commercial clips from TV streams</a></h3><div class="gs_a">LY Duan, YT Zheng, <a href="/citations?user=7_BkyxEAAAAJ&amp;hl=en&amp;oi=sra">J Wang</a>, H Lu, JS Jin - MultiMedia, IEEE, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A commercial system that performs syntactic and semantic analysis during a TV <br>advertising break could facilitate innovative new applications, such as an intelligent set-top <br>box that enhances the ability of viewers to monitor and manage commercials from TV <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9886340150223456619&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 6</a> <a href="/scholar?q=related:a0nAjvxVM4kJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/18/1F/RN227749646.html?source=googlescholar" class="gs_nph" class=yC23>BL Direct</a> <a href="/scholar?cluster=9886340150223456619&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'a0nAjvxVM4kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1386413" class=yC24>Multi-query interactive image and video retrieval-: theory and practice</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=Ade_7YoAAAAJ&amp;hl=en&amp;oi=sra">A Natsev</a>, <a href="/citations?user=8rykXfcAAAAJ&amp;hl=en&amp;oi=sra">M Campbell</a> - &hellip;  of the 2008 international conference on  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract We propose a new interactive image and video retrieval system called multi-query <br>interactive retrieval, which is designed to jointly optimize the retrieval performance on <br>multiple query topics. The proposed system employs a learning-based hybrid retrieval <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3287469760905036761&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 6</a> <a href="/scholar?q=related:2UOVbFRwny0J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'2UOVbFRwny0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://www.ee.columbia.edu/~qibin/papers/qibin2006_icme_4.pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036893" class=yC25>Classifier optimization for multimedia semantic concept detection</a></h3><div class="gs_a">S Gao, Q Sun - Multimedia and Expo, 2006 IEEE International  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present an AUC (ie, the area under the curve of receiver operating <br>characteristics (ROC)) maximization based learning algorithm to design the classifier for <br>maximizing the ranking performance. The proposed approach trains the classifier by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4183225282129186522&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 5</a> <a href="/scholar?q=related:2k6-_DrNDToJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4183225282129186522&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'2k6-_DrNDToJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm08-wanggang.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459393" class=yC27>Exploring knowledge of sub-domain in a multi-resolution bootstrapping framework for concept detection in news video</a></h3><div class="gs_a">G Wang, TS Chua, <a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a> - Proceeding of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we present a model based on a multi-resolution, multi-source and <br>multi-modal (M3) bootstrapping framework that exploits knowledge of sub-domains for <br>concept detection in news video. Because the characteristics and distributions of data in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15442298916331928942&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 5</a> <a href="/scholar?q=related:bj1dLwwRTtYJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15442298916331928942&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'bj1dLwwRTtYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5689665" class=yC29>Stability analysis for ranking algorithms</a></h3><div class="gs_a">W Gao, Y Zhang, L Liang, Y Xia - Information Theory and  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, the stability of ranking algorithms is studied by adopting a strategy <br>which adjusts the sample set by deleting one or two element from it. Relationship between <br>uniform loss stability and uniform score stability is investigated. A sufficient condition for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8655225416488815917&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 5</a> <a href="/scholar?q=related:LY2_sKiJHXgJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'LY2_sKiJHXgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Ontology Similarity Measure and Ontology Mapping Via Fast Ranking Method</h3><div class="gs_a">X Huang, T Xu, W Gao, Z Jia - International Journal of Applied Physics and  &hellip;, 2011</div><div class="gs_fl"><a href="/scholar?cites=16260523609290923132&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 3</a> <a href="/scholar?q=related:fFyjGRL8qOEJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'fFyjGRL8qOEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://xb.ynni.edu.cn/index.php/jne/article/download/2196/1752" class=yC2B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ynni.edu.cn</span><span class="gs_ggsS">ynni.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://xb.ynni.edu.cn/index.php/jne/article/viewArticle/2196" class=yC2A>ä¸ç§æ¹è¿çä¿¡æ¯æ£ç´¢æåºç®æ³ An Improved Algorithm for Ranking in Information Retrieval</a></h3><div class="gs_a">é«çGW - äºåæ°æå¤§å­¦å­¦æ¥ (èªç¶ç§å­¦ç), 2010 - xb.ynni.edu.cn</div><div class="gs_rs">æè¦ä¿¡æ¯æ£ç´¢çæ ¸å¿é®é¢å°±æ¯å¨ææ¡£éä¸­ä¸ºç¨æ·æ£ç´¢åºæç¸å³çå­ææ¡£é, <br>å¹¶ä¾é æåºç®æ³å¯¹æ£ç´¢ç»ææç§ç¸å³æ§è¿è¡æåº, å æ­¤æåºç®æ³çä¼å£ç´æ¥å½±åæ£ç´¢çæç. <br>RLR ç®æ³æ¹è¿äºæ­£åç»éªé£é©æ¨¡å, å¤§å¤§åå°äºè®¡ç®å¤æåº¦. éè¿è®¾å®ä¸å®èå´çåè®¸è¯¯å·®å¼, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=789339497650491703&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 4</a> <a href="/scholar?q=related:N5GkTBdM9AoJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=789339497650491703&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'N5GkTBdM9AoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/N743N68244275728.pdf" class=yC2C>Generalization Bounds of Ranking via Query-Level Stability I</a></h3><div class="gs_a">X He, W Gao, Z Jia - Information and Management Engineering, 2011 - Springer</div><div class="gs_rs">The quality of ranking determines the success or failure of information retrieval and the goal <br>of ranking is to learn a real-valued ranking function that induces a ranking or ordering over <br>an instance space. We focus on generalization ability of learning to rank algorithms for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6017861945709042311&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 2</a> <a href="/scholar?q=related:hxpUPY69g1MJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'hxpUPY69g1MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/95033x/200920/31860756.html" class=yC2D>åºäºè¾¹éçä¿¡æ¯æ£ç´¢æåºç®æ³ç ç©¶</a></h3><div class="gs_a">é«çï¼ å¼ è¶ï¼ æ¢ç«ï¼ å¤å¹¼æ - è®¡ç®æºå·¥ç¨ä¸è®¾è®¡, 2009 - cqvip.com</div><div class="gs_rs">ç³»ç»å°åæRLR ç®æ³æ¨¡åçä¼ç¼ºç¹, è¯æp é¶é°é¾äºæå½æ°æ¯å¸å½æ°, ä¸æ»¡è¶³2L (x/2)â¥ L (x), <br>åæ¢æ¨¡åæ¡æ¶ä¸­çäºæå½æ°, éç¨åå°ä¸ä¸çå·®è·çç­ç¥éååæ°Î±, ç±è¯æp <br>é¶é°é¾äºæå½æ°æ»¡è¶³L (x)+ L (-x)&lt;(2+| x|)&#39;è¿èå¾å°æ°çç®æ³. å®éªç»æè¡¨æ, è¯¥ç®æ³æ¯ææç, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15614385215331797824&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 3</a> <a href="/scholar?q=related:QIv28Z9wsdgJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15614385215331797824&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'QIv28Z9wsdgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1877705811019023" class=yC2E>Query-Level Stability of Ranking SVM for Replacement Case</a></h3><div class="gs_a"><a href="/citations?user=CJwLwzQAAAAJ&amp;hl=en&amp;oi=sra">Y Gao</a>, W Gao, Y Zhang - Procedia Engineering, 2011 - Elsevier</div><div class="gs_rs">The quality of ranking determines the success or failure of information retrieval and the goal <br>of ranking is to learn a real-valued ranking function that induces a ranking or ordering over <br>an instance space. We focus on stability and generalization ability of ranking SVM for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6201541802681462925&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 1</a> <a href="/scholar?q=related:jXCQkGlNEFYJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'jXCQkGlNEFYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://www.ee.columbia.edu/ln/dvmm/publications/PhD_theses/WinstonHsu07thesis.pdf" class=yC30><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ee.columbia.edu/ln/dvmm/publications/PhD_theses/WinstonHsu07thesis.pdf" class=yC2F>An information-theoretic framework towards large-scale video structuring, threading, and retrieval</a></h3><div class="gs_a"><a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a> - 2007 - ee.columbia.edu</div><div class="gs_rs">Page 1. An Information-Theoretic Framework towards Large-Scale Video Structuring, Threading,<br>and Retrieval Winston H. Hsu Submitted in partial fulfillment of the requirements for the degree<br>of Doctor of Philosophy in the Graduate School of Arts and Sciences <b>...</b> </div><div class="gs_fl"><a href="/scholar?cites=16231174863222617573&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 2</a> <a href="/scholar?q=related:5TWiqYq3QOEJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16231174863222617573&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'5TWiqYq3QOEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md25', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md25" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:5TWiqYq3QOEJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:5TWiqYq3QOEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=5485094745706648444&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://bsrc.kaist.ac.kr/nip-lr/V11N04-06/V11N04P3-83-90.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://bsrc.kaist.ac.kr/nip-lr/V11N04-06/V11N04P3-83-90.pdf" class=yC31>et, al., Cross-Model Learning-The Learning Methodology Inspired by Human&#39;s Intelligence</a></h3><div class="gs_a">B Zhang, D Ding, L Zhang - Neural Information Processing-Letters  &hellip;, 2007 - bsrc.kaist.ac.kr</div><div class="gs_rs">AbstractâHuman has an amazing cross-modal learning capability. In order to endow the <br>computers with the same ability, we use a model based on the quotient space theory. In the <br>quotient space model, representations at different modalities form a complete semi-order <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=633747098091804191&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 1</a> <a href="/scholar?q=related:H1Yyw6CFywgJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=633747098091804191&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'H1Yyw6CFywgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:H1Yyw6CFywgJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://www.worldsciencepublisher.org/journals/index.php/JOE/article/view/676/558" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from worldsciencepublisher.org</span><span class="gs_ggsS">worldsciencepublisher.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.worldsciencepublisher.org/journals/index.php/JOE/article/view/676" class=yC33>Ontology Similarity Measure and Ontology Mapping Using Half Transductive Ranking</a></h3><div class="gs_a">X Huang, T Xu, W Gao, S Gong - Proceedings of 2011  &hellip;, 2011 - worldsciencepublisher.org</div><div class="gs_rs">Abstract Ontology similarity calculation and ontology mapping are important research topics <br>in information retrieval. By analyzing the half transductive ranking algorithm, we propose the <br>new algorithm for ontology similarity measure and ontology mapping. Via the ranking <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7132304127726347757&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 2</a> <a href="/scholar?q=related:7WUuy9AI-2IJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7132304127726347757&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'7WUuy9AI-2IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/94293x/201107/38543808.html" class=yC35>ä¸¤ç±»æ°æ¨è¿æåºç®æ³</a></h3><div class="gs_a">é«çï¼ æ¢ç« - è®¡ç®æºå·¥ç¨ä¸ç§å­¦, 2011 - cqvip.com</div><div class="gs_rs">æåºå­¦ä¹ ç®æ³çç®æ æ¯å¾å°æä¼æåºå½æ°, å®ç»æ¯ä¸ªå®ä¾ä¸ä¸ªå¾å, å¹¶æ ¹æ®å¾åæå®åå®ä¾ç<br>ååæ¬¡åº. å¨æ¨è¿æåºç®æ³çæ¡æ¶ä¸, åè®¸å­¦ä¹ å­å¨ä¸å®ç¨åº¦çè¯¯å·®. è®¾å®æ­£æ°Îµ <br>ä½ä¸ºåè®¸è¯¯å·®çèå´, ç¨å¯¹ç§°Îµ-insensitive ææ°äºæå½æ°åå¯¹ç§°Îµ-insensitive å¯¹æ°äºæå½æ°<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11263816869672864479&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=39">Cited by 1</a> <a href="/scholar?q=related:30rnw8MdUZwJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11263816869672864479&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'30rnw8MdUZwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5569293" class=yC36>Dominating ranking algorithm for information retrieval</a></h3><div class="gs_a">H Liu, Z Li, J Xin, C Chen - Fuzzy Systems and Knowledge  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract There are lots of ranking algorithms used in Web information retrieval. However, <br>current algorithms have some problems: these algorithms are based on different calculation <br>formulas to calculate the documents and query similarity or train a lot of training data to get <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:y4vbFHMokjwJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'y4vbFHMokjwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://nlp.cs.nyu.edu/sk-symposium/note/P-24.pdf" class=yC38><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nyu.edu</span><span class="gs_ggsS">nyu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://nlp.cs.nyu.edu/sk-symposium/note/P-24.pdf" class=yC37>Characteristic of Textual Information in Video Data from the Perspective of Natural Language Processing</a></h3><div class="gs_a">K Shirahama, A Mizui, K Uehara - nlp.cs.nyu.edu</div><div class="gs_rs">Due to the recent advances in computing, storage and network technologies, a vast amount <br>of digital information in forms of text, images, audio and videos are widely distributed to <br>users on demand. In order to efficiently retrieve an interesting portion of information, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:kGJjKDUBjm0J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'kGJjKDUBjm0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md30', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md30" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:kGJjKDUBjm0J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://wwwhome.cs.utwente.nl/~fdejong/ieee_sicvir_fjavtw_dec06.pdf" class=yC3A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utwente.nl</span><span class="gs_ggsS">utwente.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://wwwhome.cs.utwente.nl/~fdejong/ieee_sicvir_fjavtw_dec06.pdf" class=yC39>SICVIR: Multimedia Search without Visual Analysis: The Value of Linguistic and Contextual Information</a></h3><div class="gs_a">FMG de Jong, T Westerveld, <a href="/citations?user=iH9TVHQAAAAJ&amp;hl=en&amp;oi=sra">AP de Vries</a> - 2006 - wwwhome.cs.utwente.nl</div><div class="gs_rs">AbstractâThis paper addresses the focus of this special issue by analyzing the potential <br>contribution of linguistic content and other non-image aspects to the processing of <br>audiovisual data. It summarizes the various ways in which linguistic content analysis <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:BX45MgHxu_wJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18211414505642622469&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'BX45MgHxu_wJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md31', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md31" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:BX45MgHxu_wJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/V124V8K854M032R6.pdf" class=yC3B>Ontology Mapping on Multi-ontology Graphs via Optimizing Ranking Function</a></h3><div class="gs_a">X He, <a href="/citations?user=zZD0wFEAAAAJ&amp;hl=en&amp;oi=sra">Y Wang</a>, W Gao - Emerging Research in Artificial Intelligence and  &hellip;, 2012 - Springer</div><div class="gs_rs">Ontology mapping is an important research topic in information retrieval and widely used in <br>many fields. By analyzing the ranking algorithm by optimizing NDCG measure, we propose <br>the new algorithm for ontology mapping. Via the ranking learning algorithm, the multi-<b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'3KUkSJG9MKYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Ling Zhang 2 and Bo Zhang 3</h3><div class="gs_a">RA Meyers - Springer</div><div class="gs_fl"><a href="/scholar?q=related:kONpS65C1I8J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'kONpS65C1I8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6392948" class=yC3C>Circular Reranking for Visual Search</a></h3><div class="gs_a">T Yao, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">C Ngo</a>, T Mei - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Search reranking is regarded as a common way for boosting retrieval precision. <br>The problem nevertheless is not trivial especially when there are multiple features or <br>modalities to be considered for search, which often happens in image and video retrieval. <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'W1TjVphMREgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://media.cs.tsinghua.edu.cn/~multimedia/cuipeng/papers/ICME07.pdf" class=yC3E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tsinghua.edu.cn</span><span class="gs_ggsS">tsinghua.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284777" class=yC3D>A Novel Event-Oriented Segment-of-Interest Discovery Method for Surveillance Video</a></h3><div class="gs_a">P Cui, LF Sun, Z Wang, SQ Yang - Multimedia and Expo, 2007  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract During recent years, the quick development of computer techniques has witnessed <br>the ever-increasing surveillance video data, which essentially pose great challenge on the <br>data storage, management, analysis and even retrieval. Considering that most of the high <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:8iVnXdi6rQYJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=481246173641254386&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'8iVnXdi6rQYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/D2MN0RP5V6G39042.pdf" class=yC3F>A lexicon-guided LSI method for semantic news video retrieval</a></h3><div class="gs_a">J Cao, S Tang, J Li, Y Zhang, X Pan - Advances in Multimedia Information  &hellip;, 2007 - Springer</div><div class="gs_rs">Many researchers try to utilize the semantic information extracted from visual feature to <br>directly realize the semantic video retrieval or to supplement the automated speech <br>recognition (ASR) text retrieval. But bridging the gap between the low-level visual feature <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:pmRAwF-_UA8J:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3F/57/RN221517554.html?source=googlescholar" class="gs_nph" class=yC40>BL Direct</a> <a href="/scholar?cluster=1103592326674015398&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'pmRAwF-_UA8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/16051/thesis-sub.pdf?sequence=1" class=yC42><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/16051" class=yC41>combining multimodal external resources for event-based news video retrieval and question answering</a></h3><div class="gs_a">NEOSHI YONG - 2008 - scholarbank.nus.edu</div><div class="gs_rs">The ever-increasing amount of multimedia data available online creates an urgent need on <br>how to index these information and support effective retrieval by users. In recent years, we <br>observe the gradual shift from performing retrieval solely based on analyzing one media <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:h5EV9TPOgHUJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8466994022007017863&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'h5EV9TPOgHUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://www.scholarbank.nus.edu.sg/bitstream/handle/10635/13136/Thesis_XU_Huaxin_HT016894E.pdf?sequence=1" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.scholarbank.nus.edu.sg/handle/10635/13136" class=yC43>Integrated analysis of audiovisual signals and external information sources for event detection in team sports video</a></h3><div class="gs_a">H Xu - 2007 - scholarbank.nus.edu.sg</div><div class="gs_rs">Audiovisual signals and external information sources (news reports, live commentaries, Web <br>casts, etc.) are found to have complementary strengths for detecting events in sports video. <br>This thesis reports research on integrated analysis of them, focusing on tackling the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:M-BY-RAX2tkJ:scholar.google.com/&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15697884812823552051&amp;hl=en&amp;num=39&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'M-BY-RAX2tkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
