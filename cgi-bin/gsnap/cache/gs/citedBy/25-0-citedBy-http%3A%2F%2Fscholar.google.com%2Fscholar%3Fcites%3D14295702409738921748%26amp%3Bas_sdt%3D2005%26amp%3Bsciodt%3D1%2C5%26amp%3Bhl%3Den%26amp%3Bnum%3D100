Total results = 25
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://vireo.cs.cityu.edu.hk/papers/beyond%20search%20event%20driven%20summarization%20for%20web%20videos_acmtmm10.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2043613" class=yC0>Beyond search: Event-driven summarization for web videos</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, HK Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - ACM Transactions on  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract The explosive growth of Web videos brings out the challenge of how to efficiently <br>browse hundreds or even thousands of videos at a glance. Given an event-driven query, <br>social media Web sites usually return a large number of videos that are diverse and noisy <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4403112343352155411&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=25">Cited by 13</a> <a href="/scholar?q=related:E52k-1j_Gj0J:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4403112343352155411&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'E52k-1j_Gj0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6212356" class=yC2>Visual-Textual Joint Relevance Learning for Tag-Based Social Image Search</a></h3><div class="gs_a">Y Gao, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, ZJ Zha, <a href="/citations?user=d3h-zScAAAAJ&amp;hl=en&amp;oi=sra">J Shen</a>, X Li, X Wu - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the popularity of social media websites, extensive research efforts have been <br>dedicated to tag-based social image search. Both visual information and tags have been <br>investigated in the research field. However, most existing methods use tags and visual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3875216273605194683&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=25">Cited by 4</a> <a href="/scholar?q=related:u_MMSKuIxzUJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3875216273605194683&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'u_MMSKuIxzUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2037681" class=yC3>Video accessibility enhancement for hearing-impaired users</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">XT Yuan</a>, M Xu, J Jiang&hellip; - ACM Transactions on  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract There are more than 66 million people suffering from hearing impairment and this <br>disability brings them difficulty in video content understanding due to the loss of audio <br>information. If the scripts are available, captioning technology can help them in a certain <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9071236764054744351&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=25">Cited by 3</a> <a href="/scholar?q=related:H23ygsaB430J:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'H23ygsaB430J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2043688" class=yC4>An online video recommendation framework using rich information</a></h3><div class="gs_a">X Zhao, G Li, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, S Li, X Chen, Z Li - Proceedings of the Third  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Automatic video recommendation is involved in an attempt to tackle the information-<br>overload problem, aiming to present the personalized video list to the user. This paper <br>presents a novel approach to improve the accuracy of the video recommendation by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14415617937527635185&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=25">Cited by 1</a> <a href="/scholar?q=related:8XwVECOQDsgJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'8XwVECOQDsgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212001488" class=yC5>A probabilistic graphical model for topic and preference discovery on social media</a></h3><div class="gs_a">L Liu, F Zhu, L Zhang, S Yang - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Many web applications today thrive on offering services for large-scale multimedia data, eg, <br>Flickr for photos and YouTube for videos. However, these data, while rich in content, are <br>usually sparse in textual descriptive information. For example, a video clip is often <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8168163861104227078&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=25">Cited by 1</a> <a href="/scholar?q=related:BvNMQMElW3EJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8168163861104227078&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'BvNMQMElW3EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://luzheng.org/papers/VideoAder_hu_icimcs2011.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from luzheng.org</span><span class="gs_ggsS">luzheng.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2043683" class=yC6>Videoader: a video advertising system based on intelligent analysis of visual content</a></h3><div class="gs_a">J Hu, G Li, <a href="/citations?user=Qz_zAEUAAAAJ&amp;hl=en&amp;oi=sra">Z Lu</a>, J Xiao, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a> - Proceedings of the Third International  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Recent years have witnessed the prevalence of context based video advertisement. <br>However, those advertisement systems solely take the metadata into account, such as titles, <br>descriptions and tags. In this paper, we present a novel video advertising system called <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MNCeA-0fUWEJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7012421197621022768&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'MNCeA-0fUWEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2382382" class=yC8>Personalized video recommendation based on viewing history with the study on YouTube</a></h3><div class="gs_a">X Zhao, H Luan, <a href="/citations?user=qioooCAAAAAJ&amp;hl=en&amp;oi=sra">J Cai</a>, J Yuan, X Chen&hellip; - Proceedings of the 4th  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract With internet delivery of video content surging to an un-precedented level, video <br>recommendation has become an important approach for helping people access interesting <br>videos. In this paper, we propose a novel approach to integrate viewing history for <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'cw91DuwhwUYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212009046" class=yC9>Richang Hong, Linxie Tang, Jun Hu, Guanda Li, Jiang-Guo Jiang</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, L Tang, J Hu, G Li, JG Jiang - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract We have witnessed the booming of contextual video advertising recent years. <br>However, those advertisement systems solely take the metadata into account, such as titles, <br>descriptions and tags. This kind of text-based contextual advertising reveals a number of <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'qWujGb64isAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S092523121200149X" class=yCA>Query difficulty estimation for image retrieval</a></h3><div class="gs_a">Y Li, <a href="/citations?user=gsTrHoMAAAAJ&amp;hl=en&amp;oi=sra">B Geng</a>, <a href="/citations?user=cvgKxDQAAAAJ&amp;hl=en&amp;oi=sra">L Yang</a>, C Xu, W Bian - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Query difficulty estimation predicts the performance of the search result of the given query. It <br>is a powerful tool for multimedia retrieval and receives increasing attention. It can guide the <br>pseudo relevance feedback to rerank the image search results and re-write the query by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16754922410140218058&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=25">Cited by 1</a> <a href="/scholar?q=related:yrKjHylxhegJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16754922410140218058&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'yrKjHylxhegJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S092523121200152X" class=yCB>Relationship strength estimation for online social networks with the study on Facebook</a></h3><div class="gs_a">X Zhao, G Li, J Yuan, X Chen, Z Li - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Online social network has become a popular way for users to express themselves, connect <br>and share information with each other. However, in online social networks, the connections <br>between different users are all in binary status, which neglects the relationship strengths <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:tz2dN1GiRLYJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13133800883028311479&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'tz2dN1GiRLYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212001609" class=yCC>Collaborative visual modeling for automatic image annotation via sparse model coding</a></h3><div class="gs_a">M Wang, F Li, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a> - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Building visual models provides an important way to detect visual concepts from images. <br>However, due to the problems of visual diversity and uncertainty, the estimation based on <br>these models is not satisfactory. The visual relatedness among visual models is ignored <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:64sTPjJAVpEJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10472628568030677995&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'64sTPjJAVpEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://crcv-web.eecs.ucf.edu/papers/ITMM-AudioVideoCorrelation.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucf.edu</span><span class="gs_ggsS">ucf.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://crcv-web.eecs.ucf.edu/papers/ITMM-AudioVideoCorrelation.pdf" class=yCD>Multimodal Analysis for Identification and Segmentation of Moving-Sounding Objects</a></h3><div class="gs_a"><a href="/citations?user=uDuLNHkAAAAJ&amp;hl=en&amp;oi=sra">H Izadinia</a>, <a href="/citations?user=Iu9BD-QAAAAJ&amp;hl=en&amp;oi=sra">I Saleemi</a>, <a href="/citations?user=p8gsO3gAAAAJ&amp;hl=en&amp;oi=sra">M Shah</a> - 2012 - crcv-web.eecs.ucf.edu</div><div class="gs_rs">AbstractâIn this paper, we propose a novel method that exploits correlation between audio-<br>visual dynamics of a video to segment and localize objects that are the dominant source of <br>audio. Our approach consists of a two-step spatiotemporal segmentation mechanism that <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:l1daRAOkq4kJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9920202938152277911&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'l1daRAOkq4kJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:l1daRAOkq4kJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://www.sundaychennai.com/IEEE%202012%20Dotnet%20Basepaper/Robust%20Face-Name%20Graph%20Matching%20for%20Movie%20Character%20Identification.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sundaychennai.com</span><span class="gs_ggsS">sundaychennai.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6156449" class=yCF>Robust Face-Name Graph Matching for Movie Character Identification</a></h3><div class="gs_a"><a href="/citations?user=u6ivSjgAAAAJ&amp;hl=en&amp;oi=sra">J Sang</a>, C Xu - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic face identification of characters in movies has drawn significant research <br>interests and led to many interesting applications. It is a challenging problem due to the <br>huge variation in the appearance of each character. Although existing methods <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:CI-AUvRpbt4J:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16027864621935267592&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'CI-AUvRpbt4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/P683840235086106.pdf" class=yC11>Improving image tags by exploiting web search results</a></h3><div class="gs_a">X Zhang, Z Li, W Chao - Multimedia Tools and Applications, 2011 - Springer</div><div class="gs_rs">Abstract Automatic image tagging automatically assigns image with semantic keywords <br>called tags, which significantly facilitates image search and organization. Most of present <br>image tagging approaches are constrained by the training model learned from the training <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:bJ_eVAvtPIwJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'bJ_eVAvtPIwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/fulltext.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/E67J6H23U1702840.pdf" class=yC12>Video recommendation over multiple information sources</a></h3><div class="gs_a">X Zhao, J Yuan, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, G Li, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, Z Li&hellip; - Multimedia Systems, 2012 - Springer</div><div class="gs_rs">Abstract Video recommendation is an important tool to help people access interesting <br>videos. In this paper, we propose a universal scheme to integrate rich information for <br>personalized video recommendation. Our approach regards video recommendation as a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:l6PpYxdAlR0J:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2131680467834479511&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'l6PpYxdAlR0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6092497" class=yC14>Query Difficulty Prediction for Web Image Search</a></h3><div class="gs_a">X Tian, Y Lu, <a href="/citations?user=cvgKxDQAAAAJ&amp;hl=en&amp;oi=sra">L Yang</a> - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Image search plays an important role in our daily life. Given a query, the image <br>search engine is to retrieve images related to it. However, different queries have different <br>search difficulty levels. For some queries, they are easy to be retrieved (the search engine <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:AD64Z0noKUEJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'AD64Z0noKUEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168412002204" class=yC15>Multimedia encyclopedia construction by mining web knowledge</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, ZJ Zha, Y Gao, TS Chua, X Wu - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract In recent years, we have witnessed the blooming of Web 2.0 content such as <br>Wikipedia, Flickr and YouTube, etc. How might we benefit from such rich media resources <br>available on the internet? This paper presents a novel concept called Mediapedia, a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:-PXgd9tRpOoJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'-PXgd9tRpOoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168412001661" class=yC16>Social Image Tagging Using Graph-based Reinforcement on Multi-type Interrelated Objects</a></h3><div class="gs_a">X Zhang, X Zhao, Z Li, J Xia, R Jain, W Chao - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract Social image tagging is becoming increasingly popular with the development of <br>social website, where images are annotated with arbitrary keywords called tags. Most of <br>present image tagging approaches are mainly based on the visual similarity or mapping <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10188512964859572982&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=25">Cited by 1</a> <a href="/scholar?q=related:9rLqe4_eZI0J:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'9rLqe4_eZI0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396378" class=yC17>Touch saliency</a></h3><div class="gs_a">M Xu, <a href="/citations?user=V9W87PYAAAAJ&amp;hl=en&amp;oi=sra">B Ni</a>, J Dong, Z Huang, M Wang&hellip; - Proceedings of the 20th  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In this work, we propose a new concept of touch saliency, and attempt to answer the <br>question of whether the underlying image saliency map may be implicitly derived from the <br>accumulative touch behaviors (or more specifically speaking, zoom-in and panning <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'bVcWgIwTJX4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://137.132.14.55/handle/10635/33304" class=yC18>Human Visual Perception, study and applications to understanding Images and Videos</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a> - 2011 - 137.132.14.55</div><div class="gs_rs">Assessing whether a photograph is interesting, or spotting people in conversation or <br>important objects in an images and videos, are visual tasks that we humans do effortlessly <br>and in a robust manner. In this thesis I first explore and quantify how humans distinguish <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jna1s1LyMH0J:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9020976490639357582&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'jna1s1LyMH0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jna1s1LyMH0J:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168412002447" class=yC19>An improved method of locality sensitive hashing for indexing large-scale and high-dimensional features</a></h3><div class="gs_a">X Gu, L Zhang, Y Zhang, D Zhang, J Li - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract In recent years, Locality Sensitive Hashing (LSH) has been popularly used as an <br>effective and efficient index structure of multimedia signals. LSH is originally proposed for <br>resolving the high-dimensional approximate similarity search problem. Until now, many <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:QQqMyu7L2tkJ:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'QQqMyu7L2tkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/83337436K3G00608.pdf" class=yC1A>A novel framework for concept detection on large scale video database and feature pool</a></h3><div class="gs_a">G Lv, C Zheng - Artificial Intelligence Review, 2011 - Springer</div><div class="gs_rs">Abstract Large-scale semantic concept detection from large video database suffers from <br>large variations among different semantic concepts as well as their corresponding effective <br>low-level features. In this paper, we propose a novel framework to deal with this obstacle. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:LjG79W-vzQ8J:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'LjG79W-vzQ8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/4%20MMM%202012.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/C344510K3X4177R3.pdf" class=yC1B>On video recommendation over social network</a></h3><div class="gs_a">X Zhao, J Yuan, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, Z Li&hellip; - Advances in Multimedia  &hellip;, 2012 - Springer</div><div class="gs_rs">Video recommendation is a hot research topic to help people access interesting videos. The <br>existing video recommendation approaches include CBF, CF and HF. However, these <br>approaches treat the relationships between all users as equal and neglect an important <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5632568206682281170&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=25">Cited by 1</a> <a href="/scholar?q=related:0tR0_PDmKk4J:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5632568206682281170&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'0tR0_PDmKk4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://bdtd.biblioteca.ufpb.br/tde_arquivos/16/TDE-2012-12-03T122234Z-1974/Publico/Arquivototal.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ufpb.br</span><span class="gs_ggsS">ufpb.br <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://bdtd.biblioteca.ufpb.br/tde_arquivos/16/TDE-2012-12-03T122234Z-1974/Publico/Arquivototal.pdf" class=yC1D>UMA PROPOSTA DE PROTOCOLO DE CODIFICAÃÃO DE LIBRAS PARA SISTEMAS DE TV DIGITAL</a></h3><div class="gs_a">FH LEMOS - 2012 - bdtd.biblioteca.ufpb.br</div><div class="gs_rs">A comunicaÃ§Ã£o Ã© essencial na interaÃ§Ã£o humana. Segundo Russell e Norvig [1],âa <br>comunicaÃ§Ã£o Ã© a troca intencional de informaÃ§Ãµes provocada pela produÃ§Ã£o e percepÃ§Ã£o <br>de sinais extraÃ­dos de um sistema compartilhado de sinais convencionaisâ. AtravÃ©s <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'tAHlFkwdvAkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.dbpia.co.kr/Article/?arid=1929188&amp;arid=1929188" class=yC1F>ë¹ëì¤ì ëí ë§íì  í¨ê³¼ í©ì±</a></h3><div class="gs_a">ì´ì¸ê¶ - íêµ­ì ë³´ê³¼íí 2012 íêµ­ì»´í¨í°ì¢í©íì ëí ë¼ë¬¸ì§, 2012 - dbpia.co.kr</div><div class="gs_rs">ìµê·¼ ìíë ëë¼ë§ ê°ì ë¯¸ëì´ ë°ì´í°ê° í­ë°ì ì¼ë¡ ì¦ê°íë©´ì, ë¤ìí ì¸ì´ë¡ ë²ì­ë ìë§ <br>ë°ì´í°ëì¦ê°íê³  ìë¤. ì´ë¬í ìë§ì ëë¶ë¶ íë©´ íë¨ì´ë ì°ì¸¡ì ìì¹ê° ê³ ì ëì´ <br>ëíë´ë ë°©ìì ì·¨íê³  ìë¤. ê·¸ë¬ë ì´ ë°©ììë ëª ê°ì§ íê³ì ì ê°ì§ê³  ìë¤. ìë§ê³¼ <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:FLj2b4ngV38J:scholar.google.com/&amp;hl=en&amp;num=25&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'FLj2b4ngV38J')" href="#" class="gs_nph">Cite</a></div></div></div>
