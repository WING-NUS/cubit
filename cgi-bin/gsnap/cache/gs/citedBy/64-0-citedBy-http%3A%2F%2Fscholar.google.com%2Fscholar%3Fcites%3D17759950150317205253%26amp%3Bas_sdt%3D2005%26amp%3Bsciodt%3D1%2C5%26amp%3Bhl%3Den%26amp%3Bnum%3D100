Total results = 64
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.5031&amp;rep=rep1&amp;type=pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1576260" class=yC0>Concept-based video retrieval</a></h3><div class="gs_a"><a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Foundations and Trends in Information  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we review 300 references on video retrieval, indicating when text-only <br>solutions are unsatisfactory and showing the promising alternatives which are in majority <br>concept-based. Therefore, central to our discussion is the notion of a semantic concept: an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1240556430566916602&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 140</a> <a href="/scholar?q=related:-oHEN4BXNxEJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1240556430566916602&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'-oHEN4BXNxEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md0', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md0" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:-oHEN4BXNxEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=868717410844952179&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://pdf.aminer.org/000/098/270/lessons_for_the_future_from_a_decade_of_informedia_video.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aminer.org</span><span class="gs_ggsS">aminer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/EU8X0KK3F1YNHAUD.pdf" class=yC2>Lessons for the future from a decade of informedia video analysis research</a></h3><div class="gs_a"><a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">A Hauptmann</a> - Image and Video Retrieval, 2005 - Springer</div><div class="gs_rs">Abstract. The overarching goal of the Informedia Digital Video Library project has been to <br>achieve machine understanding of video media, including all aspects of search, retrieval, <br>visualization and summarization in both contemporaneous and archival content <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3424919910863406012&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 68</a> <a href="/scholar?q=related:vAMtT4PChy8J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/12/2A/RN171992010.html?source=googlescholar" class="gs_nph" class=yC4>BL Direct</a> <a href="/scholar?cluster=3424919910863406012&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'vAMtT4PChy8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.comp.nus.edu.sg/~mohan/papers/fusion_survey.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/E31M71152774R630.pdf" class=yC5>Multimodal fusion for multimedia analysis: a survey</a></h3><div class="gs_a"><a href="/citations?user=2s3sftgAAAAJ&amp;hl=en&amp;oi=sra">PK Atrey</a>, <a href="/citations?user=Qq4AAT4AAAAJ&amp;hl=en&amp;oi=sra">MA Hossain</a>, <a href="/citations?user=VcOjgngAAAAJ&amp;hl=en&amp;oi=sra">A El Saddik</a>, MS Kankanhalli - Multimedia Systems, 2010 - Springer</div><div class="gs_rs">Abstract This survey aims at providing multimedia researchers with a state-of-the-art <br>overview of fusion strategies, which are used for combining multiple modalities in order to <br>accomplish various multimedia analysis tasks. The existing literature on multimodal fusion <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2303959858949282248&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 65</a> <a href="/scholar?q=related:yFlu6UhP-R8J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2303959858949282248&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'yFlu6UhP-R8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://vireo.cs.cityu.edu.hk/papers/csvt06.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1626302" class=yC7>Clip-based similarity measure for query-dependent clip retrieval and video summarization</a></h3><div class="gs_a">Y Peng, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Circuits and Systems for Video Technology,  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper proposes a new approach and algorithm for the similarity measure of <br>video clips. The similarity is mainly based on two bipartite graph matching algorithms: <br>maximum matching (MM) and optimal matching (OM). MM is able to rapidly filter irrelevant <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17400818532076996750&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 55</a> <a href="/scholar?q=related:jsh77UIgfPEJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/57/0B/RN196071348.html?source=googlescholar" class="gs_nph" class=yC9>BL Direct</a> <a href="/scholar?cluster=17400818532076996750&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'jsh77UIgfPEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://ciir.cs.umass.edu/pubfiles/mm-385.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from umass.edu</span><span class="gs_ggsS">umass.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1416476" class=yCA>Combining text and audio-visual features in video indexing</a></h3><div class="gs_a"><a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a>, <a href="/citations?user=_0aMq28AAAAJ&amp;hl=en&amp;oi=sra">R Manmatha</a>&hellip; - Acoustics, Speech, and  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We discuss the opportunities, state of the art, and open research issues in using <br>multi-modal features in video indexing. Specifically, we focus on how imperfect text data <br>obtained by automatic speech recognition (ASR) may be used to help solve challenging <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12681820938198234719&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 47</a> <a href="/scholar?q=related:X7ZqvvPg_q8J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12681820938198234719&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 21 versions</a> <a onclick="return gs_ocit(event,'X7ZqvvPg_q8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://doras.dcu.ie/211/1/acm_tois_apr_2006.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dcu.ie</span><span class="gs_ggsS">dcu.ie <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1148021" class=yCC>User evaluation of FÃ­schlÃ¡r-News: An automatic broadcast news delivery system</a></h3><div class="gs_a">H Lee, <a href="/citations?user=o7xnW2MAAAAJ&amp;hl=en&amp;oi=sra">AF Smeaton</a>, <a href="/citations?user=_i78M7gAAAAJ&amp;hl=en&amp;oi=sra">NE O&#39;connor</a>, <a href="/citations?user=0XvFVusAAAAJ&amp;hl=en&amp;oi=sra">B Smyth</a> - ACM Transactions on  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract Technological developments in content-based analysis of digital video information <br>are undergoing much progress, with ideas for fully automatic systems now being proposed <br>and demonstrated. Yet because we do not yet have robust operational video retrieval <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11654908758465849234&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 38</a> <a href="/scholar?q=related:kify9cONvqEJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4C/24/RN193617590.html?source=googlescholar" class="gs_nph" class=yCE>BL Direct</a> <a href="/scholar?cluster=11654908758465849234&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'kify9cONvqEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.nlpr.labs.gov.cn/2008papers/gjkw/gk12.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from labs.gov.cn</span><span class="gs_ggsS">labs.gov.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4469884" class=yCF>A multimodal scheme for program segmentation and representation in broadcast video streams</a></h3><div class="gs_a"><a href="/citations?user=7_BkyxEAAAAJ&amp;hl=en&amp;oi=sra">J Wang</a>, L Duan, Q Liu, H Lu&hellip; - &hellip; , IEEE Transactions on, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the advance of digital video recording and playback systems, the request for <br>efficiently managing recorded TV video programs is evident so that users can readily locate <br>and browse their favorite programs. In this paper, we propose a multimodal scheme to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9884284462785445419&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 25</a> <a href="/scholar?q=related:Kz49h1kILIkJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/27/46/RN226783646.html?source=googlescholar" class="gs_nph" class=yC11>BL Direct</a> <a href="/scholar?cluster=9884284462785445419&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'Kz49h1kILIkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4556923" class=yC12>Ants: A complete system for automatic news programme annotation based on multimodal analysis</a></h3><div class="gs_a">A Messina, R Borgotallo, G Dimino&hellip; - Image Analysis for  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes an integrated system for the acquisition, automatic annotation <br>and Web publication of television broadcast news programmes named ANTS (automatic <br>newscast transcription system). The system consists of several analysis components <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1659062245853073714&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 15</a> <a href="/scholar?q=related:MpFVkFIsBhcJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1659062245853073714&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'MpFVkFIsBhcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.ulb.tu-darmstadt.de/tocs/205793223.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tu-darmstadt.de</span><span class="gs_ggsS">tu-darmstadt.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=6yyj2GIhWH8C&amp;oi=fnd&amp;pg=PR5&amp;ots=5O_XGPeC64&amp;sig=CJhF-GuZGjZrr4S9uAL5bp1OSBY" class=yC13>Introduction to video search engines</a></h3><div class="gs_a">DC Gibbon, Z Liu - 2008 - books.google.com</div><div class="gs_rs">Video search engines enable users to take advantage of constantly growing video <br>resources like, for example, video on demand, Internet television and YouTube, for a wide <br>variety of applications including entertainment, education and communications. David <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=194613694663555717&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 14</a> <a href="/scholar?q=related:hfrYyh9oswIJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=194613694663555717&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'hfrYyh9oswIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:hfrYyh9oswIJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=18156452429682321218&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4959994" class=yC15>A detection-based approach to broadcast news video story segmentation</a></h3><div class="gs_a">C Ma, B Byun, I Kim, CH Lee - Acoustics, Speech and Signal  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A detection-based paradigm decomposes a complex system into small pieces, <br>solves each subproblem one by one, and combines the collected evidence to obtain a final <br>solution. In this study of video story segmentation, a set of key events are first detected <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1864104719358796898&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 12</a> <a href="/scholar?q=related:YqSCpV2h3hkJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1864104719358796898&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'YqSCpV2h3hkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4517734" class=yC16>Unsupervised anchor shot detection using multi-modal spectral clustering</a></h3><div class="gs_a">C Ma, CH Lee - &hellip;  Speech and Signal Processing, 2008. ICASSP &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a novel unsupervised method for anchor shot detection using <br>spectral clustering with multi-modal features. Unlike previous unsupervised studies where <br>the acoustic trajectory features can not be combined with visual features directly, only a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16294907706752903283&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 11</a> <a href="/scholar?q=related:cyjIpDkkI-IJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16294907706752903283&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'cyjIpDkkI-IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://mediaminer.ict.tno.nl/papers/civr05.final.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tno.nl</span><span class="gs_ggsS">tno.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/7VFLJVGNF9NLPV4V.pdf" class=yC17>Boundary error analysis and categorization in the TRECVID news story segmentation task</a></h3><div class="gs_a">J Arlandis, P Over, <a href="/citations?user=w9YiYrsAAAAJ&amp;hl=en&amp;oi=sra">W Kraaij</a> - Image and Video Retrieval, 2005 - Springer</div><div class="gs_rs">Abstract. In this paper, an error analysis based on boundary error popularity (frequency) <br>including semantic boundary categorization is applied in the context of the news story <br>segmentation task from TRECVID 1. Clusters of systems were defined based on the input <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8413405290013484784&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 10</a> <a href="/scholar?q=related:8Gal_ohrwnQJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/07/05/RN171992148.html?source=googlescholar" class="gs_nph" class=yC19>BL Direct</a> <a href="/scholar?cluster=8413405290013484784&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'8Gal_ohrwnQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://www.dcs.gla.ac.uk/~hemant/NewsStorySegmentation_MMM09.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gla.ac.uk</span><span class="gs_ggsS">gla.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/f26r1411ql135143.pdf" class=yC1A>Tv news story segmentation based on semantic coherence and content similarity</a></h3><div class="gs_a">H Misra, <a href="/citations?user=tiASHnwAAAAJ&amp;hl=en&amp;oi=sra">F Hopfgartner</a>, A Goyal, P Punitha&hellip; - Advances in Multimedia  &hellip;, 2010 - Springer</div><div class="gs_rs">Abstract. In this paper, we introduce and evaluate two novel approaches, one using video <br>stream and the other using close-caption text stream, for segmenting TV news into stories. <br>The segmentation of the video stream into stories is achieved by detecting anchor person <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12915148785805740031&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 10</a> <a href="/scholar?q=related:_8dEr2TTO7MJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12915148785805740031&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'_8dEr2TTO7MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> <a href="http://dl.acm.org/citation.cfm?id=1203343" class=yC1C>Interactive Video: Algorithms and Technologies (Signals and Communication Technology)</a></h3><div class="gs_a">R Hammoud - 2006 - dl.acm.org</div><div class="gs_fl"><a href="/scholar?cites=13256681117322818459&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 7</a> <a href="/scholar?q=related:m7OnQj0x-bcJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13256681117322818459&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'m7OnQj0x-bcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md13', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md13" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:m7OnQj0x-bcJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=3470839640616059124&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://sites.google.com/site/fbashir/14-vlbv05.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPATAPP11361829&amp;id=58OgAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC1D>Content-based video summarization using spectral clustering</a></h3><div class="gs_a"><a href="/citations?user=FSybdvkAAAAJ&amp;hl=en&amp;oi=sra">KA Peker</a>, FI Bashir - US Patent App. 11/361,829, 2006 - Google Patents</div><div class="gs_rs">A method summarizes a video including a sequence of frames. The video is partitioned into <br>segments of frames, and faces are detected in the frames of the segments. Features of the <br>frames including the faces are extracted. For each segment including the faces, a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2609677817747832253&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 7</a> <a href="/scholar?q=related:vX0TCSZwNyQJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2609677817747832253&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'vX0TCSZwNyQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0306457310000981" class=yC1F>Text segmentation: A topic modeling perspective</a></h3><div class="gs_a">H Misra, F Yvon, <a href="/citations?user=erIXTCsAAAAJ&amp;hl=en&amp;oi=sra">O CappÃ©</a>, <a href="/citations?user=ERvFJGkAAAAJ&amp;hl=en&amp;oi=sra">J Jose</a> - Information Processing &amp; Management, 2011 - Elsevier</div><div class="gs_rs">In this paper, the task of text segmentation is approached from a topic modeling perspective. <br>We investigate the use of two unsupervised topic models, latent Dirichlet allocation (LDA) <br>and multinomial mixture (MM), to segment a text into semantically coherent parts. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6220242020588640878&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 6</a> <a href="/scholar?q=related:btokVim9UlYJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6220242020588640878&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'btokVim9UlYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4624783" class=yC20>Use of probabilistic clusters supports for broadcast news segmentation</a></h3><div class="gs_a">M Di Iulio, A Messina - Database and Expert Systems  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Broadcast news segmentation is a recognised important task in audiovisual <br>archives retrieval systems. Though research is very active in the field, the problem has not <br>been satisfactorily solved yet. This paper presents a promising novel methodology at <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16630832561568867652&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 6</a> <a href="/scholar?q=related:ROVPzxmWzOYJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16630832561568867652&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ROVPzxmWzOYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://lastlaugh.inf.cs.cmu.edu/alex/cviu08_wu.pdf" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S107731420700152X" class=yC21>Measuring novelty and redundancy with multiple modalities in cross-lingual broadcast news</a></h3><div class="gs_a"><a href="/citations?user=Yt70KEIAAAAJ&amp;hl=en&amp;oi=sra">X Wu</a>, <a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Computer Vision and Image  &hellip;, 2008 - Elsevier</div><div class="gs_rs">News videos from different channels, languages are broadcast everyday, which provide <br>abundant information for users. To effectively search, retrieve, browse and track news <br>stories, news story similarity plays a critical role in assessing the novelty and redundancy <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11801897020199699987&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 5</a> <a href="/scholar?q=related:E4a-jM_CyKMJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11801897020199699987&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'E4a-jM_CyKMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm08-wanggang.pdf" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459393" class=yC23>Exploring knowledge of sub-domain in a multi-resolution bootstrapping framework for concept detection in news video</a></h3><div class="gs_a">G Wang, TS Chua, <a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a> - Proceeding of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we present a model based on a multi-resolution, multi-source and <br>multi-modal (M3) bootstrapping framework that exploits knowledge of sub-domains for <br>concept detection in news video. Because the characteristics and distributions of data in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15442298916331928942&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 5</a> <a href="/scholar?q=related:bj1dLwwRTtYJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15442298916331928942&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'bj1dLwwRTtYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://www.bilkent.edu.tr/~kpeker/publications/confpapers/2006-ICME_KP-IO-AD_Broadcast%20video%20program%20summarization%20using%20face%20tracks.pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bilkent.edu.tr</span><span class="gs_ggsS">bilkent.edu.tr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036784" class=yC25>Broadcast video program summarization using face tracks</a></h3><div class="gs_a"><a href="/citations?user=FSybdvkAAAAJ&amp;hl=en&amp;oi=sra">KA Peker</a>, I Otsuka, A Divakaran - Multimedia and Expo, 2006  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present a novel video summarization and skimming technique using face <br>detection on broadcast video programs. We take the faces in video as our primary target as <br>they constitute the focus of most consumer video programs. We detect face tracks in video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4620015395600259344&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 4</a> <a href="/scholar?q=related:ENmWP4aXHUAJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4620015395600259344&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'ENmWP4aXHUAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://www.toriwaki.nuie.nagoya-u.ac.jp/~ide/res/paper/E10-conference-ide-3pub.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nagoya-u.ac.jp</span><span class="gs_ggsS">nagoya-u.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.toriwaki.nuie.nagoya-u.ac.jp/~ide/res/paper/E10-conference-ide-3pub.pdf" class=yC27>Exploiting the chronological semantic structure in a large-scale broadcast news video archive for its efficient exploration</a></h3><div class="gs_a"><a href="/citations?user=8PXJm98AAAAJ&amp;hl=en&amp;oi=sra">I Ide</a>, T Kinoshita, <a href="/citations?user=ne-WQc4AAAAJ&amp;hl=en&amp;oi=sra">T Takahashi</a>&hellip; - &hellip;  Summit and Conf, 2010 - toriwaki.nuie.nagoya-u.ac.jp</div><div class="gs_rs">AbstractâRecent advance in digital storage technology has enabled us to archive more <br>than 1,700 hours of video data from a daily Japanese news show in the last nine years. In <br>this paper, to effectively make use of the video data in the archive, we first present a news <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=930395819418702021&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 5</a> <a href="/scholar?q=related:xdD1fBFu6QwJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=930395819418702021&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'xdD1fBFu6QwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md20', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md20" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:xdD1fBFu6QwJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://hal.inria.fr/docs/00/64/66/03/PDF/paper_434.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from inria.fr</span><span class="gs_ggsS">inria.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6011951" class=yC29>Unsupervised mining of audiovisually consistent segments in videos with application to structure analysis</a></h3><div class="gs_a">M Ben, G Gravier - Multimedia and Expo (ICME), 2011 IEEE  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, a multimodal event mining technique is proposed to discover <br>repeating video segments exhibiting audio and visual consistency in a totally unsupervised <br>manner. The mining strategy first exploits independent audio and visual cluster analysis to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=575508135340550256&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 4</a> <a href="/scholar?q=related:cChSlJmd_AcJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=575508135340550256&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'cChSlJmd_AcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00167ED1V01Y200812ICR002" class=yC2B>Automated metadata in multimedia information systems: Creation, refinement, use in surrogates, and evaluation</a></h3><div class="gs_a">MG Christel - &hellip; Lectures on Information Concepts, Retrieval, and &hellip;, 2009 - morganclaypool.com</div><div class="gs_rs">Abstract Improvements in network bandwidth along with dramatic drops in digital storage <br>and processing costs have resulted in the explosive growth of multimedia (combinations of <br>text, image, audio, and video) resources on the Internet and in digital repositories. A suite <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=207521543820141715&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 5</a> <a href="/scholar?q=related:k-Sjnr5D4QIJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=207521543820141715&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'k-Sjnr5D4QIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md22', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md22" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:k-Sjnr5D4QIJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=6795887439985526845&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://www.semedia.org/PubFolder/UGSplitMergeBasedStorySegmentationNewsVideos.pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from semedia.org</span><span class="gs_ggsS">semedia.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/UW8836Q004256040.pdf" class=yC2C>Split and merge based story segmentation in news videos</a></h3><div class="gs_a">A Goyal, P Punitha, <a href="/citations?user=tiASHnwAAAAJ&amp;hl=en&amp;oi=sra">F Hopfgartner</a>, <a href="/citations?user=ERvFJGkAAAAJ&amp;hl=en&amp;oi=sra">J Jose</a> - Advances in Information  &hellip;, 2009 - Springer</div><div class="gs_rs">Abstract. Segmenting videos into smaller, semantically related segments which ease the <br>access of the video data is a challenging open research. In this paper, we present a scheme <br>for semantic story segmentation based on anchor person detection. The proposed model <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5556784555583182104&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 4</a> <a href="/scholar?q=related:GJXpuByqHU0J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5556784555583182104&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'GJXpuByqHU0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1329665" class=yC2E>Concept-based large-scale video database browsing and retrieval via visualization</a></h3><div class="gs_a">H Luo, J Adviser-Fan - 2007 - dl.acm.org</div><div class="gs_rs">Abstract Motivated by Google&#39;s great success on text document retrieval and recent progress <br>in semantic video understanding, researchers began to build a new generation of video <br>retrieval systems that are able to support semantic video retrieval via keywords. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7552823667137218565&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 5</a> <a href="/scholar?q=related:BYjg0hsF0WgJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7552823667137218565&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'BYjg0hsF0WgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://www.hindawi.com/journals/ijdmb/aip/486487/" class=yC30><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[HTML]</span><span class="gs_ct2">[HTML]</span></span> <a href="http://www.hindawi.com/journals/ijdmb/aip/486487/" class=yC2F>Multimodal Indexing of Multilingual News Video</a></h3><div class="gs_a"><a href="/citations?user=XUwiadkAAAAJ&amp;hl=en&amp;oi=sra">H Ghosh</a>, <a href="/citations?user=2OsvtvgAAAAJ&amp;hl=en&amp;oi=sra">SK Kopparapu</a>, <a href="/citations?user=ugI69q0AAAAJ&amp;hl=en&amp;oi=sra">T Chattopadhyay</a>&hellip; - International Journal of &hellip;, 2010 - hindawi.com</div><div class="gs_rs">The problems associated with automatic analysis of news telecasts are more severe in a <br>country like India, where there are many national and regional language channels, besides <br>English. In this paper, we present a framework for multimodal analysis of multilingual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6322536354863597187&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 3</a> <a href="/scholar?q=related:g0YjB1EpvlcJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6322536354863597187&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'g0YjB1EpvlcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md25', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md25" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:g0YjB1EpvlcJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://www.merl.com/papers/docs/TR2005-155.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from merl.com</span><span class="gs_ggsS">merl.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.merl.com/papers/docs/TR2005-155.pdf" class=yC31>Browsing news and talk video on a consumer electronics platform using face detection</a></h3><div class="gs_a"><a href="/citations?user=FSybdvkAAAAJ&amp;hl=en&amp;oi=sra">KA Peker</a>, A Divakaran, T Lanning - Proc. Of SPIE, 2005 - merl.com</div><div class="gs_rs">Abstract. We present a consumer video browsing system that enables use of multiple <br>alternative summaries in a simple and effective user interface suitable for consumer <br>electronics platforms. We present a news and talk video segmentation and summary <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3794434011217831727&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 3</a> <a href="/scholar?q=related:L6dMIqGJqDQJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/60/4F/RN182452486.html?source=googlescholar" class="gs_nph" class=yC33>BL Direct</a> <a href="/scholar?cluster=3794434011217831727&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'L6dMIqGJqDQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:L6dMIqGJqDQJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607428" class=yC34>Exploiting story-level context to improve video search</a></h3><div class="gs_a">K Wan - Multimedia and Expo, 2008 IEEE International  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A trend in recent news video retrieval systems is to exploit contextual cues to <br>improve search precision. Compared to using information only at the shot-level, the <br>inclusion of story-level contextual cues can intuitively broaden query coverage and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2327599257253295464&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 3</a> <a href="/scholar?q=related:aLULPDFLTSAJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'aLULPDFLTSAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://malach.umiacs.umd.edu/pubs/PZ_06_Knowl-Based.pdf" class=yC36><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from umd.edu</span><span class="gs_ggsS">umd.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://malach.umiacs.umd.edu/pubs/PZ_06_Knowl-Based.pdf" class=yC35>Knowledge-based approaches to the segmentation of oral history interviews</a></h3><div class="gs_a">P Zhang, D Soergel - 2006 - malach.umiacs.umd.edu</div><div class="gs_rs">Abstract This paper applies discourse knowledge to the segmentation of speech transcripts. <br>The paper reviews literature on discourse structure, as well as approaches used in text <br>segmentation and speech segmentation, identifies what features are used and how the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=172380151527456047&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 3</a> <a href="/scholar?q=related:Lz0fotVqZAIJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Lz0fotVqZAIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md28', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md28" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Lz0fotVqZAIJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://www.science.uva.nl/~mdr/Publications/Files/civr2007.pdf" class=yC38><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1282322" class=yC37>The value of stories for speech-based video search</a></h3><div class="gs_a">B Huurnink, <a href="/citations?user=AVDkgFIAAAAJ&amp;hl=en&amp;oi=sra">M de Rijke</a> - Proceedings of the 6th ACM international  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Anecdotal evidence suggests that story-level information is important for the speech <br>component of video retrieval. In this paper we perform a systematic examination of the <br>combination of shot-level and story-level speech, using a document expansion approach. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8781016341545898656&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 3</a> <a href="/scholar?q=related:oFK-vNZv3HkJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8781016341545898656&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'oFK-vNZv3HkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/90287a/200711/25873702.html" class=yC39>æ°é»è§é¢æäºåååå²ææ¯ç»¼è¿°</a></h3><div class="gs_a">åä¸­ï¼ å¼ æ¥ç°ï¼ èè²æº - ä¸­å½å¾è±¡å¾å½¢å­¦æ¥, 2007 - cqvip.com</div><div class="gs_rs">æ°é»è§é¢çæäºåååå²ä¸è¬éç¨ç»è®¡å­¦æèä¿¡æ¯æ²¦çæ¹æ³, å°æ°é»èç®åå²æä¸ç³»åæåèª<br>ä¸»é¢åå®¹çæäºåå. è¿äºåååæ çæ¯è§é¢æµçé«å±è¯­ä¹, æ¯å»ºç«è§é¢ç´¢å¼çæä½³å±æ¬¡. <br>è¯¥æå¯¹è¿ä¸ææ¯è¿è¡äºç»¼è¿°, å°ç°ææ¹æ³æ ¹æ®å©ç¨ä¿¡æ¯çè§åº¦åä¸º3 ç±»: åæ¨¡æçåå²æ¹æ³, å¤<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14723120262307448603&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 6</a> <a href="/scholar?q=related:GwcxRN8HU8wJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14723120262307448603&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'GwcxRN8HU8wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/C5P467735R5N5263.pdf" class=yC3A>Pic-A-Topic: gathering information efficiently from recorded TV shows on travel</a></h3><div class="gs_a">T Sakai, T Uehara, K Sumita, T Shimomori - Information Retrieval  &hellip;, 2006 - Springer</div><div class="gs_rs">Abstract. We introduce a system called Pic-A-Topic, which analyses closed captions of <br>Japanese TV shows on travel to perform topic segmentation and topic sentence selection. <br>Our objective is to provide a table-of-contents interface that enables efficient viewing of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3359609975952247820&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 2</a> <a href="/scholar?q=related:DAx-IHm7ny4J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/56/4D/RN196682028.html?source=googlescholar" class="gs_nph" class=yC3B>BL Direct</a> <a href="/scholar?cluster=3359609975952247820&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'DAx-IHm7ny4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://smartech.gatech.edu/jspui/bitstream/1853/33889/1/ma_chengyuan_201005_phd.pdf" class=yC3D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gatech.edu</span><span class="gs_ggsS">gatech.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://smartech.gatech.edu/handle/1853/33889" class=yC3C>A detection-based pattern recognition framework and its applications</a></h3><div class="gs_a">C Ma - 2010 - smartech.gatech.edu</div><div class="gs_rs">The objective of this dissertation is to present a detection-based pattern recognition <br>framework and demonstrate its applications in automatic speech recognition and broadcast <br>news video story segmentation. Inspired by the studies of modern cognitive psychology <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8118894574215641883&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 4</a> <a href="/scholar?q=related:Gzv4eJkbrHAJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8118894574215641883&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'Gzv4eJkbrHAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md32', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md32" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:Gzv4eJkbrHAJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=9235294807336651394&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://lms.comp.nus.edu.sg/papers/media/2008/bookchapter-wanggang.pdf" class=yC3F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/L8568841V710L17G.pdf" class=yC3E>Capturing text semantics for concept detection in news video</a></h3><div class="gs_a">G Wang, TS Chua - Multimedia Content Analysis, 2009 - Springer</div><div class="gs_rs">The overwhelming amounts of multimedia contents have triggered the need for automatic <br>semantic concept detection. However, as there are large variations in the visual feature <br>space, text from automatic speech recognition (ASR) has been extensively used and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15510479479648965246&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 2</a> <a href="/scholar?q=related:flJ2oOhKQNcJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15510479479648965246&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'flJ2oOhKQNcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4604150" class=yC40>Multiple Descriptions and Adaptive Delivery of Media Content over Wireless</a></h3><div class="gs_a">TY Huang - &hellip;  Hiding and Multimedia Signal Processing, 2008.  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The dynamic and varying wireless channel makes it a challenge to efficiently <br>delivery media contents over wireless with quality guarantee. We propose a multi-<br>description scheme for the adaptive delivery of media contents over wireless. A video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12229332119189907098&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 2</a> <a href="/scholar?q=related:muaWxMRQt6kJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12229332119189907098&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'muaWxMRQt6kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://www.istudies.net/ojs/index.php/journal/article/viewFile/51/57" class=yC42><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from istudies.net</span><span class="gs_ggsS">istudies.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.istudies.net/ojs/index.php/journal/article/viewArticle/51" class=yC41>Automatic Segmentation, Aggregation and Indexing of Multimodal News Information from Television and the Internet</a></h3><div class="gs_a">M Montagnuolo, A Messina, R Borgotallo - International Journal of  &hellip;, 2010 - istudies.net</div><div class="gs_rs">Abstract The global diffusion of the Internet has enabled the distribution of informative <br>content through dynamic media such as RSS feeds and video blogs. At the same time, the <br>decreasing cost of electronic devices has increased the pervasive availability of the same <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18365464505895461101&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 2</a> <a href="/scholar?q=related:7aDx6Kk83_4J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18365464505895461101&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'7aDx6Kk83_4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="http://www.hindawi.com/journals/ijdmb/aip/732514/" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[HTML]</span><span class="gs_ct2">[HTML]</span></span> <a href="http://www.hindawi.com/journals/ijdmb/aip/732514/" class=yC43>Automatic Story Segmentation for TV News Video Using Multiple Modalities</a></h3><div class="gs_a">Ã Dumont, G QuÃ©not - International Journal of Digital Multimedia  &hellip;, 2012 - hindawi.com</div><div class="gs_rs">While video content is often stored in rather large files or broadcasted in continuous streams, <br>users are often interested in retrieving only a particular passage on a topic of interest to <br>them. It is, therefore, necessary to split video documents or streams into shorter segments <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6254370804616678151&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:B0vAFhz9y1YJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6254370804616678151&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'B0vAFhz9y1YJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md36', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md36" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:B0vAFhz9y1YJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://hal.inria.fr/docs/00/67/11/57/PDF/mmm2012_TA.pdf" class=yC46><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from inria.fr</span><span class="gs_ggsS">inria.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/40L268485G825875.pdf" class=yC45>Improving cluster selection and event modeling in unsupervised mining for automatic audiovisual video structuring</a></h3><div class="gs_a">AP Ta, M Ben, G Gravier - Advances in Multimedia Modeling, 2012 - Springer</div><div class="gs_rs">Can we discover audio-visually consistent events from videos in a totally unsupervised <br>manner? And, how to mine videos with different genres? In this paper we present our new <br>results in automatically discovering audio-visual events. A new measure is proposed to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14338097167812249803&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:y8h7Wmgn-8YJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14338097167812249803&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'y8h7Wmgn-8YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/40W105R622732265.pdf" class=yC47>Hyper media news: a fully automated platform for large scale analysis, production and distribution of multimodal news content</a></h3><div class="gs_a">A Messina, M Montagnuolo, R Di Massa&hellip; - Multimedia Tools and  &hellip;, 2011 - Springer</div><div class="gs_rs">Abstract This paper describes Hyper Media News (HMNews), a system for the automated <br>aggregation and consumption of information streams from digital television and the Internet. <br>TV newscasts are automatically segmented, annotated and indexed. Such information is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1807743842444193678&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:ji_JZXJlFhkJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'ji_JZXJlFhkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB39" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW39"><a href="http://vipl.ict.ac.cn/sites/default/files/papers/files/2010_ICME_cxliu_Event%20based%20news%20video%20people%20classification%20and%20ranking%20using%20multimodality%20features.pdf" class=yC49><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5582989" class=yC48>Event based news video people classification and ranking using multimodality features</a></h3><div class="gs_a">C Liu, Q Huang, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, C Xu - Multimedia and Expo (ICME),  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Existing research on news video analysis mainly concentrates on structure <br>analysis, semantic concept detection, annotation and search. However, little work has been <br>contributed to news video people community analysis, which is helpful for users to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2260641255092447691&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:y-lgfj5pXx8J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2260641255092447691&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'y-lgfj5pXx8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Multimodal Indexing of Multilingual News Video</h3><div class="gs_a">G Hiranmay, <a href="/citations?user=2OsvtvgAAAAJ&amp;hl=en&amp;oi=sra">K Sunil Kumar</a>&hellip; - International  &hellip;, 2010 - Hindawi Publishing Corporation</div><div class="gs_fl"><a href="/scholar?cites=3748672000631884928&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:gMwdwVL1BTQJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3748672000631884928&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'gMwdwVL1BTQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB41" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW41"><a href="http://www.lxie.nwpu-aslp.org/papers/2010-ISCSLP-LuMM-A2-EI-Conf.pdf" class=yC4B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nwpu-aslp.org</span><span class="gs_ggsS">nwpu-aslp.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5684854" class=yC4A>Multi-modal feature integration for story boundary detection in broadcast news</a></h3><div class="gs_a">MM Lu, L Xie, ZH Fu, DM Jiang&hellip; - &hellip;  (ISCSLP), 2010 7th  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper investigates how to integrate multi-modal features for story boundary <br>detection in broadcast news. The detection problem is formulated as a classification task, ie, <br>classifying each candidate into boundary/non-boundary based on a set of features. We <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5738660559837859786&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:yg-u21_Ro08J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5738660559837859786&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'yg-u21_Ro08J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Anchor Shot Detection Using Colour Histogram</h3><div class="gs_a">S Al Zahrani, Y Gotoh - 2009</div><div class="gs_fl"><a href="/scholar?cites=7249240990979999480&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:-EaQ7kV6mmQJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'-EaQ7kV6mmQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB43" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW43"><a href="http://doras.dcu.ie/375/1/interactive_video_2006.pdf" class=yC4D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dcu.ie</span><span class="gs_ggsS">dcu.ie <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/u0lk3311777t2845.pdf" class=yC4C>Interactive searching and browsing of video archives: using text and using image matching</a></h3><div class="gs_a"><a href="/citations?user=o7xnW2MAAAAJ&amp;hl=en&amp;oi=sra">A Smeaton</a>, C Gurrin, H Lee - Interactive Video, 2006 - Springer</div><div class="gs_rs">Over the last number of decades much research work has been done in the general area of <br>video and audio analysis. Initially the applications driving this included capturing video in <br>digital form and then being able to store, transmit and render it, which involved a large <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6178398020957856130&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:gmEr-EMUvlUJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6178398020957856130&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'gmEr-EMUvlUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://eprints.qut.edu.au/45473/1/Johannes_Sasongko_Thesis.pdf" class=yC4F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from qut.edu.au</span><span class="gs_ggsS">qut.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://eprints.qut.edu.au/45473/" class=yC4E>Automatic generation of effective video summaries</a></h3><div class="gs_a">J Sasongko - 2011 - eprints.qut.edu.au</div><div class="gs_rs">As the popularity of video as an information medium rises, the amount of video content that <br>we produce and archive keeps growing. This creates a demand for shorter representations <br>of videos in order to assist the task of video retrieval. The traditional solution is to let <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:EEkqKPbqbTEJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'EEkqKPbqbTEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md44', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md44" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:EEkqKPbqbTEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=13292123502572703559&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2382348" class=yC50>Cross community news event summary generation based on collaborative ranking</a></h3><div class="gs_a">C Liu, W Zhang, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, Q Huang - Proceedings of the 4th International  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In order to make the users to access their interested news content conveniently, <br>news analysis has been a hot research topic for a long time. However, most of the previous <br>works only focus on news event detection, tracking, etc. Little attention has been paid to <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'dtC7PhOJyEMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB46" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW46"><a href="https://tech.ebu.ch/docs/techreview/trev_2008-Q1_ants-dimino.pdf" class=yC52><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ebu.ch</span><span class="gs_ggsS">ebu.ch <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://tech.ebu.ch/docs/techreview/trev_2008-Q1_ants-dimino.pdf" class=yC51>ANTS</a></h3><div class="gs_a">G Dimino, A Messina, R Borgotallo - tech.ebu.ch</div><div class="gs_rs">The use of automatic speech-to-text transcripts introduces several issues in the news <br>segmentation task, due to some typical errors such as missing words, word deletions and <br>insertions, and wrongly transcribed words. The current system does not use Newsroom <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:T3UqRVAddSUJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2699095782273480015&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 29 versions</a> <a onclick="return gs_ocit(event,'T3UqRVAddSUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md46', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md46" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:T3UqRVAddSUJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB47" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW47"><a href="http://www.guicarmail.it/Television/DVB%20Proj/EBU/trev_2008-Q1.pdf" class=yC54><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from guicarmail.it</span><span class="gs_ggsS">guicarmail.it <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.guicarmail.it/Television/DVB%20Proj/EBU/trev_2008-Q1.pdf" class=yC53>DTT</a></h3><div class="gs_a">A Louis, M Roger - guicarmail.it</div><div class="gs_rs">On the technical side, the broadcasting and compression standards are defined through <br>government decisions. Spectrum planning is under the responsibility of the CSA, in <br>partnership with the Agence Nationale des FrÃ©quences (ANFr), the French spectrum <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jbgfR0Bk1G0J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7914060672430749837&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 39 versions</a> <a onclick="return gs_ocit(event,'jbgfR0Bk1G0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md47', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md47" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jbgfR0Bk1G0J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/15829/final_thesis_wanggang_nus_phdx.pdf?sequence=1" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/15829" class=yC55>A multi-resolution multi-source and multi-modal (M3) transductive framework for concept detection in news video</a></h3><div class="gs_a">W Gang - 2009 - scholarbank.nus.sg</div><div class="gs_rs">We study the problem of detecting concepts in news video. Most existing algorithms for news <br>video concept detection are based on single-resolution (shot), single source (training data), <br>and multi-modal fusion methods under a supervised inductive inference framework. In this <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:O24GTu3y5YoJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10008672847931010619&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'O24GTu3y5YoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7796860&amp;id=riXWAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC57>Method and system for playing back videos at speeds adapted to content</a></h3><div class="gs_a"><a href="/citations?user=FSybdvkAAAAJ&amp;hl=en&amp;oi=sra">KA Peker</a> - US Patent 7,796,860, 2010 - Google Patents</div><div class="gs_rs">A method plays back a video at speeds adapted to content of the video. A video is <br>partitioned into summary segments and skipped segments. The summary segments are <br>played back sequentially at a normal play back speed, and the skipped segments are <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:DVTqU6VKV3MJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8311193711273464845&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'DVTqU6VKV3MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5348521" class=yC58>A novel audiovisual analysis for news video indexing</a></h3><div class="gs_a">H Yubin, D Yuan, D Chengyu&hellip; - Broadband Network &amp;  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Anchor person can provide a topic-oriented navigation for news videos. In this <br>paper we present a novel method based on multiple filtering for fusion of audiovisual <br>analysis results, by analyzing their repetition and duration. Experiments results have <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10701913048319288061&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:_WIwXjfVhJQJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'_WIwXjfVhJQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.138.7627&amp;rep=rep1&amp;type=pdf" class=yC5A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1931392" class=yC59>Pic-A-Topic: efficient viewing of informative TV contents on travel, cooking, food and more</a></h3><div class="gs_a">T Sakai, T Uehara, T Shimomori, M Koyama&hellip; - Large Scale Semantic  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Pic-A-Topic is a prototype system designed for enabling the user to view topical <br>segments of recorded TV shows selectively. By analysing closed captions and eletronic <br>program guide texts, it performs topic segmentation and topic sentence selection, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7003366408046202054&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:xkx_h6T0MGEJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7003366408046202054&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'xkx_h6T0MGEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/Q536052L8W142412.pdf" class=yC5B>Performance evaluation of media segmentation heuristics using non-Markovian multi-class arrival processes</a></h3><div class="gs_a">P Piazzolla, M Gribaudo, R Borgotallo&hellip; - Analytical and Stochastic  &hellip;, 2010 - Springer</div><div class="gs_rs">Abstract. In the current scenario of a multitude of digital audiovisual sources it is valuable to <br>set up systems capable to automatically analyze, classify and index the material for further <br>usage. In this paper we propose a technique to study the performance of a system for the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7_Z-lB1uDd0J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15928508530403768047&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'7_Z-lB1uDd0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://pdf.aminer.org/000/246/882/new_trends_in_multimedia_systems_introduction.pdf" class=yC5D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aminer.org</span><span class="gs_ggsS">aminer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://pdf.aminer.org/000/246/882/new_trends_in_multimedia_systems_introduction.pdf" class=yC5C>Emerging Trends in Multimedia Systems</a></h3><div class="gs_a">MS Kankanhalli - 2004 - pdf.aminer.org</div><div class="gs_rs">There has been a steadily growing interest in multimedia systems research over the last <br>decade. While there was initially a lot of activity in a few popular areas like video-on-<br>demand, video segmentation and content-based image retrieval, the range and depth of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:r_LOmvN-q_wJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18206785503314768559&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'r_LOmvN-q_wJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md53', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md53" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:r_LOmvN-q_wJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212007096" class=yC5E>An Adaptive Video Shot Segmentation Scheme Based on Dual-detection Model</a></h3><div class="gs_a">X Jiang, T Sun, J Liu, J Chao, W Zhang - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Abstract To efficiently segment the video shots is the important and foundational work for the <br>research of video content retrieval and analysis. A video shot segmentation scheme based <br>on a dual-detection model is proposed, which includes the pre-detection and re-detection <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'DZK-qLtyvO4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB55" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW55"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202012/pdfs/0001417.pdf" class=yC60><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6288156" class=yC5F>Multi-modal information fusion for news story segmentation in broadcast video</a></h3><div class="gs_a">B Feng, P Ding, J Chen, J Bai, S Xu&hellip; - Acoustics, Speech and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the fast development of high-speed network and digital video recording <br>technologies, broadcast video has been playing a more and more important role in our daily <br>life. In this paper, we propose a novel news story segmentation scheme which can <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:zWuGsf2GKBgJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1740789680141921229&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'zWuGsf2GKBgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/01H6050H20380470.pdf" class=yC61>On-line video abstract generation of multimedia news</a></h3><div class="gs_a">V ValdÃ©s, <a href="/citations?user=k_b4Fp4AAAAJ&amp;hl=en&amp;oi=sra">JM MartÃ­nez</a> - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract The amount of video content available nowadays makes video abstraction <br>techniques a necessary tool to ease the access to the already huge and ever growing video <br>databases. Nevertheless, many of the existing video abstraction approaches have high <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:tf59s8dAYxIJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1324973941831106229&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'tf59s8dAYxIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB57" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW57"><a href="http://www.murase.nuie.nagoya-u.ac.jp/publications/960-pdf.pdf" class=yC63><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nagoya-u.ac.jp</span><span class="gs_ggsS">nagoya-u.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://search.ieice.org/bin/summary.php?id=e95-d_5_1288" class=yC62>Efficient Tracking of News Topics Based on Chronological Semantic Structures in a Large-Scale News Video Archive</a></h3><div class="gs_a"><a href="/citations?user=8PXJm98AAAAJ&amp;hl=en&amp;oi=sra">IDE Ichiro</a>, T Kinoshita, <a href="/citations?user=ne-WQc4AAAAJ&amp;hl=en&amp;oi=sra">T Takahashi</a>&hellip; - &hellip;  on Information and  &hellip;, 2012 - search.ieice.org</div><div class="gs_rs">Recent advance in digital storage technology has enabled us to archive a large volume of <br>video data. Thanks to this trend, we have archived more than 1,800 hours of video data from <br>a daily Japanese news show in the last ten years. When considering the effective use of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16386894314651706633&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:CeWuxI7xaeMJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16386894314651706633&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'CeWuxI7xaeMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB58" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW58"><a href="http://theses.gla.ac.uk/2132/01/2010hopfgartner1phd.pdf" class=yC65><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gla.ac.uk</span><span class="gs_ggsS">gla.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://theses.gla.ac.uk/2132/01/2010hopfgartner1phd.pdf" class=yC64>Personalized Video Retrieval: Application of Implicit Feedback and Semantic User Profiles</a></h3><div class="gs_a"><a href="/citations?user=tiASHnwAAAAJ&amp;hl=en&amp;oi=sra">F Hopfgartner</a> - SIGIR Forum, 2010 - theses.gla.ac.uk</div><div class="gs_rs">Abstract A challenging problem in the user profiling domain is to create profiles of users of <br>retrieval systems. This problem even exacerbates in the multimedia domain. Due to the <br>Semantic Gap, the difference between low-level data representation of videos and the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:vXrMzuDouqsJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12374458978393684669&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'vXrMzuDouqsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md58', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md58" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:vXrMzuDouqsJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB59" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW59"><a href="http://hal.inria.fr/docs/00/71/89/85/PDF/ICME2012.pdf" class=yC67><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from inria.fr</span><span class="gs_ggsS">inria.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6298455" class=yC66>Unsupervised mining of multiple audiovisually consistent clusters for video structure analysis</a></h3><div class="gs_a">AP Ta, G Gravier - Multimedia and Expo (ICME), 2012 IEEE  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We address the problem of detecting multiple audiovisual events related to the edit <br>structure of a video by incorporating an unsupervised cluster analysis technique into a <br>cluster selection method designed to measure coherence between audio and visual <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:p1WZ4S21rXkJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8767863258223302055&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'p1WZ4S21rXkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Introduction to Video Search Engines</h3><div class="gs_a">DCGZ Liu</div><div class="gs_fl"><a href="/scholar?q=related:9DTh5mVZFkAJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'9DTh5mVZFkAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB61" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW61"><a href="http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Conference/data/4711a973.pdf" class=yC69><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6298529" class=yC68>A local temporal context-based approach for TV news story segmentation</a></h3><div class="gs_a">E Dumont, G QuÃ©not - Multimedia and Expo (ICME), 2012 IEEE &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Users are often interested in retrieving only a particular passage on a topic of <br>interest to them. It is therefore necessary to split videos into shorter segments corresponding <br>to appropriate retrieval units. We propose here a method based on a local temporal <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:0E-s6P95Te8J:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17243572688298725328&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'0E-s6P95Te8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB62" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW62"><a href="http://www.scholarbank.nus.edu.sg/bitstream/handle/10635/13136/Thesis_XU_Huaxin_HT016894E.pdf?sequence=1" class=yC6B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.scholarbank.nus.edu.sg/handle/10635/13136" class=yC6A>Integrated analysis of audiovisual signals and external information sources for event detection in team sports video</a></h3><div class="gs_a">H Xu - 2007 - scholarbank.nus.edu.sg</div><div class="gs_rs">Audiovisual signals and external information sources (news reports, live commentaries, Web <br>casts, etc.) are found to have complementary strengths for detecting events in sports video. <br>This thesis reports research on integrated analysis of them, focusing on tackling the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:M-BY-RAX2tkJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15697884812823552051&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'M-BY-RAX2tkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB63" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW63"><a href="http://hal.archives-ouvertes.fr/docs/00/63/21/19/PDF/manuscritTheseBigot2011.pdf" class=yC6D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/tel-00632119/" class=yC6C>Recherche du rÃ´le des intervenants et de leurs interactions pour la structuration de documents audiovisuels</a></h3><div class="gs_a">B Bigot - 2011 - hal.archives-ouvertes.fr</div><div class="gs_rs">2.2 Enrichissement de la structuration fondÃ©e sur des Ã©vÃ©nements audio... 111 2.3 <br>Exploitation d&#39;informations extraites de la vidÃ©o pour la structuration.. 111 2.4 Pistes de <br>recherche autour de la caractÃ©risation locale des intervenants.. 113 2.5 Investigation sur <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13044200438820409743&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=64">Cited by 1</a> <a href="/scholar?q=related:j5HiHjBPBrUJ:scholar.google.com/&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13044200438820409743&amp;hl=en&amp;num=64&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'j5HiHjBPBrUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
