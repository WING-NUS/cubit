Total results = 28
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://pensivepuffin.com/dwmcphd/syllabi/insc547_wi11/papers/overview/king-overview-socialcomp.sna-IJCNN09.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from pensivepuffin.com</span><span class="gs_ggsS">pensivepuffin.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5178967" class=yC0>A brief survey of computational approaches in social computing</a></h3><div class="gs_a"><a href="/citations?user=MXvC7tkAAAAJ&amp;hl=en&amp;oi=sra">I King</a>, J Li, KT Chan - Neural Networks, 2009. IJCNN 2009.  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Web 2.0 technologies have brought new ways of connecting people in social <br>networks for collaboration in various on-line communities. Social Computing is a novel and <br>emerging computing paradigm that involves a multi-disciplinary approach in analyzing <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10039176603577056795&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 22</a> <a href="/scholar?q=related:G36fXe5RUosJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10039176603577056795&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'G36fXe5RUosJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a514582.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dtic.mil</span><span class="gs_ggsS">dtic.mil <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA514582" class=yC2>A comparison of query-by-example methods for spoken term detection</a></h3><div class="gs_a">W Shen, CM White, <a href="/citations?user=t1UaPDgAAAAJ&amp;hl=en&amp;oi=sra">TJ Hazen</a> - 2009 - DTIC Document</div><div class="gs_rs">Abstract: In this paper we examine an alternative interface for phonetic search, namely query-<br>by-example, that avoids OOV issues associated with both standard word-based and <br>phonetic search methods. We develop three methods that compare query lattices derived <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14913548011405867007&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 17</a> <a href="/scholar?q=related:_3eUqOaQ984J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14913548011405867007&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'_3eUqOaQ984J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:_3eUqOaQ984J:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=13134895811387309825&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://berlin.csie.ntnu.edu.tw/Berlin_Research/Talks/20090504-ntust-WTM.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntnu.edu.tw</span><span class="gs_ggsS">ntnu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4960495" class=yC4>Latent topic modelling of word co-occurence information for spoken document retrieval</a></h3><div class="gs_a"><a href="/citations?user=-2c31OsAAAAJ&amp;hl=en&amp;oi=sra">B Chen</a> - Acoustics, Speech and Signal Processing, 2009.  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present a word topic model (WTM) approach, discovering the co-<br>occurrence relationship between words as well as the long-span latent topic information, for <br>spoken document retrieval (SDR). A given document as a whole is modeled as a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11368123571092247154&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 16</a> <a href="/scholar?q=related:ctKyMSiww50J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11368123571092247154&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'ctKyMSiww50J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://old-site.clsp.jhu.edu/~carolinap/papers/qbye_oovs_asru_09.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jhu.edu</span><span class="gs_ggsS">jhu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5373341" class=yC6>Query-by-example spoken term detection for OOV terms</a></h3><div class="gs_a">C Parada, A Sethy&hellip; - &hellip;  Speech Recognition &amp;  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The goal of spoken term detection (STD) technology is to allow open vocabulary <br>search over large collections of speech content. In this paper, we address cases where <br>search term (s) of interest (queries) are acoustic examples. This is provided either by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10596553453171710731&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 16</a> <a href="/scholar?q=related:Cxu9AzmFDpMJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10596553453171710731&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'Cxu9AzmFDpMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://berlin.csie.ntnu.edu.tw/Berlin_Research/manuscripts/2009/p15635.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntnu.edu.tw</span><span class="gs_ggsS">ntnu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Improved speech summarization with multiple-hypothesis representations and Kullback-Leibler divergence measures</h3><div class="gs_a">SH Lin, <a href="/citations?user=-2c31OsAAAAJ&amp;hl=en&amp;oi=sra">B Chen</a> - Proceedings of Interspeech, 2009</div><div class="gs_fl"><a href="/scholar?cites=3352264964649888008&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 16</a> <a href="/scholar?q=related:CKnxajmjhS4J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3352264964649888008&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'CKnxajmjhS4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://doc.utwente.nl/71235/1/story-event-searching.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utwente.nl</span><span class="gs_ggsS">utwente.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/b216521113704351.pdf" class=yC9>Beyond shot retrieval: Searching for broadcast news items using language models of concepts</a></h3><div class="gs_a"><a href="/citations?user=wL5km8kAAAAJ&amp;hl=en&amp;oi=sra">R Aly</a>, <a href="/citations?user=IZN89qQAAAAJ&amp;hl=en&amp;oi=sra">A Doherty</a>, <a href="/citations?user=SN0MvYwAAAAJ&amp;hl=en&amp;oi=sra">D Hiemstra</a>, <a href="/citations?user=o7xnW2MAAAAJ&amp;hl=en&amp;oi=sra">A Smeaton</a> - Advances in Information  &hellip;, 2010 - Springer</div><div class="gs_rs">Abstract. Current video search systems commonly return video shots as results. We believe <br>that users may better relate to longer, semantic video units and propose a retrieval <br>framework for news story items, which consist of multiple shots. The framework is divided <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3014787615808285075&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 11</a> <a href="/scholar?q=related:k-l_EV2t1ikJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3014787615808285075&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 18 versions</a> <a onclick="return gs_ocit(event,'k-l_EV2t1ikJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.aclweb.org/anthology-new/N/N10/N10-1006.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aclweb.org</span><span class="gs_ggsS">aclweb.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1858005" class=yCB>Using confusion networks for speech summarization</a></h3><div class="gs_a"><a href="/citations?user=jl89ugsAAAAJ&amp;hl=en&amp;oi=sra">S Xie</a>, Y Liu - Human Language Technologies: The 2010 Annual  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract For extractive meeting summarization, previous studies have shown performance <br>degradation when using speech recognition transcripts because of the relatively high <br>speech recognition errors on meeting recordings. In this paper we investigated using <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1218446269255612540&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 10</a> <a href="/scholar?q=related:fHjhIG3K6BAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1218446269255612540&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'fHjhIG3K6BAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1871568" class=yCD>Faceted search and browsing of audio content on spoken web</a></h3><div class="gs_a">M Diao, S Mukherjea, <a href="/citations?user=2f4HU1gAAAAJ&amp;hl=en&amp;oi=sra">N Rajput</a>&hellip; - Proceedings of the 19th  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Spoken Web is a web of VoiceSites that can be accessed by a phone. The content <br>in a VoiceSite is audio. Therefore Spoken Web provides an alternate to the World Wide Web <br>(WWW) in developing regions where low Internet penetration and low literacy are barriers <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11616339876883571543&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 11</a> <a href="/scholar?q=related:V6PZb5KHNaEJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'V6PZb5KHNaEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://140.122.185.120/Berlin_Research/Manuscripts/2011-/2011-IEEEASLP-Leveraging%20Kullback%E2%80%93Leibler%20Divergence%20Measures%20and%20Information-Rich%20Cues%20for%20Speech%20Summarization%20.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 140.122.185.120</span><span class="gs_ggsS">140.122.185.120 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5549862" class=yCE>Leveraging KullbackâLeibler Divergence Measures and Information-Rich Cues for Speech Summarization</a></h3><div class="gs_a">SH Lin, YM Yeh, <a href="/citations?user=-2c31OsAAAAJ&amp;hl=en&amp;oi=sra">B Chen</a> - Audio, Speech, and Language  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Imperfect speech recognition often leads to degraded performance when exploiting <br>conventional text-based methods for speech summarization. To alleviate this problem, this <br>paper investigates various ways to robustly represent the recognition hypotheses of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16653215220472787109&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 6</a> <a href="/scholar?q=related:pbBosAIbHOcJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16653215220472787109&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'pbBosAIbHOcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://www.speech.sri.com/people/wwang/papers/slt2008-clssr.pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sri.com</span><span class="gs_ggsS">sri.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4777895" class=yC10>Phonetic name matching for cross-lingual spoken sentence retrieval</a></h3><div class="gs_a"><a href="/citations?user=z7GCqT4AAAAJ&amp;hl=en&amp;oi=sra">H Ji</a>, <a href="/citations?user=blwKAkUAAAAJ&amp;hl=en&amp;oi=sra">R Grishman</a>, W Wang - Spoken Language Technology  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Cross-lingual spoken sentence retrieval (CLSSR) remains a challenge, especially <br>for queries including OOV words such as person names. This paper proposes a simple <br>method of fuzzy matching between query names and phones of candidate audio <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13885566217006223362&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 6</a> <a href="/scholar?q=related:AhDSzedws8AJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13885566217006223362&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'AhDSzedws8AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://doc.utwente.nl/72019/1/thesis_R_Aly.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utwente.nl</span><span class="gs_ggsS">utwente.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://doc.utwente.nl/72019/1/thesis_R_Aly.pdf" class=yC12>Modeling representation uncertainty in concept-based multimedia retrieval</a></h3><div class="gs_a"><a href="/citations?user=wL5km8kAAAAJ&amp;hl=en&amp;oi=sra">RBN Aly</a> - 2010 - doc.utwente.nl</div><div class="gs_rs">This thesis considers concept-based multimedia retrieval, where documents are <br>represented by the occurrence of concepts (also referred to as semantic concepts or high-<br>level features). A concept can be thought of as a kind of label, which is attached to (parts <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15711303683337853314&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 4</a> <a href="/scholar?q=related:gpmjO3XDCdoJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15711303683337853314&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'gpmjO3XDCdoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:gpmjO3XDCdoJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=1317474973774950399&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://cl.naist.jp/~junta-m/slt2008_thesis.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from naist.jp</span><span class="gs_ggsS">naist.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4777899" class=yC14>A similar content retrieval method for podcast episodes</a></h3><div class="gs_a">J Mizuno, J Ogata, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Spoken Language Technology  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Given podcasts (audio blogs) that are sets of speech files called episodes, this <br>paper describes a method for retrieving episodes that have similar content. Although most <br>previous retrieval methods were based on bibliographic information, tags, or users&#39; <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9046081681620002579&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 4</a> <a href="/scholar?q=related:E-cgelwjin0J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9046081681620002579&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'E-cgelwjin0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5983479" class=yC16>Performance analysis and improvement of Turkish broadcast news retrieval</a></h3><div class="gs_a"><a href="/citations?user=GvYk2Z8AAAAJ&amp;hl=en&amp;oi=sra">S Parlak</a>, <a href="/citations?user=H04j-VEAAAAJ&amp;hl=en&amp;oi=sra">M Saraclar</a> - Audio, Speech, and Language  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents our work on the retrieval of spoken information in Turkish. <br>Traditional speech retrieval systems perform indexing and retrieval over automatic speech <br>recognition (ASR) transcripts, which include errors either because of out-of-vocabulary (<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=914946196474523781&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 3</a> <a href="/scholar?q=related:hXB11beKsgwJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=914946196474523781&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'hXB11beKsgwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://wwwconference.org/proceedings/www2011/companion/p503.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from wwwconference.org</span><span class="gs_ggsS">wwwconference.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1963364" class=yC17>Two-stream indexing for spoken web search</a></h3><div class="gs_a">J Ajmera, <a href="/citations?user=sJ7wlksAAAAJ&amp;hl=en&amp;oi=sra">A Joshi</a>, S Mukherjea, <a href="/citations?user=2f4HU1gAAAAJ&amp;hl=en&amp;oi=sra">N Rajput</a>&hellip; - Proceedings of the 20th &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract This paper presents two-stream processing of audio to index the audio content for <br>Spoken Web search. The first stream indexes the meta-data associated with a particular <br>audio document. The meta-data is usually very sparse, but accurate. This therefore results <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1389579601600964530&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 3</a> <a href="/scholar?q=related:sueNDEnHSBMJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1389579601600964530&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'sueNDEnHSBMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://kusu.comp.nus.edu/proceedings/mm09/sscs/p3.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631129" class=yC19>Topic modeling for spoken document retrieval using word-and syllable-level information</a></h3><div class="gs_a">SH Lin, <a href="/citations?user=-2c31OsAAAAJ&amp;hl=en&amp;oi=sra">B Chen</a> - Proceedings of the third workshop on Searching  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Topic modeling for information retrieval (IR) has attracted significant attention and <br>demonstrated good performance in a wide variety of tasks over the years. In this article, we <br>first present a comprehensive comparison among various topic modeling approaches, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7264211174683058199&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 3</a> <a href="/scholar?q=related:FyBocJOpz2QJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7264211174683058199&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'FyBocJOpz2QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2328971" class=yC1B>Comparison of methods for language-dependent and language-independent Query-by-Example spoken term detection</a></h3><div class="gs_a">J Tejedor, M FapÅ¡o, I SzÃ¶ke, J ÄernockÃ½&hellip; - ACM Transactions on  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract This article investigates query-by-example (QbE) spoken term detection (STD), in <br>which the query is not entered as text, but selected in speech data or spoken. Two feature <br>extractors based on neural networks (NN) are introduced: the first producing phone-state <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9406020948282484007&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 2</a> <a href="/scholar?q=related:J6VCnDfmiIIJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'J6VCnDfmiIIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.hlt.utdallas.edu/~shasha/dissertation/dissertation.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utdallas.edu</span><span class="gs_ggsS">utdallas.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.hlt.utdallas.edu/~shasha/dissertation/dissertation.pdf" class=yC1C>Automatic extractive summarization on meeting corpus</a></h3><div class="gs_a"><a href="/citations?user=jl89ugsAAAAJ&amp;hl=en&amp;oi=sra">S Xie</a>, Y Liu, <a href="/citations?user=hfADwdIAAAAJ&amp;hl=en&amp;oi=sra">JHL Hansen</a>, S Harabagiu, V Ng - 2010 - hlt.utdallas.edu</div><div class="gs_rs">This dissertation (or thesis) was produced in accordance with guidelines which permit the <br>inclusion as part of the dissertation (or thesis) the text of an original paper or papers <br>submitted for publication. The dissertation (or thesis) must still conform to all other <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12361683304118837954&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 1</a> <a href="/scholar?q=related:wmLhV3iFjasJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12361683304118837954&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'wmLhV3iFjasJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md16', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md16" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:wmLhV3iFjasJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:wmLhV3iFjasJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=13399309807659920762&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://eprints2008.lib.hokudai.ac.jp/dspace/bitstream/2115/39702/1/TA-SS1-1.pdf" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hokudai.ac.jp</span><span class="gs_ggsS">hokudai.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://eprints2008.lib.hokudai.ac.jp/dspace/handle/2115/39702" class=yC1E>Query-by-Example Spoken Document Retrieval: The Star Challenge 2008</a></h3><div class="gs_a"><a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, <a href="/citations?user=jnU62sUAAAAJ&amp;hl=en&amp;oi=sra">KC Sim</a>, V Singh, KM Lye - &hellip; : APSIPA ASC 2009 &hellip;, 2009 - eprints2008.lib.hokudai.ac.jp</div><div class="gs_rs">In this paper, we give an update of recent research activities in HLT department of I2R in <br>query-by-example spoken document retrieval (SDR) and report an evaluation campaign, the <br>Star Challenge 2008, which was organized by A* STAR, Singapore. It is suggested that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16360638948118022001&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 1</a> <a href="/scholar?q=related:ccf-4HCqDOMJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16360638948118022001&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ccf-4HCqDOMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://speech.ee.ntu.edu.tw/~RA/lab/html/thesis/Voice-based%20Information%20Retrieval_981016.pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5372952" class=yC20>Voice-based information retrievalâhow far are we from the text-based information retrieval?</a></h3><div class="gs_a">L Lee, Y Pan - &hellip;  &amp; Understanding, 2009. ASRU 2009. IEEE  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Although network content access is primarily text-based today, almost all roles of <br>text can be accomplished by voice. Voice-based information retrieval refers to the situation <br>that the user query and/or the content to be retried are in form of voice. This paper tries to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9906306604307563029&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 2</a> <a href="/scholar?q=related:FcLb6V5FeokJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9906306604307563029&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'FcLb6V5FeokJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://filebox.vt.edu/users/skhater/CS5604/hw01.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from vt.edu</span><span class="gs_ggsS">vt.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://filebox.vt.edu/users/skhater/CS5604/hw01.pdf" class=yC22>HW01 for CS5604, Information Storage and Retrieval, Spring 2012</a></h3><div class="gs_a">S Khater - Information Storage and Retrieval, 2012 - filebox.vt.edu</div><div class="gs_rs">B. Summary This paper discusses the issue of spoken document retrieval. The paper first described <br>two different ways of document re- trieval. One way is to run 1-best automatic speech recognition <br>(ASR) transcripts of spoken documents for retrieval. However, it was found that 1-best <b> ...</b> </div><div class="gs_fl"><a href="/scholar?q=related:2sU4GIsSAwYJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'2sU4GIsSAwYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:2sU4GIsSAwYJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://www.mpkato.net/wp-content/uploads/2012/08/fp070-kato.pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpkato.net</span><span class="gs_ggsS">mpkato.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2348392" class=yC24>Content-based retrieval for heterogeneous domains: domain adaptation by relative aggregation points</a></h3><div class="gs_a"><a href="/citations?user=Gr2Q2dQAAAAJ&amp;hl=en&amp;oi=sra">MP Kato</a>, H Ohshima, K Tanaka - &hellip;  of the 35th international ACM SIGIR  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract We introduce the problem of domain adaptation for content-based retrieval and <br>propose a domain adaptation method based on relative aggregation points (RAPs). Content-<br>based retrieval including image retrieval and spoken document retrieval enables a user to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Udc9IqNUe2cJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7456646667672868689&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'Udc9IqNUe2cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2063840" class=yC26>Social ranking for spoken web search</a></h3><div class="gs_a">S Sahay, <a href="/citations?user=2f4HU1gAAAAJ&amp;hl=en&amp;oi=sra">N Rajput</a>, N Pansare - Proceedings of the 20th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Spoken Web is an alternative Web for low-literacy users in the developing world. <br>People can create audio content over phone and share on the Spoken Web. This enables <br>easy creation of locally relevant content. Even on the World Wide Web in developed <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17277018544863538394&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 1</a> <a href="/scholar?q=related:2mxtONRMxO8J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'2mxtONRMxO8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://doras.dcu.ie/17144/1/urr-framework.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dcu.ie</span><span class="gs_ggsS">dcu.ie <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/301257252827313q.pdf" class=yC27>The uncertain representation ranking framework for concept-based video retrieval</a></h3><div class="gs_a"><a href="/citations?user=wL5km8kAAAAJ&amp;hl=en&amp;oi=sra">R Aly</a>, <a href="/citations?user=IZN89qQAAAAJ&amp;hl=en&amp;oi=sra">A Doherty</a>, <a href="/citations?user=SN0MvYwAAAAJ&amp;hl=en&amp;oi=sra">D Hiemstra</a>, F de Jong&hellip; - Information Retrieval, 2012 - Springer</div><div class="gs_rs">Abstract Concept based video retrieval often relies on imperfect and uncertain concept <br>detectors. We propose a general ranking framework to define effective and robust ranking <br>functions, through explicitly addressing detector uncertainty. It can cope with multiple <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:he5JOv1q-eAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16211105969534004869&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'he5JOv1q-eAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://140.109.19.106/clclp/v17n1/v17n1a4.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 140.109.19.106</span><span class="gs_ggsS">140.109.19.106 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://140.109.19.106/clclp/v17n1/v17n1a4.pdf" class=yC29>A Comparative Study of Methods for Topic Modeling in Spoken Document Retrieval</a></h3><div class="gs_a">SH Lin, <a href="/citations?user=-2c31OsAAAAJ&amp;hl=en&amp;oi=sra">B Chen</a> - ä¸­æè¨ç®èªè¨å­¸æå, 2012 - 140.109.19.106</div><div class="gs_rs">Abstract Topic modeling for information retrieval (IR) has attracted significant attention and <br>demonstrated good performance in a wide variety of tasks over the years. In this paper, we <br>first present a comprehensive comparison of various topic modeling approaches, <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'bhOyT1IbowgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md23', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md23" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:bhOyT1IbowgJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://cs.iupui.edu/~alhasan/papers/fp006-bhuiyan.pdf" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iupui.edu</span><span class="gs_ggsS">iupui.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cs.iupui.edu/~alhasan/papers/fp006-bhuiyan.pdf" class=yC2B>Interactive Pattern Mining on Hidden Data: A Sampling-based Solution</a></h3><div class="gs_a">M Bhuiyan, S Mukhopadhyay, <a href="/citations?user=fsCnri8AAAAJ&amp;hl=en&amp;oi=sra">M Al Hasan</a> - 2012 - cs.iupui.edu</div><div class="gs_rs">ABSTRACT Mining frequent patterns from a hidden dataset is an important task with various <br>real-life applications. In this research, we propose a solution to this problem that is based on <br>Markov Chain Monte Carlo (MCMC) sampling of frequent patterns. Instead of returning all <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:sft0XDGicwsJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=825181489627200433&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'sft0XDGicwsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md24', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md24" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:sft0XDGicwsJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://www.searchingspeech.org/sscs2008/sscs08_proceedings.pdf#page=6" class=yC2E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from searchingspeech.org</span><span class="gs_ggsS">searchingspeech.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.searchingspeech.org/sscs2008/sscs08_proceedings.pdf#page=6" class=yC2D>Query-by-Example Spoken Document Retrieval</a></h3><div class="gs_a"><a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - CIP GEGEVENS KONINKLIJKE BIBLIOTHEEK, DEN  &hellip;, 2008 - searchingspeech.org</div><div class="gs_rs">Query-by-Example Spoken Document Retrieval Haizhou Li Institute for Infocomm Research <br>(I2R) Agency for Science, Technology and Research (A* STAR), Singapore. hli@ i2r. a-star. <br>edu. sg 1. INTRODUCTION In this presentation, we gave an overview of ongoing <b> ...</b> </div><div class="gs_fl"><a href="/scholar?q=related:kfYvZLN7uOgJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16769289222924269201&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'kfYvZLN7uOgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md25', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md25" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:kfYvZLN7uOgJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://www.ll.mit.edu/mission/communications/publications/publication-files/book_chapter/2011_05_03_Hazen_Speech_Retrieval_FP.pdf" class=yC30><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mit.edu</span><span class="gs_ggsS">mit.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ll.mit.edu/mission/communications/publications/publication-files/book_chapter/2011_05_03_Hazen_Speech_Retrieval_FP.pdf" class=yC2F>Speech Retrieval</a></h3><div class="gs_a"><a href="/citations?user=Rtg5ZY8AAAAJ&amp;hl=en&amp;oi=sra">C Chelba</a>, <a href="/citations?user=t1UaPDgAAAAJ&amp;hl=en&amp;oi=sra">TJ Hazen</a>, B Ramabhadran&hellip; - &hellip; : Systems for Extracting  &hellip;, 2011 - ll.mit.edu</div><div class="gs_rs">Abstract In this chapter we discuss the retrieval and browsing of spoken audio documents. <br>We focus primarily on the application of document search where a user provides a query <br>and the system returns a set of audio documents that best match the query. The primary <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:2CdPl8Mgm0oJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5375926604626077656&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'2CdPl8Mgm0oJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:2CdPl8Mgm0oJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5593872" class=yC31>Using N-Best Lists and Confusion Networks for Meeting Summarization</a></h3><div class="gs_a"><a href="/citations?user=jl89ugsAAAAJ&amp;hl=en&amp;oi=sra">S Xie</a>, Y Liu - Audio, Speech, and Language Processing, IEEE  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The incorrect speech recognition results usually have a negative impact on the <br>speech summarization task, especially on the meeting domain where the word error rate is <br>often higher than other speech genres. In this paper we investigate using rich speech <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:fqPw2q5RoDAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3503890321533608830&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'fqPw2q5RoDAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
