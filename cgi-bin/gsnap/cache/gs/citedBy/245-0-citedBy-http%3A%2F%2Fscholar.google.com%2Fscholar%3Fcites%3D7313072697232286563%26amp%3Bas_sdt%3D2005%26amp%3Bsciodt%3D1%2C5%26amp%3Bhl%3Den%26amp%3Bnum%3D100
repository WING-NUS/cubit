Total results = 245
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm09-jinhui.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631305" class=yC0>Inferring semantic concepts from community-contributed images and noisy tags</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=Nut-uvoAAAAJ&amp;hl=en&amp;oi=sra">GJ Qi</a>, TS Chua - Proceedings of the 17th ACM  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we exploit the problem of inferring images&#39; semantic concepts from <br>community-contributed images and their associated noisy tags. To infer the concepts more <br>accurately, we propose a novel sparse graph-based semi-supervised learning approach <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=406595127280536812&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 67</a> <a href="/scholar?q=related:7Az2MB2EpAUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=406595127280536812&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'7Az2MB2EpAUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.decom.ufop.br/fabricio/download/tomccap09.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ufop.br</span><span class="gs_ggsS">ufop.br <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1596994" class=yC2>Video interactions in online video social networks</a></h3><div class="gs_a"><a href="/citations?user=iOnt0iMAAAAJ&amp;hl=en&amp;oi=sra">F Benevenuto</a>, <a href="/citations?user=1Ee6KSoAAAAJ&amp;hl=en&amp;oi=sra">T Rodrigues</a>, <a href="/citations?user=Wbi6RfAAAAAJ&amp;hl=en&amp;oi=sra">V Almeida</a>&hellip; - ACM Transactions on  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract This article characterizes video-based interactions that emerge from YouTube&#39;s <br>video response feature, which allows users to discuss themes and to provide reviews for <br>products or places using much richer media than text. Based on crawled data covering a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11858314114017739572&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 42</a> <a href="/scholar?q=related:NEMH0dsxkaQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11858314114017739572&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'NEMH0dsxkaQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://yima.csl.illinois.edu/psfile/ACM-MM10.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from illinois.edu</span><span class="gs_ggsS">illinois.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874028" class=yC4>Image tag refinement towards low-rank, content-tag prior and error sparsity</a></h3><div class="gs_a">G Zhu, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, <a href="/citations?user=XqLiBQMAAAAJ&amp;hl=en&amp;oi=sra">Y Ma</a> - Proceedings of the international conference on  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract The vast user-provided image tags on the popular photo sharing websites may <br>greatly facilitate image retrieval and management. However, these tags are often imprecise <br>and/or incomplete, resulting in unsatisfactory performances in tag related applications. In <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6026626523088077998&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 45</a> <a href="/scholar?q=related:rsScbuTgolMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6026626523088077998&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'rsScbuTgolMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www.cs.clemson.edu/~jzwang/ustc11/mm2009/p115-liu.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631291" class=yC6>Label to region by bi-layer sparsity priors</a></h3><div class="gs_a">X Liu, B Cheng, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, TS Chua&hellip; - Proceedings of the 17th  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract In this work, we investigate how to automatically reassign the manually annotated <br>labels at the image-level to those contextually derived semantic regions. First, we propose a <br>bi-layer sparse coding formulation for uncovering how an image or semantic region can <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10412793761950922351&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 40</a> <a href="/scholar?q=related:b_Z81MGsgZAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10412793761950922351&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'b_Z81MGsgZAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www.haoli.me/uploads/9/0/6/7/9067538/_2009_icdmw_msra-mm_2.0-a_large-scale_web_multimedia_dataset.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from haoli.me</span><span class="gs_ggsS">haoli.me <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5360509" class=yC8>MSRA-MM 2.0: A large-scale web multimedia dataset</a></h3><div class="gs_a">H Li, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a> - Data Mining Workshops, 2009. ICDMW&#39; &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we introduce the second version of Microsoft Research Asia <br>Multimedia (MSRA-MM), a dataset that aims to facilitate research in multimedia information <br>retrieval and related areas. The images and videos in the dataset are collected from a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8049577185041825639&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 30</a> <a href="/scholar?q=related:Z7-BIs3XtW8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8049577185041825639&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'Z7-BIs3XtW8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://mcg.ict.ac.cn/mcg-webv.files/downloads/MCG-WEBV%20paper%20for%20release.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mcg.ict.ac.cn/mcg-webv.files/downloads/MCG-WEBV%20paper%20for%20release.pdf" class=yCA>MCG-WEBV: A benchmark dataset for web video analysis</a></h3><div class="gs_a">J Cao, YD Zhang, YC Song, ZN Chen&hellip; - &hellip;  Technol., Tech. Rep.  &hellip;, 2009 - mcg.ict.ac.cn</div><div class="gs_rs">ABSTRACT This report introduces a WEB Video (MCG-WEBV) benchmark dataset including <br>80,031 1 most viewed videos for every month from Dec. 2008 to Feb. 2009 on YouTube. <br>These videos are most valuable to do mining for their high quality and popular contents. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7554850718492964437&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 33</a> <a href="/scholar?q=related:VbbgfrM42GgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7554850718492964437&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'VbbgfrM42GgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md5', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md5" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:VbbgfrM42GgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www-connex.lip6.fr/~gallinar/gallinari/uploads/Teaching/MM10-wu-Mult-Label-Boosting.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lip6.fr</span><span class="gs_ggsS">lip6.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1873957" class=yCC>Multi-label boosting for image annotation by structural grouping sparsity</a></h3><div class="gs_a">F Wu, <a href="/citations?user=t4283loAAAAJ&amp;hl=en&amp;oi=sra">Y Han</a>, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, Y Zhuang - Proceedings of the international  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract We can obtain high-dimensional heterogenous features from real-world images to <br>describe their various aspects of visual characteristics, such as color, texture and shape etc. <br>Different kinds of heterogenous features have different intrinsic discriminative power for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5598484222619586028&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 34</a> <a href="/scholar?q=related:7I3eCb3PsU0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5598484222619586028&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'7I3eCb3PsU0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.ntu.edu.sg/home/dongxu/TPAMI-Retrieval.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5989829" class=yCE>A multimedia retrieval framework based on semi-supervised ranking and relevance feedback</a></h3><div class="gs_a"><a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, <a href="/citations?user=2oB4nAIAAAAJ&amp;hl=en&amp;oi=sra">F Nie</a>, D Xu, <a href="/citations?user=CcbnBvgAAAAJ&amp;hl=en&amp;oi=sra">J Luo</a>, Y Zhuang&hellip; - Pattern Analysis and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present a new framework for multimedia content analysis and retrieval which <br>consists of two independent algorithms. First, we propose a new semi-supervised algorithm <br>called ranking with Local Regression and Global Alignment (LRGA) to learn a robust <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5223703788302730835&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 34</a> <a href="/scholar?q=related:U4alSetSfkgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5223703788302730835&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'U4alSetSfkgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://staff.science.uva.nl/~cgmsnoek/pub/li-multifeature-civr10.pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816044" class=yC10>Unsupervised multi-feature tag relevance learning for social image retrieval</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - &hellip;  of the ACM International Conference on  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Interpreting the relevance of a user-contributed tag with respect to the visual content <br>of an image is an emerging problem in social image retrieval. In the literature this problem is <br>tackled by analyzing the correlation between tags and images represented by specific <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1995228259615344882&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 25</a> <a href="/scholar?q=related:8ogx7Id5sBsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1995228259615344882&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'8ogx7Id5sBsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://kusu.comp.nus.edu/proceedings/mm09/wsmc/p1.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631137" class=yC12>Visual tag dictionary: interpreting tags with visual words</a></h3><div class="gs_a"><a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=g2gAY_0AAAAJ&amp;hl=en&amp;oi=sra">K Yang</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, HJ Zhang - &hellip;  of the 1st workshop on Web- &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Visual-word based image representation has shown effectiveness in a wide variety <br>of applications such as categorization, annotation and search. By detecting keypoints in <br>images and treating their patterns as visual words, an image can be represented as a bag <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13684631048843383814&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 22</a> <a href="/scholar?q=related:BlSXRG-T6b0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13684631048843383814&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'BlSXRG-T6b0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.ntu.edu.sg/home/dongxu/ACM%20MM%202009.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/authorize?128577" class=yC14>Using large-scale web data to facilitate textual query based retrieval of consumer photos</a></h3><div class="gs_a">Y Liu, D Xu, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">IW Tsang</a>, <a href="/citations?user=CcbnBvgAAAAJ&amp;hl=en&amp;oi=sra">J Luo</a> - Proceedings of the 17th ACM international &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract The rapid popularization of digital cameras and mobile phone cameras has lead to <br>an explosive growth of consumer photo collections. In this paper, we present a (quasi) real-<br>time textual query based personal photo retrieval system by leveraging millions of web <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15292246957967860512&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 23</a> <a href="/scholar?q=related:IOM4I5b5ONQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15292246957967860512&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'IOM4I5b5ONQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://kusu.comp.nus.edu/proceedings/mm09/wsm/p19.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631150" class=yC16>Image tag clarity: in search of visual-representative tags for social images</a></h3><div class="gs_a"><a href="/citations?user=wyKGVKUAAAAJ&amp;hl=en&amp;oi=sra">A Sun</a>, <a href="/citations?user=o0F3sqEAAAAJ&amp;hl=en&amp;oi=sra">SS Bhowmick</a> - Proceedings of the first SIGMM workshop on  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Tags associated with images in various social media sharing web sites are <br>valuable information source for superior image retrieval experiences. Due to the nature of <br>tagging, many tags associated with images are not visually descriptive. In this paper, we <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3395905500379337183&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 21</a> <a href="/scholar?q=related:39HncA-uIC8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3395905500379337183&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'39HncA-uIC8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://www.cs.clemson.edu/~jzwang/1201863/mm2010/p25-liu.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/ft_gateway.cfm?id=1873958&amp;type=pdf" class=yC18>Unified tag analysis with multi-edge graph</a></h3><div class="gs_a"><a href="/citations?user=NvU30c4AAAAJ&amp;hl=en&amp;oi=sra">D Liu</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, <a href="/citations?user=uOJH_AEAAAAJ&amp;hl=en&amp;oi=sra">Y Rui</a>, HJ Zhang - vertex, 2010 - dl.acm.org</div><div class="gs_rs">ABSTRACT Image tags have become a key intermediate vehicle to organize, index and <br>search the massive online image repositories. Extensive research has been conducted on <br>different yet related tag analysis tasks, eg, tag refinement, tag-to-region assignment, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15209635009575387589&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 20</a> <a href="/scholar?q=related:xQEWBXV6E9MJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15209635009575387589&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'xQEWBXV6E9MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.7743&amp;rep=rep1&amp;type=pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5551148" class=yC1A>Textual query of personal photos facilitated by large-scale web data</a></h3><div class="gs_a">Y Liu, D Xu, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">IWH Tsang</a>, <a href="/citations?user=CcbnBvgAAAAJ&amp;hl=en&amp;oi=sra">J Luo</a> - Pattern Analysis and Machine  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The rapid popularization of digital cameras and mobile phone cameras has led to <br>an explosive growth of personal photo collections by consumers. In this paper, we present a <br>real-time textual query-based personal photo retrieval system by leveraging millions of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10441465032319460318&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 18</a> <a href="/scholar?q=related:3mMQKiCJ55AJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10441465032319460318&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'3mMQKiCJ55AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://www.klmp.pku.edu.cn/Paper/UsrFile/525.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from pku.edu.cn</span><span class="gs_ggsS">pku.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5740601" class=yC1C>DAML: Domain adaptation metric learning</a></h3><div class="gs_a"><a href="/citations?user=gsTrHoMAAAAJ&amp;hl=en&amp;oi=sra">B Geng</a>, D Tao, C Xu - Image Processing, IEEE Transactions  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The state-of-the-art metric-learning algorithms cannot perform well for domain <br>adaptation settings, such as cross-domain face recognition, image annotation, etc., because <br>labeled data in the source domain and unlabeled ones in the target domain are drawn <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15979701902890586817&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 21</a> <a href="/scholar?q=related:wVJtcjdOw90J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15979701902890586817&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'wVJtcjdOw90J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://hub.hku.hk/bitstream/10722/127357/1/Content.pdf?accept=1" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hku.hk</span><span class="gs_ggsS">hku.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5308445" class=yC1E>Evolutionary cross-domain discriminative hessian eigenmaps</a></h3><div class="gs_a"><a href="/citations?user=eAJfUeIAAAAJ&amp;hl=en&amp;oi=sra">S Si</a>, D Tao, KP Chan - Image Processing, IEEE Transactions  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Is it possible to train a learning model to separate tigers from elks when we have 1) <br>labeled samples of leopard and zebra and 2) unlabelled samples of tiger and elk at hand? <br>Cross-domain learning algorithms can be used to solve the above problem. However, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11099538453802486519&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 17</a> <a href="/scholar?q=related:94qvdWp7CZoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11099538453802486519&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'94qvdWp7CZoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://oz.berkeley.edu/users/binyu/ps/Conferencepapers/HanWJZY10.pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from berkeley.edu</span><span class="gs_ggsS">berkeley.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/download/1903/2021" class=yC20>Multi-task sparse discriminant analysis (MtSDA) with overlapping categories</a></h3><div class="gs_a"><a href="/citations?user=t4283loAAAAJ&amp;hl=en&amp;oi=sra">Y Han</a>, F Wu, J Jia, Y Zhuang, B Yu - &hellip;  of the AAAI Conference on Artificial  &hellip;, 2010 - aaai.org</div><div class="gs_rs">Abstract Multi-task learning aims at combining information across tasks to boost prediction <br>performance, especially when the number of training samples is small and the number of <br>predictors is very large. In this paper, we first extend the Sparse Discriminate Analysis (<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10348024582837540319&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 15</a> <a href="/scholar?q=related:35UKK4iRm48J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10348024582837540319&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'35UKK4iRm48J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm09-richang.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631154" class=yC22>Event driven summarization for web videos</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, HK Tan, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">C Ngo</a>&hellip; - Proceedings of the first  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract The explosive growth of web videos brings out the challenge of how to efficiently <br>browse hundreds or even thousands of videos at a glance. Given an event-driven query, <br>social media web sites can easily return a ranked list of large but diverse and somewhat <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8154543978622150517&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 15</a> <a href="/scholar?q=related:decSP4vCKnEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8154543978622150517&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'decSP4vCKnEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://sites.google.com/site/guojunq/Home/fullpub/mm10687-qi.pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631307" class=yC24>Learning semantic distance from community-tagged media collection</a></h3><div class="gs_a"><a href="/citations?user=Nut-uvoAAAAJ&amp;hl=en&amp;oi=sra">GJ Qi</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, HJ Zhang - Proceedings of the 17th ACM international  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract This paper proposes a novel semantic-aware distance metric for images by mining <br>multimedia data on the Internet, in particular, web images and their associated tags. As well <br>known, a proper distance metric between images is a key ingredient in many realistic web <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4643724970553933749&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:tX962EHTcUAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4643724970553933749&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'tX962EHTcUAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://lms.comp.nus.edu.sg/papers/media/2010/tist-jinhui.pdf" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1899418" class=yC26>Image annotation by k NN-sparse graph-based label propagation over noisily tagged web images</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, TS Chua, <a href="/citations?user=Nut-uvoAAAAJ&amp;hl=en&amp;oi=sra">GJ Qi</a>&hellip; - ACM Transactions on  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract In this article, we exploit the problem of annotating a large-scale image corpus by <br>label propagation over noisily tagged web images. To annotate the images more accurately, <br>we propose a novel kNN-sparse graph-based semi-supervised learning approach for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13238107845729306771&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 28</a> <a href="/scholar?q=related:k8gBe_I0t7cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13238107845729306771&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'k8gBe_I0t7cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5709997" class=yC28>m-SNE: Multiview stochastic neighbor embedding</a></h3><div class="gs_a">B Xie, Y Mu, D Tao, <a href="/citations?user=dGo1COMAAAAJ&amp;hl=en&amp;oi=sra">K Huang</a> - Systems, Man, and Cybernetics,  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Dimension reduction has been widely used in real-world applications such as <br>image retrieval and document classification. In many scenarios, different features (or <br>multiview data) can be obtained, and how to duly utilize them is a challenge. It is not <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13815091715712775826&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 15</a> <a href="/scholar?q=related:knaATrgQub8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13815091715712775826&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'knaATrgQub8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://210.75.252.83/bitstream/344010/4141/1/10029.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 210.75.252.83</span><span class="gs_ggsS">210.75.252.83 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1835947" class=yC29>Semi-supervised sparse metric learning using alternating linearization optimization</a></h3><div class="gs_a"><a href="/citations?user=AjxoEpIAAAAJ&amp;hl=en&amp;oi=sra">W Liu</a>, <a href="/citations?user=kkzUrUgAAAAJ&amp;hl=en&amp;oi=sra">S Ma</a>, D Tao, J Liu, P Liu - Proceedings of the 16th ACM SIGKDD  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract In plenty of scenarios, data can be represented as vectors and then mathematically <br>abstracted as points in a Euclidean space. Because a great number of machine learning <br>and data mining applications need proximity measures over data, a simple and universal <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10095884706034426588&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 19</a> <a href="/scholar?q=related:3CJ7XKbJG4wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10095884706034426588&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'3CJ7XKbJG4wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://www.cais.ntu.edu.sg/~assourav/papers/TagRep-MM-2010.pdf" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874029" class=yC2B>Quantifying tag representativeness of visual content of social images</a></h3><div class="gs_a"><a href="/citations?user=wyKGVKUAAAAJ&amp;hl=en&amp;oi=sra">A Sun</a>, <a href="/citations?user=o0F3sqEAAAAJ&amp;hl=en&amp;oi=sra">SS Bhowmick</a> - Proceedings of the international conference on  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Social tags describe images from many aspects including the visual content <br>observable from the images, the context and usage of images, user opinions and others. Not <br>all tags are therefore useful for image search and are appropriate for tag recommendation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9270399511018333765&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:ReYzFD0Tp4AJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9270399511018333765&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ReYzFD0Tp4AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://www.cs.cmu.edu/~epxing/papers/2010/Chen_Zhu_Xing_NIPS10.pdf" class=yC2E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cs.cmu.edu/~epxing/papers/2010/Chen_Zhu_Xing_NIPS10.pdf" class=yC2D>Predictive subspace learning for multi-view data: a large margin approach</a></h3><div class="gs_a"><a href="/citations?user=cSxeVz0AAAAJ&amp;hl=en&amp;oi=sra">N Chen</a>, <a href="/citations?user=axsP38wAAAAJ&amp;hl=en&amp;oi=sra">J Zhu</a>, EP Xing - Advances in Neural Information Processing  &hellip;, 2010 - cs.cmu.edu</div><div class="gs_rs">Abstract Learning from multi-view data is important in many applications, such as image <br>classification and annotation. In this paper, we present a large-margin learning framework to <br>discover a predictive latent subspace representation shared by multiple views. Our <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18197769626023604288&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 18</a> <a href="/scholar?q=related:QLCHFw93i_wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18197769626023604288&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'QLCHFw93i_wJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md23', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md23" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:QLCHFw93i_wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://www.cs.clemson.edu/~jzwang/1201863/mm2010/p35-chen.pdf" class=yC30><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1873959" class=yC2F>Efficient large-scale image annotation by probabilistic collaborative multi-label propagation</a></h3><div class="gs_a">X Chen, <a href="/citations?user=Fqqx4HsAAAAJ&amp;hl=en&amp;oi=sra">Y Mu</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, TS Chua - Proceedings of the international  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Annotating large-scale image corpus requires huge amount of human efforts and is <br>thus generally unaffordable, which directly motivates recent development of semi-<br>supervised or active annotation methods. In this paper we revisit this notoriously <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17532080644981185834&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 17</a> <a href="/scholar?q=related:Kt2aKXV2TvMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17532080644981185834&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'Kt2aKXV2TvMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://www.ee.columbia.edu/~yjiang/publication/civr10_sampling.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816051" class=yC31>On the sampling of web images for learning visual concept classifiers</a></h3><div class="gs_a">S Zhu, G Wang, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a> - Proceedings of the ACM  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Visual concept learning often requires a large set of training images. In practice, <br>nevertheless, acquiring noise-free training labels with sufficient positive examples is always <br>expensive. A plausible solution for training data collection is by sampling the largely <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11972773823535266513&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:0Trqp1vWJ6YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11972773823535266513&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'0Trqp1vWJ6YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://lms.comp.nus.edu.sg/papers/media/2010/mmm10-richang.pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/925p41335j22184n.pdf" class=yC33>Mediapedia: Mining web knowledge to construct multimedia encyclopedia</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, ZJ Zha, <a href="/citations?user=0UkdiUT1ooUC&amp;hl=en&amp;oi=sra">Z Luo</a>, TS Chua - Advances in Multimedia  &hellip;, 2010 - Springer</div><div class="gs_rs">Abstract. In recent years, we have witnessed the blooming of Web 2.0 content such as <br>Wikipedia, Flickr and YouTube, etc. How might we benefit from such rich media resources <br>available on the internet? This paper presents a novel concept called Mediapedia, a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14978197780089934046&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 13</a> <a href="/scholar?q=related:3hDO6YU_3c8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14978197780089934046&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'3hDO6YU_3c8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://infolab.stanford.edu/~wangz/project/imsearch/review/MTA/neela.pdf" class=yC36><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from stanford.edu</span><span class="gs_ggsS">stanford.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/07NJ448H48433W7P.pdf" class=yC35>Automatic image semantic interpretation using social action and tagging data</a></h3><div class="gs_a">N Sawant, <a href="/citations?user=4Nmf18IAAAAJ&amp;hl=en&amp;oi=sra">J Li</a>, <a href="/citations?user=inVzWAcAAAAJ&amp;hl=en&amp;oi=sra">JZ Wang</a> - Multimedia Tools and Applications, 2011 - Springer</div><div class="gs_rs">Abstract The plethora of social actions and annotations (tags, comments, ratings) from online <br>media sharing Websites and collaborative games have induced a paradigm shift in the <br>research on image semantic interpretation. Social inputs with their added context <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4265122887020118349&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:TQE5DKvCMDsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4265122887020118349&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'TQE5DKvCMDsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://www.ifp.illinois.edu/~qi4/papers/2011_CVPR_CCTL-gjqi.pdf" class=yC38><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from illinois.edu</span><span class="gs_ggsS">illinois.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995312" class=yC37>Towards cross-category knowledge propagation for learning visual concepts</a></h3><div class="gs_a"><a href="/citations?user=Nut-uvoAAAAJ&amp;hl=en&amp;oi=sra">GJ Qi</a>, <a href="/citations?user=x_wsduUAAAAJ&amp;hl=en&amp;oi=sra">C Aggarwal</a>, <a href="/citations?user=uOJH_AEAAAAJ&amp;hl=en&amp;oi=sra">Y Rui</a>, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>&hellip; - Computer Vision and &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In recent years, knowledge transfer algorithms have become one of most the active <br>research areas in learning visual concepts. Most of the existing learning algorithms focuses <br>on leveraging the knowledge transfer process which is specific to a given category. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6327455598443497657&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 13</a> <a href="/scholar?q=related:uQhIrVejz1cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6327455598443497657&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'uQhIrVejz1cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://www.ee.columbia.edu/~dongliu/Papers/Survey.pdf" class=yC3A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/341820526WVG768J.pdf" class=yC39>Content-based tag processing for internet social images</a></h3><div class="gs_a"><a href="/citations?user=NvU30c4AAAAJ&amp;hl=en&amp;oi=sra">D Liu</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, HJ Zhang - Multimedia Tools and Applications, 2011 - Springer</div><div class="gs_rs">Abstract Online social media services such as Flickr and Zooomr allow users to share their <br>images with the others for social interaction. An important feature of these services is that the <br>users manually annotate their images with the freely-chosen tags, which can be used as <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9560297346392626769&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 14</a> <a href="/scholar?q=related:UZbXrsj_rIQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9560297346392626769&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'UZbXrsj_rIQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://www.ee.columbia.edu/~yjiang/publication/tcsvt_cdvs.pdf" class=yC3C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5686924" class=yC3B>Concept-driven multi-modality fusion for video search</a></h3><div class="gs_a">XY Wei, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Circuits and Systems for Video  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract As it is true for human perception that we gather information from different sources <br>in natural and multi-modality forms, learning from multi-modalities has become an effective <br>scheme for various information retrieval problems. In this paper, we propose a novel multi-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9058963900236874973&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 14</a> <a href="/scholar?q=related:3cS0uqvnt30J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9058963900236874973&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'3cS0uqvnt30J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://www.viplab.cs.nott.ac.uk/publications/Papers/TPAMI_10.7.pdf" class=yC3E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nott.ac.uk</span><span class="gs_ggsS">nott.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5639019" class=yC3D>A hybrid probabilistic model for unified collaborative and content-based image tagging</a></h3><div class="gs_a">N Zhou, <a href="/citations?user=e42JkYIAAAAJ&amp;hl=en&amp;oi=sra">WK Cheung</a>, <a href="/citations?user=pHkKtyMAAAAJ&amp;hl=en&amp;oi=sra">G Qiu</a>&hellip; - Pattern Analysis and  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The increasing availability of large quantities of user contributed images with labels <br>has provided opportunities to develop automatic tools to tag images to facilitate image <br>search and retrieval. In this paper, we present a novel hybrid probabilistic model (HPM) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11387811574683480230&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 13</a> <a href="/scholar?q=related:pjAXvkqiCZ4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11387811574683480230&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'pjAXvkqiCZ4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://jmlr.csail.mit.edu/papers/volume10/madani09a/madani09a.pdf" class=yC40><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mit.edu</span><span class="gs_ggsS">mit.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1755872" class=yC3F>Learning when concepts abound</a></h3><div class="gs_a"><a href="/citations?user=bfogkZUAAAAJ&amp;hl=en&amp;oi=sra">O Madani</a>, M Connor, W Greiner - The Journal of Machine Learning  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Many learning tasks, such as large-scale text categorization and word prediction, <br>can benefit from efficient training and classification when the number of classes, in addition <br>to instances and features, is large, that is, in the thousands and beyond. We investigate <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15763216418774436984&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 11</a> <a href="/scholar?q=related:eMyn1tAxwtoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15763216418774436984&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'eMyn1tAxwtoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://www.ee.columbia.edu/~dongliu/Papers/RetaggingTMM.pdf" class=yC42><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5740369" class=yC41>Image retagging using collaborative tag propagation</a></h3><div class="gs_a"><a href="/citations?user=NvU30c4AAAAJ&amp;hl=en&amp;oi=sra">D Liu</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, HJ Zhang - &hellip; , IEEE Transactions on, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Photo sharing websites such as Flickr host a massive amount of social images with <br>user-provided tags. However, these tags are often imprecise and incomplete, which <br>essentially limits tag-based image indexing and related applications. To tackle this issue, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2969282129434413639&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 9</a> <a href="/scholar?q=related:R4raYl0CNSkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2969282129434413639&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'R4raYl0CNSkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://arxiv.org/pdf/1008.4000" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5694022" class=yC43>NESVM: a fast gradient method for support vector machines</a></h3><div class="gs_a"><a href="/citations?user=OKvgizMAAAAJ&amp;hl=en&amp;oi=sra">T Zhou</a>, D Tao, X Wu - Data Mining (ICDM), 2010 IEEE 10th  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Support vector machines (SVMs) are invaluable tools for many practical <br>applications in artificial intelligence, eg, classification and event recognition. However, <br>popular SVM solvers are not sufficiently efficient for applications with a great deal of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=857670448975037290&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 10</a> <a href="/scholar?q=related:asOBrboO5wsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=857670448975037290&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'asOBrboO5wsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://domino.mpi-inf.mpg.de/intranet/d2/d2publ.nsf/e127ff338913b2a3c12565f4005ef860/dfef9e01f92efaf0c125783800326762/$FILE/rohrbach11cvpr.pdf" class=yC46><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995627" class=yC45>Evaluating knowledge transfer and zero-shot learning in a large-scale setting</a></h3><div class="gs_a"><a href="/citations?user=3kDtybgAAAAJ&amp;hl=en&amp;oi=sra">M Rohrbach</a>, <a href="/citations?user=cCda-zQAAAAJ&amp;hl=en&amp;oi=sra">M Stark</a>, <a href="/citations?user=z76PBfYAAAAJ&amp;hl=en&amp;oi=sra">B Schiele</a> - Computer Vision and Pattern  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract While knowledge transfer (KT) between object classes has been accepted as a <br>promising route towards scalable recognition, most experimental KT studies are surprisingly <br>limited in the number of object classes considered. To support claims of KT wrt scalability <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5433387856486716476&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 15</a> <a href="/scholar?q=related:PNTP2HdFZ0sJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5433387856486716476&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'PNTP2HdFZ0sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/civr10-hongrichang.pdf" class=yC48><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816055" class=yC47>Exploring large scale data for multimedia QA: an initial study</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, G Li, L Nie, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, TS Chua - Proceedings of the ACM  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract With the explosive growth of multimedia contents on the internet, multimedia search <br>has become more and more important. However, users are often bewildered by the vast <br>quantity of information content returned by the search engines. In this scenario, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4416393598065709541&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:5REEEZQuSj0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4416393598065709541&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'5REEEZQuSj0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.7358&amp;rep=rep1&amp;type=pdf" class=yC4A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1992008" class=yC49>Social negative bootstrapping for visual categorization</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>&hellip; - Proceedings of the 1st  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract To learn classifiers for many visual categories, obtaining labeled training examples <br>in an efficient way is crucial. Since a classifier tends to misclassify negative examples which <br>are visually similar to positive examples, inclusion of such informative negatives should be <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2766355420324308099&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 10</a> <a href="/scholar?q=related:gyToFpkRZCYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2766355420324308099&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'gyToFpkRZCYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://sites.google.com/site/kuiyuanyang/taggingTags.pdf" class=yC4C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874035" class=yC4B>Tagging tags</a></h3><div class="gs_a"><a href="/citations?user=g2gAY_0AAAAJ&amp;hl=en&amp;oi=sra">K Yang</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, HJ Zhang - Proceedings of the international  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Social image sharing websites like Flickr have successfully motivated users around <br>the world to annotate images with tags, which greatly facilitate search and organization of <br>social image content. However, these manually-input tags are far from a comprehensive <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16046832757195583447&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:16--dl_Nsd4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16046832757195583447&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'16--dl_Nsd4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072336" class=yC4D>Exploiting the entire feature space with sparsity for automatic image annotation</a></h3><div class="gs_a">Z Ma, <a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, <a href="/citations?user=2oB4nAIAAAAJ&amp;hl=en&amp;oi=sra">F Nie</a>, <a href="/citations?user=jInmtEkAAAAJ&amp;hl=en&amp;oi=sra">J Uijlings</a>, <a href="/citations?user=fOmROdkAAAAJ&amp;hl=en&amp;oi=sra">N Sebe</a> - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract The explosive growth of digital images requires effective methods to manage these <br>images. Among various existing methods, automatic image annotation has proved to be an <br>important technique for image management tasks, eg, image retrieval over large-scale <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3583457953198903850&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 13</a> <a href="/scholar?q=related:KhL4vwUAuzEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'KhL4vwUAuzEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874129" class=yC4E>Heterogeneous feature selection by group lasso with logistic regression</a></h3><div class="gs_a">F Wu, Y Yuan, Y Zhuang - &hellip;  of the international conference on Multimedia, 2010 - dl.acm.org</div><div class="gs_rs">Abstract The selection of groups of discriminative features is critical for image understanding <br>since the irrelevant features could deteriorate the performance of image understanding. This <br>paper formulates the selection of groups of discriminative features by the extension of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12147245600961016045&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 9</a> <a href="/scholar?q=related:7TC15IOvk6gJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'7TC15IOvk6gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631524" class=yC4F>Google challenge: incremental-learning for web video categorization on robust semantic feature space</a></h3><div class="gs_a">Y Song, Y Zhang, <a href="/citations?user=WyYnBkEAAAAJ&amp;hl=en&amp;oi=sra">X Zhang</a>, J Cao, JT Li - Proceedings of the 17th ACM  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract With the advent of video sharing websites, the amount of videos on the internet <br>grows rapidly. Web video categorization is an efficient methodology to organize the huge <br>amount of data. In this paper, we propose an effective web video categorization algorithm <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15558891620098309177&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:OfzCcn5J7NcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15558891620098309177&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'OfzCcn5J7NcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB42" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW42"><a href="http://archive.itee.uq.edu.au/~uqyyan10/papers/wwwj.pdf" class=yC51><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uq.edu.au</span><span class="gs_ggsS">uq.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/732M8G373RV73181.pdf" class=yC50>Mining multi-tag association for image tagging</a></h3><div class="gs_a"><a href="/citations?user=PVv2xDYAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, Z Huang, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, <a href="/citations?user=y6m820wAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a> - World Wide Web, 2011 - Springer</div><div class="gs_rs">Abstract Automatic media tagging plays a critical role in modern tag-based media retrieval <br>systems. Existing tagging schemes mostly perform tag assignment based on community <br>contributed media resources, where the tags are provided by users interactively. However, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14467407038118961623&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:1yERsAyOxsgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14467407038118961623&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'1yERsAyOxsgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5540033" class=yC52>Nonparametric label-to-region by search</a></h3><div class="gs_a">X Liu, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, <a href="/citations?user=CcbnBvgAAAAJ&amp;hl=en&amp;oi=sra">J Luo</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>&hellip; - Computer Vision and  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this work, we investigate how to propagate annotated labels for a given single <br>image from the image-level to their corresponding semantic regions, namely Label-to-<br>Region (L2R), by utilizing the auxiliary knowledge from Internet image search with the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8081740356259420413&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:_RjI_QgcKHAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8081740356259420413&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'_RjI_QgcKHAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://www.yugangjiang.info/publication/icmr11-lostinbinarization.pdf" class=yC54><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from yugangjiang.info</span><span class="gs_ggsS">yugangjiang.info <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1992012" class=yC53>Lost in binarization: query-adaptive ranking for similar image search with compact codes</a></h3><div class="gs_a"><a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a>, <a href="/citations?user=i0KkESEAAAAJ&amp;hl=en&amp;oi=sra">J Wang</a>, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Proceedings of the 1st ACM International  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract With the proliferation of images on the Web, fast search of visually similar images <br>has attracted significant attention. State-of-the-art techniques often embed high-dimensional <br>visual features into low-dimensional Hamming space, where search can be performed in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11855863591256266127&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:jxVRBh99iKQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11855863591256266127&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'jxVRBh99iKQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB45" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW45"><a href="http://research.microsoft.com/en-us/um/people/jingdw/searchbycolorsketch/..%5Cpubs%5CTIST12-ColorSketch.pdf" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from microsoft.com</span><span class="gs_ggsS">microsoft.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2036276" class=yC55>Interactive image search by color map</a></h3><div class="gs_a"><a href="/citations?user=z5SPCmgAAAAJ&amp;hl=en&amp;oi=sra">J Wang</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a> - ACM Transactions on Intelligent Systems and  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract The availability of large-scale images from the Internet has made the research on <br>image search attract a lot of attention. Text-based image search engines, for example, <br>Google/Microsoft Bing/Yahoo! image search engines using the surrounding text, have <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8795776931183450955&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:S9vDUoTgEHoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8795776931183450955&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'S9vDUoTgEHoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1891010" class=yC57>Tag refinement in an image folksonomy using visual similarity and tag co-occurrence statistics</a></h3><div class="gs_a">S Lee, <a href="/citations?user=VpjWb7wAAAAJ&amp;hl=en&amp;oi=sra">W De Neve</a>, YM Ro - Image Communication, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Noisy tag assignments lower the effectiveness of multimedia applications that rely <br>on the availability of user-supplied tags for retrieving user-contributed images for further <br>processing. This paper discusses a novel tag refinement technique that aims at <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15795776413396468713&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:6YP9V_XeNdsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15795776413396468713&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'6YP9V_XeNdsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB47" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW47"><a href="https://netfiles.uiuc.edu/xinjin3/homepage/paper/www10rankcom.pdf" class=yC59><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uiuc.edu</span><span class="gs_ggsS">uiuc.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1772809" class=yC58>RankCompete: simultaneous ranking and clustering of web photos</a></h3><div class="gs_a"><a href="/citations?user=S-hBSfIAAAAJ&amp;hl=en&amp;oi=sra">L Cao</a>, A Del Pozo, X Jin, <a href="/citations?user=CcbnBvgAAAAJ&amp;hl=en&amp;oi=sra">J Luo</a>, J Han&hellip; - Proceedings of the 19th  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract With the explosive growth of digital cameras and online media, it has become <br>crucial to design efficient methods that help users browse and search large image <br>collections. The recent VisualRank algorithm [4] employs visual similarity to represent the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9946072065692375997&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:vZsH4tiLB4oJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9946072065692375997&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'vZsH4tiLB4oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://grid.hust.edu.cn/xbliu/papers/ICDM09.pdf" class=yC5B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hust.edu.cn</span><span class="gs_ggsS">hust.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5360256" class=yC5A>Unified solution to nonnegative data factorization problems</a></h3><div class="gs_a">X Liu, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, J Yan, <a href="/citations?user=o02W0aEAAAAJ&amp;hl=en&amp;oi=sra">H Jin</a> - Data Mining, 2009. ICDM&#39;09. Ninth  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we restudy the non-convex data factorization problems (regularized or <br>not, unsupervised or supervised), where the optimization is confined in the nonnegative <br>orthant, and provide a unified convergency provable solution based on multiplicative <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5386426152144602524&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:nAV0_wtuwEoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5386426152144602524&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'nAV0_wtuwEoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm09-chua2.pdf" class=yC5D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631143" class=yC5C>Moviebase: a movie database for event detection and behavioral analysis</a></h3><div class="gs_a">TS Chua, S Tang, R Trichet, HK Tan&hellip; - &hellip; of the 1st workshop on Web &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract The overwhelming amount of multimedia entities shared over the web has given <br>rise to the need for semantic identification and classification of these entities. Numerous <br>research efforts have tackled this problem by developing advanced content analysis <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17228695373834831186&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:UkXXNCmfGO8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17228695373834831186&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'UkXXNCmfGO8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6146459" class=yC5E>Web image annotation via subspace-sparsity collaborated feature selection</a></h3><div class="gs_a">Z Ma, <a href="/citations?user=2oB4nAIAAAAJ&amp;hl=en&amp;oi=sra">F Nie</a>, <a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, <a href="/citations?user=jInmtEkAAAAJ&amp;hl=en&amp;oi=sra">JRR Uijlings</a>&hellip; - &hellip; , IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The number of web images has been explosively growing due to the development <br>of network and storage technology. These images make up a large amount of current <br>multimedia data and are closely related to our daily life. To efficiently browse, retrieve and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17340288242148473925&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:RXwkkEgUpfAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17340288242148473925&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'RXwkkEgUpfAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://www.cais.ntu.edu.sg/~assourav/papers/JASIST-TagIR-2011.pdf" class=yC60><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21659/full" class=yC5F>Tagbased social image retrieval: An empirical evaluation</a></h3><div class="gs_a"><a href="/citations?user=wyKGVKUAAAAJ&amp;hl=en&amp;oi=sra">A Sun</a>, <a href="/citations?user=o0F3sqEAAAAJ&amp;hl=en&amp;oi=sra">SS Bhowmick</a>, N Nguyen&hellip; - Journal of the  &hellip;, 2011 - Wiley Online Library</div><div class="gs_rs">Abstract Tags associated with social images are valuable information source for superior <br>image search and retrieval experiences. Although various heuristics are valuable to boost <br>tag-based search for images, there is a lack of general framework to study the impact of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12821041757159329039&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:D43uoYx97bEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12821041757159329039&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'D43uoYx97bEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6022804" class=yC61>Utilizing related samples to enhance interactive concept-based video search</a></h3><div class="gs_a">J Yuan, ZJ Zha, YT Zheng, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>&hellip; - Multimedia, IEEE  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract One of the main challenges in interactive concept-based video search is the <br>problem of insufficient relevant samples, especially for queries with complex semantics. In <br>this paper,related samples are exploited to enhance interactive video search. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16391758592167997752&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:OHEhopc5e-MJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16391758592167997752&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'OHEhopc5e-MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://kusu.comp.nus.edu/proceedings/mm09/wsmc/p33.pdf" class=yC63><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631142" class=yC62>Large scale incremental web video categorization</a></h3><div class="gs_a"><a href="/citations?user=WyYnBkEAAAAJ&amp;hl=en&amp;oi=sra">X Zhang</a>, YC Song, J Cao, YD Zhang, JT Li - &hellip; of the 1st workshop on Web &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract With the advent of video sharing websites, the amount of videos on the internet <br>grows rapidly. Web video categorization is an efficient methodology for organizing the huge <br>amount of videos. In this paper we investigate the characteristics of web videos, and make <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7250344862404031585&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:YbSrBj1mnmQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7250344862404031585&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'YbSrBj1mnmQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5756484" class=yC64>Tag tagging: towards more descriptive keywords of image content</a></h3><div class="gs_a"><a href="/citations?user=g2gAY_0AAAAJ&amp;hl=en&amp;oi=sra">K Yang</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>&hellip; - &hellip; , IEEE Transactions on, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Tags have been demonstrated to be effective and efficient for organizing and <br>searching social image content. However, these human-provided keywords are far from a <br>comprehensive description of the image content, which limits their effectiveness in tag-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6046709255871788737&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:wfZNCAg66lMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6046709255871788737&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'wfZNCAg66lMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB55" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW55"><a href="http://137.132.145.151/lms/sites/default/files/mm10-jinhui.pdf" class=yC66><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874139" class=yC65>One person labels one million images</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, Q Chen, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, TS Chua, R Jain - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Targeting the same objective of alleviating the manual work as automatic <br>annotation, in this paper, we propose a novel framework with minimal human effort to <br>manually annotate a large-scale image corpus. In this framework, a dynamic multi-scale <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18179452320848807319&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:lwVYI5FjSvwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18179452320848807319&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'lwVYI5FjSvwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/EL767Q764473Q628.pdf" class=yC67>m-sne: Multiview stochastic neighbor embedding</a></h3><div class="gs_a">B Xie, Y Mu, D Tao - Neural Information Processing. Theory and  &hellip;, 2010 - Springer</div><div class="gs_rs">In many real world applications, different features (or multiview data) can be obtained and <br>how to duly utilize them in dimension reduction is a challenge. Simply concatenating them <br>into a long vector is not appropriate because each view has its specific statistical property <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8755071550095699453&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:_S2VPjBDgHkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8755071550095699453&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'_S2VPjBDgHkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB57" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW57"><a href="http://eprints.pascal-network.org/archive/00009343/01/steggink-name-it-game-mmsys.pdf" class=yC69><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from pascal-network.org</span><span class="gs_ggsS">pascal-network.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/T567101UM02J4468.pdf" class=yC68>Adding semantics to image-region annotations with the Name-It-Game</a></h3><div class="gs_a">J Steggink, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a> - Multimedia systems, 2011 - Springer</div><div class="gs_rs">Abstract In this paper we present the Name-It-Game, an interactive multimedia game <br>fostering the swift creation of a large data set of region-based image annotations. Compared <br>to existing annotation games, we consider an added semantic structure, by means of the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8394035972231984021&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:lfO8RD6bfXQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8394035972231984021&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'lfO8RD6bfXQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB58" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW58"><a href="http://www.science.uva.nl/research/publications/2012/LiITM2012/li_biconcept_tmm.pdf" class=yC6B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6175138" class=yC6A>Harvesting social images for bi-concept search</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>&hellip; - &hellip; , IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Searching for the co-occurrence of two visual concepts in unlabeled images is an <br>important step towards answering complex user queries. Traditional visual search methods <br>use combinations of the confidence scores of individual concept detectors to tackle such <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=20645694324272890&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:-u5WTCZZSQAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=20645694324272890&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'-u5WTCZZSQAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072049" class=yC6C>Image annotation by composite kernel learning with group structure</a></h3><div class="gs_a">Y Yuan, F Wu, Y Zhuang, <a href="/citations?user=VUN-9cQAAAAJ&amp;hl=en&amp;oi=sra">J Shao</a> - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract We can obtain more and more kinds of heterogeneous features (such as color, <br>shape and texture) in images which can be extracted to describe various aspects of visual <br>characteristics. Those high-dimensional heterogeneous visual features are intrinsically <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5773631620472008471&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:F28yal4PIFAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'F28yal4PIFAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB60" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW60"><a href="http://algorithmofsaintqdd.googlecode.com/svn/trunk/Papers/ML/ICML2011/icml2011proceedings/374_icmlpaper.pdf" class=yC6E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from googlecode.com</span><span class="gs_ggsS">googlecode.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://algorithmofsaintqdd.googlecode.com/svn/trunk/Papers/ML/ICML2011/icml2011proceedings/374_icmlpaper.pdf" class=yC6D>Infinite SVM: a Dirichlet process mixture of large-margin kernel machines</a></h3><div class="gs_a"><a href="/citations?user=axsP38wAAAAJ&amp;hl=en&amp;oi=sra">J Zhu</a>, <a href="/citations?user=cSxeVz0AAAAJ&amp;hl=en&amp;oi=sra">N Chen</a>, EP Xing - &hellip;  of the 28th  &hellip;, 2011 - algorithmofsaintqdd.googlecode. &hellip;</div><div class="gs_rs">Abstract We present Infinite SVM (iSVM), a Dirichlet process mixture of large-margin kernel <br>machines for multi-way classification. An iSVM enjoys the advantages of both Bayesian <br>nonparametrics in handling the unknown number of mixing components, and large-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10917526158327492429&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:TS8ljTLYgpcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10917526158327492429&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'TS8ljTLYgpcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md60', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md60" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TS8ljTLYgpcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0031320310004851" class=yC6F>Efficient region-aware large graph construction towards scalable multi-label propagation</a></h3><div class="gs_a"><a href="/citations?user=lDppvmoAAAAJ&amp;hl=en&amp;oi=sra">BK Bao</a>, <a href="/citations?user=V9W87PYAAAAJ&amp;hl=en&amp;oi=sra">B Ni</a>, <a href="/citations?user=Fqqx4HsAAAAJ&amp;hl=en&amp;oi=sra">Y Mu</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a> - Pattern Recognition, 2011 - Elsevier</div><div class="gs_rs">With fast growing number of images on photo-sharing websites such as Flickr and Picasa, it <br>is in urgent need to develop scalable multi-label propagation algorithms for image indexing, <br>management and retrieval. It has been well acknowledged that analysis in semantic <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1395369859629856533&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:FTeGjX5ZXRMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1395369859629856533&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'FTeGjX5ZXRMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6025297" class=yC70>Web and personal image annotation by mining label correlation with relaxed visual graph embedding</a></h3><div class="gs_a"><a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, F Wu, <a href="/citations?user=2oB4nAIAAAAJ&amp;hl=en&amp;oi=sra">F Nie</a>, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>&hellip; - Image Processing,  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The number of digital images rapidly increases, and it becomes an important <br>challenge to organize these resources effectively. As a way to facilitate image categorization <br>and retrieval, automatic image annotation has received much research attention. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15614467088638220519&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:5xhPjxa7sdgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15614467088638220519&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'5xhPjxa7sdgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB63" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW63"><a href="http://c2inet.sce.ntu.edu.sg/ivor/publication/GMI.pdf" class=yC72><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5872043" class=yC71>Improving Web Image Search by Bag-Based Reranking</a></h3><div class="gs_a"><a href="/citations?user=inRIcS0AAAAJ&amp;hl=en&amp;oi=sra">L Duan</a>, <a href="/citations?user=yjG4Eg4AAAAJ&amp;hl=en&amp;oi=sra">W Li</a>, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">IWH Tsang</a>, D Xu - Image Processing, IEEE  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Given a textual query in traditional text-based image retrieval (TBIR), relevant <br>images are to be reranked using visual features after the initial text-based search. In this <br>paper, we propose a new bag-based reranking framework for large-scale TBIR. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8498587916291489528&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:-D77sq4M8XUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8498587916291489528&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'-D77sq4M8XUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB64" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW64"><a href="http://vipl.ict.ac.cn/sites/default/files/papers/files/2010_ACMMM_zpwu_Vicept_%20Link%20Visual%20Features%20to%20Concepts%20for%20Large-scale%20Image%20Understanding.pdf" class=yC74><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874059" class=yC73>Vicept: link visual features to concepts for large-scale image understanding</a></h3><div class="gs_a">Z Wu, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, L Li, P Cui, Q Huang&hellip; - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract On noticing the paradox of visual polysemia and concept poly-morphism, this paper <br>proposes a new perspective called&quot; Vicept&quot; to associate elementary visual features and <br>cognitive concepts. Firstly, a carefully prepared large image dataset and associate <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15314954435448674535&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:5xg2vOqlidQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15314954435448674535&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'5xg2vOqlidQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB65" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW65"><a href="http://www.cais.ntu.edu.sg/~assourav/papers/MM-11-TagRec.pdf" class=yC76><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2071969" class=yC75>Social image tag recommendation by concept matching</a></h3><div class="gs_a"><a href="/citations?user=wyKGVKUAAAAJ&amp;hl=en&amp;oi=sra">A Sun</a>, <a href="/citations?user=o0F3sqEAAAAJ&amp;hl=en&amp;oi=sra">SS Bhowmick</a>, JA Chong - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Tags associated with social images are valuable information source for superior <br>image search and retrieval experiences. In this paper, we propose a novel tag <br>recommendation technique that exploits the user-given tags associated with images. Each <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3357586024013872144&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:ELwpGLOKmC4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3357586024013872144&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ELwpGLOKmC4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1992042" class=yC77>Active learning through notes data in flickr: an effortless training data acquisition approach for object localization</a></h3><div class="gs_a"><a href="/citations?user=7hogLBYAAAAJ&amp;hl=en&amp;oi=sra">L Zhang</a>, J Ma, C Cui, P Li - Proceedings of the 1st ACM International  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Most of the state-of-the-art systems for object localization rely on supervised <br>machine learning techniques, and are thus limited by the lack of labeled training data. In this <br>paper, our motivation is to provide training dataset for object localization effectively and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18332098546666447452&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:XCKCJYCyaP4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'XCKCJYCyaP4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:333"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB67" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW67"><a href="http://repository.tudelft.nl/assets/uuid:cd337eab-3330-4005-8006-b0c5d6b1d99b/tsikrika.pdf" class=yC79><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tudelft.nl</span><span class="gs_ggsS">tudelft.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/3T85R31556030WV1.pdf" class=yC78>Reliability and effectiveness of clickthrough data for automatic image annotation</a></h3><div class="gs_a">T Tsikrika, C Diou, <a href="/citations?user=iH9TVHQAAAAJ&amp;hl=en&amp;oi=sra">AP de Vries</a>&hellip; - Multimedia Tools and  &hellip;, 2011 - Springer</div><div class="gs_rs">Abstract Automatic image annotation using supervised learning is performed by concept <br>classifiers trained on labelled example images. This work proposes the use of clickthrough <br>data collected from search logs as a source for the automatic generation of concept <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3599037107118200803&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:49c6Ly5Z8jEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3599037107118200803&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'49c6Ly5Z8jEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:332"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/R830386W66L11531.pdf" class=yC7A>Web video retagging</a></h3><div class="gs_a">Z Chen, J Cao, T Xia, Y Song, Y Zhang, J Li - Multimedia Tools and  &hellip;, 2011 - Springer</div><div class="gs_rs">Abstract Tags associated with web videos play a crucial role in organizing and accessing <br>large-scale video collections. However, the raw tag list (RawL) is usually incomplete, <br>imprecise and unranked, which reduces the usability of tags. Meanwhile, compared with <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13722599603110798911&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:P-ZVMqJ3cL4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13722599603110798911&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'P-ZVMqJ3cL4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:331"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB69" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW69"><a href="http://psy2.fau.edu/~barenholtz/Papers/Marques_et_al_2010.pdf" class=yC7C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fau.edu</span><span class="gs_ggsS">fau.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/FR1M4Q447121R316.pdf" class=yC7B>Context modeling in computer vision: techniques, implications, and applications</a></h3><div class="gs_a"><a href="/citations?user=ZgWULzYAAAAJ&amp;hl=en&amp;oi=sra">O Marques</a>, <a href="/citations?user=2grAjZsAAAAJ&amp;hl=en&amp;oi=sra">E Barenholtz</a>, V Charvillat - Multimedia Tools and Applications, 2011 - Springer</div><div class="gs_rs">Abstract In recent years there has been a surge of interest in context modeling for numerous <br>applications in computer vision. The basic motivation behind these diverse efforts is <br>generally the sameattempting to enhance current image analysis technologies by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7457324632844757168&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:sGh7Mz69fWcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7457324632844757168&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'sGh7Mz69fWcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:330"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB70" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW70"><a href="http://kusu.comp.nus.edu/proceedings/mm09/wsmc/p9.pdf" class=yC7E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631138" class=yC7D>Understanding tag-cloud and visual features for better annotation of concepts in NUS-WIDE dataset</a></h3><div class="gs_a"><a href="/citations?user=fe-1v0MAAAAJ&amp;hl=en&amp;oi=sra">S Gao</a>, <a href="/citations?user=Eeolw80AAAAJ&amp;hl=en&amp;oi=sra">LT Chia</a>, X Cheng - Proceedings of the 1st workshop on Web- &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Large-scale dataset construction will require a significant large amount of well <br>labeled ground truth. For the NUS-WIDE dataset, a less labor-intensive annotation process <br>was used and this paper will focuses on improving the semi-manual annotation method <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6885952313000836822&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:1r6xNCXRj18J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6885952313000836822&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'1r6xNCXRj18J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:329"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB71" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW71"><a href="http://home.ustc.edu.cn/~junjcai/1876.pdf" class=yC80><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ustc.edu.cn</span><span class="gs_ggsS">ustc.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5583896" class=yC7F>Evaluation of histogram based interest point detector in web image classification and search</a></h3><div class="gs_a"><a href="/citations?user=qioooCAAAAAJ&amp;hl=en&amp;oi=sra">J Cai</a>, ZJ Zha, Y Zhao, Z Wang - Multimedia and Expo (ICME),  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Local image feature has received increasing attention in various applications, such <br>as web image classification and search. The process of local feature extraction consists of <br>two main steps: interest point detection and local feature description. A wealth of interest <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=461953506403695871&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:_zzwO0QwaQYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=461953506403695871&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'_zzwO0QwaQYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:328"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816084" class=yC81>Beyond tag relevance: integrating visual attention model and multi-instance learning for tag saliency ranking</a></h3><div class="gs_a">S Feng, C Lang, D Xu - &hellip;  of the ACM International Conference on Image  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Tag ranking has emerged as an important research topic recently due to its <br>potential application on web image search. Conventional tag ranking approaches mainly <br>rank the tags according to their relevance levels with respect to a given image. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15015146307116709129&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:CbU1PgOEYNAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'CbU1PgOEYNAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:327"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB73" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW73"><a href="http://books.nips.cc/papers/files/nips25/NIPS2012_1319.pdf" class=yC83><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nips.cc</span><span class="gs_ggsS">nips.cc <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://books.nips.cc/papers/files/nips25/NIPS2012_1319.pdf" class=yC82>Accelerated training for matrix-norm regularization: A boosting approach</a></h3><div class="gs_a"><a href="/citations?user=jrkrn3sAAAAJ&amp;hl=en&amp;oi=sra">X Zhang</a>, Y Yu, <a href="/citations?user=xaQuPloAAAAJ&amp;hl=en&amp;oi=sra">D Schuurmans</a> - Advances in Neural Information  &hellip;, 2012 - books.nips.cc</div><div class="gs_rs">Abstract Sparse learning models typically combine a smooth loss with a nonsmooth penalty, <br>such as trace norm. Although recent developments in sparse approximation have offered <br>promising solution methods, current approaches either apply only to matrix-norm <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15762838188973860521&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a onclick="return gs_ocit(event,'qaa_WdHZwNoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md73', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md73" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:qaa_WdHZwNoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:326"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB74" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW74"><a href="http://charuaggarwal.net/2010_TPAMI_revised_v2.pdf" class=yC85><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from charuaggarwal.net</span><span class="gs_ggsS">charuaggarwal.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6035718" class=yC84>Exploring context and content links in social media: A latent space method</a></h3><div class="gs_a"><a href="/citations?user=Nut-uvoAAAAJ&amp;hl=en&amp;oi=sra">GJ Qi</a>, <a href="/citations?user=x_wsduUAAAAJ&amp;hl=en&amp;oi=sra">C Aggarwal</a>, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, <a href="/citations?user=z7GCqT4AAAAJ&amp;hl=en&amp;oi=sra">H Ji</a>&hellip; - Pattern Analysis and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Social media networks contain both content and context-specific information. Most <br>existing methods work with either of the two for the purpose of multimedia mining and <br>retrieval. In reality, both content and context information are rich sources of information for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=456745240989403318&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:tggGcmCvVgYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=456745240989403318&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'tggGcmCvVgYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:325"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB75" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW75"><a href="http://c2inet.sce.ntu.edu.sg/ivor/publication/MLGSc.pdf" class=yC87><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995454" class=yC86>Multi-layer group sparse codingFor concurrent image classification and annotation</a></h3><div class="gs_a"><a href="/citations?user=fe-1v0MAAAAJ&amp;hl=en&amp;oi=sra">S Gao</a>, <a href="/citations?user=Eeolw80AAAAJ&amp;hl=en&amp;oi=sra">LT Chia</a>, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">IWH Tsang</a> - Computer Vision and Pattern  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present a multi-layer group sparse coding framework for concurrent image <br>classification and annotation. By leveraging the dependency between image class label and <br>tags, we introduce a multi-layer group sparse structure of the reconstruction coefficients. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7762187774515327005&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:HbDds6fUuGsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7762187774515327005&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'HbDds6fUuGsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:324"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB76" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW76"><a href="http://www.cs.nott.ac.uk/~axk/Publications/siq.pdf" class=yC89><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nott.ac.uk</span><span class="gs_ggsS">nott.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cs.nott.ac.uk/~axk/Publications/siq.pdf" class=yC88>Social image quality</a></h3><div class="gs_a"><a href="/citations?user=pHkKtyMAAAAJ&amp;hl=en&amp;oi=sra">G Qiu</a>, A Kheiri - Image Quality and System Performance VIII, 2011 - cs.nott.ac.uk</div><div class="gs_rs">ABSTRACT Current subjective image quality assessments have been developed in the <br>laboratory environments, under controlledconditions, and are dependent on the participation <br>of limited numbers of observers. In this research, with the help of Web 2.0 and social <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2463233179840622605&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:DTBbu4YpLyIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2463233179840622605&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'DTBbu4YpLyIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md76', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md76" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:DTBbu4YpLyIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:323"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6189803" class=yC8A>On Combining Multiple Features for Cartoon Character Retrieval and Clip Synthesis</a></h3><div class="gs_a">J Yu, D Liu, D Tao, HS Seah - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract How do we retrieve cartoon characters accurately? Or how to synthesize new <br>cartoon clips smoothly and efficiently from the cartoon library? Both questions are important <br>for animators and cartoon enthusiasts to design and create new cartoons by utilizing <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17195421315561920469&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:1Zc_zZRoou4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17195421315561920469&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'1Zc_zZRoou4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:322"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB78" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW78"><a href="http://www.idiap.ch/~gatica/publications/NegoescuGatica-book10.pdf" class=yC8C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from idiap.ch</span><span class="gs_ggsS">idiap.ch <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.idiap.ch/~gatica/publications/NegoescuGatica-book10.pdf" class=yC8B>Internet multimedia search and mining</a></h3><div class="gs_a"><a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>, TS Chua - Internet Multimedia Search and Mining, 2010 - idiap.ch</div><div class="gs_rs">Abstract: We present in this chapter a review of current work that leverages on large online <br>social networks&#39; meta-information, in particular Flickr Groups. We briefly present this hugely <br>successful feature in Flickr and discuss the various ways in which metadata stemming <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5011184445285179392&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:AKAv3bNNi0UJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5011184445285179392&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'AKAv3bNNi0UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md78', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md78" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:AKAv3bNNi0UJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:321"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB79" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW79"><a href="http://research.microsoft.com/en-us/um/people/akapoor/papers/CVPR%202011a.pdf" class=yC8E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from microsoft.com</span><span class="gs_ggsS">microsoft.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995439" class=yC8D>Collaborative personalization of image enhancement</a></h3><div class="gs_a"><a href="/citations?user=U50zLvkAAAAJ&amp;hl=en&amp;oi=sra">JC Caicedo</a>, <a href="/citations?user=4D1n8scAAAAJ&amp;hl=en&amp;oi=sra">A Kapoor</a>, SB Kang - Computer Vision and Pattern  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract While most existing enhancement tools for photographs have universal auto-<br>enhancement functionality, recent research shows that users can have personalized <br>preferences. In this paper, we explore whether such personalized preferences in image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15963187154414592315&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:O-H17SOiiN0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15963187154414592315&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'O-H17SOiiN0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:320"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6202346" class=yC8F>Spectral hashing with semantically consistent graph for image indexing</a></h3><div class="gs_a">P Li, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=o8PT69EAAAAJ&amp;hl=en&amp;oi=sra">J Cheng</a>, C Xu, H Lu - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The ability of fast similarity search in a large-scale dataset is of great importance to <br>many multimedia applications. Semantic hashing is a promising way to accelerate similarity <br>search, which designs compact binary codes for a large number of images so that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18036589592035755599&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:T4bMCKzWTvoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'T4bMCKzWTvoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:319"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5628899" class=yC90>Automatic concept detector refinement for large-scale video semantic annotation</a></h3><div class="gs_a"><a href="/citations?user=-BFEdeMAAAAJ&amp;hl=en&amp;oi=sra">X Liu</a>, <a href="/citations?user=KdPSGmAAAAAJ&amp;hl=en&amp;oi=sra">B Huet</a> - Semantic Computing (ICSC), 2010 IEEE Fourth  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the explosion of content sharing web site, an unprecedented amount of <br>multimedia items are made available online on a day to day basis. Since search engine <br>technologies rely essentially on textual information there is an urgent need to infer <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9310175242193858029&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:7e2ZKw5jNIEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9310175242193858029&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'7e2ZKw5jNIEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:318"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB82" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW82"><a href="http://202.114.89.42/resource/pdf/5546.pdf" class=yC92><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 202.114.89.42</span><span class="gs_ggsS">202.114.89.42 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320310001033" class=yC91>Web image concept annotation with better understanding of tags and visual features</a></h3><div class="gs_a"><a href="/citations?user=fe-1v0MAAAAJ&amp;hl=en&amp;oi=sra">S Gao</a>, <a href="/citations?user=Eeolw80AAAAJ&amp;hl=en&amp;oi=sra">LT Chia</a>, X Cheng - Journal of Visual Communication and Image  &hellip;, 2010 - Elsevier</div><div class="gs_rs">This paper focuses on improving the semi-manual method for web image concept <br>annotation. By sufficiently studying the characteristics of tag and visual feature, we propose <br>the Grouping-Based-Precision &amp; Recall-Aided (GBPRA) feature selection strategy for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3589741737423602662&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:5gOHRhdT0TEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3589741737423602662&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'5gOHRhdT0TEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:317"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6212356" class=yC93>Visual-Textual Joint Relevance Learning for Tag-Based Social Image Search</a></h3><div class="gs_a">Y Gao, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, ZJ Zha, <a href="/citations?user=d3h-zScAAAAJ&amp;hl=en&amp;oi=sra">J Shen</a>, X Li, X Wu - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the popularity of social media websites, extensive research efforts have been <br>dedicated to tag-based social image search. Both visual information and tags have been <br>investigated in the research field. However, most existing methods use tags and visual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3875216273605194683&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:u_MMSKuIxzUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3875216273605194683&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'u_MMSKuIxzUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:316"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB84" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW84"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.5646&amp;rep=rep1&amp;type=pdf" class=yC95><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072330" class=yC94>Personalizing automated image annotation using cross-entropy</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, E Gavves, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>&hellip; - Proceedings of the 19th &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Annotating the increasing amounts of user-contributed images in a personalized <br>manner is in great demand. However, this demand is largely ignored by the mainstream of <br>automated image annotation research. In this paper we aim for personalizing automated <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15388403704339045842&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:0h1vSaOXjtUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15388403704339045842&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'0h1vSaOXjtUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:315"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB85" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW85"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/MM2011.pdf" class=yC97><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072334" class=yC96>Towards multi-semantic image annotation with graph regularized exclusive group lasso</a></h3><div class="gs_a">X Chen, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">X Yuan</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, J Tang, <a href="/citations?user=uOJH_AEAAAAJ&amp;hl=en&amp;oi=sra">Y Rui</a>&hellip; - Proceedings of the 19th  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract To bridge the semantic gap between low level feature and human perception, most <br>of the existing algorithms aim mainly at annotating images with concepts coming from only <br>one semantic space, eg cognitive or affective. The naive combination of the outputs from <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11933754482419175968&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:IBJSWXk2naUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11933754482419175968&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'IBJSWXk2naUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:314"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB86" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW86"><a href="http://www.lv-nus.org/papers/2009/2009_imcs_bkb.pdf" class=yC99><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lv-nus.org</span><span class="gs_ggsS">lv-nus.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1734613" class=yC98>Hidden-concept driven image decomposition towards semi-supervised multi-label image annotation</a></h3><div class="gs_a"><a href="/citations?user=lDppvmoAAAAJ&amp;hl=en&amp;oi=sra">BK Bao</a>, T Li, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a> - Proceedings of the First International Conference  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Conventional semi-supervised learning algorithms over multi-label image data <br>propagate labels predominantly via the holistic image similarities, ignoring that each label <br>essentially only characterizes a local region within an image. In this paper, we present a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12242115211520195072&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:ADoRYuy65KkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12242115211520195072&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ADoRYuy65KkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:313"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/R141W2PMT86473P7.pdf" class=yC9A>Tagging image by merging multiple features in a integrated manner</a></h3><div class="gs_a">X Zhang, Z Li, W Chao - Journal of Intelligent Information Systems, 2012 - Springer</div><div class="gs_rs">Abstract Image tagging is a task that automatically assigns the query image with semantic <br>keywords called tags, which significantly facilitates image search and organization. Since <br>tags and image visual content are represented in different feature space, how to merge <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4024339810954317437&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:fQ5ld7xT2TcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4024339810954317437&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'fQ5ld7xT2TcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:312"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/81Q648J023045684.pdf" class=yC9B>Towards a universal and limited visual vocabulary</a></h3><div class="gs_a"><a href="/citations?user=QwSTX6EAAAAJ&amp;hl=en&amp;oi=sra">J Hou</a>, ZS Feng, Y Yang, NM Qi - Advances in Visual Computing, 2011 - Springer</div><div class="gs_rs">Bag-of-visual-words is a popular image representation and attains wide application in image <br>processing community. While its potential has been explored in many aspects, its operation <br>still follows a basic mode, namely for a given dataset, using k-means-like clustering <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6250044168646352171&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:K92qpg6evFYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6250044168646352171&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'K92qpg6evFYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:311"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874332" class=yC9C>Towards a universal detector by mining concepts with small semantic gaps</a></h3><div class="gs_a"><a href="/citations?user=Q8iay0gAAAAJ&amp;hl=en&amp;oi=sra">J Feng</a>, Y Zheng, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a> - Proceedings of the international conference on &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Can we have a universal detector that could recognize unseen objects with no <br>training exemplars available? Such a detector is so desirable, as there are hundreds of <br>thousands of object concepts in human vocabulary but few available labeled image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11345425452790866222&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:LmFGs1YMc50J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11345425452790866222&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'LmFGs1YMc50J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:310"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5583296" class=yC9D>Image tag refinement along the &#39;what&#39;dimension using tag categorization and neighbor voting</a></h3><div class="gs_a">S Lee, <a href="/citations?user=VpjWb7wAAAAJ&amp;hl=en&amp;oi=sra">W De Neve</a>, YM Ro - Multimedia and Expo (ICME), 2010 &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Online sharing of images is increasingly becoming popular, resulting in the <br>availability of vast collections of user-contributed images that have been annotated with user-<br>supplied tags. However, user-supplied tags are often not related to the actual image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14333672889554803712&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:AJyPzIxv68YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14333672889554803712&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'AJyPzIxv68YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:309"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB91" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW91"><a href="http://hub.hku.hk/bitstream/10722/125704/1/Content.pdf?accept=1" class=yC9F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hku.hk</span><span class="gs_ggsS">hku.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/8r3712r7v0080p16.pdf" class=yC9E>Transfer discriminative logmaps</a></h3><div class="gs_a"><a href="/citations?user=eAJfUeIAAAAJ&amp;hl=en&amp;oi=sra">S Si</a>, D Tao, KP Chan - &hellip;  in Multimedia Information Processing-PCM 2009, 2009 - Springer</div><div class="gs_rs">Abstract. In recent years, transfer learning has attracted much attention in multimedia. In this <br>paper, we propose an efficient transfer dimensionality reduction algorithm called transfer <br>discriminative Logmaps (TDL). TDL finds a common feature so that 1) the quadratic <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12632101508177351779&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:Y1SW2WQ9Tq8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12632101508177351779&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Y1SW2WQ9Tq8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:308"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB92" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW92"><a href="http://vc.sce.ntu.edu.sg/index_files/1218.pdf" class=yCA1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6126478" class=yCA0>Text-based image retrieval using progressive multi-instance learning</a></h3><div class="gs_a"><a href="/citations?user=yjG4Eg4AAAAJ&amp;hl=en&amp;oi=sra">W Li</a>, <a href="/citations?user=inRIcS0AAAAJ&amp;hl=en&amp;oi=sra">L Duan</a>, D Xu, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">IWH Tsang</a> - Computer Vision (ICCV), 2011 &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Relevant and irrelevant images collected from the Web (eg, Flickr. com) have been <br>employed as loosely labeled training data for image categorization and retrieval. In this <br>work, we propose a new approach to learn a robust classifier for text-based image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8928032819226444165&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:hYEF3Yu-5nsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8928032819226444165&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'hYEF3Yu-5nsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:307"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1873951.1874164" class=yCA2>Automatic image tagging via category label and web data</a></h3><div class="gs_a"><a href="/citations?user=fe-1v0MAAAAJ&amp;hl=en&amp;oi=sra">S Gao</a>, <a href="/citations?user=43G9x0YAAAAJ&amp;hl=en&amp;oi=sra">Z Wang</a>, <a href="/citations?user=Eeolw80AAAAJ&amp;hl=en&amp;oi=sra">LT Chia</a>, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">IWH Tsang</a> - Proceedings of the international  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Image tagging is an important technique for the image content understanding and <br>text based image processing. Given a selection of images, how to tag these images <br>efficiently and effectively is an interesting problem. In this paper, a novel semi-auto image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=562064742947459082&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:CpAn4OfazAcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'CpAn4OfazAcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:306"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB94" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW94"><a href="http://vldb.org/pvldb/vldb2010/pvldb_vol3/D24.pdf" class=yCA4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from vldb.org</span><span class="gs_ggsS">vldb.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1921051" class=yCA3>i AVATAR: an interactive tool for finding and visualizing visual-representative tags in image search</a></h3><div class="gs_a"><a href="/citations?user=wyKGVKUAAAAJ&amp;hl=en&amp;oi=sra">A Sun</a>, <a href="/citations?user=o0F3sqEAAAAJ&amp;hl=en&amp;oi=sra">SS Bhowmick</a>, Y Liu - Proceedings of the VLDB Endowment, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Tags associated with social images are valuable information source for superior <br>image search and retrieval experiences. Due to the nature of tagging, many tags associated <br>with images are not visually descriptive. Consequently, presence of these noisy tags may <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12584751513079310869&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:FeqmA9MEpq4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12584751513079310869&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'FeqmA9MEpq4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:305"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB95" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW95"><a href="http://www.csie.ntu.edu.tw/~cyy/publications/papers/Weng2012CVR.pdf" class=yCA6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2169003" class=yCA5>Collaborative video reindexing via matrix factorization</a></h3><div class="gs_a"><a href="/citations?user=5BeBtpgAAAAJ&amp;hl=en&amp;oi=sra">MF Weng</a>, <a href="/citations?user=uSS3FwQAAAAJ&amp;hl=en&amp;oi=sra">YY Chuang</a> - ACM Transactions on Multimedia Computing,  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Concept-based video indexing generates a matrix of scores predicting the <br>possibilities of concepts occurring in video shots. Based on the idea of collaborative filtering, <br>this article presents unsupervised methods to refine the initial scores generated by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12315205893946737913&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:-QR-BINm6KoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12315205893946737913&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'-QR-BINm6KoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:304"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB96" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW96"><a href="http://www.ee.columbia.edu/~wliu/TSMCB11_calibration.pdf" class=yCA8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5682066" class=yCA7>Distribution Calibration in Riemannian Symmetric Space</a></h3><div class="gs_a"><a href="/citations?user=eAJfUeIAAAAJ&amp;hl=en&amp;oi=sra">S Si</a>, <a href="/citations?user=AjxoEpIAAAAJ&amp;hl=en&amp;oi=sra">W Liu</a>, D Tao, KP Chan - Systems, Man, and Cybernetics,  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Distribution calibration plays an important role in cross-domain learning. However, <br>existing distribution distance metrics are not geodesic; therefore, they cannot measure the <br>intrinsic distance between two distributions. In this paper, we calibrate two distributions by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9688803679880228643&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:Iy_CBZ2LdYYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9688803679880228643&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'Iy_CBZ2LdYYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:303"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/pdf/10.1007/s11042-010-0567-2" class=yCA9>Social image annotation via cross-domain subspace learning</a></h3><div class="gs_a"><a href="/citations?user=eAJfUeIAAAAJ&amp;hl=en&amp;oi=sra">S Si</a>, D Tao, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, KP Chan - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract In recent years, cross-domain learning algorithms have attracted much attention to <br>solve labeled data insufficient problem. However, these cross-domain learning algorithms <br>cannot be applied for subspace learning, which plays a key role in multimedia processing. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6024443664525864448&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:AK5qEZgfm1MJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6024443664525864448&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'AK5qEZgfm1MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:302"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6199988" class=yCAA>Discriminating Joint Feature Analysis for Multimedia Data Understanding</a></h3><div class="gs_a">Z Ma, <a href="/citations?user=2oB4nAIAAAAJ&amp;hl=en&amp;oi=sra">F Nie</a>, <a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, <a href="/citations?user=jInmtEkAAAAJ&amp;hl=en&amp;oi=sra">J Uijlings</a>, <a href="/citations?user=fOmROdkAAAAJ&amp;hl=en&amp;oi=sra">N Sebe</a>&hellip; - 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose a novel semi-supervised feature analyzing framework for <br>multimedia data understanding and apply it to three different applications: image annotation, <br>video concept detection and 3D motion data analysis. Our method is built upon two <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9564380930356098073&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:GTRlKsiBu4QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'GTRlKsiBu4QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:301"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB99" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW99"><a href="http://users.cis.fiu.edu/~lzhen001/activities/KDD2011Program/docs/p1199.pdf" class=yCAC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fiu.edu</span><span class="gs_ggsS">fiu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2020592" class=yCAB>Mining partially annotated images</a></h3><div class="gs_a">Z Qi, M Yang, ZM Zhang, Z Zhang - Proceedings of the 17th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we study the problem of mining partially annotated images. We first <br>define what the problem of mining partially annotated images is, and argue that in many real-<br>world applications annotated images are typically partially annotated and thus that the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12075587384407991607&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:N6XTNb8alacJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12075587384407991607&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'N6XTNb8alacJ')" href="#" class="gs_nph">Cite</a></div></div></div>
