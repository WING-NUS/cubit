Total results = 5
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.ee.columbia.edu/~yjiang/publication/civr10_sampling.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816051" class=yC0>On the sampling of web images for learning visual concept classifiers</a></h3><div class="gs_a">S Zhu, G Wang, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a> - Proceedings of the ACM  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Visual concept learning often requires a large set of training images. In practice, <br>nevertheless, acquiring noise-free training labels with sufficient positive examples is always <br>expensive. A plausible solution for training data collection is by sampling the largely <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11972773823535266513&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5">Cited by 16</a> <a href="/scholar?q=related:0Trqp1vWJ6YJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11972773823535266513&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'0Trqp1vWJ6YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/97390x/201001/32844764.html" class=yC2>äºèç½è·¨åªä½åæä¸æ£ç´¢: çè®ºä¸ç®æ³</a></h3><div class="gs_a">å´é£ï¼ åºè¶æº - è®¡ç®æºè¾å©è®¾è®¡ä¸å¾å½¢å­¦å­¦æ¥, 2010 - cqvip.com</div><div class="gs_rs">äºèç½ä¿¡æ¯ä¸ä»è§æ¨¡å·¨å¤§, èä¸å­å¨çååå¹¿æ³, éç»¼å¤æçäº¤åå³è, åç°è·¨åªä½ç¹æ§. <br>å®ç°äºèç½ä¸­è·¨åªä½æ°æ®çç²¾ç¡®ææä¸æ£ç´¢æ¯é«æå©ç¨äºèç½èµæºè¿«åéè¦è§£å³é®é¢. <br>æä¸­å¯¹è¯¥é¢åææ¶åçäººèè®¤ç¥è·¨åªä½ç¹æ§, å¤åªä½æ£ç´¢æ æ³¨ä¸è¯­ä¹çè§£, è·¨åªä½è¡¨è¾¾ä¸è¯å«, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16479291768950520932&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5">Cited by 11</a> <a href="/scholar?q=related:ZHxQho00suQJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16479291768950520932&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ZHxQho00suQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://newdesign.aclweb.org/anthology/C/C10/C10-2149.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aclweb.org</span><span class="gs_ggsS">aclweb.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1944715" class=yC3>Automatic generation of semantic fields for annotating web images</a></h3><div class="gs_a">G Wang, TS Chua, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, YC Wang - Proceedings of the 23rd  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract The overwhelming amounts of multimedia contents have triggered the need for <br>automatically detecting the semantic concepts within the media contents. With the <br>development of photo sharing websites such as Flickr, we are able to obtain millions of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16309016145247876043&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5">Cited by 1</a> <a href="/scholar?q=related:yzOcdcZDVeIJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16309016145247876043&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'yzOcdcZDVeIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www.cs.cmu.edu/~caiyang/papers/med-ma.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2393414" class=yC5>Knowledge adaptation for ad hoc multimedia event detection with few exemplars</a></h3><div class="gs_a">Z Ma, <a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, <a href="/citations?user=KzJWndQAAAAJ&amp;hl=en&amp;oi=sra">Y Cai</a>, <a href="/citations?user=fOmROdkAAAAJ&amp;hl=en&amp;oi=sra">N Sebe</a>&hellip; - Proceedings of the 20th  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Multimedia event detection (MED) has a significant impact on many applications. <br>Though video concept annotation has received much research effort, video event detection <br>remains largely unaddressed. Current research mainly focuses on sports and news event <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2615067704466082346&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=5">Cited by 1</a> <a onclick="return gs_ocit(event,'KgpM1DiWSiQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2168755" class=yC7>Robust Video Content Analysis via Transductive Learning</a></h3><div class="gs_a">R Ewerth, M MÃ¼hling, B Freisleben - ACM Transactions on Intelligent  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Reliable video content analysis is an essential prerequisite for effective video <br>search. An important current research question is how to develop robust video content <br>analysis methods that produce satisfactory results for a large variety of video sources, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:yU_UBTUZHQkJ:scholar.google.com/&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=656708836187721673&amp;hl=en&amp;num=5&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'yU_UBTUZHQkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
