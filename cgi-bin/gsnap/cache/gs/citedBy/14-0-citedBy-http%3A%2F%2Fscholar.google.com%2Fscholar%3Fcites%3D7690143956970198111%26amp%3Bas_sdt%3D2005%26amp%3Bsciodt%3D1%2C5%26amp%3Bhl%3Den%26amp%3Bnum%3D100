Total results = 14
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://webhome.cs.uvic.ca/~gtzan/work/pubs/tsalp2008gtzan.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uvic.ca</span><span class="gs_ggsS">uvic.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432646" class=yC0>Normalized cuts for predominant melodic source separation</a></h3><div class="gs_a"><a href="/citations?user=Z-D1LQwAAAAJ&amp;hl=en&amp;oi=sra">M Lagrange</a>, <a href="/citations?user=hj8OIrcAAAAJ&amp;hl=en&amp;oi=sra">LG Martins</a>, J Murdoch&hellip; - Audio, Speech, and  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The predominant melodic source, frequently the singing voice, is an important <br>component of musical signals. In this paper, we describe a method for extracting the <br>predominant source and corresponding melody from ldquoreal-worldrdquo polyphonic <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1806671621810067218&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 32</a> <a href="/scholar?q=related:EpfEk0SWEhkJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/1A/3B/RN222855380.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=1806671621810067218&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'EpfEk0SWEhkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://users.cis.fiu.edu/~lli003/Music/clu/4.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fiu.edu</span><span class="gs_ggsS">fiu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://users.cis.fiu.edu/~lli003/Music/clu/4.pdf" class=yC3>Polyphonic instrument recognition using spectral clustering</a></h3><div class="gs_a"><a href="/citations?user=hj8OIrcAAAAJ&amp;hl=en&amp;oi=sra">LG Martins</a>, JJ Burred, <a href="/citations?user=yPgxxpwAAAAJ&amp;hl=en&amp;oi=sra">G Tzanetakis</a>&hellip; - &hellip;  Conference on Music  &hellip;, 2007 - users.cis.fiu.edu</div><div class="gs_rs">ABSTRACT The identification of the instruments playing in a polyphonic music signal is an <br>important and unsolved problem in Music Information Retrieval. In this paper, we propose a <br>framework for the sound source separation and timbre classification of polyphonic, multi-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12006125889840478544&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 20</a> <a href="/scholar?q=related:UPmPuOJTnqYJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12006125889840478544&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 23 versions</a> <a onclick="return gs_ocit(event,'UPmPuOJTnqYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:UPmPuOJTnqYJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.ee.kth.se/php/modules/publications/reports/2004/sriram-ICASSP-04.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kth.se</span><span class="gs_ggsS">kth.se <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1326826" class=yC5>Auditory blobs</a></h3><div class="gs_a">SH Srinivasan - &hellip; ICASSP&#39;04). IEEE International Conference on, 2004 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Auditory scene analysis (ASA) tries to segment an auditory signal (scene) into <br>objects. Most of the intermediate representations currently proposed based on ASA are <br>difficult to compute. We propose auditory strands and blobs as intermediate <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7370362223093864695&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 18</a> <a href="/scholar?q=related:99h3XWTJSGYJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7370362223093864695&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 34 versions</a> <a onclick="return gs_ocit(event,'99h3XWTJSGYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://ispl.korea.ac.kr/conference/ICASSP2004/pdfs/0400321.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from korea.ac.kr</span><span class="gs_ggsS">korea.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1326828" class=yC7>Harmonicity and dynamics-based features for audio</a></h3><div class="gs_a">H Srinivasan, M Kankanhalli - Acoustics, Speech, and Signal  &hellip;, 2004 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Features are very important for audio processing. Tasks like speech recognition <br>and instrument identification are based on features. Most low-level features currently used <br>are based on LPC and cepstral analysis. We propose a class of features based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7430893446913178125&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 15</a> <a href="/scholar?q=related:DboJKzjWH2cJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7430893446913178125&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'DboJKzjWH2cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4258809" class=yC9>Transform domain steganography in DVD video and audio content</a></h3><div class="gs_a">S Badura, S Rymaszewski - Imaging Systems and Techniques,  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper is dealing with steganography in DVD content. Classical steganography <br>concerns itself on ways of embedding a secret message in a cover message such as a video <br>stream or audio recording. The embedding is typically parameterized by a key. Without <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2610282143209148520&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 6</a> <a href="/scholar?q=related:aNCGhceVOSQJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'aNCGhceVOSQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://recherche.ircam.fr/equipes/analyse-synthese/lagrange/research/papers/lagrangeAes07.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ircam.fr</span><span class="gs_ggsS">ircam.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.aes.org/e-lib/browse.cfm?elib=14027" class=yCA>Semi-automatic mono to stereo up-mixing using sound source formation</a></h3><div class="gs_a"><a href="/citations?user=Z-D1LQwAAAAJ&amp;hl=en&amp;oi=sra">M Lagrange</a>, <a href="/citations?user=hj8OIrcAAAAJ&amp;hl=en&amp;oi=sra">LG Martins</a>, <a href="/citations?user=yPgxxpwAAAAJ&amp;hl=en&amp;oi=sra">G Tzanetakis</a> - Watermark, 2012 - aes.org</div><div class="gs_rs">In this paper, we propose an original method to include spatial panning information when <br>converting monophonic recordings to stereophonic ones. Sound sources are first identified <br>using perceptively motivated clustering of spectral components. Correlations between <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1689953913514867938&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 3</a> <a href="/scholar?q=related:4rzVbSHscxcJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1689953913514867938&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'4rzVbSHscxcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://hal.archives-ouvertes.fr/docs/00/66/23/23/PDF/regnier_2010_1.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5495744" class=yCC>Partial clustering using a time-varying frequency model for singing voice detection</a></h3><div class="gs_a">L Regnier, G Peeters - Acoustics Speech and Signal  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We propose a new method to group partials produced by each instrument of a <br>polyphonic audio mixture. This method works for pitched and harmonic instruments and is <br>specially adapted to singing voice. In our approach, we model time-varying frequencies of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3755242542120970524&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 4</a> <a href="/scholar?q=related:HCmOKDJNHTQJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3755242542120970524&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'HCmOKDJNHTQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.cs.uvic.ca/~gtzan/work/pubs/icassp08gtzan.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uvic.ca</span><span class="gs_ggsS">uvic.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4517572" class=yCE>A computationally efficient scheme for dominant harmonic source separation</a></h3><div class="gs_a"><a href="/citations?user=Z-D1LQwAAAAJ&amp;hl=en&amp;oi=sra">M Lagrange</a>, <a href="/citations?user=hj8OIrcAAAAJ&amp;hl=en&amp;oi=sra">LG Martins</a>&hellip; - Acoustics, Speech and  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The leading voice is an important feature of musical pieces and can often be <br>considered as the dominant harmonic source. We propose in this paper a new scheme for <br>the purpose of efficient dominant harmonic source separation. This is achieved by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13628615979528599817&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 2</a> <a href="/scholar?q=related:CbkDoAaSIr0J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13628615979528599817&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'CbkDoAaSIr0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.esenf.ipsantarem.pt/WEB-Esenfs/cursos/ANEXO%20II%20CPLEE%20COMUNITARIA.pdf" class=yC10>J5W4</a></h3><div class="gs_a">CGPSE SERIAÃÃO, DOS CANDIDATOS - EducaÃ§Ã£o, 2009 - esenf.ipsantarem.pt</div><div class="gs_rs">Â«J5W4Â» 44-Ã¯ INSTITUTO POLITECNICO DE SANTARÃM ESCOLA SUPERIOR DE SAÃDE DE <br>SANTARÃM 4 ANEXO II Io. CURSO DE PÃS-LICENCIATURA DE ESPECIALIZAÃÃO EM ENFERMAGEM <br>COMUNITÃRIA (Portaria nÂ° 245 / 2009, de 6 de MArÃ§o) CRITÃRIOS GERAIS PARA <b> ...</b> </div><div class="gs_fl"><a href="/scholar?cites=15867700513579987182&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 3</a> <a href="/scholar?q=related:7vA34otlNdwJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15867700513579987182&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'7vA34otlNdwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://webhome.cs.uvic.ca/~lagrange/papers/lagrangeDafx07.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uvic.ca</span><span class="gs_ggsS">uvic.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://webhome.cs.uvic.ca/~lagrange/papers/lagrangeDafx07.pdf" class=yC11>Adaptive harmonization and pitch correction of polyphonic audio using spectral clustering</a></h3><div class="gs_a"><a href="/citations?user=Z-D1LQwAAAAJ&amp;hl=en&amp;oi=sra">M Lagrange</a>, <a href="/citations?user=xCnDkXAAAAAJ&amp;hl=en&amp;oi=sra">G Percival</a>&hellip; - Proceedings of the  &hellip;, 2007 - webhome.cs.uvic.ca</div><div class="gs_rs">ABSTRACT There are several well known harmonization and pitch correction techniques <br>that can be applied to monophonic sound sources. They are based on automatic pitch <br>detection and frequency shifting without time stretching. In many applications it is desired <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13852470033679903033&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 1</a> <a href="/scholar?q=related:Oek7IxrcPcAJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13852470033679903033&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'Oek7IxrcPcAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Oek7IxrcPcAJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7974420&amp;id=ZWXnAQAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC13>Mixed audio separation apparatus</a></h3><div class="gs_a">S Yoshizawa, T Suzuki, Y Nakatoh - US Patent 7,974,420, 2011 - Google Patents</div><div class="gs_rs">A mixed audio separation system (100) which separates a specific audio from among a <br>mixed audio (S100) includes a local frequency information generation unit (105) which <br>obtains pieces of local frequency information (S103) corresponding to local reference <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18117165759416160343&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 1</a> <a href="/scholar?q=related:V9De8kQabfsJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18117165759416160343&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'V9De8kQabfsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://etheses.whiterose.ac.uk/1504/1/Siamantas09_thesis_final.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from whiterose.ac.uk</span><span class="gs_ggsS">whiterose.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://etheses.whiterose.ac.uk/1504/" class=yC14>An iterative, residual-based approach to unsupervised musical source separation in single-channel mixtures</a></h3><div class="gs_a">G Siamantas - 2009 - etheses.whiterose.ac.uk</div><div class="gs_rs">This thesis concentrates on a major problem within audio signal processing, the separation <br>of source signals from musical mixtures when only a single mixture channel is available. <br>Source separation is the process by which signals that correspond to distinct sources are <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jMeYLu2Gmy8J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3430483894419900300&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'jMeYLu2Gmy8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://recherche.ircam.fr/equipes/analyse-synthese/lagrange/research/papers/lagrangeNips06.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ircam.fr</span><span class="gs_ggsS">ircam.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://recherche.ircam.fr/equipes/analyse-synthese/lagrange/research/papers/lagrangeNips06.pdf" class=yC16>Temporal Constraints for Sound Source Formation using the Normalized Cut</a></h3><div class="gs_a"><a href="/citations?user=Z-D1LQwAAAAJ&amp;hl=en&amp;oi=sra">M Lagrange</a>, J Murdoch, <a href="/citations?user=yPgxxpwAAAAJ&amp;hl=en&amp;oi=sra">G Tzanetakis</a> - 2008 - recherche.ircam.fr</div><div class="gs_rs">Abstract In this paper, we explore the use of a graph algorithm called the normalized cut in <br>order to organize prominent components of the auditory scene. We focus specifically on <br>defining a time-constrained similarity metric. We show that such a metric can be <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:KhzuOrjw3JkJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11087001056732650538&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'KhzuOrjw3JkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md12', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md12" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:KhzuOrjw3JkJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> POLYPHONIC INSTRUMENT RECOGNITION USING SPECTRAL CLUSTERING</h3><div class="gs_a">LGMJJ Burred, <a href="/citations?user=yPgxxpwAAAAJ&amp;hl=en&amp;oi=sra">G Tzanetakis</a>&hellip; - &hellip;  2007: Proceedings of &hellip;, 2007 - Austrian Computer Society</div><div class="gs_fl"><a href="/scholar?q=related:XDtpvG6C7GQJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'XDtpvG6C7GQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
