Total results = 13
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://lms.comp.nus.edu.sg/papers/media/2010/mm10-gaoyue.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1873970" class=yC0>W2go: a travel guidance system by automatic landmark ranking</a></h3><div class="gs_a">Y Gao, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, Q Dai, TS Chua&hellip; - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we present a travel guidance system W2Go (Where to Go), which can <br>automatically recognize and rank the landmarks for travellers. In this system, a novel <br>Automatic Landmark Ranking (ALR) method is proposed by utilizing the tag and geo-tag <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14138977723431265510&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 20</a> <a href="/scholar?q=related:5iRI9FO9N8QJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14138977723431265510&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'5iRI9FO9N8QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://vireo.cs.cityu.edu.hk/papers/beyond%20search%20event%20driven%20summarization%20for%20web%20videos_acmtmm10.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2043613" class=yC2>Beyond search: Event-driven summarization for web videos</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, HK Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - ACM Transactions on  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract The explosive growth of Web videos brings out the challenge of how to efficiently <br>browse hundreds or even thousands of videos at a glance. Given an event-driven query, <br>social media Web sites usually return a large number of videos that are diverse and noisy <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4403112343352155411&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 13</a> <a href="/scholar?q=related:E52k-1j_Gj0J:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4403112343352155411&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'E52k-1j_Gj0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/civr10-hongrichang.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816055" class=yC4>Exploring large scale data for multimedia QA: an initial study</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, G Li, L Nie, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, TS Chua - Proceedings of the ACM  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract With the explosive growth of multimedia contents on the internet, multimedia search <br>has become more and more important. However, users are often bewildered by the vast <br>quantity of information content returned by the search engines. In this scenario, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4416393598065709541&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 8</a> <a href="/scholar?q=related:5REEEZQuSj0J:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4416393598065709541&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'5REEEZQuSj0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://137.132.145.151/lms/sites/default/files/mm10-richang2.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874033" class=yC6>Movie2comics: a feast of multimedia artwork</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">XT Yuan</a>, M Xu, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract As a type of artwork, comics are prevalent and popular around the world. However, <br>although there are several assistive software and tools available, the creation of comics is <br>still a tedious and labor intensive process. This paper proposes a scheme that is able to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14261038913768144478&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 7</a> <a href="/scholar?q=related:Xj4sd1Vj6cUJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14261038913768144478&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Xj4sd1Vj6cUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874120" class=yC8>Representative views re-ranking for 3D model retrieval with multi-bipartite graph reinforcement model</a></h3><div class="gs_a">Y Gao, Y Yang, Q Dai, N Zhang - Proceedings of the international  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose a multi-bipartite graph reinforcement model for <br>representative views re-ranking in 3D model retrieval. Given the views of one query 3D <br>model, all query views are grouped into clusters to generate representative views and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15528047021886435910&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 4</a> <a href="/scholar?q=related:RnIay360ftcJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15528047021886435910&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'RnIay360ftcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874014" class=yC9>The third eye: mining the visual cognition across multi-language communities</a></h3><div class="gs_a">C Liu, Q Huang, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, C Xu - Proceedings of the international  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Existing research work in the multimedia domain mainly focuses on image/video <br>indexing, retrieval, annotation, tagging, re-ranking, etc. However, little work has been <br>contributed to people&#39;s visual cognition. In this paper, we propose a novel framework to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3219043812031669782&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 3</a> <a href="/scholar?q=related:FiaRtEpXrCwJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'FiaRtEpXrCwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://luzheng.org/papers/VideoAder_hu_icimcs2011.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from luzheng.org</span><span class="gs_ggsS">luzheng.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2043683" class=yCA>Videoader: a video advertising system based on intelligent analysis of visual content</a></h3><div class="gs_a">J Hu, G Li, <a href="/citations?user=Qz_zAEUAAAAJ&amp;hl=en&amp;oi=sra">Z Lu</a>, J Xiao, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a> - Proceedings of the Third International  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Recent years have witnessed the prevalence of context based video advertisement. <br>However, those advertisement systems solely take the metadata into account, such as titles, <br>descriptions and tags. In this paper, we present a novel video advertising system called <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MNCeA-0fUWEJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7012421197621022768&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'MNCeA-0fUWEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212009046" class=yCC>Richang Hong, Linxie Tang, Jun Hu, Guanda Li, Jiang-Guo Jiang</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, L Tang, J Hu, G Li, JG Jiang - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract We have witnessed the booming of contextual video advertising recent years. <br>However, those advertisement systems solely take the metadata into account, such as titles, <br>descriptions and tags. This kind of text-based contextual advertising reveals a number of <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'qWujGb64isAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6135507" class=yCD>Event Driven Web Video Summarization by Tag Localization and Key-Shot Identification</a></h3><div class="gs_a"><a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, G Li, ZJ Zha, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - &hellip; , IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the explosive growth of web videos on the Internet, it becomes challenging to <br>efficiently browse hundreds or even thousands of videos. When searching an event query, <br>users are often bewildered by the vast quantity of web videos returned by search engines. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17159319909387631227&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 6</a> <a href="/scholar?q=related:e3pWKosmIu4J:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'e3pWKosmIu4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5673753" class=yCE>A Collaborative Approach for Image Annotation</a></h3><div class="gs_a">F Sun, Y Ge, D Wang, X Wang - Image and Video Technology ( &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic image annotation is a promising solution to enable more effective image <br>retrieval by keywords. Different statistical models and machine learning methods have been <br>introduced for image auto-annotation. In this paper, we propose a collaborative approach, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:28RkbF_hARkJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1801969125881595099&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'28RkbF_hARkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5999642" class=yCF>Multimedia Question Answering</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, M Wang, G Li, Z Zha, T Chua - 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Recent explosive growth of multimedia content on the Web has led to the popularity <br>and proliferation of search technology. However, faced the vast quantity of information <br>content returned by search engines, users are often bewildered and have to painstakingly <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3052222141799642610&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 1</a> <a href="/scholar?q=related:8rWM2d2rWyoJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3052222141799642610&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'8rWM2d2rWyoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/83337436K3G00608.pdf" class=yC10>A novel framework for concept detection on large scale video database and feature pool</a></h3><div class="gs_a">G Lv, C Zheng - Artificial Intelligence Review, 2011 - Springer</div><div class="gs_rs">Abstract Large-scale semantic concept detection from large video database suffers from <br>large variations among different semantic concepts as well as their corresponding effective <br>low-level features. In this paper, we propose a novel framework to deal with this obstacle. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:LjG79W-vzQ8J:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'LjG79W-vzQ8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://japanlinkcenter.org/JST.JSTAGE/iieej/39.924?from=Google" class=yC11>6-12 ç»åãã¼ã¿ãã¼ã¹</a></h3><div class="gs_a">å¤ç°æè£ï¼ å è¤ä¿ä¸ - ç»åé»å­å­¦ä¼èª, 2010 - J-STAGE</div><div class="gs_rs">ç»åãã¼ã¿ãã¼ã¹ã»ç»åæ¤ç´¢ã¯,ãç»åããæ½åºãããç¹å¾´éãã­ã¼ã¨ãã¦ç»åãç®¡çã, <br>ãããæ¤ç´¢ããææ³ã ã¨,ãç»åã«ä»ä¸ãããä»å çãªæå ±ãã­ã¼ã¨ãã¦ç»åãç®¡çã, <br>ãããæ¤ç´¢ããææ³ã ãåºç¤æè¡ã¨ãã¦ãã, ãã®ä¸¡èããã¾ãèåãããã¨, ã¤ã¾ã, ã¤ã³ã¿ã¼ãã£<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:mTyj5VH0RKcJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12053027135332498585&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'mTyj5VH0RKcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
