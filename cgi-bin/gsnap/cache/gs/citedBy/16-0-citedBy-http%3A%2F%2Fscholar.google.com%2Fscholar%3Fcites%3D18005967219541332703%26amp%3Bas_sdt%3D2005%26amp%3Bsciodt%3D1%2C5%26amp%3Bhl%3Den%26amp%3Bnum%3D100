Total results = 16
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.5031&amp;rep=rep1&amp;type=pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1576260" class=yC0>Concept-based video retrieval</a></h3><div class="gs_a"><a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Foundations and Trends in Information  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we review 300 references on video retrieval, indicating when text-only <br>solutions are unsatisfactory and showing the promising alternatives which are in majority <br>concept-based. Therefore, central to our discussion is the notion of a semantic concept: an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1240556430566916602&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=16">Cited by 140</a> <a href="/scholar?q=related:-oHEN4BXNxEJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1240556430566916602&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'-oHEN4BXNxEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md0', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md0" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:-oHEN4BXNxEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=868717410844952179&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv8.papers/mcg-ict-cas.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv8.papers/mcg-ict-cas.pdf" class=yC2>TRECVID 2008 high-level feature extraction by MCG-ICT-CAS</a></h3><div class="gs_a">S Tang, JT Li, M Li, C Xie, YZ Liu, K Tao&hellip; - Proc. TRECVID  &hellip;, 2008 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT For TRECVID 2008 concept detection task, we principally focus on:(1) Early <br>fusion of texture, edge and color features TECM, abbreviation of the combined TF* IDF <br>weights based on SIFT features, Edge Histogram, and Color Moments.(2) To improve the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9754870363319714301&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=16">Cited by 34</a> <a href="/scholar?q=related:_UGoc-lCYIcJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9754870363319714301&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'_UGoc-lCYIcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_UGoc-lCYIcJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://staff.science.uva.nl/~cgmsnoek/pub/rooij-thread-based-navigation-civr2008.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1386352.1386414" class=yC4>Balancing thread based navigation for targeted video search</a></h3><div class="gs_a"><a href="/citations?user=znjsUsYAAAAJ&amp;hl=en&amp;oi=sra">O De Rooij</a>, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Proceedings of the 2008  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Various query methods for video search exist. Because of the semantic gap each <br>method has its limitations. We argue that for effective retrieval query methods need to be <br>combined at retrieval time. However, switching query methods often involves a change in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14318453898070121584&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=16">Cited by 16</a> <a href="/scholar?q=related:cNxGN_VdtcYJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14318453898070121584&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'cNxGN_VdtcYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www.dcs.bbk.ac.uk/~sjmaybank/survey%20video%20indexing.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bbk.ac.uk</span><span class="gs_ggsS">bbk.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5729374" class=yC6>A survey on visual content-based video indexing and retrieval</a></h3><div class="gs_a">W Hu, N Xie, L Li, X Zeng&hellip; - Systems, Man, and  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video indexing and retrieval have a wide spectrum of promising applications, <br>motivating the interest of researchers worldwide. This paper offers a tutorial and an overview <br>of the landscape of general strategies in visual content-based video indexing and retrieval<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5775956488998965554&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=16">Cited by 24</a> <a href="/scholar?q=related:MlHlDNNRKFAJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5775956488998965554&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'MlHlDNNRKFAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www.science.uva.nl/research/publications/2010/deRooijITM2010/deRooij_RotorBrowser2010.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5340554" class=yC8>Browsing video along multiple threads</a></h3><div class="gs_a"><a href="/citations?user=znjsUsYAAAAJ&amp;hl=en&amp;oi=sra">O De Rooij</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Multimedia, IEEE Transactions on, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes a novel method for browsing a large video collection. It links <br>various forms of related video fragments together as threads. These threads are based on <br>query results, the timeline as well as visual and semantic similarity. We design two <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3141577060277481148&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=16">Cited by 11</a> <a href="/scholar?q=related:vO5aCLEfmSsJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3141577060277481148&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'vO5aCLEfmSsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://lms.comp.nus.edu.sg/papers/media/2008/civr08-luan.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1386352.1386411" class=yCA>Adaptive multiple feedback strategies for interactive video search</a></h3><div class="gs_a">H Luan, Y Zheng, SY Neo, Y Zhang, S Lin&hellip; - Proceedings of the 2008 &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose adaptive multiple feedback strategies for interactive video <br>retrieval. We first segregate interactive feedback into 3 distinct types (recall-driven relevance <br>feedback, precision-driven active learning and locality-driven relevance feedback) so that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12517552171279368847&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=16">Cited by 8</a> <a href="/scholar?q=related:j8KGAmFHt60J:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12517552171279368847&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'j8KGAmFHt60J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv7.papers/ict_nus.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv7.papers/ict_nus.pdf" class=yCC>Trecvid 2007 search tasks by nus-ict</a></h3><div class="gs_a">TS Chua, SY Neo, YT Zheng, HK Goh&hellip; - TRECVID  &hellip;, 2007 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT This paper describes the details of our systems for our automated and <br>interactive search in TRECVID 2007. The shift from news video to documentary video this <br>year has prompted a series of changes in processing techniques from that developed over <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=173529439328118389&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=16">Cited by 6</a> <a href="/scholar?q=related:daY5DRuAaAIJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=173529439328118389&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'daY5DRuAaAIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md6', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md6" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:daY5DRuAaAIJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yCE>TRECVID 2010 Known-item Search by NUS</a></h3><div class="gs_a">XY Chen, J Yuan, L Nie, ZJ Zha, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - TRECVID  &hellip;, 2010 - www-nlpir.nist.gov</div><div class="gs_rs">Abstract. This paper describes our system for auto search and interactive search in the <br>known-item search (KIS) task in TRECVID 2010. KIS task aims to find an unique video <br>answer for each text query. The shift from traditional video search has prompted a series <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12664714192218118309&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=16">Cited by 3</a> <a href="/scholar?q=related:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'pVgCEXUawq8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.185.1903&amp;rep=rep1&amp;type=pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/YPK1127L2261P326.pdf" class=yC10>Size matters! How thumbnail number, size, and motion influence mobile video retrieval</a></h3><div class="gs_a">W HÃ¼rst, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">C Snoek</a>, WJ Spoel, M Tomin - Advances in Multimedia Modeling, 2011 - Springer</div><div class="gs_rs">Various interfaces for video browsing and retrieval have been proposed that provide <br>improved usability, better retrieval performance, and richer user experience compared to <br>simple result lists that are just sorted by relevance. These browsing interfaces take <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=404889215682760585&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=16">Cited by 4</a> <a href="/scholar?q=related:iU8osph0ngUJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=404889215682760585&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'iU8osph0ngUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/VisionGo-%20Towards%20video%20retrieval%20with%20joint%20exploration%20of%20human%20and%20computer.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0020025511002672" class=yC12>VisionGo: towards video retrieval with joint exploration of human and computer</a></h3><div class="gs_a">H Luan, YT Zheng, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, TS Chua - Information Sciences, 2011 - Elsevier</div><div class="gs_rs">Abstract This paper introduces an effective interactive video retrieval system named <br>VisionGo. It jointly explores human and computer to accomplish video retrieval with high <br>effectiveness and efficiency. It assists the interactive video retrieval process in different <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15423134762805473061&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=16">Cited by 2</a> <a href="/scholar?q=related:Ja__-1n7CdYJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15423134762805473061&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Ja__-1n7CdYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://mcg.ict.ac.cn/download/pdf/paper/2008/29.VisionGo%20A%20High-performance%20and%20Multifunctional.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mcg.ict.ac.cn/download/pdf/paper/2008/29.VisionGo%20A%20High-performance%20and%20Multifunctional.pdf" class=yC14>VisionGo: A High-performance and Multi-functional Interactive Video Retrieval System</a></h3><div class="gs_a">H Luan, S Lin, Y Zhang, SY Neo, TS Chua - mcg.ict.ac.cn</div><div class="gs_rs">Abstract One of the most critical tasks in multimedia retrieval is the interactive video search. <br>This paper proposes an effective interactive video retrieval system named VisionGo to <br>improve the overall search performance. VisionGo provides three key functions to assist <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'D5KOBLqaupIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:D5KOBLqaupIJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://vireo.cs.cityu.edu.hk/papers/mm11-tan.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072331" class=yC16>Cross media hyperlinking for search topic browsing</a></h3><div class="gs_a">S Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, HK Tan, L Pang - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract With the rapid growth of social media, there are plenty of information sources freely <br>available online for use. Nevertheless, how to synchronize and leverage these diverse forms <br>of information for multimedia applications remains a problem yet to be seriously studied. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:uHuya3a24vIJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17501751721644424120&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'uHuya3a24vIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://www.willfulwreckords.com/GinsuScience/CVPR2012/data/papers/workshops/W17_07.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from willfulwreckords.com</span><span class="gs_ggsS">willfulwreckords.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6239341" class=yC18>A consumer video search system by audio-visual concept classification</a></h3><div class="gs_a">W Jiang, AC Loui, P Lei - Computer Vision and Pattern  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Indexing and searching the massive amount of consumer videos in the open <br>domain is increasingly important. Due to the lack of text descriptions as well as the <br>difficulties in analyzing the content of consumer videos, little work has been conducted to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:OaDlyW8EwaEJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11655602188786704441&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'OaDlyW8EwaEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://proxy.azgor.com/www.science.uva.nl/research/publications/2012/deRooijTMCCA2012/deRooij_TOMCCAP_ForkBrowser2012.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from azgor.com</span><span class="gs_ggsS">azgor.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://proxy.azgor.com/www.science.uva.nl/research/publications/2012/deRooijTMCCA2012/deRooij_TOMCCAP_ForkBrowser2012.pdf" class=yC1A>Efficient Targeted Search Using a Focus and Context Video Browser</a></h3><div class="gs_a"><a href="/citations?user=znjsUsYAAAAJ&amp;hl=en&amp;oi=sra">ORK DE ROOIJ</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M WORRING</a> - proxy.azgor.com</div><div class="gs_rs">Currently there are several interactive content-based video retrieval techniques and systems <br>available. However, retrieval performance depends heavily on the means of interaction. We <br>argue that effective CBVR requires efficient, specialized user interfaces. In this article we <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'DBdo7aDDu9EJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md13', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md13" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:DBdo7aDDu9EJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/16051/thesis-sub.pdf?sequence=1" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/16051" class=yC1C>combining multimodal external resources for event-based news video retrieval and question answering</a></h3><div class="gs_a">NEOSHI YONG - 2008 - scholarbank.nus.edu</div><div class="gs_rs">The ever-increasing amount of multimedia data available online creates an urgent need on <br>how to index these information and support effective retrieval by users. In recent years, we <br>observe the gradual shift from performing retrieval solely based on analyzing one media <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:h5EV9TPOgHUJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8466994022007017863&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'h5EV9TPOgHUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/QG464XP786T36T73.pdf" class=yC1E>User Feedback for Improving Question Categorization in Web-Based Question Answering Systems</a></h3><div class="gs_a">W Song, L Wenyin, N Gu, X Quan - &hellip; in Web and Network Technologies, and &hellip;, 2009 - Springer</div><div class="gs_rs">Abstract. Question categorization, which automatically suggests a few categories to host a <br>user&#39;s question, is a useful technique in Web-based question answering systems. In this <br>paper, we propose a question categorization method which makes use of user feedback <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:WpPz_qFcvPEJ:scholar.google.com/&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17418899309599101786&amp;hl=en&amp;num=16&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'WpPz_qFcvPEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
