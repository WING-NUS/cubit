Total results = 60
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://brutal.googlecode.com/svn/trunk/Adding%20Semantics%20to%20Detectors%20for%20Video%20Retrieval.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from googlecode.com</span><span class="gs_ggsS">googlecode.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4276717" class=yC0>Adding semantics to detectors for video retrieval</a></h3><div class="gs_a"><a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, B Huurnink, <a href="/citations?user=T6b4scsAAAAJ&amp;hl=en&amp;oi=sra">L Hollink</a>&hellip; - Multimedia, IEEE  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose an automatic video retrieval method based on high-level <br>concept detectors. Research in video analysis has reached the point where over 100 <br>concept detectors can be learned in a generic fashion, albeit with mixed performance. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7315134674580442679&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 126</a> <a href="/scholar?q=related:N46CsjqUhGUJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/39/51/RN213262190.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=7315134674580442679&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 26 versions</a> <a onclick="return gs_ocit(event,'N46CsjqUhGUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.5031&amp;rep=rep1&amp;type=pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1576260" class=yC3>Concept-based video retrieval</a></h3><div class="gs_a"><a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Foundations and Trends in Information  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we review 300 references on video retrieval, indicating when text-only <br>solutions are unsatisfactory and showing the promising alternatives which are in majority <br>concept-based. Therefore, central to our discussion is the notion of a semantic concept: an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1240556430566916602&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 140</a> <a href="/scholar?q=related:-oHEN4BXNxEJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1240556430566916602&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'-oHEN4BXNxEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:-oHEN4BXNxEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=868717410844952179&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.ee.columbia.edu/~winston/papers/rerank06h.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1180654" class=yC5>Video search reranking via information bottleneck principle</a></h3><div class="gs_a"><a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a>, LS Kennedy, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Proceedings of the 14th annual ACM  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract We propose a novel and generic video/image reranking algorithm, IB reranking, <br>which reorders results from text-only searches by discovering the salient visual patterns of <br>relevant and irrelevant shots from the approximate relevance provided by text results. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11409587040090823489&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 123</a> <a href="/scholar?q=related:QZuDXvb-Vp4J:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11409587040090823489&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'QZuDXvb-Vp4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www.ee.columbia.edu/dvmm/publications/07/hsu07video.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291446" class=yC7>Video search reranking through random walk over document-level context graph</a></h3><div class="gs_a"><a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a>, LS Kennedy, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Proceedings of the 15th international  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Multimedia search over distributed sources often result in recurrent images or <br>videos which are manifested beyond the textual modality. To exploit such contextual <br>patterns and keep the simplicity of the keyword-based search, we propose novel <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16025218997205144395&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 119</a> <a href="/scholar?q=related:SxNB0MUDZd4J:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16025218997205144395&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'SxNB0MUDZd4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www.mirlab.org/conference_papers/International_Conference/ACM%202005/docs/mm598.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101288" class=yC9>Learning the semantics of multimedia queries and concepts from a small number of examples</a></h3><div class="gs_a"><a href="/citations?user=Ade_7YoAAAAJ&amp;hl=en&amp;oi=sra">AP Natsev</a>, MR Naphade, J TeÅ¡iÄ - Proceedings of the 13th annual ACM &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract In this paper we unify two supposedly distinct tasks in multimedia retrieval. One task <br>involves answering queries with a few examples. The other involves learning models for <br>semantic concepts, also with a few examples. In our view these two tasks are identical <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8482438592720975995&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 112</a> <a href="/scholar?q=related:e7gyTPWst3UJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8482438592720975995&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'e7gyTPWst3UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.8546&amp;rep=rep1&amp;type=pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1460126" class=yCB>Learning tag relevance by neighbor voting for social image retrieval</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Proceedings of the 1st ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Social image retrieval is important for exploiting the increasing amounts of amateur-<br>tagged multimedia such as Flickr images. Since amateur tagging is known to be <br>uncontrolled, ambiguous, and personalized, a fundamental problem is how to reliably <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7054374523984623744&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 97</a> <a href="/scholar?q=related:gDir_kAs5mEJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7054374523984623744&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'gDir_kAs5mEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.175.759&amp;rep=rep1&amp;type=pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5208328" class=yCD>Learning social tag relevance by neighbor voting</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Multimedia, IEEE Transactions  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Social image analysis and retrieval is important for helping people organize and <br>access the increasing amount of user tagged multimedia. Since user tagging is known to be <br>uncontrolled, ambiguous, and overly personalized, a fundamental problem is how to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9432624991788059948&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 97</a> <a href="/scholar?q=related:LM3_LnRq54IJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9432624991788059948&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'LM3_LnRq54IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.comp.nus.edu.sg/~kanmy/dossier/papers/Neo_1568987456.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/78446033686p4420.pdf" class=yCF>Video retrieval using high level features: Exploiting query matching and confidence-based weighting</a></h3><div class="gs_a">SY Neo, J Zhao, <a href="/citations?user=aNVcd3EAAAAJ&amp;hl=en&amp;oi=sra">MY Kan</a>, TS Chua - Image and Video Retrieval, 2006 - Springer</div><div class="gs_rs">Abstract. Recent research in video retrieval has focused on automated, high-level feature <br>indexing on shots or frames. One important application of such indexing is to support precise <br>video retrieval. We report on extensions of this semantic indexing on news video retrieval. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9652872861403202182&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 67</a> <a href="/scholar?q=related:hkLybbnk9YUJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/02/23/RN192552113.html?source=googlescholar" class="gs_nph" class=yC11>BL Direct</a> <a href="/scholar?cluster=9652872861403202182&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 27 versions</a> <a onclick="return gs_ocit(event,'hkLybbnk9YUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.ee.columbia.edu/~lyndon/pubs/acmmm2005-queryclass.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101339" class=yC12>Automatic discovery of query-class-dependent models for multimodal search</a></h3><div class="gs_a">LS Kennedy, <a href="/citations?user=Ade_7YoAAAAJ&amp;hl=en&amp;oi=sra">AP Natsev</a>, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Proceedings of the 13th annual  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract We develop a framework for the automatic discovery of query classes for query-<br>class-dependent search models in multimodal retrieval. The framework automatically <br>discovers useful query classes by clustering queries in a training set according to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2404302615015925925&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 58</a> <a href="/scholar?q=related:pSiRZH3MXSEJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2404302615015925925&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'pSiRZH3MXSEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.4571&amp;rep=rep1&amp;type=pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1282331" class=yC14>A reranking approach for context-based concept fusion in video indexing and retrieval</a></h3><div class="gs_a">LS Kennedy, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Proceedings of the 6th ACM international  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract We propose to incorporate hundreds of pre-trained concept detectors to provide <br>contextual information for improving the performance of multimodal video search. The <br>approach takes initial search results from established video search methods (which <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5319002075635562310&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 61</a> <a href="/scholar?q=related:RrfS3TTk0EkJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5319002075635562310&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'RrfS3TTk0EkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://ciir.cs.umass.edu/pubfiles/mm-385.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from umass.edu</span><span class="gs_ggsS">umass.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1416476" class=yC16>Combining text and audio-visual features in video indexing</a></h3><div class="gs_a"><a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a>, <a href="/citations?user=_0aMq28AAAAJ&amp;hl=en&amp;oi=sra">R Manmatha</a>&hellip; - Acoustics, Speech, and  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We discuss the opportunities, state of the art, and open research issues in using <br>multi-modal features in video indexing. Specifically, we focus on how imperfect text data <br>obtained by automatic speech recognition (ASR) may be used to help solve challenging <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12681820938198234719&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 47</a> <a href="/scholar?q=related:X7ZqvvPg_q8J:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12681820938198234719&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 21 versions</a> <a onclick="return gs_ocit(event,'X7ZqvvPg_q8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.8383&amp;rep=rep1&amp;type=pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291233.1291279" class=yC18>Video search re-ranking via multi-graph propagation</a></h3><div class="gs_a"><a href="/citations?user=ELlCxBQAAAAJ&amp;hl=en&amp;oi=sra">J Liu</a>, W Lai, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, Y Huang, <a href="/citations?user=2sQYtYwAAAAJ&amp;hl=en&amp;oi=sra">S Li</a> - Proceedings of the 15th  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract This paper 1 is concerned with the problem of multimodal fusion in video search. <br>First, we employ an object-sensitive approach to query analysis to improve the baseline <br>result of text-based video search. Then, we propose a PageRank-like graph-based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10948019822910666767&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 44</a> <a href="/scholar?q=related:D3iGGgYu75cJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10948019822910666767&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'D3iGGgYu75cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://166.111.138.19/paper/xirongli_civr2007_conceptsubspace.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 166.111.138.19</span><span class="gs_ggsS">166.111.138.19 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://166.111.138.19/paper/xirongli_civr2007_conceptsubspace.pdf" class=yC1A>Video search in concept subspace: a text-like paradigm</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, D Wang, J Li, B Zhang - &hellip;  Video Retrieval: Proceedings of the 6 &hellip;, 2007 - 166.111.138.19</div><div class="gs_rs">ABSTRACT Though both quantity and quality of semantic concept detection in video are <br>continuously improving, it still remains unclear how to exploit these detected concepts as <br>semantic indices in video search, given a specific query. In this paper, we tackle this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=321217045020478241&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 45</a> <a href="/scholar?q=related:IQs_WDMxdQQJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=321217045020478241&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'IQs_WDMxdQQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md12', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md12" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:IQs_WDMxdQQJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.1241&amp;rep=rep1&amp;type=pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.1241&amp;rep=rep1&amp;type=pdf" class=yC1C>Intelligent multimedia group of Tsinghua University at TRECVID 2006</a></h3><div class="gs_a">J Cao, Y Lan, <a href="/citations?user=ahUibskAAAAJ&amp;hl=en&amp;oi=sra">J Li</a>, Q Li, <a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, F Lin, X Liu, <a href="/citations?user=fqubyX0AAAAJ&amp;hl=en&amp;oi=sra">L Luo</a>&hellip; - TRECVID Video  &hellip;, 2006 - Citeseer</div><div class="gs_rs">Abstract Our shot boundary detection system of this year is basically the same as that of last <br>year. However, we have made three minor improvements on the system, including the <br>detection of FOIs, flashlight and short gradual transitions. On the data set of last year, the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=728004654307401615&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 40</a> <a href="/scholar?q=related:j9_SPmBkGgoJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=728004654307401615&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'j9_SPmBkGgoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md13', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md13" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:j9_SPmBkGgoJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://admis.fudan.edu.cn/intraGroup/p2pSeminar/seminar.files/061230-2.pdf" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fudan.edu.cn</span><span class="gs_ggsS">fudan.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1148228" class=yC1E>Probabilistic latent query analysis for combining multiple retrieval sources</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a> - Proceedings of the 29th annual international  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract Combining the output from multiple retrieval sources over the same document <br>collection is of great importance to a number of retrieval tasks such as multimedia retrieval, <br>web retrieval and meta-search. To merge retrieval sources adaptively according to query <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18131018423274545824&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 39</a> <a href="/scholar?q=related:oKJXlzFRnvsJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18131018423274545824&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'oKJXlzFRnvsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.6328&amp;rep=rep1&amp;type=pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4287412" class=yC20>Reranking methods for visual search</a></h3><div class="gs_a"><a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a>, LS Kennedy, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Multimedia, IEEE, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Most semantic video search methods use text-keyword queries or example video <br>clips and images. But such methods have limitations. To address the problems of example-<br>based video search approaches and avoid the use of specialized models, we conduct <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13225036989505212029&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 42</a> <a href="/scholar?q=related:fQ7cnBLFiLcJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/50/3D/RN214871109.html?source=googlescholar" class="gs_nph" class=yC22>BL Direct</a> <a href="/scholar?cluster=13225036989505212029&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'fQ7cnBLFiLcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv5.papers/nus.pdf" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv5.papers/nus.pdf" class=yC23>Trecvid 2005 by nus pris</a></h3><div class="gs_a">TS Chua, SY Neo, HK Goh, <a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a>, Y Xiao&hellip; - NIST TRECVID- &hellip;, 2005 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT We participated in the high-level feature extraction and search task for <br>TRECVID 2005. For the high-level feature extraction task, we make use of the available <br>collaborative annotation results for training, and develop 2 methods to perform automated <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6139068261712169763&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 39</a> <a href="/scholar?q=related:I4s6zw5aMlUJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6139068261712169763&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'I4s6zw5aMlUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md16', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md16" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:I4s6zw5aMlUJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/r742245481q23631.pdf" class=yC25>A review of text and image retrieval approaches for broadcast news video</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a> - Information Retrieval, 2007 - Springer</div><div class="gs_rs">Abstract The effectiveness of a video retrieval system largely depends on the choice of <br>underlying text and image retrieval components. The unique properties of video collections <br>(eg, multiple sources, noisy features and temporal relations) suggest we examine the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9278500574809399146&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 35</a> <a href="/scholar?q=related:ap_F-Rzbw4AJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0D/2C/RN216038550.html?source=googlescholar" class="gs_nph" class=yC26>BL Direct</a> <a href="/scholar?cluster=9278500574809399146&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'ap_F-Rzbw4AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://www.lti.cs.cmu.edu/research/thesis/2006/rong_yan.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.lti.cs.cmu.edu/research/thesis/2006/rong_yan.pdf" class=yC27>Probabilistic models for combining diverse knowledge sources in multimedia retrieval</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a> - 2006 - lti.cs.cmu.edu</div><div class="gs_rs">Abstract In recent years, the multimedia retrieval community is gradually shifting its <br>emphasis from analyzing one media source at a time to exploring the opportunities of <br>combining diverse knowledge sources from correlated media types and context. In order <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1282854589144669096&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 32</a> <a href="/scholar?q=related:qJtytHOdzREJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1282854589144669096&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'qJtytHOdzREJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md18', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md18" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:qJtytHOdzREJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:qJtytHOdzREJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=9607173453927529710&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://166.111.138.19/paper/Tsinghua_TRECVID2005.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 166.111.138.19</span><span class="gs_ggsS">166.111.138.19 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://166.111.138.19/paper/Tsinghua_TRECVID2005.pdf" class=yC29>Tsinghua university at TRECVID 2005</a></h3><div class="gs_a">J Yuan, L Xiao, D Wang, D Ding, Y Zuo, Z Tong&hellip; - NIST TRECVID  &hellip;, 2005 - 166.111.138.19</div><div class="gs_rs">Abstract Our shot boundary determination system consists of three components, including a <br>FOI detector, a generalized CUT detector, and a long gradual transition detector. One <br>support vector machine, taking score vector calculated with graph partition model as input, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16454118311791924237&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 23</a> <a href="/scholar?q=related:DZSbSm3FWOQJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16454118311791924237&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'DZSbSm3FWOQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:DZSbSm3FWOQJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://www.wocc.org/doc/presentation/MM3-Lexing/LexingXie.pdf" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from wocc.org</span><span class="gs_ggsS">wocc.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284946" class=yC2B>Dynamic multimodal fusion in video search</a></h3><div class="gs_a"><a href="/citations?user=u0xUDSoAAAAJ&amp;hl=en&amp;oi=sra">L Xie</a>, <a href="/citations?user=Ade_7YoAAAAJ&amp;hl=en&amp;oi=sra">A Natsev</a>, J Tesic - Multimedia and Expo, 2007 IEEE  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We propose effective multimodal fusion strategies for video search. Multimodal <br>search is a widely applicable information-retrieval problem, and fusion strategies are <br>essential to the system in order to utilize all available retrieval experts and to boost the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8708676926528158780&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 18</a> <a href="/scholar?q=related:PPxZLYZv23gJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8708676926528158780&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'PPxZLYZv23gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://www.csie.ntu.edu.tw/~winston/doc/kuo09query.pdf" class=yC2E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631284" class=yC2D>Query expansion for hash-based image object retrieval</a></h3><div class="gs_a">YH Kuo, KT Chen, CH Chiang, <a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a> - Proceedings of the 17th ACM  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract An efficient indexing method is essential for content-based image retrieval with the <br>exponential growth in large-scale videos and photos. Recently, hash-based methods (eg, <br>locality sensitive hashing-LSH) have been shown efficient for similarity search. We extend <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7569738195210375615&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 23</a> <a href="/scholar?q=related:v5E4SMgcDWkJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7569738195210375615&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'v5E4SMgcDWkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://www.mmsp2011.org/download/papers/InternationalJournals/2010_Vlogging_A_Survey_of_Video_Blogging_Technology.pdf" class=yC30><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mmsp2011.org</span><span class="gs_ggsS">mmsp2011.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1749606" class=yC2F>Vlogging: A survey of videoblogging technology on the web</a></h3><div class="gs_a">W Gao, Y Tian, <a href="/citations?user=knvEK4AAAAAJ&amp;hl=en&amp;oi=sra">T Huang</a>, <a href="/citations?user=1LxWZLQAAAAJ&amp;hl=en&amp;oi=sra">Q Yang</a> - ACM Computing Surveys (CSUR), 2010 - dl.acm.org</div><div class="gs_rs">Abstract In recent years, blogging has become an exploding passion among Internet <br>communities. By combining the grassroots blogging with the richness of expression <br>available in video, videoblogs (vlogs for short) will be a powerful new media adjunct to our <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10873998803957870053&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 17</a> <a href="/scholar?q=related:5W3unUw06JYJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10873998803957870053&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'5W3unUw06JYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2009_Comprehensive_Query-Dependent_Fusion_using_Regression-on-Folksonomies-A_Case_Study_of_Multimodal_Music_Search.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631303" class=yC31>Comprehensive query-dependent fusion using regression-on-folksonomies: a case study of multimodal music search</a></h3><div class="gs_a">B Zhang, Q Xiang, H Lu, <a href="/citations?user=d3h-zScAAAAJ&amp;hl=en&amp;oi=sra">J Shen</a>, Y Wang - Proceedings of the 17th ACM  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract The combination of heterogeneous knowledge sources has been widely regarded <br>as an effective approach to boost retrieval accuracy in many information retrieval domains. <br>While various technologies have been recently developed for information retrieval, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9683550243293838679&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 11</a> <a href="/scholar?q=related:Vz0tAKThYoYJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9683550243293838679&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'Vz0tAKThYoYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://www.mirlab.org/conference_papers/International_Conference/ACM%202005/docs/mir17.pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101832" class=yC33>Mutual relevance feedback for multimodal query formulation in video retrieval</a></h3><div class="gs_a"><a href="/citations?user=8Ut6gTcAAAAJ&amp;hl=en&amp;oi=sra">A Amir</a>, M Berg, <a href="/citations?user=0qfCL-QAAAAJ&amp;hl=en&amp;oi=sra">H Permuter</a> - Proceedings of the 7th ACM SIGMM  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract Video indexing and retrieval systems allow users to find relevant video segments <br>for a given information need. A multimodal video index may include speech indices, a text-<br>from-screen (OCR) index, semantic visual concepts, content-based image features, audio <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4776358565755538908&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 13</a> <a href="/scholar?q=related:3M3AL88ISUIJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4776358565755538908&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'3M3AL88ISUIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://ntur.lib.ntu.edu.tw/retrieve/171165/05.pdf" class=yC36><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607427" class=yC35>Video search reranking via online ordinal reranking</a></h3><div class="gs_a"><a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, <a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a> - Multimedia and Expo, 2008 IEEE  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract To exploit co-occurrence patterns among features and target semantics while <br>keeping the simplicity of the keyword-based visual search, a novel reranking methods is <br>proposed. The approach, ordinal reranking, reranks an initial search list by utilizing the co-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16352072491241330086&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 11</a> <a href="/scholar?q=related:plXgNUs77uIJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16352072491241330086&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'plXgNUs77uIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://www.ee.columbia.edu/dvmm/publications/05/thesis-xlx.pdf" class=yC38><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ee.columbia.edu/dvmm/publications/05/thesis-xlx.pdf" class=yC37>Unsupervised pattern discovery for multimedia sequences</a></h3><div class="gs_a"><a href="/citations?user=u0xUDSoAAAAJ&amp;hl=en&amp;oi=sra">L Xie</a> - 2005 - ee.columbia.edu</div><div class="gs_rs">Page 1. Unsupervised Pattern Discovery for Multimedia Sequences Lexing Xie Submitted in<br>partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Graduate School<br>of Arts and Sciences Columbia University 2005 Page 2. c 2005 Lexing Xie <b>...</b> </div><div class="gs_fl"><a href="/scholar?cites=15648979171219232293&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 10</a> <a href="/scholar?q=related:JVYC9qRXLNkJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15648979171219232293&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'JVYC9qRXLNkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:JVYC9qRXLNkJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:JVYC9qRXLNkJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=6559857903015864032&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://www.mingzhao.name/publications/2006_MMM_PersonX.pdf" class=yC3A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mingzhao.name</span><span class="gs_ggsS">mingzhao.name <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1651320" class=yC39>Multi-faceted contextual model for person identification in news video</a></h3><div class="gs_a"><a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a>, SY Neo, HK Goh&hellip; - Multi-Media Modelling  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Person identification is very important in the domain of multimedia news as it is <br>often the focus of events in news stories and interest of searchers. However, this detection is <br>impeded by the imprecise audio/visual analysis tools. In this paper, we describe a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4346048044377015775&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 9</a> <a href="/scholar?q=related:34XfhatDUDwJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4346048044377015775&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'34XfhatDUDwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://vireo.cs.cityu.edu.hk/papers/mm08_hktan1.pdf" class=yC3C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459453" class=yC3B>Modeling video hyperlinks with hypergraph for web video reranking</a></h3><div class="gs_a">HK Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=Yt70KEIAAAAJ&amp;hl=en&amp;oi=sra">X Wu</a> - Proceedings of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we investigate a novel approach of exploiting visual-duplicates for <br>web video reranking using hypergraph. Current graph-based reranking approaches <br>consider mainly the pair-wise linking of keyframes and ignore reliability issues that are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2994283768866861338&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 8</a> <a href="/scholar?q=related:GvEGMDnVjSkJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2994283768866861338&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'GvEGMDnVjSkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://mpac.ee.ntu.edu.tw/~yihsuan/pub/TCSVT09.pdf" class=yC3E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5159452" class=yC3D>Online reranking via ordinal informative concepts for context fusion in concept detection and video search</a></h3><div class="gs_a"><a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, <a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a>, HH Chen - Circuits and Systems for Video  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract To exploit the co-occurrence patterns of semantic concepts while keeping the <br>simplicity of context fusion, a novel reranking approach is proposed in this paper. The <br>approach, called ordinal reranking, adjusts the ranking of an initial search (or detection) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11286279959416668923&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 7</a> <a href="/scholar?q=related:-wKkaNProJwJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11286279959416668923&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'-wKkaNProJwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://arxiv.org/pdf/cs/0611113" class=yC40><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://arxiv.org/abs/cs/0611113" class=yC3F>An anthological review of research utilizing montylingua, a python-based end-to-end text processor</a></h3><div class="gs_a"><a href="/citations?user=a7LWLEoAAAAJ&amp;hl=en&amp;oi=sra">MHT Ling</a> - arXiv preprint cs/0611113, 2006 - arxiv.org</div><div class="gs_rs">Abstract: MontyLingua, an integral part of ConceptNet which is currently the largest <br>commonsense knowledge base, is an English text processor developed using Python <br>programming language in MIT Media Lab. The main feature of MontyLingua is the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11987117416037951935&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 6</a> <a href="/scholar?q=related:vynXh8fLWqYJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11987117416037951935&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'vynXh8fLWqYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://dbappl.cs.utwente.nl/cirquid/pubs/amr05.pdf" class=yC42><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utwente.nl</span><span class="gs_ggsS">utwente.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/C4243166G6320776.pdf" class=yC41>Surface features in video retrieval</a></h3><div class="gs_a">T Westerveld, <a href="/citations?user=iH9TVHQAAAAJ&amp;hl=en&amp;oi=sra">A de Vries</a>, <a href="/citations?user=aeCjQlwAAAAJ&amp;hl=en&amp;oi=sra">G RamÃ­rez</a> - Adaptive Multimedia Retrieval: User &hellip;, 2006 - Springer</div><div class="gs_rs">This paper assesses the usefulness of surface features in a multimedia retrieval setting. <br>Surface features describe the metadata or structure of a document rather than the content. <br>We note that the distribution of these features varies across topics. The paper shows how <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2557414877343398206&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 7</a> <a href="/scholar?q=related:Pjla-0fDfSMJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/10/1B/RN183647258.html?source=googlescholar" class="gs_nph" class=yC43>BL Direct</a> <a href="/scholar?cluster=2557414877343398206&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 24 versions</a> <a onclick="return gs_ocit(event,'Pjla-0fDfSMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://doras.dcu.ie/14877/1/wilkins_thesis.pdf" class=yC45><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dcu.ie</span><span class="gs_ggsS">dcu.ie <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://doras.dcu.ie/14877/" class=yC44>An investigation into weighted data fusion for content-based multimedia information retrieval</a></h3><div class="gs_a">P Wilkins - 2009 - doras.dcu.ie</div><div class="gs_rs">Content Based Multimedia Information Retrieval (CBMIR) is characterised by the <br>combination of noisy sources of information which, in unison, are able to achieve strong <br>performance. In this thesis we focus on the combination of ranked results from the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6563010496097017445&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 5</a> <a href="/scholar?q=related:Zd4FHUZ_FFsJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6563010496097017445&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Zd4FHUZ_FFsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm08-wanggang.pdf" class=yC47><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459393" class=yC46>Exploring knowledge of sub-domain in a multi-resolution bootstrapping framework for concept detection in news video</a></h3><div class="gs_a">G Wang, TS Chua, <a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a> - Proceeding of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we present a model based on a multi-resolution, multi-source and <br>multi-modal (M3) bootstrapping framework that exploits knowledge of sub-domains for <br>concept detection in news video. Because the characteristics and distributions of data in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15442298916331928942&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 5</a> <a href="/scholar?q=related:bj1dLwwRTtYJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15442298916331928942&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'bj1dLwwRTtYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://www.hindawi.com/journals/ijdmb/aip/486487/" class=yC49><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[HTML]</span><span class="gs_ct2">[HTML]</span></span> <a href="http://www.hindawi.com/journals/ijdmb/aip/486487/" class=yC48>Multimodal Indexing of Multilingual News Video</a></h3><div class="gs_a"><a href="/citations?user=XUwiadkAAAAJ&amp;hl=en&amp;oi=sra">H Ghosh</a>, <a href="/citations?user=2OsvtvgAAAAJ&amp;hl=en&amp;oi=sra">SK Kopparapu</a>, <a href="/citations?user=ugI69q0AAAAJ&amp;hl=en&amp;oi=sra">T Chattopadhyay</a>&hellip; - International Journal of &hellip;, 2010 - hindawi.com</div><div class="gs_rs">The problems associated with automatic analysis of news telecasts are more severe in a <br>country like India, where there are many national and regional language channels, besides <br>English. In this paper, we present a framework for multimodal analysis of multilingual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6322536354863597187&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 3</a> <a href="/scholar?q=related:g0YjB1EpvlcJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6322536354863597187&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'g0YjB1EpvlcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md34', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md34" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:g0YjB1EpvlcJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv6.papers/iis_bj.pdf" class=yC4B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv6.papers/iis_bj.pdf" class=yC4A>BJTU TRECVID 2006 Video Retrieval System</a></h3><div class="gs_a">SK Wei, <a href="/citations?user=474TbQYAAAAJ&amp;hl=en&amp;oi=sra">Y Zhao</a>, Z Zhu, N Liu, Y Zhao&hellip; - TREC Video  &hellip;, 2006 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT In this paper, we describe our experiments of search task for TRECVID 2006. <br>This year we participated in the automatic video search subtask and the interactive video <br>search subtask, and submitted two runs to NIST, one for the automatic search (runid: <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13626501212458682580&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 3</a> <a href="/scholar?q=related:1IgHDKgOG70J:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13626501212458682580&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'1IgHDKgOG70J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md35', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md35" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:1IgHDKgOG70J:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC4D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC4C>TRECVID 2010 Known-item Search by NUS</a></h3><div class="gs_a">XY Chen, J Yuan, L Nie, ZJ Zha, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - TRECVID  &hellip;, 2010 - www-nlpir.nist.gov</div><div class="gs_rs">Abstract. This paper describes our system for auto search and interactive search in the <br>known-item search (KIS) task in TRECVID 2010. KIS task aims to find an unique video <br>answer for each text query. The shift from traditional video search has prompted a series <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12664714192218118309&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 3</a> <a href="/scholar?q=related:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'pVgCEXUawq8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md36', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md36" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/vt737521865008m3.pdf" class=yC4E>Filling the semantic gap in video retrieval: An exploration</a></h3><div class="gs_a"><a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">A Hauptmann</a>, <a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=PlBXfHEAAAAJ&amp;hl=en&amp;oi=sra">WH Lin</a>, M Christel&hellip; - Semantic multimedia and &hellip;, 2008 - Springer</div><div class="gs_rs">Digital images and motion video have proliferated in the past few years, ranging from ever-<br>growing personal photo and video collections to professional news and documentary <br>archives. In searching through these archives, digital imagery indexing based on low-level <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7245539427021096793&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 3</a> <a href="/scholar?q=related:WQeuabhTjWQJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7245539427021096793&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'WQeuabhTjWQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://www.idiap.ch/~gatica/publications/NegoescuGatica-book10.pdf" class=yC50><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from idiap.ch</span><span class="gs_ggsS">idiap.ch <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.idiap.ch/~gatica/publications/NegoescuGatica-book10.pdf" class=yC4F>Internet multimedia search and mining</a></h3><div class="gs_a"><a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>, TS Chua - Internet Multimedia Search and Mining, 2010 - idiap.ch</div><div class="gs_rs">Abstract: We present in this chapter a review of current work that leverages on large online <br>social networks&#39; meta-information, in particular Flickr Groups. We briefly present this hugely <br>successful feature in Flickr and discuss the various ways in which metadata stemming <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5011184445285179392&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 3</a> <a href="/scholar?q=related:AKAv3bNNi0UJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5011184445285179392&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'AKAv3bNNi0UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md38', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md38" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:AKAv3bNNi0UJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4731670" class=yC51>A novel video searching model based on ontology inference and multimodal information fusion</a></h3><div class="gs_a"><a href="/citations?user=nYl5trAAAAAJ&amp;hl=en&amp;oi=sra">J Zhang</a> - &hellip;  and Computational Technology, 2008. ISCSCT&#39;08.  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video comprises multiple types of textual, audio and visual information, and each of <br>them contains abundant semantic information. Therefore multimodal features query and <br>fusion are necessary in video retrieval. In this paper, we propose a new video retrieval <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4029170368148424840&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 2</a> <a href="/scholar?q=related:iEgENRp96jcJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4029170368148424840&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'iEgENRp96jcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB40" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW40"><a href="http://lms.comp.nus.edu.sg/papers/media/2008/bookchapter-wanggang.pdf" class=yC53><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/L8568841V710L17G.pdf" class=yC52>Capturing text semantics for concept detection in news video</a></h3><div class="gs_a">G Wang, TS Chua - Multimedia Content Analysis, 2009 - Springer</div><div class="gs_rs">The overwhelming amounts of multimedia contents have triggered the need for automatic <br>semantic concept detection. However, as there are large variations in the visual feature <br>space, text from automatic speech recognition (ASR) has been extensively used and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15510479479648965246&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 2</a> <a href="/scholar?q=related:flJ2oOhKQNcJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15510479479648965246&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'flJ2oOhKQNcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4197411" class=yC54>Automatic Query Type Classification for Web Image Retrieval</a></h3><div class="gs_a">K Cai, J Bu, C Chen, P Huang - Multimedia and Ubiquitous  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, a framework of query classification is proposed for text-based image <br>retrieval. The classification process in this framework consists of three phases. In the first <br>phase, two basic classifiers are employed to classify the image query into certain <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7171910662986000785&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 2</a> <a href="/scholar?q=related:kekd5L--h2MJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7171910662986000785&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'kekd5L--h2MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB42" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW42"><a href="http://www.ee.columbia.edu/ln/dvmm/publications/PhD_theses/WinstonHsu07thesis.pdf" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ee.columbia.edu/ln/dvmm/publications/PhD_theses/WinstonHsu07thesis.pdf" class=yC55>An information-theoretic framework towards large-scale video structuring, threading, and retrieval</a></h3><div class="gs_a"><a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a> - 2007 - ee.columbia.edu</div><div class="gs_rs">Page 1. An Information-Theoretic Framework towards Large-Scale Video Structuring, Threading,<br>and Retrieval Winston H. Hsu Submitted in partial fulfillment of the requirements for the degree<br>of Doctor of Philosophy in the Graduate School of Arts and Sciences <b>...</b> </div><div class="gs_fl"><a href="/scholar?cites=16231174863222617573&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 2</a> <a href="/scholar?q=related:5TWiqYq3QOEJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16231174863222617573&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'5TWiqYq3QOEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md42', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md42" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:5TWiqYq3QOEJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:5TWiqYq3QOEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=5485094745706648444&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Multimodal Indexing of Multilingual News Video</h3><div class="gs_a">G Hiranmay, <a href="/citations?user=2OsvtvgAAAAJ&amp;hl=en&amp;oi=sra">K Sunil Kumar</a>&hellip; - International  &hellip;, 2010 - Hindawi Publishing Corporation</div><div class="gs_fl"><a href="/scholar?cites=3748672000631884928&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=60">Cited by 1</a> <a href="/scholar?q=related:gMwdwVL1BTQJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3748672000631884928&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'gMwdwVL1BTQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2011_Document_Dependent_Fusion_in_Multimodal_Music_Retrieval.pdf" class=yC58><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2071949" class=yC57>Document dependent fusion in multimodal music retrieval</a></h3><div class="gs_a"><a href="/citations?user=TF8WdBMAAAAJ&amp;hl=en&amp;oi=sra">Z Li</a>, B Zhang, Y Wang - Proceedings of the 19th ACM international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose a novel multimodal fusion framework, document <br>dependent fusion (DDF), which derives the optimal combination strategy for each individual <br>document in the fusion process. For each document, we derive a document weight vector <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:JyU9BhZI5j4J:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4532389334426133799&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'JyU9BhZI5j4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB45" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW45"><a href="http://137.132.14.55/bitstream/handle/10635/20949/ZhangBJ.pdf?sequence=1" class=yC5A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.14.55</span><span class="gs_ggsS">137.132.14.55 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://137.132.14.55/handle/10635/20949" class=yC59>Adaptive multimodal fusion based similarity measures in music information retrieval</a></h3><div class="gs_a">Z BINGJUN - 2010 - 137.132.14.55</div><div class="gs_rs">In the field of music information retrieval (MIR), one fundamental research problem is the <br>measuring of the similarity between music documents. Based on a viable similarity measure, <br>MIR systems can be made more effective to help users retrieve relevant music information. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9FjpHoxTlRIJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1339068325491726580&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'9FjpHoxTlRIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB46" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW46"><a href="http://www.commit-nl.nl/sites/default/files/675955_survey_mm-search-optimization.pdf" class=yC5C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from commit-nl.nl</span><span class="gs_ggsS">commit-nl.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.commit-nl.nl/sites/default/files/675955_survey_mm-search-optimization.pdf" class=yC5B>A Survey on Multimedia Search Optimization based on Multimodal Information Resources</a></h3><div class="gs_a"><a href="/citations?user=Fhmp7lQAAAAJ&amp;hl=en&amp;oi=sra">C Kofler</a>, M Larson, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - commit-nl.nl</div><div class="gs_rs">Abstract. This survey constitutes a literature study that overviews the state-of-the-art in <br>multimedia search. Techniques that are covered include multimodal re-ranking, pseudo-<br>relevance feedback, query classification and query suggestion. Discussion of the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1DJ7H6YdmWsJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'1DJ7H6YdmWsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md46', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md46" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:1DJ7H6YdmWsJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB47" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW47"><a href="http://wing.comp.nus.edu.sg/publications/theses/jinZhaoThesis.pdf" class=yC5E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://wing.comp.nus.edu.sg/publications/theses/jinZhaoThesis.pdf" class=yC5D>Refining and Expanding WordNet for Video Retrieval</a></h3><div class="gs_a">Z Jin - wing.comp.nus.edu.sg</div><div class="gs_rs">Abstract Recent research in video retrieval has shown that correctly chosen high-level <br>feature detectors are able to improve retrieval performance. This has motivated us to design <br>and implement a system to deduce the correct high-level feature for a query with WordNet <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:SXoHMtkOU-cJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16668682971838380617&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'SXoHMtkOU-cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md47', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md47" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:SXoHMtkOU-cJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://www.comp.nus.edu.sg/~subolan/files/News%20Video%20Retrieval%20by%20Learning%20Multimodal%20Semantic%20Information.pdf" class=yC60><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/8532G89W71T24007.pdf" class=yC5F>News video retrieval by learning multimodal semantic information</a></h3><div class="gs_a">H Yu, <a href="/citations?user=ymlKC0EAAAAJ&amp;hl=en&amp;oi=sra">B Su</a>, H Lu, X Xue - Advances in Visual Information Systems, 2007 - Springer</div><div class="gs_rs">With the explosion of multimedia data especially that of video data, requirement of efficient <br>video retrieval has becoming more and more important. Years of TREC Video Retrieval <br>Evaluation (TRECVID) research gives benchmark for video search task. The video data in <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:SVSTQKDMAQIJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/10/3B/RN221154550.html?source=googlescholar" class="gs_nph" class=yC61>BL Direct</a> <a href="/scholar?cluster=144621651702797385&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'SVSTQKDMAQIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/15829/final_thesis_wanggang_nus_phdx.pdf?sequence=1" class=yC63><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/15829" class=yC62>A multi-resolution multi-source and multi-modal (M3) transductive framework for concept detection in news video</a></h3><div class="gs_a">W Gang - 2009 - scholarbank.nus.sg</div><div class="gs_rs">We study the problem of detecting concepts in news video. Most existing algorithms for news <br>video concept detection are based on single-resolution (shot), single source (training data), <br>and multi-modal fusion methods under a supervised inductive inference framework. In this <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:O24GTu3y5YoJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10008672847931010619&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'O24GTu3y5YoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB50" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW50"><a href="http://mpac.ee.ntu.edu.tw/~yihsuan/pub/TCSVT08_2_5.pdf" class=yC65><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mpac.ee.ntu.edu.tw/~yihsuan/pub/TCSVT08_2_5.pdf" class=yC64>Online Reranking via Ordinal Informative Concepts for Context Fusion in Video Detection and Search</a></h3><div class="gs_a"><a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, <a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a>, HH Chen - mpac.ee.ntu.edu.tw</div><div class="gs_rs">AbstractâTo exploit co-occurrence patterns among semantic concepts while keeping the <br>simplicity of context fusion, a novel reranking methods is proposed for video detection and <br>search. The approach, ordinal reranking, reranks an initial search (or detection) list by <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:KK2KVJhqq6oJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'KK2KVJhqq6oJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md50', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md50" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:KK2KVJhqq6oJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://www.cmlab.csie.ntu.edu.tw/~majorrei/pcm/old/pcm_v20.pdf" class=yC67><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/74421m2p5225lt5g.pdf" class=yC66>Adaptive Learning for Multimodal Fusion in Video Search</a></h3><div class="gs_a">WY Lee, PT Wu, <a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">W Hsu</a> - &hellip; in Multimedia Information Processing-PCM 2009, 2009 - Springer</div><div class="gs_rs">Abstract. Multimodal fusion had been shown prominent in video search for the sheer volume <br>of video data. The state-of-the-art methods address the problem by query-dependent fusion, <br>where modality weights vary across query classes (eg, object, sports, scenes, people, etc.)<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:UtO6WkiBzPkJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17999903958452851538&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'UtO6WkiBzPkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Manual and Automatic Multimedia Annotation Automatic</h3><div class="gs_a">T Pototschnig, C Tutsch - 2005</div><div class="gs_fl"><a href="/scholar?q=related:KYcPgK69IYYJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'KYcPgK69IYYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/43MM2R47K8675575.pdf" class=yC68>A Cooperative Learning Scheme for Interactive Video Search</a></h3><div class="gs_a">S Wei, <a href="/citations?user=474TbQYAAAAJ&amp;hl=en&amp;oi=sra">Y Zhao</a>, Z Zhu, N Liu - Journal of Signal Processing Systems, 2010 - Springer</div><div class="gs_rs">Abstract The main idea of an interactive search is to gradually improve search quality of <br>retrieval system via user interaction. While a large amount of work has been made in the <br>past, most of the existing approaches typically require labeling effort for updating the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:X0LZhcms-OsJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17003530374671319647&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'X0LZhcms-OsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Information-theoretic Cue-X Reranking for Video Retrieval</h3><div class="gs_a"><a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a>, LS Kennedy, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a></div><div class="gs_fl"><a href="/scholar?q=related:se_lkKhlrBsJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'se_lkKhlrBsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/p562x62763465n52.pdf" class=yC69>A Novel Method for Spoken Text Feature Extraction in Semantic Video Retrieval</a></h3><div class="gs_a">J Cao, J Li, Y Zhang, S Tang - Advances in Multimedia Information  &hellip;, 2006 - Springer</div><div class="gs_rs">We propose a novel method for extracting text feature from the automatic speech recognition <br>(ASR) results in semantic video retrieval. We combine HowNet-rule-based knowledge with <br>statistic information to build special concept lexicons, which can rapidly narrow the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:splexUzwDyAJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/20/0A/RN198327747.html?source=googlescholar" class="gs_nph" class=yC6A>BL Direct</a> <a href="/scholar?cluster=2310329346383845810&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'splexUzwDyAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB56" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW56"><a href="http://dare.uva.nl/document/355661" class=yC6C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dare.uva.nl/en/record/410580" class=yC6B>Content-based visual search learned from social media</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a> - 2012 - dare.uva.nl</div><div class="gs_rs">Abstract In een wereld waarin de hoeveelheid digitale afbeeldingen alsmaar groeit is het <br>belangrijk te kunnen zoeken op basis van beeldinhoud. Xirong Li liet zich inspireren door <br>sociale media en onderzocht de waarde van beelden met social tags voor visueel zoeken. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:xTQX0HIOAtIJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15132673584198530245&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'xTQX0HIOAtIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB57" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW57"><a href="https://scholarworks.iu.edu/dspace/bitstream/handle/2022/7666/umi-indiana-1772.pdf?sequence=1" class=yC6E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iu.edu</span><span class="gs_ggsS">iu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=GFXaoaPlZ20C&amp;oi=fnd&amp;pg=PA1&amp;ots=EtZbw4ecez&amp;sig=gc6VdQH9Y1RX-CqhnNTX7nekLmg" class=yC6D>A domain-centric approach to designing user interfaces of video retrieval systems</a></h3><div class="gs_a">DE Albertson - 2007 - books.google.com</div><div class="gs_rs">User-and task-centric efforts in video information retrieval (IR) research are needed because <br>current experiments are showing few significant results. It is our belief that unsatisfactory <br>results in video IR can be partially attributed to the overemphasis on technologically-<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lQrc5--3vbcJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13239940720554150549&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'lQrc5--3vbcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB58" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW58"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/16051/thesis-sub.pdf?sequence=1" class=yC70><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/16051" class=yC6F>combining multimodal external resources for event-based news video retrieval and question answering</a></h3><div class="gs_a">NEOSHI YONG - 2008 - scholarbank.nus.edu</div><div class="gs_rs">The ever-increasing amount of multimedia data available online creates an urgent need on <br>how to index these information and support effective retrieval by users. In recent years, we <br>observe the gradual shift from performing retrieval solely based on analyzing one media <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:h5EV9TPOgHUJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8466994022007017863&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'h5EV9TPOgHUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB59" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW59"><a href="http://www.scholarbank.nus.edu.sg/bitstream/handle/10635/13136/Thesis_XU_Huaxin_HT016894E.pdf?sequence=1" class=yC72><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.scholarbank.nus.edu.sg/handle/10635/13136" class=yC71>Integrated analysis of audiovisual signals and external information sources for event detection in team sports video</a></h3><div class="gs_a">H Xu - 2007 - scholarbank.nus.edu.sg</div><div class="gs_rs">Audiovisual signals and external information sources (news reports, live commentaries, Web <br>casts, etc.) are found to have complementary strengths for detecting events in sports video. <br>This thesis reports research on integrated analysis of them, focusing on tackling the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:M-BY-RAX2tkJ:scholar.google.com/&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15697884812823552051&amp;hl=en&amp;num=60&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'M-BY-RAX2tkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
