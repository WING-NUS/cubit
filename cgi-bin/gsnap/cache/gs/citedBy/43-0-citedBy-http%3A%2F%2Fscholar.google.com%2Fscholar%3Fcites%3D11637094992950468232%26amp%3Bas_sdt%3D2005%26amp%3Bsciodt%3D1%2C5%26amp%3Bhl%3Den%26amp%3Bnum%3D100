Total results = 43
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.cse.iitk.ac.in/users/vision/dipen/references/53_truong-venkatesh-2007_video-abstraction-review-classification.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iitk.ac.in</span><span class="gs_ggsS">iitk.ac.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1198305" class=yC0>Video abstraction: A systematic review and classification</a></h3><div class="gs_a">BT Truong, <a href="/citations?user=AEkRUQcAAAAJ&amp;hl=en&amp;oi=sra">S Venkatesh</a> - ACM Transactions on Multimedia Computing,  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract The demand for various multimedia applications is rapidly increasing due to the <br>recent advance in the computing and network infrastructure, together with the widespread <br>use of digital video technology. Among the key elements for the success of these <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13499285334123051192&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 262</a> <a href="/scholar?q=related:uHgLNn0YV7sJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13499285334123051192&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'uHgLNn0YV7sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://pdf.aminer.org/000/331/288/video_abstract_a_hybrid_approach_to_generate_semantically_meaningful_video.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aminer.org</span><span class="gs_ggsS">aminer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://pdf.aminer.org/000/331/288/video_abstract_a_hybrid_approach_to_generate_semantically_meaningful_video.pdf" class=yC2>An overview of video abstraction techniques</a></h3><div class="gs_a">Y Li, T Zhang, D Tretter - HP Laboratories Palo Alto, 2001 - pdf.aminer.org</div><div class="gs_rs">Digital video is an emerging force in today&#39;s computer and telecommunication industries. <br>The rapid growth of the Internet, in terms of both bandwidth and the number of users, has <br>pushed all multimedia technology forward including video streaming. Continuous <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3939094753137160237&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 112</a> <a href="/scholar?q=related:LXBYyc55qjYJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3939094753137160237&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'LXBYyc55qjYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:LXBYyc55qjYJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1621451" class=yC4>Techniques for movie content analysis and skimming: tutorial and overview on video abstraction techniques</a></h3><div class="gs_a">Y Li, SH Lee, CH Yeh, <a href="/citations?user=81d60okAAAAJ&amp;hl=en&amp;oi=sra">CCJ Kuo</a> - Signal Processing Magazine, &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the proliferation of digital video, video summarization and skimming has <br>become an indispensable tool of any practical video content management system. This <br>paper provides a tutorial on the existing abstraction work for generic videos and presents <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4522640630412431396&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 82</a> <a href="/scholar?q=related:JFB7_rClwz4J:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4D/33/RN186343588.html?source=googlescholar" class="gs_nph" class=yC5>BL Direct</a> <a href="/scholar?cluster=4522640630412431396&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'JFB7_rClwz4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=8u5R70UFaG0C&amp;oi=fnd&amp;pg=PR11&amp;ots=Ki3AodZjS8&amp;sig=S9QNPAkiPDAkuNZ48w47G5yrsEM" class=yC6>Video content analysis using multimodal information: for movie content extraction, indexing and representation</a></h3><div class="gs_a">Y Li, <a href="/citations?user=81d60okAAAAJ&amp;hl=en&amp;oi=sra">CCJ Kuo</a> - 2003 - books.google.com</div><div class="gs_rs">Video Content Analysis Using Multimodal Information For Movie Content Extraction, <br>Indexing and Representation is on content-based multimedia analysis, indexing, <br>representation and applications with a focus on feature films. Presented are the state-of-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3202940274079693996&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 38</a> <a href="/scholar?q=related:rPxPmTUhcywJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3202940274079693996&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'rPxPmTUhcywJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:rPxPmTUhcywJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=15524422622802607628&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www.bmva.org/bmvc/2002/papers/152/full_152.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bmva.org</span><span class="gs_ggsS">bmva.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.bmva.org/bmvc/2002/papers/152/full_152.pdf" class=yC7>Rapid summarization and browsing of video sequences</a></h3><div class="gs_a">J Vermaak, <a href="/citations?user=8Cph5uQAAAAJ&amp;hl=en&amp;oi=sra">P Perez</a>, M Gangnet, <a href="/citations?user=tDgCcOEAAAAJ&amp;hl=en&amp;oi=sra">A Blake</a> - British Machine Vision  &hellip;, 2002 - bmva.org</div><div class="gs_rs">Abstract This paper presents a strategy for rapid summarisation and browsing of video <br>sequences. The input video is first transformed into a sequence of representative feature <br>vectors. Using this representation a utility function is designed that assigns high reward to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3636841611805663617&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 34</a> <a href="/scholar?q=related:gSEJWC2oeDIJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3636841611805663617&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'gSEJWC2oeDIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:gSEJWC2oeDIJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://kawicho.com/iat333/IAT455%20_IGNOR/useful_.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kawicho.com</span><span class="gs_ggsS">kawicho.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1026719" class=yC9>Key-frame extraction algorithm using entropy difference</a></h3><div class="gs_a">M Mentzelopoulos, <a href="/citations?user=SWn6HA0AAAAJ&amp;hl=en&amp;oi=sra">A Psarrou</a> - Proceedings of the 6th ACM SIGMM  &hellip;, 2004 - dl.acm.org</div><div class="gs_rs">Abstract The fast evolution of the digital video technology has opened new areas of <br>research. The most important aspect will be to develop algorithms to perform video <br>cataloguing, indexing and retrieval. The basic step is to find a way for video abstraction, as <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12706425783654322650&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 23</a> <a href="/scholar?q=related:2pGUv-1KVrAJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12706425783654322650&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'2pGUv-1KVrAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://cvsp.cs.ntua.gr/publications/confr/EvangelopoulosRapantzikosEtAl_MovieSum_ICIP2008_fancyhead.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntua.gr</span><span class="gs_ggsS">ntua.gr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4712308" class=yCB>Movie summarization based on audiovisual saliency detection</a></h3><div class="gs_a"><a href="/citations?user=VBKA-CMAAAAJ&amp;hl=en&amp;oi=sra">G Evangelopoulos</a>, <a href="/citations?user=-DJUVIUAAAAJ&amp;hl=en&amp;oi=sra">K Rapantzikos</a>&hellip; - &hellip; , 2008. ICIP 2008.  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Based on perceptual and computational attention modeling studies, we formulate <br>measures of saliency for an audiovisual stream. Audio saliency is captured by signal <br>modulations and related multi-frequency band features, extracted through nonlinear <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4736478776377429315&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 19</a> <a href="/scholar?q=related:Q00nKFpau0EJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4736478776377429315&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'Q00nKFpau0EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICME%202003/pdfs/0300329.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1221315" class=yCD>Semantic video summarization in compressed domain MPEG video</a></h3><div class="gs_a">JCS Yu, MS Kankanhalli&hellip; - Multimedia and Expo, 2003 &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present a semantic summarization algorithm that interfaces with <br>the metadata and that works in compressed domain, in particular MPEG-1 and MPEG-2 <br>videos. In enabling a summarization algorithm through high-level semantic content, we try <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12980372427007217146&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 12</a> <a href="/scholar?q=related:-k0KDPOLI7QJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12980372427007217146&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'-k0KDPOLI7QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.9887&amp;rep=rep1&amp;type=pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1230812.1230813" class=yCF>Computational approaches to temporal sampling of video sequences</a></h3><div class="gs_a">T Liu, JR Kender - ACM Transactions on Multimedia Computing,  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Video key frame extraction is one of the most important research problems for video <br>summarization, indexing, and retrieval. For a variety of applications such as ubiquitous <br>media access and video streaming, the temporal boundaries between video key frames <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3874024195260707398&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 12</a> <a href="/scholar?q=related:RnLI6XpMwzUJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3874024195260707398&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'RnLI6XpMwzUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4606055" class=yC11>Automatic video summarization by affinity propagation clustering and semantic content mining</a></h3><div class="gs_a">X Xie, F Wu - &hellip;  and Security, 2008 International Symposium on, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video summarization has become an indispensable tool of any practical video <br>content management system in large volume video data. In this paper, we propose a novel <br>approach to automatically generate the video summary for broadcast news videos. Firstly, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7646310086642825893&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 7</a> <a href="/scholar?q=related:pfrUDIMmHWoJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7646310086642825893&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'pfrUDIMmHWoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.9383&amp;rep=rep1&amp;type=pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.9383&amp;rep=rep1&amp;type=pdf" class=yC12>Adaptive Video Summarization</a></h3><div class="gs_a">P Mulhem, J Gensel, <a href="/citations?user=QwWiE6AAAAAJ&amp;hl=en&amp;oi=sra">H Martin</a> - The Handbook of Video Databases  &hellip;, 2003 - Citeseer</div><div class="gs_rs">One of the specific characteristics of the video medium is to be a temporal medium: it has an <br>inherent duration and the time spent to find information present in a video depends <br>somehow on its duration. Without any knowledge about the video, it is necessary to use <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11973387459082960438&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 7</a> <a href="/scholar?q=related:Nr7B0HQEKqYJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11973387459082960438&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'Nr7B0HQEKqYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Nr7B0HQEKqYJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.75&amp;rep=rep1&amp;type=pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/6L9D8Q20JXTUR2EH.pdf" class=yC14>Compressed domain summarization of digital video</a></h3><div class="gs_a">C Chew, M Kankanhalli - Advances in Multimedia Information Processing &hellip;, 2001 - Springer</div><div class="gs_rs">Video data is usually voluminous and it is desirable that one be able to get a quick idea of <br>the content before actually watching a video or downloading it from the web. In this paper, <br>we present a video summarization algorithm that works in the compressed domain, in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14101924864602173620&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 7</a> <a href="/scholar?q=related:tHDc-_IZtMMJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/15/1A/RN103714060.html?source=googlescholar" class="gs_nph" class=yC16>BL Direct</a> <a href="/scholar?cluster=14101924864602173620&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'tHDc-_IZtMMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.worldscientific.com/doi/abs/10.1142/S1793351X07000044" class=yC17>Overview and future trends of multimedia research for content access and distribution</a></h3><div class="gs_a">ML Shyu, C SHU-CHING, Q Sun&hellip; - International Journal of  &hellip;, 2007 - World Scientific</div><div class="gs_rs">The advances in information technology, computational capability, and communication <br>networks have enabled large-scale data collection and distribution of vast amounts of <br>multimedia data available to consumer and enterprise applications. With the proliferation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7975338722911631865&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 7</a> <a href="/scholar?q=related:-bmQNlAYrm4J:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/32/19/RN215410193.html?source=googlescholar" class="gs_nph" class=yC18>BL Direct</a> <a href="/scholar?cluster=7975338722911631865&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'-bmQNlAYrm4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1260057" class=yC19>An approach to generating two-level video abstraction</a></h3><div class="gs_a">WG Cheng, D Xu - Machine Learning and Cybernetics, 2003  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video abstraction is a short summary of the content of a longer video document. <br>Most existing video abstraction methods are based on shot-level, which is not sufficient to <br>meaningful browsing and is too fine to users sometimes. In this paper, we propose a novel <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2384763415363077596&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 6</a> <a href="/scholar?q=related:3EUQ9rBhGCEJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'3EUQ9rBhGCEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://dl.acm.org/citation.cfm?id=1354647" class=yC1A>A framework for video annotation, visualization, and interaction</a></h3><div class="gs_a">DR Goldman, DH Adviser-Salesin, B Adviser-Curless - 2007 - dl.acm.org</div><div class="gs_rs">Abstract Existing approaches to interaction with digital video are complex, and some <br>operations lack the immediacy of interactive feedback. In this thesis, I present a framework <br>for video annotation, visualization, and interaction that harnesses computer vision to aid <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12303567963395212666&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 5</a> <a href="/scholar?q=related:ekUMKOANv6oJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12303567963395212666&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ekUMKOANv6oJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md14', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md14" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:ekUMKOANv6oJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=6697763415310741974&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/h4m374421627608n.pdf" class=yC1B>Hierarchical structuring of video previews by Leading-Cluster-Analysis</a></h3><div class="gs_a">S Benini, <a href="/citations?user=Fm5DhucAAAAJ&amp;hl=en&amp;oi=sra">P Migliorati</a>, <a href="/citations?user=nojVrZgAAAAJ&amp;hl=en&amp;oi=sra">R Leonardi</a> - Signal, image and video processing, 2010 - Springer</div><div class="gs_rs">Abstract Clustering of shots is frequently used for accessing video data and enabling quick <br>grasping of the associated content. In this work we first group video shots by a classic <br>hierarchical algorithm, where shot content is described by a codebook of visual words and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=219305657975495919&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 5</a> <a href="/scholar?q=related:79zcgFUhCwMJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=219305657975495919&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'79zcgFUhCwMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/N4056057T7RQ5226.pdf" class=yC1C>Advances in video summarization and skimming</a></h3><div class="gs_a">R Jiang, A Sadka, D Crookes - Recent Advances in Multimedia Signal  &hellip;, 2009 - Springer</div><div class="gs_rs">This chapter summarizes recent advances in video abstraction for fast content browsing, <br>skimming, transmission, and retrieval of massive video database which are demanded in <br>many system applications, such as web multimedia, mobile multimedia, interactive TV, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5588758588001488528&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 5</a> <a href="/scholar?q=related:kA4-RVNCj00J:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5588758588001488528&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'kA4-RVNCj00J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5349317" class=yC1D>Video summarization using genetic algorithm and information theory</a></h3><div class="gs_a"><a href="/citations?user=Ha5PITUAAAAJ&amp;hl=en&amp;oi=sra">ZZ Tabrizi</a>, BM Bidgoli, M Fathi - Computer Conference, 2009.  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video processing techniques based on pattern recognition methods and machine <br>vision is one of the interesting research fields which attract many researchers. In this paper, <br>we proposed a novel method for video summarization using genetic algorithm based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5118283509121197951&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 4</a> <a href="/scholar?q=related:f6fo1LvLB0cJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'f6fo1LvLB0cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://image.ntua.gr/iva/files/papers/springer08.muscle.pdf" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntua.gr</span><span class="gs_ggsS">ntua.gr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/v5t5n7058200622h.pdf" class=yC1E>Audiovisual attention modeling and salient event detection</a></h3><div class="gs_a"><a href="/citations?user=VBKA-CMAAAAJ&amp;hl=en&amp;oi=sra">G Evangelopoulos</a>, <a href="/citations?user=-DJUVIUAAAAJ&amp;hl=en&amp;oi=sra">K Rapantzikos</a>, P Maragos&hellip; - Multimodal Processing  &hellip;, 2008 - Springer</div><div class="gs_rs">In analyzing the visual and aural information of video streams the main issues that arise are: <br>i) choosing appropriate features that capture important signal properties, ii) combining the <br>information corresponding to the different modalities to allow for interaction and iii) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18170350812852719194&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 3</a> <a href="/scholar?q=related:Wg5RLMsNKvwJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18170350812852719194&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'Wg5RLMsNKvwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/90287x/200401/9625017.html" class=yC20>ä¸ç§å±æ¬¡è§é¢æè¦çææ¹æ³</a></h3><div class="gs_a">ç¨æåï¼ é¡»å¾· - ä¸­å½å¾è±¡å¾å½¢å­¦æ¥: A è¾, 2004 - cqvip.com</div><div class="gs_rs">è§é¢æè¦æ¯è§é¢åå®¹çä¸ç§åç¼©è¡¨ç¤ºæ¹å¼. ä¸ºäºè½å¤æ´å¥½å°æµè§è§é¢, æåºäºä¸ç§æ ¹æ®æµè§æ<br>æ£ç´¢çç²åº¦ä¸åæ¥å»ºç«ä¸¤ç§å±æ¬¡è§é¢æè¦(éå¤´çº§ååºæ¯çº§) çææ³, å¹¶ç»åºäºä¸ç§è§é¢æè¦çæ<br>æ¹æ³: é¦åç¨ä¸ç§æ ¹æ®åå®¹ååèªå¨æåéå¤´åå³é®å¸§çæ¹æ³æ¥å®ç°å³é®å¸§çæå; ç»§èç¨ä¸<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3502125373079869430&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 5</a> <a href="/scholar?q=related:9tdOxXgMmjAJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3502125373079869430&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'9tdOxXgMmjAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://mrim.imag.fr/publications/2003/PM001/mulhem03bISI.pdf" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from imag.fr</span><span class="gs_ggsS">imag.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mrim.imag.fr/publications/2003/PM001/mulhem03bISI.pdf" class=yC21>ModÃ¨les pour rÃ©sumÃ©s adaptatifs de vidÃ©os</a></h3><div class="gs_a">P Mulhem, J Gensel, <a href="/citations?user=QwWiE6AAAAAJ&amp;hl=en&amp;oi=sra">H Martin</a> - INGENIERIE DES SYSTEMS D  &hellip;, 2002 - mrim.imag.fr</div><div class="gs_rs">RÃSUMÃ. La vidÃ©o est un mÃ©dia qui pose des problÃ¨mes complexes en raison du volume <br>important de donnÃ©es Ã  traiter et de la difficultÃ© de reprÃ©senter et d&#39;extraire des informations <br>de son contenu. Nous proposons d&#39;annoter le contenu d&#39;une vidÃ©o Ã  l&#39;aide de Graphes <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1550349735689949545&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 5</a> <a href="/scholar?q=related:aZFr2d7ygxUJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/22/0B/RN130434150.html?source=googlescholar" class="gs_nph" class=yC23>BL Direct</a> <a href="/scholar?cluster=1550349735689949545&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'aZFr2d7ygxUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md20', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md20" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:aZFr2d7ygxUJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://hal.archives-ouvertes.fr/docs/00/12/27/87/PDF/guironnet.pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/tel-00122787/" class=yC24>MÃ©thodes de rÃ©sumÃ© de vidÃ©o Ã  partir d&#39;informations bas niveau, du mouvement de camÃ©ra ou de l&#39;attention visuelle</a></h3><div class="gs_a">M Guironnet - 2006 - hal.archives-ouvertes.fr</div><div class="gs_rs">Durant cette derniÃ¨re dÃ©cennie, la technologie Â«numÃ©riqueÂ» a rÃ©volutionnÃ© les moyens de <br>communication avec l&#39;arrivÃ©e entre autres des tÃ©lÃ©phones portables, de l&#39;internet Ã  haut <br>dÃ©bit et de la tÃ©lÃ©vision numÃ©rique. L&#39;explosion des moyens de communication a conduit <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9455501545496907007&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 4</a> <a href="/scholar?q=related:_wyA6I6wOIMJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9455501545496907007&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'_wyA6I6wOIMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md21', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md21" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:_wyA6I6wOIMJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=10451482474085386994&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S2090123210000408" class=yC26>Particle swarm optimisation based video abstraction</a></h3><div class="gs_a">MB Fayk, HA El Nemr, MM Moussa - Journal of Advanced Research, 2010 - Elsevier</div><div class="gs_rs">Video abstraction is a basic step for intelligent access to video and multimedia databases <br>which facilitates content-based video indexing, retrieving and browsing. This paper presents <br>a new video abstraction scheme. The proposed method relies on two stages. First, video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10665289419024272620&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 4</a> <a href="/scholar?q=related:7IysLzi4ApQJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10665289419024272620&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'7IysLzi4ApQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1957872" class=yC27>Hierarchical keyframe-based video summarization using QR-decomposition and modified k-means clustering</a></h3><div class="gs_a">A Amiri, M Fathy - EURASIP Journal on Advances in Signal Processing, 2010 - dl.acm.org</div><div class="gs_rs">Abstract We propose a novel hierarchical keyframe-based video summarization system <br>using QR-decomposition. Specially, we attend to the challenges of defining some measures <br>to detect the dynamicity of a shot and video and extracting appropriate keyframes that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4622628974223385672&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 3</a> <a href="/scholar?q=related:SGQDcY_gJkAJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4622628974223385672&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'SGQDcY_gJkAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://ipac.kacst.edu.sa/eDoc/2006/156426_1.pdf" class=yC29><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kacst.edu.sa</span><span class="gs_ggsS">kacst.edu.sa <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ipac.kacst.edu.sa/eDoc/2006/156426_1.pdf" class=yC28>Iterative image based video summarization by node segmentation</a></h3><div class="gs_a">N Vasudevan, <a href="/citations?user=elmWdycAAAAJ&amp;hl=en&amp;oi=sra">A Jain</a>, H Agrawal - 18th National Computer  &hellip;, 2006 - ipac.kacst.edu.sa</div><div class="gs_rs">Abstract In this paper, we propose a simple video summarization system based on removal <br>of similar frames and the maintenance of unique frames. It tries to capture the temporal <br>content of the frame and to output the video with a length specified by the user. It aims at <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12228482401643765306&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 2</a> <a href="/scholar?q=related:Op4XfPRLtKkJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12228482401643765306&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'Op4XfPRLtKkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md24', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md24" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Op4XfPRLtKkJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Content-based video analysis for knowledge discovery</h3><div class="gs_a">CH Yeh, SH Lee, <a href="/citations?user=81d60okAAAAJ&amp;hl=en&amp;oi=sra">CCJ Kuo</a> - Handbook of Pattern Recognition and  &hellip;, 2005 - World Scientific</div><div class="gs_fl"><a href="/scholar?cites=8606594468402681040&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 2</a> <a href="/scholar?q=related:0NT7vhHEcHcJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'0NT7vhHEcHcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ci.nii.ac.jp/naid/110003670954/" class=yC2A>æ··ååå¸è¡¨ç¾ã«åºã¥ããããªã®é¡ä¼¼æ¤ç´¢</a></h3><div class="gs_a">å ç°æ¿äºï¼ äºä¸åå¹³ï¼ æµ¦æµåä¸ - æ åæå ±ã¡ãã£ã¢å­¦ä¼èª: æ åæå ±ã¡ãã£ã¢, 2002 - ci.nii.ac.jp</div><div class="gs_rs">æé² A method is described for searching for videos similar to a query. The video frames are <br>segmented into shots by clustering consecutive frames based on their color histograms. <br>Segmentation performance is improved by adding temporal information to the color data. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12003503654116253800&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 2</a> <a href="/scholar?q=related:aOCz4_kClaYJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12003503654116253800&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'aOCz4_kClaYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:aOCz4_kClaYJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=eT4V2TBhE6UC&amp;oi=fnd&amp;pg=PA311&amp;ots=BW3rV-Oj2k&amp;sig=g4fI5WdycIFS3VFYXv58JIHJLYc" class=yC2B>15.1 Evolution of Mobile Multimedia</a></h3><div class="gs_a">S Siltanen, C Woodward, S Valli&hellip; - &hellip; : Audio, Video, Text, 2008 - books.google.com</div><div class="gs_rs">Multimedia applications have existed as long as there have been personal computers <br>supporting the playback of audiovisual (AV) content. Especially after PCs were equipped <br>with a network access for downloading content, the development of multimedia <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14200121506104112256&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 1</a> <a href="/scholar?q=related:gHS4Okb3EMUJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'gHS4Okb3EMUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://vision.ece.ucsb.edu/publications/04ThesisXinding.pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucsb.edu</span><span class="gs_ggsS">ucsb.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://vision.ece.ucsb.edu/publications/04ThesisXinding.pdf" class=yC2C>Motion Activity for Video Indexing</a></h3><div class="gs_a">X Sun - 2004 - vision.ece.ucsb.edu</div><div class="gs_rs">Digital video is playing an increasingly important role in our daily life. In recent years, the <br>development of software and hardware technology has enabled the creation of a large <br>amount of digital video content. Due to the rapid increase in the size of digital video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12772188113583056007&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 1</a> <a href="/scholar?q=related:h6SVWmvtP7EJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12772188113583056007&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'h6SVWmvtP7EJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md28', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md28" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:h6SVWmvtP7EJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:h6SVWmvtP7EJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=680131113377766680&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1204912" class=yC2E>Content-based summarization for personal image library</a></h3><div class="gs_a">JH Lim, J Li, P Mulhem, Q Tan - Digital Libraries, 2003.  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the accumulation of consumer&#39;s personal image library, the problem of <br>managing, browsing, querying and presenting photos effectively and efficiently would <br>become critical. We propose a framework for automatic organization of personal image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10044142509412809671&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 1</a> <a href="/scholar?q=related:x3vzamX2Y4sJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10044142509412809671&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'x3vzamX2Y4sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Histogram Evolution-Video Summarisation</h3><div class="gs_a">T Wan - 2003 - University of Bristol</div><div class="gs_fl"><a href="/scholar?cites=8140153627418276619&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 1</a> <a href="/scholar?q=related:C7Nfg5ii93AJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'C7Nfg5ii93AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://144.206.159.178/ft/CONF/16408942/16408956.pdf" class=yC30><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 144.206.159.178</span><span class="gs_ggsS">144.206.159.178 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=812719" class=yC2F>Semantic video indexing using context-dependent fusion</a></h3><div class="gs_a">DJ Kim, H Frigui, A Fadeev - Electronic  &hellip;, 2008 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract We present a novel method for fusing the results of multiple semantic video <br>indexing algorithms that use different types of feature descriptors and different classification <br>methods. This method, called Context-Dependent Fusion (CDF), is motivated by the fact <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17021153682732630719&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 1</a> <a href="/scholar?q=related:v0KcrhdJN-wJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17021153682732630719&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'v0KcrhdJN-wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> AJ Theft Prevention Alarm Based Video Summarization Algorithm</h3><div class="gs_a">A Javed, S Noman</div><div class="gs_fl"><a href="/scholar?q=related:c9ayWtw-KjUJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'c9ayWtw-KjUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Video Summarization Using Genetic Algorithm and Information Theory</h3><div class="gs_a"><a href="/citations?user=Ha5PITUAAAAJ&amp;hl=en&amp;oi=sra">Z Zeinalpour</a>, BM Bidgoli, M Fathi</div><div class="gs_fl"><a href="/scholar?q=related:BGjIaiqs0PUJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17712846632628283396&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'BGjIaiqs0PUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5568462" class=yC31>Application of information theory in video abstraction extraction</a></h3><div class="gs_a">Y Jian, D Meng, Y Xiu - Environmental Science and Information &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the development of Internet and broadband multimedia business, video <br>becomes one of the main-stream vehicles in modern information dissemination. How to <br>reduce the cost of video storage, classification and indexing by analyzing the video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14127665361522704258&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=43">Cited by 1</a> <a href="/scholar?q=related:gr8mc8uMD8QJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'gr8mc8uMD8QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="ftp://image.ntua.gr/pub/4dkonto/WIAMIS-2004/wiamis/papers/cr1068.pdf" class=yC33><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntua.gr</span><span class="gs_ggsS">ntua.gr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="ftp://image.ntua.gr/pub/4dkonto/WIAMIS-2004/wiamis/papers/cr1068.pdf" class=yC32>SHOT-ADAPTIVE SAMPLING FOR HOME VIDEO SUMMARIZATION</a></h3><div class="gs_a">N COOHAROJANANANONE, K AIZAWA - image.ntua.gr</div><div class="gs_rs">ABSTRACT In this paper, we propose a new home video summarization by shot-based <br>adaptive sampling using shot information. The characteristics of shots in video should be <br>useful information for summarization algorithm. Our algorithm make use of shot <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:5UC2gtFcsdYJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15470248249905266917&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'5UC2gtFcsdYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md35', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md35" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:5UC2gtFcsdYJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.fujipress.jp/finder/xslt.php?mode=present&amp;inputfile=JACII001100090011.xml" class=yC34>Arbitrary-Shaped Cluster Separation Using One-Dimensional Data Mapping and Histogram Segmentation</a></h3><div class="gs_a">S Hotta, S Kiyasu, S Miyahara - Journal ref: Journal of Advanced  &hellip;, 2007 - fujipress.jp</div><div class="gs_rs">Abstract Of the many clustering methods proposed for separating arbitrarily shaped clusters, <br>most had drawbacks in parameter sensitivity and high-computational cost requiring large <br>amounts of memory. We propose one-dimensional (1D) mapping for separating arbitrarily <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:_wvt7VhR_CIJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'_wvt7VhR_CIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md36', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md36" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_wvt7VhR_CIJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5957780" class=yC35>Key Frames Extraction Algorithm Based on GA</a></h3><div class="gs_a">L Liu, X Wang, S Zhou - Computational Sciences and  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The key frame is a set of discrete frames which contain the most important <br>information in the video. It can greatly reduce the time of video library retrieval by extracting <br>key frames according to an efficient algorithm, and some important information can also <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:VPyC95DpK1oJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6497543696249191508&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'VPyC95DpK1oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://tel.archives-ouvertes.fr/docs/00/12/27/87/PDF/guironnet.pdf" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://tel.archives-ouvertes.fr/docs/00/12/27/87/PDF/guironnet.pdf" class=yC36>Titre: MÃTHODES DE RÃSUMÃ DE VIDÃO Ã PARTIR D&#39;INFORMATIONS BAS NIVEAU, DU MOUVEMENT DE CAMÃRA OU DE L&#39;ATTENTION VISUELLE</a></h3><div class="gs_a">M GUIRONNET - 2007 - tel.archives-ouvertes.fr</div><div class="gs_rs">Durant cette derniÃ¨re dÃ©cennie, la technologie Â«numÃ©riqueÂ» a rÃ©volutionnÃ© les moyens de <br>communication avec l&#39;arrivÃ©e entre autres des tÃ©lÃ©phones portables, de l&#39;internet Ã  haut <br>dÃ©bit et de la tÃ©lÃ©vision numÃ©rique. L&#39;explosion des moyens de communication a conduit <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:aKEiBgoxw14J:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6828355379146170728&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'aKEiBgoxw14J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md38', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md38" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:aKEiBgoxw14J:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Indexation et rÃ©sumÃ© de vidÃ©os</h3><div class="gs_a">M GUIRONNET</div><div class="gs_fl"><a href="/scholar?q=related:zRUACCl73SAJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2368184395224847821&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'zRUACCl73SAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> <a href="http://ci.nii.ac.jp/naid/110003239794/" class=yC38>D-12-71 Shot importance based for home video summarization</a></h3><div class="gs_a">N COOHAROJANANONE, K AIZAWA - é»å­æå ±éä¿¡å­¦ä¼ç·åå¤§ä¼è¬æ¼ &hellip;, 2003 - ci.nii.ac.jp</div><div class="gs_fl"><a href="/scholar?q=related:pa_uH2UZbV0J:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6732064940106887077&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'pa_uH2UZbV0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md40', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md40" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pa_uH2UZbV0J:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> First Draft und Diskussionsgrundlage</h3><div class="gs_a">B Baumann, C Meinel, H Sack, C Willems&hellip; - 2009</div><div class="gs_fl"><a href="/scholar?q=related:tUZEi6_TG98J:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'tUZEi6_TG98J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ci.nii.ac.jp/naid/110006837253/" class=yC39>ã·ã§ããç¹å¾´ãç¨ãããã¼ã ãããªè¦ç´</a></h3><div class="gs_a">N Cooharojananone, K Aizawa - æ åæå ±ã¡ãã£ã¢å­¦ä¼èª: æ åæå ±ã¡ãã£ã¢, 2004 - ci.nii.ac.jp</div><div class="gs_rs">æé² We propose a home video summarization representing by a group of representative <br>frames (R-frames) from each shot. Each shot, a number of R-frames is calculated by the shot <br>duration and motion activity. When events of shots are not related to each other (One tape <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:bgYBpxRZKLgJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13269954247283246702&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'bgYBpxRZKLgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md42', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md42" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:bgYBpxRZKLgJ:scholar.google.com/&amp;hl=en&amp;num=43&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
