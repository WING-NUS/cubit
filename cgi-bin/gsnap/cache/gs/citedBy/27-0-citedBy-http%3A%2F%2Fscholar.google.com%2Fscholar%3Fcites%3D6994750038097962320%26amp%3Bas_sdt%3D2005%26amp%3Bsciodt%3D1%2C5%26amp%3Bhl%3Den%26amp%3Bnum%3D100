Total results = 27
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://ismir2008.ismir.net/papers/ISMIR2008_126.pdf.." class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ismir.net</span><span class="gs_ggsS">ismir.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA395&amp;ots=oDOSrGiza3&amp;sig=kVRw7nokt6M8mvDFGnkwKEgfrK4" class=yC0>Segmentation-based lyrics-audio alignment using dynamic programming</a></h3><div class="gs_a">K Lee, M Cremer - &hellip;  of the 9th International Conference on Music  &hellip;, 2008 - books.google.com</div><div class="gs_rs">ABSTRACT In this paper, we present a system for automatic alignment of textual lyrics with <br>musical audio. Given an input audio signal, structural segmentation is ï¬rst performed and <br>similar segments are assigned a label by computing the distance between the segment <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13686225526189618520&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 14</a> <a href="/scholar?q=related:WHVqdJo9770J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13686225526189618520&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'WHVqdJo9770J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://asmp.eurasipjournals.com/content/pdf/1687-4722-2010-546047.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurasipjournals.com</span><span class="gs_ggsS">eurasipjournals.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1863626" class=yC2>Automatic recognition of lyrics in singing</a></h3><div class="gs_a">A Mesaros, <a href="/citations?user=hRjwVoMAAAAJ&amp;hl=en&amp;oi=sra">T Virtanen</a> - EURASIP Journal on Audio, Speech, and Music  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract The paper considers the task of recognizing phonemes and words from a singing <br>input by using a phonetic hidden Markov model recognizer. The system is targeted to both <br>monophonic singing and singing in polyphonic music. A vocal separation algorithm is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16913021310027574744&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 8</a> <a href="/scholar?q=related:2C3OvkUft-oJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16913021310027574744&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'2C3OvkUft-oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://staff.aist.go.jp/m.goto/PAPER/AES2011goto.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.aes.org/e-lib/browse.cfm?elib=15970" class=yC4>Music Listening in the Future: Augmented Music-Understanding Interfaces and Crowd Music Listening</a></h3><div class="gs_a"><a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Watermark, 2012 - aes.org</div><div class="gs_rs">In the future, music listening can be more active, more immersive, richer, and deeper by <br>using automatic music-understanding technologies (semantic audio analysis). In the first half <br>of this invited talk, four Augmented Music-Understanding Interfaces that facilitate deeper <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15463436103312633042&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 4</a> <a href="/scholar?q=related:0mzmCDUpmdYJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15463436103312633042&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'0mzmCDUpmdYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072531" class=yC6>The need for music information retrieval with user-centered and multimodal strategies</a></h3><div class="gs_a">C Liem, <a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a>, <a href="/citations?user=JICtaa0AAAAJ&amp;hl=en&amp;oi=sra">D Eck</a>, <a href="/citations?user=yPgxxpwAAAAJ&amp;hl=en&amp;oi=sra">G Tzanetakis</a>&hellip; - Proceedings of the 1st  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Music is a widely enjoyed content type, existing in many multifaceted <br>representations. With the digital information age, a lot of digitized music information has <br>theoretically become available at the user&#39;s fingertips. However, the abundance of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7516642515667629651&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 4</a> <a href="/scholar?q=related:U7b3CYt6UGgJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'U7b3CYt6UGgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://staff.aist.go.jp/m.goto/PAPER/ICASSP2010goto.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5495212" class=yC7>Singing information processing based on singing voice modeling</a></h3><div class="gs_a"><a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a>, T Saitou, <a href="/citations?user=qkzorZgAAAAJ&amp;hl=en&amp;oi=sra">T Nakano</a>&hellip; - Acoustics Speech and  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose a novel area of research referred to as singing <br>information processing. To shape the concept of this area, we first introduce singing <br>understanding systems for synchronizing between vocal melody and corresponding lyrics, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11024960718757932610&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 3</a> <a href="/scholar?q=related:Qq4zaFyHAJkJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11024960718757932610&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Qq4zaFyHAJkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://staff.aist.go.jp/h.fujihara/pdf/waspaa2009_fujihara.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5346497" class=yC9>A novel framework for recognizing phonemes of singing voice in polyphonic music</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a>, <a href="/citations?user=8cX3pewAAAAJ&amp;hl=en&amp;oi=sra">HG Okuno</a> - Applications of Signal  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A novel method is described that can be used to recognize the phoneme of a <br>singing voice (vocal) in polyphonic music. Though we focus on the voiced phoneme in this <br>paper, this method is design to concurrently recognize other elements of a singing voice <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2904691984875507359&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 3</a> <a href="/scholar?q=related:n85tdPiJTygJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2904691984875507359&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'n85tdPiJTygJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.matthiasmauch.net/_pdf/mauch_laa_2010.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from matthiasmauch.net</span><span class="gs_ggsS">matthiasmauch.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.matthiasmauch.net/_pdf/mauch_laa_2010.pdf" class=yCB>Lyrics-to-audio alignment and phrase-level segmentation using incomplete internet-style chord annotations</a></h3><div class="gs_a"><a href="/citations?user=_gfIN8AAAAAJ&amp;hl=en&amp;oi=sra">M Mauch</a>, H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - &hellip;  of the 7th Sound and Music  &hellip;, 2010 - matthiasmauch.net</div><div class="gs_rs">ABSTRACT We propose two novel lyrics-to-audio alignment methods which make use of <br>additional chord information. In the first method we extend an existing hidden Markov model <br>(HMM) for lyrics alignment [1] by adding a chord model based on the chroma features <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11978336140451915224&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 3</a> <a href="/scholar?q=related:2CXce0GZO6YJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11978336140451915224&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'2CXce0GZO6YJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md6', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md6" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:2CXce0GZO6YJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf" class=yCD>æ­å£°æå ±å¦çã®æè¿ã®ç ç©¶</a></h3><div class="gs_a">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å° - æ¥æ¬é³é¿å­¦ä¼èª, 2008 - staff.aist.go.jp</div><div class="gs_rs">â Recent studies on singing information processing. ââ Masataka Goto, Takeshi Saitou, Tomoyasu <br>Nakano and Hiromasa Fujihara (National Institute of Ad- vanced Industrial Science and Technology <br>(AIST), Tsukuba, 305â8568) e-mail: m.goto@aist.go.jp  </div><div class="gs_fl"><a href="/scholar?cites=3835003938146142193&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 6</a> <a href="/scholar?q=related:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3835003938146142193&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'8cdsVcOrODUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/0/65cd0224ccb5368ec12579950043afe0/$FILE/2011_Mueller_NewDevelopmentsMIR_AES42-Ilmenau.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.aes.org/e-lib/browse.cfm?elib=15944" class=yCF>New Developments in Music Information Retrieval</a></h3><div class="gs_a"><a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a> - Watermark, 2012 - aes.org</div><div class="gs_rs">The digital revolution has brought about a massive increase in the availability and <br>distribution of music-related documents of various modalities comprising textual, audio, as <br>well as visual material. Therefore, the development of techniques and tools for organizing, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15945428088121631443&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 2</a> <a href="/scholar?q=related:06IGGF2KSd0J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15945428088121631443&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'06IGGF2KSd0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://staff.aist.go.jp/m.goto/PAPER/IEEETASLP201201mauch.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5876304" class=yC11>Integrating Additional Chord Information into HMM-Based Lyrics-to-Audio Alignment</a></h3><div class="gs_a"><a href="/citations?user=_gfIN8AAAAAJ&amp;hl=en&amp;oi=sra">M Mauch</a>, H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Audio, Speech, and Language  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Aligning lyrics to audio has a wide range of applications such as the automatic <br>generation of karaoke scores, song-browsing by lyrics, and the generation of audio <br>thumbnails. Existing methods are restricted to using only lyrics and match them to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3269594826642557923&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 3</a> <a href="/scholar?q=related:4wt8-SvvXy0J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3269594826642557923&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'4wt8-SvvXy0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Singing Phoneme Class Detection In Polyphonic Music Recordings</h3><div class="gs_a">V Ourania - 2008 - magistrska naloga, Univerza  &hellip;</div><div class="gs_fl"><a href="/scholar?cites=11653457682537027000&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 1</a> <a href="/scholar?q=related:uJWqBAVmuaEJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11653457682537027000&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'uJWqBAVmuaEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://www.nuriaoliver.com/papers/MuViSync_ICME2010.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nuriaoliver.com</span><span class="gs_ggsS">nuriaoliver.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5583863" class=yC13>Muvisync: Realtime music video alignment</a></h3><div class="gs_a">R Macrae, <a href="/citations?user=Sg0XkAsAAAAJ&amp;hl=en&amp;oi=sra">X Anguera</a>, N Oliver - Multimedia and Expo (ICME),  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In recent years, the popularity of compressed music files and online music <br>downloads has increased dramatically. Today&#39;s users own large digital collections of high <br>quality music on their computers and portable devices to be played in their homes or on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17653439979110655143&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 2</a> <a href="/scholar?q=related:p-SRCiGe_fQJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17653439979110655143&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'p-SRCiGe_fQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/q321132ru207p862.pdf" class=yC15>Simultaneous synchronization of text and speech for broadcast news subtitling</a></h3><div class="gs_a">J Gao, Q Zhao, T Li, Y Yan - Advances in Neural NetworksâISNN 2009, 2009 - Springer</div><div class="gs_rs">Abstract. In this paper, we present our initial effort in automatic generation of subtitle for live <br>broadcast news programs, utilizing the fact that nearly perfect transcriptions are available. <br>Instead of using the former error-prone automatic-speech-recognition (ASR)-based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8580359405998043081&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 1</a> <a href="/scholar?q=related:ye-xR2uPE3cJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8580359405998043081&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'ye-xR2uPE3cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Online detecting end times of spoken utterances for synchronization of live speech and its transcripts</h3><div class="gs_a">J Gao, Q Zhao, T Li, Y Yan - Proc. Interspeech-2009, ISCA, 2009</div><div class="gs_fl"><a href="/scholar?cites=1869832719323846617&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 1</a> <a href="/scholar?q=related:2VezkPP68hkJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1869832719323846617&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'2VezkPP68hkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://141.84.8.93/pubdb/publications/pub/baur2010ismir/baur2010ismir.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 141.84.8.93</span><span class="gs_ggsS">141.84.8.93 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://141.84.8.93/pubdb/publications/pub/baur2010ismir/baur2010ismir.pdf" class=yC16>SongWords: Exploring Music Collections Through Lyrics</a></h3><div class="gs_a"><a href="/citations?user=zqUTdkkAAAAJ&amp;hl=en&amp;oi=sra">D Baur</a>, B Steinmayr, <a href="/citations?user=8_z5YpQAAAAJ&amp;hl=en&amp;oi=sra">A Butz</a> - 2010 - 141.84.8.93</div><div class="gs_rs">ABSTRACT The lyrics of a song are an interesting, yet underused type of symbolic music <br>data. We present SongWords, an application for tabletop computers that allows browsing <br>and exploring a music collection based on its lyrics. Song-Words can present the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:_mxTG_jYneAJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16185331195961371902&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'_mxTG_jYneAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md14', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md14" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_mxTG_jYneAJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www.doc.gold.ac.uk/~mas03dm/papers/Ewertetal_HarmonicAnalysis_2012.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gold.ac.uk</span><span class="gs_ggsS">gold.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6165370" class=yC18>Towards Cross-Version Harmonic Analysis of Music</a></h3><div class="gs_a"><a href="/citations?user=kkwnObwAAAAJ&amp;hl=en&amp;oi=sra">S Ewert</a>, <a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M Muller</a>, V Konz&hellip; - Multimedia, IEEE  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract For a given piece of music, there often exist multiple versions belonging to the <br>symbolic (eg, MIDI representations), acoustic (audio recordings), or visual (sheet music) <br>domain. Each type of information allows for applying specialized, domain-specific <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1976494073348921011&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=27">Cited by 1</a> <a href="/scholar?q=related:s5agRuPqbRsJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1976494073348921011&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'s5agRuPqbRsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.unix.eng.ua.edu/~mrrao/acm-paper.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ua.edu</span><span class="gs_ggsS">ua.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2184559" class=yC1A>RRA: An audio format for single-source music and lyrics</a></h3><div class="gs_a">M Rao, JC Lusth - Proceedings of the 50th Annual Southeast Regional  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Karaoke music has world-wide appeal, especially for non-professional singers. <br>However, most karaoke-audio architectures involve separate text and audio data streams <br>which run in different threads. Such an approach suffers from timing synchronization <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:wZrLWAYqsUsJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5454186830470290113&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'wZrLWAYqsUsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://drops.dagstuhl.de/opus/volltexte/2012/3464/pdf/3.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dagstuhl.de</span><span class="gs_ggsS">dagstuhl.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://drops.dagstuhl.de/opus/volltexte/2012/3464/" class=yC1C>Lyrics-to-Audio Alignment and its Application}}</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Multimodal Music Processing} - drops.dagstuhl.de</div><div class="gs_rs">Abstract Automatic lyrics-to-audio alignment techniques have been drawing attention in the <br>last years and various studies have been made in this field. The objective of lyrics-to-audio <br>alignment is to estimate a temporal relationship between lyrics and musical audio signals <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:pR0KaNFrpC0J:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3288872175025135013&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'pR0KaNFrpC0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0167639311000021" class=yC1E>Towards precise and robust automatic synchronization of live speech and its transcripts</a></h3><div class="gs_a">J Gao, Q Zhao, Y Yan - Speech Communication, 2011 - Elsevier</div><div class="gs_rs">This paper presents our efforts in automatically synchronizing spoken utterances with their <br>transcripts (textual contents)(ASUT), where the speech is a live stream and its corresponding <br>transcripts are known. This task is first simplified to the problem of online detecting the end <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:flSdhSNB6hwJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2083549398433617022&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'flSdhSNB6hwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.173.5164&amp;rep=rep1&amp;type=pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.173.5164&amp;rep=rep1&amp;type=pdf" class=yC1F>Linking music-related information and audio data</a></h3><div class="gs_a">R Macrae - 2008 - Citeseer</div><div class="gs_rs">Abstract Due to recent technological advancement we now have a near endless supply of <br>musical content. There is now a growing interest in new ways of interacting with and filtering <br>this music. Examples are music editing suites that can identify audio and align segments <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MKyRnl4l-DkJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4177129742703635504&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'MKyRnl4l-DkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:MKyRnl4l-DkJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://www.audiolabs-erlangen.de/meinard/students/thesis/2012_EwertSebastian_SignalProcessingMethods_Phd-Thesis.pdf" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from audiolabs-erlangen.de</span><span class="gs_ggsS">audiolabs-erlangen.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.audiolabs-erlangen.de/meinard/students/thesis/2012_EwertSebastian_SignalProcessingMethods_Phd-Thesis.pdf" class=yC21>Signal Processing Methods for Music Synchronization, Audio Matching, and Source Separation</a></h3><div class="gs_a"><a href="/citations?user=kkwnObwAAAAJ&amp;hl=en&amp;oi=sra">S Ewert</a> - 2012 - audiolabs-erlangen.de</div><div class="gs_rs">Abstract The field of music information retrieval (MIR) aims at developing techniques and <br>tools for organizing, understanding, and searching multimodal information in large music <br>collections in a robust, efficient and intelligent manner. In this context, this thesis presents <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'Li_TocU8mCEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md20', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md20" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Li_TocU8mCEJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://downloads.hindawi.com/journals/asmp/2010/546047.pdf" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://downloads.hindawi.com/journals/asmp/2010/546047.pdf" class=yC23>Automatic Recognition of Lyrics in Singing</a></h3><div class="gs_a">M Annamaria, <a href="/citations?user=hRjwVoMAAAAJ&amp;hl=en&amp;oi=sra">V Tuomas</a> - EURASIP Journal on Audio,  &hellip;, 2010 - downloads.hindawi.com</div><div class="gs_rs">The paper considers the task of recognizing phonemes and words from a singing input by <br>using a phonetic hidden Markov model recognizer. The system is targeted to both <br>monophonic singing and singing in polyphonic music. A vocal separation algorithm is <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:UoAHx5uLsEAJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4661379115503812690&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'UoAHx5uLsEAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md21', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md21" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:UoAHx5uLsEAJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://drops.dagstuhl.de/opus/volltexte/2012/3463/pdf/2.pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dagstuhl.de</span><span class="gs_ggsS">dagstuhl.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://drops.dagstuhl.de/opus/volltexte/2012/3463/" class=yC25>Linking Sheet Music and AudioâChallenges and New Approaches</a></h3><div class="gs_a">V Thomas, C Fremerey, <a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a>, M Clausen - 2012 - drops.dagstuhl.de</div><div class="gs_rs">Abstract Score and audio files are the two most important ways to represent, convey, record, <br>store, and experience music. While score describes a piece of music on an abstract level <br>using symbols such as notes, keys, and measures, audio files allow for reproducing a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:8cjl0nuN9lgJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6410466682567248113&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'8cjl0nuN9lgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5496295" class=yC27>Automatic Synchronization of live speech and its Transcripts based on a frame-synchronous likelihood ratio test</a></h3><div class="gs_a">J Gao, Q Zhao, Y Yan - Acoustics Speech and Signal  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present our initial efforts in the task of Automatically Synchronizing <br>live spoken Utterances with their Transcripts (textual contents)(ASUT) when the texts are <br>known. We treat it as a online speech-text alignment problem. And it is further simplified <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:hb7403l05OcJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16709608584047541893&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'hb7403l05OcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://ismir2012.ismir.net/event/papers/361-ismir-2012.pdf" class=yC29><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ismir.net</span><span class="gs_ggsS">ismir.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ismir2012.ismir.net/event/papers/361-ismir-2012.pdf" class=yC28>RANKING LYRICS FOR ONLINE SEARCH</a></h3><div class="gs_a">R Macrae, <a href="/citations?user=9GvyqXEAAAAJ&amp;hl=en&amp;oi=sra">S Dixon</a> - ismir2012.ismir.net</div><div class="gs_rs">ABSTRACT When someone wishes to find the lyrics for a song they typically go online and <br>use a search engine. There are a large number of lyrics available on the internet as the effort <br>required to transcribe and post lyrics is minimal. These lyrics are promptly returned to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:882m1oY-pkoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'882m1oY-pkoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md24', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md24" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:882m1oY-pkoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://staff.aist.go.jp/m.goto/PAPER/ASJ200909fujihara.pdf" class=yC2B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://staff.aist.go.jp/m.goto/PAPER/ASJ200909fujihara.pdf" class=yC2A>æ¥½æ²ä¸­ã®æ­å£°ã®åºæ¬å¨æ³¢æ°ã¨é³ç´ ãåææ¨å®å¯è½ãªãã¬ã¼ã ã¯ã¼ã¯</a></h3><div class="gs_a">è¤åå¼å°ï¼ å¾è¤çå­ï¼ å¥¥ä¹å(äº¬å¤§ - staff.aist.go.jp</div><div class="gs_rs">é³æ¥½ã¯, ç£æ¥­çã«ãæåçã«ãéè¦ãªã³ã³ãã³ãã§ãã, ãã®ä¸­ã§ãæ­å£°ã¯éè¦ãªå½¹å²ãæããã¦<br>ãã. æ¬ç¨¿ã§ã¯, æ··åé³ä¸­ã®æ­å£°ã®æ­è© (é³ç´ ) ã¨åºæ¬å¨æ³¢æ° (F0) ãåæã«èªè­ããããã®ææ³, <br>WPST (Weighted composition of Probabilistic Spectral Template) æ³ãææ¡ã, F0 æ¨å®ã¨<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:uGdvSfHJOboJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'uGdvSfHJOboJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md25', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md25" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:uGdvSfHJOboJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007goto.pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007goto.pdf" class=yC2C>æ­å£°æå ±å¦ç: æ­å£°ãå¯¾è±¡ã¨ããé³æ¥½æå ±å¦ç</a></h3><div class="gs_a">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å° - 2010 - staff.aist.go.jp</div><div class="gs_rs">æ¬ç¨¿ã§ã¯,ãæ­å£°æå ±å¦çã ã¨åä»ããæ°ããç ç©¶é åã«ãããæãã®ç ç©¶äºä¾ãç´¹ä»ãã. <br>ããã¯æ­å£°ã«å¯¾ããé³æ¥½æå ±å¦çã§ãã, ãã®ç ç©¶å¯¾è±¡ã¯å¤å²ã«æ¸¡ãã, æ¬ç¨¿ã§ã¯, <br>æ­å£°çè§£ã·ã¹ãã , æ­å£°ã«åºã¥ãé³æ¥½æå ±æ¤ç´¢ã·ã¹ãã , æ­å£°åæã·ã¹ãã ã®ä¸ã¤ã®éè¦ãª<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:uzd8aAf29qoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12319304342396745659&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'uzd8aAf29qoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:uzd8aAf29qoJ:scholar.google.com/&amp;hl=en&amp;num=27&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
