Total results = 80
<div class="gs_r" style="z-index:400"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320307000247" class=yC0>Video summarisation: A conceptual framework and survey of the state of the art</a></h3><div class="gs_a">AG Money, H Agius - Journal of Visual Communication and Image  &hellip;, 2008 - Elsevier</div><div class="gs_rs">Video summaries provide condensed and succinct representations of the content of a video <br>stream through a combination of still images, video segments, graphical representations and <br>textual descriptors. This paper presents a conceptual framework for video summarisation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7380321859487654168&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 129</a> <a href="/scholar?q=related:GOHP6qArbGYJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7380321859487654168&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'GOHP6qArbGYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.4406&amp;rep=rep1&amp;type=pdf" class=yC2><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1027661" class=yC1>Learning query-class dependent weights in automatic video retrieval</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, J Yang, <a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a> - Proceedings of the 12th annual ACM  &hellip;, 2004 - dl.acm.org</div><div class="gs_rs">Abstract Combining retrieval results from multiple modalities plays a crucial role for video <br>retrieval systems, especially for automatic video retrieval systems without any user feedback <br>and query expansion. However, most of current systems only utilize query independent <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16723400768940408431&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 105</a> <a href="/scholar?q=related:b_4vHWV0FegJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16723400768940408431&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 28 versions</a> <a onclick="return gs_ocit(event,'b_4vHWV0FegJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://avss2012.org/2008papers/gjkw/gk8.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from avss2012.org</span><span class="gs_ggsS">avss2012.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4469885" class=yC3>A novel framework for semantic annotation and personalized retrieval of sports video</a></h3><div class="gs_a">C Xu, J Wang, H Lu, Y Zhang - Multimedia, IEEE Transactions  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Sports video annotation is important for sports video semantic analysis such as <br>event detection and personalization. In this paper, we propose a novel approach for sports <br>video semantic annotation and personalized retrieval. Different from the state of the art <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9150671654910331118&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 70</a> <a href="/scholar?q=related:7ogqR2O3_X4J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/1A/16/RN226783660.html?source=googlescholar" class="gs_nph" class=yC5>BL Direct</a> <a href="/scholar?cluster=9150671654910331118&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'7ogqR2O3_X4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://124.207.250.90/staff/lingyu/ACM-MM-Ad2006.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 124.207.250.90</span><span class="gs_ggsS">124.207.250.90 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1180697" class=yC6>Segmentation, categorization, and identification of commercial clips from TV streams using multimodal analysis</a></h3><div class="gs_a">LY Duan, <a href="/citations?user=7_BkyxEAAAAJ&amp;hl=en&amp;oi=sra">J Wang</a>, Y Zheng, JS Jin, H Lu&hellip; - Proceedings of the 14th  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract TV advertising is ubiquitous, perseverant, and economically vital. Millions of <br>people&#39;s living and working habits are affected by TV commercials. In this paper, we present <br>a multimodal (&quot; visual+ audio+ text&quot;) commercial video digest scheme to segment <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11446355647284931839&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 59</a> <a href="/scholar?q=related:_2w02dCf2Z4J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11446355647284931839&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'_2w02dCf2Z4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://ciir.cs.umass.edu/pubfiles/mm-385.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from umass.edu</span><span class="gs_ggsS">umass.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1416476" class=yC8>Combining text and audio-visual features in video indexing</a></h3><div class="gs_a"><a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a>, <a href="/citations?user=_0aMq28AAAAJ&amp;hl=en&amp;oi=sra">R Manmatha</a>&hellip; - Acoustics, Speech, and  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We discuss the opportunities, state of the art, and open research issues in using <br>multi-modal features in video indexing. Specifically, we focus on how imperfect text data <br>obtained by automatic speech recognition (ASR) may be used to help solve challenging <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12681820938198234719&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 47</a> <a href="/scholar?q=related:X7ZqvvPg_q8J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12681820938198234719&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 21 versions</a> <a onclick="return gs_ocit(event,'X7ZqvvPg_q8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1026733" class=yCA>The fusion of audio-visual features and external knowledge for event detection in team sports video</a></h3><div class="gs_a">H Xu, TS Chua - Proceedings of the 6th ACM SIGMM international  &hellip;, 2004 - dl.acm.org</div><div class="gs_rs">Abstract Most existing systems detect events in broadcast team sports video using only <br>internal audio-visual (AV) features with limited success. We found that there are many widely <br>available external knowledge sources-such as match reports and real-time game logs in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10308022348565130020&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 39</a> <a href="/scholar?q=related:JCsDN7ZzDY8J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10308022348565130020&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'JCsDN7ZzDY8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://pdf.aminer.org/000/098/325/topic_threading_for_structuring_a_large_scale_news_video_archive.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aminer.org</span><span class="gs_ggsS">aminer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/XXRKX4EW4B2B3UC8.pdf" class=yCB>Topic threading for structuring a large-scale news video archive</a></h3><div class="gs_a"><a href="/citations?user=8PXJm98AAAAJ&amp;hl=en&amp;oi=sra">I Ide</a>, H Mo, N Katayama, <a href="/citations?user=7aEF5cQAAAAJ&amp;hl=en&amp;oi=sra">S Satoh</a> - Image and Video Retrieval, 2004 - Springer</div><div class="gs_rs">We are building a broadcast news video archive where topics of interest can be retrieved <br>and tracked easily. This paper introduces a structuring method applied to the accumulated <br>news videos. First they are segmented into topic units and then threaded according to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8692386434893137812&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 36</a> <a href="/scholar?q=related:lMc6hmiPoXgJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/58/48/RN153246063.html?source=googlescholar" class="gs_nph" class=yCD>BL Direct</a> <a href="/scholar?cluster=8692386434893137812&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'lMc6hmiPoXgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://old-site.clsp.jhu.edu/ws2004/groups/ws04vstxt/documents/ACMpaper.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jhu.edu</span><span class="gs_ggsS">jhu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101149.1101154" class=yCE>Joint visual-text modeling for automatic retrieval of multimedia documents</a></h3><div class="gs_a">G Iyengar, <a href="/citations?user=1KEMrHkAAAAJ&amp;hl=en&amp;oi=sra">P Duygulu</a>, S Feng, <a href="/citations?user=JhnGUTgAAAAJ&amp;hl=en&amp;oi=sra">P Ircing</a>&hellip; - Proceedings of the 13th &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract In this paper we describe a novel approach for jointly modeling the text and the <br>visual components of multimedia documents for the purpose of information retrieval (IR). We <br>propose a novel framework where individual components are developed to model <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8103894176717922851&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 33</a> <a href="/scholar?q=related:I-KgRNHQdnAJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8103894176717922851&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 24 versions</a> <a onclick="return gs_ocit(event,'I-KgRNHQdnAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.cs.clemson.edu/~jzwang/ustc11/mm2008/p389-yeh.pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459412" class=yC10>Photo-based question answering</a></h3><div class="gs_a"><a href="/citations?user=mDNhPjAAAAAJ&amp;hl=en&amp;oi=sra">T Yeh</a>, JJ Lee, <a href="/citations?user=bh-uRFMAAAAJ&amp;hl=en&amp;oi=sra">T Darrell</a> - Proceedings of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Photo-based question answering is a useful way of finding information about <br>physical objects. Current question answering (QA) systems are text-based and can be <br>difficult to use when a question involves an object with distinct visual features. A photo-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14516586603822883264&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 34</a> <a href="/scholar?q=related:wKEOoJpGdckJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14516586603822883264&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'wKEOoJpGdckJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/r742245481q23631.pdf" class=yC12>A review of text and image retrieval approaches for broadcast news video</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a> - Information Retrieval, 2007 - Springer</div><div class="gs_rs">Abstract The effectiveness of a video retrieval system largely depends on the choice of <br>underlying text and image retrieval components. The unique properties of video collections <br>(eg, multiple sources, noisy features and temporal relations) suggest we examine the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9278500574809399146&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 35</a> <a href="/scholar?q=related:ap_F-Rzbw4AJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0D/2C/RN216038550.html?source=googlescholar" class="gs_nph" class=yC13>BL Direct</a> <a href="/scholar?cluster=9278500574809399146&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'ap_F-Rzbw4AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.lti.cs.cmu.edu/research/thesis/2006/rong_yan.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.lti.cs.cmu.edu/research/thesis/2006/rong_yan.pdf" class=yC14>Probabilistic models for combining diverse knowledge sources in multimedia retrieval</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a> - 2006 - lti.cs.cmu.edu</div><div class="gs_rs">Abstract In recent years, the multimedia retrieval community is gradually shifting its <br>emphasis from analyzing one media source at a time to exploring the opportunities of <br>combining diverse knowledge sources from correlated media types and context. In order <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1282854589144669096&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 32</a> <a href="/scholar?q=related:qJtytHOdzREJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1282854589144669096&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'qJtytHOdzREJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:qJtytHOdzREJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:qJtytHOdzREJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=9607173453927529710&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.119.3200&amp;rep=rep1&amp;type=pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432643" class=yC16>LyricAlly: Automatic synchronization of textual lyrics to acoustic music signals</a></h3><div class="gs_a"><a href="/citations?user=aNVcd3EAAAAJ&amp;hl=en&amp;oi=sra">MY Kan</a>, Y Wang, D Iskandar, TL New&hellip; - Audio, Speech, and  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present LyricAlly, a prototype that automatically aligns acoustic musical signals <br>with their corresponding textual lyrics, in a manner similar to manually-aligned karaoke. We <br>tackle this problem based on a multimodal approach, using an appropriate pairing of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6994750038097962320&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 27</a> <a href="/scholar?q=related:UKV6kRlYEmEJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/47/0A/RN222855432.html?source=googlescholar" class="gs_nph" class=yC18>BL Direct</a> <a href="/scholar?cluster=6994750038097962320&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'UKV6kRlYEmEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.119.7376&amp;rep=rep1&amp;type=pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1336123" class=yC19>Question answering on lecture videos: a multifaceted approach</a></h3><div class="gs_a">J Cao, JF Nunamaker Jr - &hellip; , 2004. Proceedings of the 2004 Joint  &hellip;, 2004 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we introduce a multifaceted approach for question answering on <br>lecture videos. Text extracted from PowerPoint slides associated with the lecture videos is <br>used as a source of domain knowledge to boost the answer extraction performance on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14280652674327303023&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 20</a> <a href="/scholar?q=related:b9MC9vERL8YJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14280652674327303023&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'b9MC9vERL8YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm07-neosyOFF.PDF" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291278" class=yC1B>The use of topic evolution to help users browse and find answers in news video corpus</a></h3><div class="gs_a">SY Neo, Y Ran, HK Goh, Y Zheng, TS Chua&hellip; - Proceedings of the 15th  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Earlier research in news video has been focusing mainly on improving retrieval <br>accuracies given the limited amount of extractable video semantics. In this paper, we <br>propose an enhancement to news video searching by leveraging extractable video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2755699895959884965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 18</a> <a href="/scholar?q=related:pRRFfnQ2PiYJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2755699895959884965&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'pRRFfnQ2PiYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://lms.comp.nus.edu.sg/papers/media/2009/acmmm09-guangda.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631411" class=yC1D>Video reference: question answering on YouTube</a></h3><div class="gs_a">G Li, <a href="/citations?user=BrND3vgAAAAJ&amp;hl=en&amp;oi=sra">Z Ming</a>, H Li, TS Chua - Proceedings of the 17th ACM international  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Community-based question answering systems have become very popular for <br>providing answers to a wide variety of&quot; how-to&quot; questions. However most such systems <br>present only textual answers. In many cases, users would prefer visual answers such as <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16991029052326473054&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 14</a> <a href="/scholar?q=related:Xu1CmOZCzOsJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16991029052326473054&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'Xu1CmOZCzOsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://web.cs.hacettepe.edu.tr/~ilyas/PDF/2008_INFOSCIENCE.pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hacettepe.edu.tr</span><span class="gs_ggsS">hacettepe.edu.tr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0020025508000492" class=yC1F>Natural language querying for video databases</a></h3><div class="gs_a">G Erozel, NK Cicekli, I Cicekli - Information Sciences, 2008 - Elsevier</div><div class="gs_rs">The video databases have become popular in various areas due to the recent advances in <br>technology. Video archive systems need user-friendly interfaces to retrieve video frames. In <br>this paper, a user interface based on natural language processing (NLP) to a video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2725150670489488997&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 14</a> <a href="/scholar?q=related:ZY7xqRiu0SUJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2725150670489488997&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'ZY7xqRiu0SUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://sites.google.com/site/mohanrajunotes/22680043b.pdf" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1385320" class=yC21>Automated question answering from lecture videos: NLP vs. pattern matching</a></h3><div class="gs_a">J Cao, JA Robles-Flores, D Roussinov&hellip; - &hellip; , 2005. HICSS&#39;05.  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper explores the feasibility of automated question answering from lecture <br>video materials used in conjunction with PowerPoint slides. Two popular approaches to <br>question answering are discussed, each separately tested on the text extracted from <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3912459527654463508&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 13</a> <a href="/scholar?q=related:FNjvFzbZSzYJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3912459527654463508&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'FNjvFzbZSzYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://murase.m.is.nagoya-u.ac.jp/publications/141-pdf.pdf" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nagoya-u.ac.jp</span><span class="gs_ggsS">nagoya-u.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/f7469311n7846777.pdf" class=yC23>trackThem: Exploring a large-scale news video archive by tracking human relations</a></h3><div class="gs_a"><a href="/citations?user=8PXJm98AAAAJ&amp;hl=en&amp;oi=sra">I Ide</a>, T Kinoshita, H Mo, N Katayama&hellip; - Information Retrieval  &hellip;, 2005 - Springer</div><div class="gs_rs">Abstract. We propose a novel retrieval method for a very large-scale news video archive <br>based on human relations extracted from the archive itself. This paper presents the idea and <br>the implementation of the method, and also introduces the trackThem interface that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13592340892719707047&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 13</a> <a href="/scholar?q=related:p_OS0AayobwJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/38/29/RN177076786.html?source=googlescholar" class="gs_nph" class=yC25>BL Direct</a> <a href="/scholar?cluster=13592340892719707047&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'p_OS0AayobwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://140.115.112.118/paper%20list/04/CLVQ.pdf" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 140.115.112.118</span><span class="gs_ggsS">140.115.112.118 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1376675" class=yC26>CLVQ: Cross-language video question/answering system</a></h3><div class="gs_a">YC Wu, CH Chang, <a href="/citations?user=tiClE2YAAAAJ&amp;hl=en&amp;oi=sra">YS Lee</a> - Multimedia Software Engineering, &hellip;, 2004 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Multilanguage information retrieval promotes users to browse documents in the <br>form of their mother language, and more and more peoples interested in retrieves short <br>answers rather than a full document. In this paper, we present a cross-language video QA <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9399343266272323911&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 12</a> <a href="/scholar?q=related:RykfnOYscYIJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9399343266272323911&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'RykfnOYscYIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/GW22P0L8G3121812.pdf" class=yC28>METIS: a flexible foundation for the unified management of multimedia assets</a></h3><div class="gs_a">R King, N Popitsch, U Westermann - Multimedia Tools and Applications, 2007 - Springer</div><div class="gs_rs">Abstract Multimedia database systems typically focus on specific kinds of media and/or <br>applications. This article gives an architectural overview of METIS, a database foundation for <br>a unified management of any variety of media that is flexibly customizable to different <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2109864515572971568&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 10</a> <a href="/scholar?q=related:MLjtLpm-Rx0J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/55/63/RN206405427.html?source=googlescholar" class="gs_nph" class=yC29>BL Direct</a> <a href="/scholar?cluster=2109864515572971568&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'MLjtLpm-Rx0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://vireo.cs.cityu.edu.hk/papers/beyond%20search%20event%20driven%20summarization%20for%20web%20videos_acmtmm10.pdf" class=yC2B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2043613" class=yC2A>Beyond search: Event-driven summarization for web videos</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, HK Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - ACM Transactions on  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract The explosive growth of Web videos brings out the challenge of how to efficiently <br>browse hundreds or even thousands of videos at a glance. Given an event-driven query, <br>social media Web sites usually return a large number of videos that are diverse and noisy <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4403112343352155411&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 13</a> <a href="/scholar?q=related:E52k-1j_Gj0J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4403112343352155411&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'E52k-1j_Gj0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://www.mingzhao.name/publications/2006_MMM_PersonX.pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mingzhao.name</span><span class="gs_ggsS">mingzhao.name <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1651320" class=yC2C>Multi-faceted contextual model for person identification in news video</a></h3><div class="gs_a"><a href="/citations?user=9Be5CtEAAAAJ&amp;hl=en&amp;oi=sra">M Zhao</a>, SY Neo, HK Goh&hellip; - Multi-Media Modelling  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Person identification is very important in the domain of multimedia news as it is <br>often the focus of events in news stories and interest of searchers. However, this detection is <br>impeded by the imprecise audio/visual analysis tools. In this paper, we describe a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4346048044377015775&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 9</a> <a href="/scholar?q=related:34XfhatDUDwJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4346048044377015775&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'34XfhatDUDwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://140.115.112.118/paper%20list/08/TCSVT-%20TVQS-Final.pdf" class=yC2F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 140.115.112.118</span><span class="gs_ggsS">140.115.112.118 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4633656" class=yC2E>A robust passage retrieval algorithm for video question answering</a></h3><div class="gs_a">YC Wu, JC Yang - Circuits and Systems for Video Technology,  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present a robust passage retrieval algorithm to extend the <br>conventional text question answering (Q/A) to videos. Users interact with our videoQ/A <br>system through natural language queries, while the top-ranked passage fragments with <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4290824077760933806&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 10</a> <a href="/scholar?q=related:ri-i1MMRjDsJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4290824077760933806&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'ri-i1MMRjDsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.497&amp;rep=rep1&amp;type=pdf" class=yC31><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.497&amp;rep=rep1&amp;type=pdf" class=yC30>Design of content-based multimedia retrieval</a></h3><div class="gs_a">CH Wei, CT Li - &hellip;  of Multimedia Technology and Networking, Idea Group &hellip;, 2005 - Citeseer</div><div class="gs_rs">ABSTRACT As the size of multimedia database grows, the difficulty in finding desired <br>information increases and it becomes impractical to manually annotate all contents and <br>attributes of the media. To copy with these challenges, content-based multimedia retrieval <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3459148438134963008&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 7</a> <a href="/scholar?q=related:QLthWC1dATAJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3459148438134963008&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'QLthWC1dATAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md23', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md23" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:QLthWC1dATAJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://140.115.112.118/paper%20list/08/2009_JASIST_Lee.pdf" class=yC33><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 140.115.112.118</span><span class="gs_ggsS">140.115.112.118 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21002/full" class=yC32>BVideoQA: Online English/Chinese bilingual video question answering</a></h3><div class="gs_a"><a href="/citations?user=tiClE2YAAAAJ&amp;hl=en&amp;oi=sra">YS Lee</a>, YC Wu, JC Yang - Journal of the American Society for  &hellip;, 2009 - Wiley Online Library</div><div class="gs_rs">Abstract This article presents a bilingual video question answering (QA) system, namely <br>BVideoQA, which allows users to retrieve Chinese videos through English or Chinese <br>natural language questions. Our method first extracts an optimal one-to-one string pattern <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7690154811152803922&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 8</a> <a href="/scholar?q=related:UiiGeg7ruGoJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7690154811152803922&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'UiiGeg7ruGoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://cs.fiu.edu/~chens/PDF/MDDM06.pdf" class=yC35><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fiu.edu</span><span class="gs_ggsS">fiu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1623805" class=yC34>Video database modeling and temporal pattern retrieval using hierarchical Markov model mediator</a></h3><div class="gs_a">N Zhao, SC Chen, ML Shyu - Data Engineering Workshops,  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The dream of pervasive multimedia retrieval and reuse will not be realized without <br>incorporating semantics in the multimedia database. As video data is penetrating many <br>information systems, the need for database support for video data evolves. Hence, we <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6868147076290018921&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 7</a> <a href="/scholar?q=related:aX5ne2CPUF8J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6868147076290018921&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'aX5ne2CPUF8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://scholarbank.nus.edu.sg/bitstream/handle/10635/14865/ChaisornL.pdf?sequence=1" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu.sg/handle/10635/14865" class=yC36>A Hierarchical Multi-Modal approach to story segmentation in news video</a></h3><div class="gs_a">L Chaisorn - 2005 - scholarbank.nus.edu.sg</div><div class="gs_rs">This research presents a multi-modal two-level framework for news story segmentation. We <br>divide our system into two levels: shot level that classifies the input video shots into one of <br>the predefined categories using a hybrid of heuristic and learning based approaches; and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7879365373768026534&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 7</a> <a href="/scholar?q=related:psn0jg8hWW0J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7879365373768026534&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'psn0jg8hWW0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://140.115.112.118/bcbb/TVQS3/PSIVT.pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 140.115.112.118</span><span class="gs_ggsS">140.115.112.118 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/l31g152l13667lk2.pdf" class=yC38>A new passage ranking algorithm for video question answering</a></h3><div class="gs_a">YC Wu, <a href="/citations?user=tiClE2YAAAAJ&amp;hl=en&amp;oi=sra">YS Lee</a>, JC Yang, SJ Yen - Advances in Image and Video  &hellip;, 2006 - Springer</div><div class="gs_rs">Developing a question answering (Q/A) system involves in integrating abundant linguistic <br>resources such as syntactic parsers, named entity recognizers which are not only impose <br>time cost but also unavailable in other languages. Ranking-based approaches take the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9070552822175907556&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 6</a> <a href="/scholar?q=related:5D424rsT4X0J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/38/5E/RN201261786.html?source=googlescholar" class="gs_nph" class=yC3A>BL Direct</a> <a href="/scholar?cluster=9070552822175907556&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'5D424rsT4X0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://cecs.uci.edu/~papers/icme06/pdfs/0000497.pdf" class=yC3C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uci.edu</span><span class="gs_ggsS">uci.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036645" class=yC3B>TV commercial classification by using multi-modal textual information</a></h3><div class="gs_a">Y Zheng, L Duan, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, JS Jin - Multimedia and Expo, 2006  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose an approach for TV commercial video classification by the <br>categories of advertised products or services (eg automobiles, healthcare products, etc). <br>Since automatic speech recognition (ASR) and optical character recognition (OCR) can <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3906888106822172332&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 5</a> <a href="/scholar?q=related:rH5VmAgOODYJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3906888106822172332&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'rH5VmAgOODYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://in2.csie.ncu.edu.tw/~yangjc/paper/2008_ESWA_Wu.pdf" class=yC3E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ncu.edu.tw</span><span class="gs_ggsS">ncu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0957417407001509" class=yC3D>A weighted string pattern matching-based passage ranking algorithm for video question answering</a></h3><div class="gs_a">YC Wu, JC Yang, <a href="/citations?user=tiClE2YAAAAJ&amp;hl=en&amp;oi=sra">YS Lee</a> - Expert Systems with Applications, 2008 - Elsevier</div><div class="gs_rs">Video question answering aims to pinpoint answers in response to user&#39;s specified <br>questions. However, most question answering technologies involve in integrating rich <br>specific external knowledge such as syntactic parsers, which are often unavailable for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6574765696690657083&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 4</a> <a href="/scholar?q=related:O5erCJFCPlsJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6574765696690657083&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'O5erCJFCPlsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://vireo.cs.cityu.edu.hk/papers/civr05_b.pdf" class=yC40><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/BF9F72QFVTTU0UBB.pdf" class=yC3F>Hot event detection and summarization by graph modeling and matching</a></h3><div class="gs_a">Y Peng, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Image and Video Retrieval, 2005 - Springer</div><div class="gs_rs">Abstract. This paper proposes a new approach for hot event detection and summarization of <br>news videos. The approach is mainly based on two graph algorithms: optimal matching <br>(OM) and normalized cut (NC). Initially, OM is employed to measure the visual similarity <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4473150745825308660&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 5</a> <a href="/scholar?q=related:9O-wT-fSEz4J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/14/26/RN171992290.html?source=googlescholar" class="gs_nph" class=yC41>BL Direct</a> <a href="/scholar?cluster=4473150745825308660&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'9O-wT-fSEz4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ieee10-guangda.pdf" class=yC43><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ieee10-guangda.pdf" class=yC42>Question answering over community-contributed web videos</a></h3><div class="gs_a">G Li, <a href="/citations?user=BrND3vgAAAAJ&amp;hl=en&amp;oi=sra">ZY Ming</a>, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, TS Chua, H Li, S Tang - IEEE MultiMedia, 2010 - 137.132.145.151</div><div class="gs_rs">The amount of information on the Web has grown exponentially over the years, with content <br>covering almost any topic. As a result, when looking for information, users are often <br>bewildered by the vast quantity of results from search engines. Users usually have to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1834089433792853375&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 6</a> <a href="/scholar?q=related:fzWFY5_-cxkJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1834089433792853375&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'fzWFY5_-cxkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md31', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md31" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:fzWFY5_-cxkJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://people.csail.mit.edu/chiu/paper/similarity%20retrieval%20of%20videos%20by%203D%20C-string.pdf" class=yC45><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mit.edu</span><span class="gs_ggsS">mit.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320304000823" class=yC44>Similarity retrieval of videos by using 3D C-string knowledge representation</a></h3><div class="gs_a">AJT Lee, HP Chiu, P Yu - Journal of Visual Communication and Image  &hellip;, 2005 - Elsevier</div><div class="gs_rs">We have proposed a new spatio-temporal knowledge structure called 3D C-string to <br>represent symbolic videos accompanying with the string generation and video <br>reconstruction algorithms. In this paper, we extend the idea behind the similarity retrieval <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4766772935148261745&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 4</a> <a href="/scholar?q=related:cVF1pLr6JkIJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4766772935148261745&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'cVF1pLr6JkIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.75.3220&amp;rep=rep1&amp;type=pdf" class=yC47><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.75.3220&amp;rep=rep1&amp;type=pdf" class=yC46>Content analysis and summarization for video documents</a></h3><div class="gs_a">S Lu - 2004 - Citeseer</div><div class="gs_rs">Abstract Video is increasingly becoming the favorite medium for many communication <br>entities for its extraordinary expressive power. With the fast advancement of network <br>bandwidth and high-capacity storage devices, large scale digital video library systems are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1671014352239301396&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 3</a> <a href="/scholar?q=related:FJPNL7OiMBcJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1671014352239301396&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'FJPNL7OiMBcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md33', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md33" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:FJPNL7OiMBcJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:FJPNL7OiMBcJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=18401798845679756018&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://140.115.112.118/bcbb/TVQS3/IEEE%20MSE-04-2.pdf" class=yC49><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 140.115.112.118</span><span class="gs_ggsS">140.115.112.118 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1376676" class=yC48>VSUM: summarizing from videos</a></h3><div class="gs_a">YC Wu, <a href="/citations?user=tiClE2YAAAAJ&amp;hl=en&amp;oi=sra">YS Lee</a>, CH Chang - Multimedia Software Engineering, &hellip;, 2004 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Summarization on produced type of video data (like news or movies) is to find <br>important segments that contain rich information. Users could obtain the important <br>messages by reading summaries rather than full documents. The research in this area <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=227179942505429763&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 3</a> <a href="/scholar?q=related:AzshPfQaJwMJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=227179942505429763&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'AzshPfQaJwMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1651369" class=yC4A>A blog search method using news video scene order</a></h3><div class="gs_a">D Kitayama, K Sumiya - Multi-Media Modelling Conference  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract When we want information on current events, we often view news programs on TV <br>or news streams on Web sites. A news video stream consists of several scenes, and viewers <br>often gain a broad understanding of the news by viewing scenes in the given order. Since <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17868364765348213282&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 3</a> <a href="/scholar?q=related:IrKvWxUv-fcJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17868364765348213282&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'IrKvWxUv-fcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21257/full" class=yC4B>Analyzing user interaction with the ViewFinder video retrieval system</a></h3><div class="gs_a">D Albertson - Journal of the American Society for Information  &hellip;, 2010 - Wiley Online Library</div><div class="gs_rs">Abstract This study investigates interactive video retrieval. The basis for this study is that <br>user-and search task-centric research in video information retrieval can assist efforts for <br>developing effective user interfaces and help complement the existing corpus of video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15577849111740564752&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 3</a> <a href="/scholar?q=related:EEXubTujL9gJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15577849111740564752&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'EEXubTujL9gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://acl.ldc.upenn.edu/N/N07/N07-1068.pdf" class=yC4D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from upenn.edu</span><span class="gs_ggsS">upenn.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://acl.ldc.upenn.edu/N/N07/N07-1068.pdf" class=yC4C>Toward multimedia: A String pattern-based passage ranking model for video question answering</a></h3><div class="gs_a">YC Wu, JC Yang - Proceedings of NAACL HLT, 2007 - acl.ldc.upenn.edu</div><div class="gs_rs">Abstract In this paper, we present a new string pattern matching-based passage ranking <br>algorithm for extending traditional textbased QA toward videoQA. Users interact with our <br>videoQA system through natural language questions, while our system returns passage <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15703727436147934260&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 3</a> <a href="/scholar?q=related:NEQnuebY7tkJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15703727436147934260&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'NEQnuebY7tkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md37', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md37" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:NEQnuebY7tkJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/94832x/200801/26236542.html" class=yC4E>åºäºéå¤«åæ¢çå¾åè¿å¨æ¨¡ç³è§åº¦è¯å«æ³çæ¹è¿</a></h3><div class="gs_a">é»ç¦ï¼ å¼ å½åºï¼ ååäº¬ - è®¡ç®æºåºç¨, 2008 - cqvip.com</div><div class="gs_rs">è¿å¨æ¨¡ç³å¾åçå¤åä¸»è¦ä¾èµäºå¯¹è¿å¨æ¨¡ç³åæ°çä¼°è®¡. éå¯¹R. Lokhande ç­äººæåºçåºäºéå¤«<br>åæ¢çè¿å¨æ¨¡ç³è§åº¦ä¼°è®¡æ³è¿è¡æ¹è¿, éè¿å¢å é¢å¤çæ­¥éª¤, å¹¶å©ç¨éå¤«åæ¢å¾å°æ´å¤åéç´çº¿, <br>ç¶ååå æåå¼å¾å°æåä¼°è®¡è§åº¦å¼. å®éªè¡¨æ, æ¹è¿æ¹æ³è½å¾å°æ¯åæ¹æ³æ´ç²¾ç¡®çè§åº¦å¼, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17186190127061600829&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 7</a> <a href="/scholar?q=related:PQrVPd2cge4J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17186190127061600829&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'PQrVPd2cge4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/94832x/200801/26236538.html" class=yC4F>ä¸ç§å¤æ¨¡æä¿¡æ¯èåçè§é¢æ£ç´¢æ¨¡å</a></h3><div class="gs_a">å¼ éï¼ ä¿è¾ - è®¡ç®æºåºç¨, 2008 - cqvip.com</div><div class="gs_rs">éå¯¹åå«å¤æè¯­ä¹ä¿¡æ¯çè§é¢æ£ç´¢çéè¦, æåºäºä¸ç§åºäºå³ç³»ä»£æ°çå¤æ¨¡æä¿¡æ¯èåè§é¢æ£ç´¢<br>æ¨¡å, è¯¥æ¨¡åååå©ç¨è§é¢åå«çææ¬, å¾å, é«å±è¯­ä¹æ¦å¿µç­å¤æ¨¡æç¹å¾, <br>æé äºå¯¹åºäºå¤ä¸ªè§é¢ç¹å¾çæ¥è¯¢æ¨¡å, å¹¶åæ°å°ä½¿ç¨å³ç³»ä»£æ°è¡¨è¾¾å¼å¯¹æ¥è¯¢å¾å°çå¤æ¨¡æ<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=832966605744139164&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 6</a> <a href="/scholar?q=related:nGsk9bZKjwsJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=832966605744139164&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'nGsk9bZKjwsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB40" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW40"><a href="http://hal.archives-ouvertes.fr/docs/00/12/27/87/PDF/guironnet.pdf" class=yC51><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/tel-00122787/" class=yC50>MÃ©thodes de rÃ©sumÃ© de vidÃ©o Ã  partir d&#39;informations bas niveau, du mouvement de camÃ©ra ou de l&#39;attention visuelle</a></h3><div class="gs_a">M Guironnet - 2006 - hal.archives-ouvertes.fr</div><div class="gs_rs">Durant cette derniÃ¨re dÃ©cennie, la technologie Â«numÃ©riqueÂ» a rÃ©volutionnÃ© les moyens de <br>communication avec l&#39;arrivÃ©e entre autres des tÃ©lÃ©phones portables, de l&#39;internet Ã  haut <br>dÃ©bit et de la tÃ©lÃ©vision numÃ©rique. L&#39;explosion des moyens de communication a conduit <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9455501545496907007&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 4</a> <a href="/scholar?q=related:_wyA6I6wOIMJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9455501545496907007&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'_wyA6I6wOIMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md40', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md40" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:_wyA6I6wOIMJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=10451482474085386994&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4731670" class=yC52>A novel video searching model based on ontology inference and multimodal information fusion</a></h3><div class="gs_a"><a href="/citations?user=nYl5trAAAAAJ&amp;hl=en&amp;oi=sra">J Zhang</a> - &hellip;  and Computational Technology, 2008. ISCSCT&#39;08.  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video comprises multiple types of textual, audio and visual information, and each of <br>them contains abundant semantic information. Therefore multimodal features query and <br>fusion are necessary in video retrieval. In this paper, we propose a new video retrieval <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4029170368148424840&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 2</a> <a href="/scholar?q=related:iEgENRp96jcJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4029170368148424840&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'iEgENRp96jcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/m451105862562337.pdf" class=yC53>Multimedia summarization in law courts: a clustering-based environment for browsing and consulting judicial folders</a></h3><div class="gs_a">E Fersini, E Messina, F Archetti - Advances in Data Mining. Applications  &hellip;, 2010 - Springer</div><div class="gs_rs">Digital videos represent a fundamental informative source of those events that occur during <br>a penal proceedings, which thanks to the technologies available nowadays, can be stored, <br>organized and retrieved in short time and with low cost. However, considering the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10796164881368310791&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 4</a> <a href="/scholar?q=related:B2SBUsKu05UJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10796164881368310791&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'B2SBUsKu05UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB43" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW43"><a href="http://t2r2.star.titech.ac.jp/rrws/file/CTT100548700/ATD100000413/E07-31secured.pdf" class=yC55><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from titech.ac.jp</span><span class="gs_ggsS">titech.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4430112" class=yC54>A language modeling approach to question answering on speech transcripts</a></h3><div class="gs_a">MH Heie, EWD Whittaker, JR Novak&hellip; - &hellip;  Speech Recognition &amp;  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a language modeling approach to sentence retrieval for <br>Question Answering (QA) that we used in Question Answering on speech transcripts (QAst), <br>a pilot task at the Cross Language Evaluation Forum (CLEF) evaluations 2007. A <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16206805236425326641&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 2</a> <a href="/scholar?q=related:MYASxX4j6uAJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16206805236425326641&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'MYASxX4j6uAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://eprints.cs.univie.ac.at/864/1/metis2c.pdf" class=yC57><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from univie.ac.at</span><span class="gs_ggsS">univie.ac.at <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> <a href="http://eprints.cs.univie.ac.at/864" class=yC56>METIS-A Flexible Database Solution for the Management of Multimedia Assets</a></h3><div class="gs_a">R King, GU Westermann, N Popitsch - 2004 - eprints.cs.univie.ac.at</div><div class="gs_rs">AbstractâMultimedia database systems largely focus on the management of media of one <br>particular type and suffer from inflexible architectures, which makes it difficult to adapt them <br>to individual application needs. This paper gives an overview of METIS, a flexible <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11473002034608156121&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 3</a> <a href="/scholar?q=related:2S0nXJBKOJ8J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'2S0nXJBKOJ8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB45" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW45"><a href="http://db-event.jpn.org/deim2009/proceedings/files/E9-2.pdf" class=yC59><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jpn.org</span><span class="gs_ggsS">jpn.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://db-event.jpn.org/deim2009/proceedings/files/E9-2.pdf" class=yC58>è¬æ¼åç»ã¢ã¼ã«ã¤ãã¹ããã­ã¼ã¯ã¼ãã«åè´ããè¬æ¼ãã¤ã¸ã§ã¹ãã®èªåä½æ</a></h3><div class="gs_a">åæ¡ï¼ æ¸¡è¾ºé½ä»ï¼ æ¨ªç°æ²»å¤« - DEIM ãã©ã¼ã©ã , 2009 - db-event.jpn.org</div><div class="gs_rs">ããã¾ã è¿å¹´, å­¦ä¼ãªã©ã«ãããè¬æ¼ãåç»ã³ã³ãã³ãã¨ãã¦å¤ãèç©ããã¦ãã. ããã, <br>å¤§éã®è¬æ¼ã³ã³ãã³ããå¨ã¦é²è¦§ãããã¨ã¯å¤ãã®æéãè¦ãã. ããã§, æ¬ç ç©¶ã§ã¯, <br>ããããè¬æ¼åç»ã¢ã¼ã«ã¤ãã¹ã«å¯¾ã, ã¦ã¼ã¶ãç­æéã§èå³ã®ãããããã¯ã¹ã«é¢é£ããè¬æ¼ã®<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10368716413803885372&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 2</a> <a href="/scholar?q=related:PPOKfaQU5Y8J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10368716413803885372&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'PPOKfaQU5Y8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md45', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md45" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:PPOKfaQU5Y8J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ri"><h3 class="gs_rt"><a href="http://japanlinkcenter.org/JST.JSTAGE/itej/64.306?from=Google" class=yC5A>ãã¥ã¼ã¹æ åã®æ¤ç´¢</a></h3><div class="gs_a">äºæä¸é - æ åæå ±ã¡ãã£ã¢å­¦ä¼èª, 2010 - J-STAGE</div><div class="gs_rs">ä¸è¬ã«ãã¥ã¼ã¹ã¨ããã³ã³ãã³ããèããéã«, ãããã 5W1H ã¨å¼ã°ããå±æ§ãéè¦è¦ããã. <br>ãã¥ã¼ã¹ã®æ§é åãèããéã«ã¯, ãã®ãªãã§ç¹ã«å·ä½çãª 4W (When, Where, Who, What) <br>ã®å±æ§ã«æ³¨ç®ãããã¨ãå¤ã. ä¸æ¹, ãã®ãããªã¹ãã¼ãªã¼éã®æ§é åã¨ã¯å¥ã«, åä¸ã®ã¤ãã³ãã<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7222040150399292147&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 2</a> <a href="/scholar?q=related:85IotkDXOWQJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7222040150399292147&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'85IotkDXOWQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB47" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW47"><a href="http://www.scholarpedia.org/article/Multimedia_Question_Answering" class=yC5C><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from scholarpedia.org</span><span class="gs_ggsS">scholarpedia.org <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[HTML]</span><span class="gs_ct2">[HTML]</span></span> <a href="http://www.scholarpedia.org/article/Multimedia_Question_Answering" class=yC5B>Multimedia question answering</a></h3><div class="gs_a">C Tat-Seng, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, J Tang - Scholarpedia, 2010 - scholarpedia.org</div><div class="gs_rs">With the proliferation of text and multimedia information, users are now able to find answers <br>to almost any questions on the Web. Meanwhile, they are also bewildered by the huge <br>amount of information routinely presented to them. Question-answering (QA) is a natural <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13904536500342593696&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:oJhfFUfW9sAJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'oJhfFUfW9sAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md47', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md47" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:oJhfFUfW9sAJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://www2.lifl.fr/~martinej/papers/sano07image.pdf" class=yC5E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lifl.fr</span><span class="gs_ggsS">lifl.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284958" class=yC5D>Image-based quizzes from news video archives</a></h3><div class="gs_a">M Sano, N Yagi, J Martinet&hellip; - Multimedia and Expo, &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper proposes a method for generating image-based quizzes from news <br>video achieves. Although there are many types of quizzes, in this work we focus on matching <br>quizzes in which an image is to be matched to one of several choices that are statements. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9064645138719829419&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:q-1dJLoWzH0J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9064645138719829419&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'q-1dJLoWzH0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://www.theseus.joint-research.org/wp-content/uploads/2011/07/dunker2008Personal-Television-A-Crossmodal-Analysis-Approach.pdf" class=yC60><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from joint-research.org</span><span class="gs_ggsS">joint-research.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4559572" class=yC5F>Personal television: A crossmodal analysis approach</a></h3><div class="gs_a">P Dunker, M Gruhne, S Sturtz - Consumer Electronics, 2008.  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Personal information consumption became more and more important due to the <br>huge number of existing information channels and the broad range of available information. <br>While obtaining information from the Internet, it is usual to create individual profiles in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15181063315714121848&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:eKDaJaf4rdIJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15181063315714121848&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'eKDaJaf4rdIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20685/full" class=yC61>Questionâdriven segmentation of lecture speech text: Towards intelligent eâlearning systems</a></h3><div class="gs_a">M Lin, Z Zhang - Journal of the American Society for Information &hellip;, 2008 - Wiley Online Library</div><div class="gs_rs">Abstract Recently, lecture videos have been widely used in e-learning systems. Envisioning <br>intelligent e-learning systems, this article addresses the challenge of information seeking in <br>lecture videos by retrieving relevant video segments based on user queries, through <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13424008104015571376&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:sCnQRT-oS7oJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/37/1A/RN221364026.html?source=googlescholar" class="gs_nph" class=yC62>BL Direct</a> <a href="/scholar?cluster=13424008104015571376&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'sCnQRT-oS7oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://arizona.openrepository.com/arizona/bitstream/10150/193843/1/azu_etd_1730_sip1_m.pdf" class=yC64><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from openrepository.com</span><span class="gs_ggsS">openrepository.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://arizona.openrepository.com/arizona/handle/10150/193843" class=yC63>Automated Lecture Video Segmentation: Facilitate Content Browsing and Retrieval</a></h3><div class="gs_a">M Lin - 2006 - arizona.openrepository.com</div><div class="gs_rs">People often have difficulties finding specific information in video because of its linear and <br>unstructured nature. Segmenting long videos into small clips by topics and providing <br>browsing and search functionalities is beneficial for information searching. However, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10110024824933819945&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:KaaPWAMGTowJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10110024824933819945&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'KaaPWAMGTowJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md51', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md51" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:KaaPWAMGTowJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=14262242278056286698&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB52" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW52"><a href="http://home.engineering.iastate.edu/~sparsh/VersatileQAMittal.pdf" class=yC66><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iastate.edu</span><span class="gs_ggsS">iastate.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://inderscience.metapress.com/index/2M61H18325817774.pdf" class=yC65>Versatile question answering systems: seeing in synthesis</a></h3><div class="gs_a">S Mittal, A Mittal - International Journal of Intelligent Information and  &hellip;, 2011 - Inderscience</div><div class="gs_rs">Recent advances in massive information storage and growth of internet have led to <br>increased necessity of tools such as search engines and QA systems to get meaningful <br>answers from the vast amount of information. The complex questions appearing in real-life <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2629002218834624807&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 2</a> <a href="/scholar?q=related:JwFax5YXfCQJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2629002218834624807&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'JwFax5YXfCQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://www.iis.sinica.edu.tw/page/jise/2007/200701_01.pdf" class=yC68><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sinica.edu.tw</span><span class="gs_ggsS">sinica.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.iis.sinica.edu.tw/page/jise/2007/200701_01.pdf" class=yC67>Video algebra for spatio-temporal reasoning of iconic videos represented in 3D C-string</a></h3><div class="gs_a">AJT Lee, P Yu, HP Chiu, HH Lin - Journal of information science  &hellip;, 2007 - iis.sinica.edu.tw</div><div class="gs_rs">The video content management has attracted increasing attention in recent years. We have <br>proposed a new spatio-temporal knowledge structure, called 3D C-string, to represent the <br>spatio-temporal relations between the objects in a video and to keep track of the motions <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1032201054262699146&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:iqQc2GMdUw4J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/18/5E/RN204245494.html?source=googlescholar" class="gs_nph" class=yC69>BL Direct</a> <a href="/scholar?cluster=1032201054262699146&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'iqQc2GMdUw4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md53', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md53" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:iqQc2GMdUw4J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/T2KX7JLKVW3EW31Y.pdf" class=yC6A>Structuring soccer video based on audio classification and segmentation using hidden markov model</a></h3><div class="gs_a">J Chen, Y Li, S Lao, L Wu, L Bai - Image and Video Retrieval, 2004 - Springer</div><div class="gs_rs">This paper presents a novel scheme for indexing and segmentation of video by analyzing <br>the audio track using Hidden Markov Model. This analysis is then applied to structuring the <br>soccer video. Based on the attributes of soccer video, we define three audio classes in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=604214216413599308&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:TNaS6aCZYggJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3F/59/RN153246036.html?source=googlescholar" class="gs_nph" class=yC6B>BL Direct</a> <a href="/scholar?cluster=604214216413599308&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'TNaS6aCZYggJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB55" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW55"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.3106&amp;rep=rep1&amp;type=pdf" class=yC6D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.3106&amp;rep=rep1&amp;type=pdf" class=yC6C>METIS-A Flexible Database Foundation for the Unified Management of Multimedia Content</a></h3><div class="gs_a">R King, N Popitsch, GU Westermann - Proceedings of the 10th  &hellip;, 2004 - Citeseer</div><div class="gs_rs">AbstractâMultimedia database systems largely focus on the management of media of one <br>particular type and suffer from inflexible architectures, which makes it difficult to adapt them <br>to individual application needs. This paper gives an overview of METIS, a flexible <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2488299997180844922&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:etuR8qk3iCIJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2488299997180844922&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'etuR8qk3iCIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md55', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md55" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:etuR8qk3iCIJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB56" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW56"><a href="http://www.ime.usp.br/~cpaz/downloads/mac5701_relatorio.pdf" class=yC6F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from usp.br</span><span class="gs_ggsS">usp.br <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ime.usp.br/~cpaz/downloads/mac5701_relatorio.pdf" class=yC6E>AplicaÃ§Ã£o para a RecuperaÃ§Ã£o de vÃ­deos indexados por conceitos</a></h3><div class="gs_a"><a href="/citations?user=_NmB4gIAAAAJ&amp;hl=en&amp;oi=sra">CDP Trillo</a> - RelatÃ³rio de Estudos de TÃ³picos em CiÃªncia da  &hellip;, 2004 - ime.usp.br</div><div class="gs_rs">Resumo Procurar informaÃ§Ã£o em vÃ­deos compridos pode consumir muito tempo quando <br>nÃ£o se possui uma ferramenta apropriada para buscar o que se procura dentro dele. O <br>enfoque do estudo apresentado Ã© conhecer as ferramentas para conseguir uma <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10720594746336891903&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:_1NUJB80x5QJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10720594746336891903&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'_1NUJB80x5QJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md56', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md56" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_1NUJB80x5QJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB57" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW57"><a href="http://t2r2.star.titech.ac.jp/rrws/file/CTT100601021/ATD100000413/deim2010-goi%2Bsecured.pdf" class=yC71><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from titech.ac.jp</span><span class="gs_ggsS">titech.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://t2r2.star.titech.ac.jp/rrws/file/CTT100601021/ATD100000413/deim2010-goi%2Bsecured.pdf" class=yC70>è¬æ¼ã³ã³ãã³ãã«ãããè¦è¦çå¹æã«åºã¥ããã¤ã¸ã§ã¹ãã®èªåçæ</a></h3><div class="gs_a">åæ¡ï¼ æ¸¡è¾ºé½ä»ï¼ æ¨ªç°æ²»å¤« - DEIM Forum 2010 D4-3, 2010 - t2r2.star.titech.ac.jp</div><div class="gs_rs">Abstract A large amount of presentation contents which record presentation slides and <br>videos have been accumulated in archives to be provided to the Internet. However, it takes <br>users a long time to find what they really want to get from a lot of presentation videos. Thus<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14355828122638916135&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:J5p0-p0lOscJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14355828122638916135&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'J5p0-p0lOscJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> <a href="http://thesis.lib.ncu.edu.tw/ETD-db/ETD-search-c/getfile?URN=985204011&amp;filename=985204011.pdf" class=yC72>Effects of Multimodal Surrogates in a Video Summarization</a></h3><div class="gs_a">W Chang - 2011 - thesis.lib.ncu.edu.tw</div><div class="gs_rs">Abstract The traditional video-based learning seldom considers the issue of time constraint <br>that learners may keep viewing videos until the whole video stream finished or the suitable <br>videos are explored. They spend a lot of time grasping the important gist of videos, but yet <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ExkZAjtfsYYJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'ExkZAjtfsYYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md58', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md58" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:ExkZAjtfsYYJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB59" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW59"><a href="http://www.ntu.edu.sg/eee/urop/Congress2003/Proceedings/abstract/NUS_SoC/NUS-NeoShiYong-SoC.pdf" class=yC74><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ntu.edu.sg/eee/urop/Congress2003/Proceedings/abstract/NUS_SoC/NUS-NeoShiYong-SoC.pdf" class=yC73>NUROP CONGRESS PAPER SEARCHING FOR MULTIMEDIA NEWS ON THE WEB</a></h3><div class="gs_a">SY NEO, TS CHUA - ntu.edu.sg</div><div class="gs_rs">ABSTRACT Current search engine is too general and often return too much of irrelevant <br>information in respond to user&#39;s free-text queries. Many redundant documents are returned <br>and this often takes users a long time to find the actual piece of news that they want. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:TPQsCV-wtKsJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12372707998515917900&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'TPQsCV-wtKsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md59', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md59" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TPQsCV-wtKsJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB60" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW60"><a href="http://ceur-ws.org/Vol-582/paper8.pdf" class=yC76><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ceur-ws.org</span><span class="gs_ggsS">ceur-ws.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ceur-ws.org/Vol-582/paper8.pdf" class=yC75>Multimedia Summarization in Law Courts: An Environment for Browsing and Consulting</a></h3><div class="gs_a">E Fersini, G Arosio, E Messina, F Archetti, D Toscani - ceur-ws.org</div><div class="gs_rs">Abstract. Digital videos represent a fundamental informative source of those events that <br>occur during a penal proceedings, which thanks to the technologies available nowadays, <br>can be stored, organized and retrieved in short time and with low cost. Considering the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:NGw-CUXGGZUJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10743836385879157812&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'NGw-CUXGGZUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md60', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md60" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:NGw-CUXGGZUJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB61" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW61"><a href="http://it.mesce.ac.in/icist/sites/default/files/12-15.pdf" class=yC78><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mesce.ac.in</span><span class="gs_ggsS">mesce.ac.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://it.mesce.ac.in/icist/sites/default/files/12-15.pdf" class=yC77>Question Answering System using Artificial Intelligence and Fuzzy System</a></h3><div class="gs_a">A Kumar, L Rawtani - &hellip;  International Conference on Information Systems and  &hellip; - it.mesce.ac.in</div><div class="gs_rs">AbstractâWe will be given sentences from the story, based on this story questions will be <br>asked and the answers to those questions will be given. In this paper we have considered <br>the story of Hare and Tortoise and answered all possible questions based on this story. It <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lMCPFbRtfqgJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12141262265660719252&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'lMCPFbRtfqgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md61', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md61" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:lMCPFbRtfqgJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607645" class=yC79>Personalized event-based news video retrieval with dynamic user-log</a></h3><div class="gs_a">M Li, Y Zheng, SY Neo, X Wang&hellip; - Multimedia and Expo,  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Personalization especially in the domain of information retrieval is essentially <br>important, as users might pose the same query even when they are searching for different <br>information. It is thus necessary to create a retrieval engine which takes into consideration <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:nYuFN91PGNIJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'nYuFN91PGNIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB63" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW63"><a href="http://140.115.112.118/paper%20list/2011/A%20Webpage-Based%20and%20Video%20Summarization-Based%20Learning%20Platform%20for%20Online%20Multimedia%20Learning.pdf" class=yC7B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 140.115.112.118</span><span class="gs_ggsS">140.115.112.118 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/A7M06K0Q01J453Q7.pdf" class=yC7A>Webpage-based and video summarization-based learning platform for online multimedia learning</a></h3><div class="gs_a">WH Chang, YC Wu, JC Yang - Edutainment Technologies. Educational  &hellip;, 2011 - Springer</div><div class="gs_rs">In general, watch and view the whole video is often time-consuming which involves <br>displaying the video film linearly. In this paper, we propose a webpage-based and video <br>summarization-based learning platform (WVSUM). The learning platform provides not only <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:cEyTQFulPdIJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15149446532888218736&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'cEyTQFulPdIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB64" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW64"><a href="http://www.comp.nus.edu.sg/~subolan/files/News%20Video%20Retrieval%20by%20Learning%20Multimodal%20Semantic%20Information.pdf" class=yC7D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/8532G89W71T24007.pdf" class=yC7C>News video retrieval by learning multimodal semantic information</a></h3><div class="gs_a">H Yu, <a href="/citations?user=ymlKC0EAAAAJ&amp;hl=en&amp;oi=sra">B Su</a>, H Lu, X Xue - Advances in Visual Information Systems, 2007 - Springer</div><div class="gs_rs">With the explosion of multimedia data especially that of video data, requirement of efficient <br>video retrieval has becoming more and more important. Years of TREC Video Retrieval <br>Evaluation (TRECVID) research gives benchmark for video search task. The video data in <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:SVSTQKDMAQIJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/10/3B/RN221154550.html?source=googlescholar" class="gs_nph" class=yC7E>BL Direct</a> <a href="/scholar?cluster=144621651702797385&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'SVSTQKDMAQIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB65" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW65"><a href="http://vireo.cs.cityu.edu.hk/papers/mm12-zhang.pdf" class=yC80><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://vireo.cs.cityu.edu.hk/papers/mm12-zhang.pdf" class=yC7F>Snap-and-Ask: Answering Multimodal Question by Naming Visual Instance</a></h3><div class="gs_a">W Zhang, L Pang, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - 2012 - vireo.cs.cityu.edu.hk</div><div class="gs_rs">ABSTRACT In real-life, it is easier to provide a visual cue when asking a question about a <br>possibly unfamiliar topic, for example, asking the question,âWhere was this crop circle <br>found?â. Providing an image of the instance is far more convenient than texting a verbose <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7671120657956869866&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 1</a> <a href="/scholar?q=related:6hauVphLdWoJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'6hauVphLdWoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md65', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md65" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:6hauVphLdWoJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Tat-Seng CHUA et al.(2009), Scholarpedia, 5 (5): 9546. doi: 10.4249/scholarpedia. 9546 revision# 91536 [link to/cite this article]</h3><div class="gs_a"><a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a></div><div class="gs_fl"><a href="/scholar?q=related:1IxXY7WSOosJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'1IxXY7WSOosJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:333"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5992259" class=yC81>A Keyword-based Video Summarization Learning Platform with Multimodal Surrogates</a></h3><div class="gs_a">WH Chang, JC Yang, YC Wu - &hellip;  Learning Technologies (ICALT) &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In general, video-based learning contains rich media information, but displaying an <br>entire video linearly is time-consuming. As an alternative, video summarization techniques <br>extract important content provides short but informative fragments. In this paper, a video <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Z1ujZYVpP0sJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5422168498034137959&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Z1ujZYVpP0sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:332"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/VJ5V8W70KT3273GT.pdf" class=yC82>A Human-Centered Computing Framework to Enable Personalized News Video Recommendation</a></h3><div class="gs_a">H Luo, J Fan - Multimedia Analysis, Processing and Communications, 2011 - Springer</div><div class="gs_rs">In this chapter, an interactive framework is developed to enable personalized news video <br>recommendation and allow news seekers to access large-scale news videos more <br>effectively. First, multiple information sources (audio, video and closed captions) are <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:62NNfpHoeekJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16823733594606101483&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'62NNfpHoeekJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:331"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dx.doi.org/10.1561/1500000020" class=yC83>Automatic Summarization</a></h3><div class="gs_a">M Larson - Foundations and TrendsÂ® in Information Retrieval, 2011 - dx.doi.org</div><div class="gs_rs">Abstract Speech media, ie, digital audio and video containing spoken content, has <br>blossomed in recent years. Large collections are accruing on the Internet as well as in <br>private and enterprise settings. This growth has motivated extensive research work on <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:W3ZDjicVdc4J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'W3ZDjicVdc4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:330"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/tt4476m52366k10k.pdf" class=yC84>Words and Pictures: An HCI Perspective</a></h3><div class="gs_a">TJ Siddiqui, US Tiwary - Proceedings of the First International Conference  &hellip;, 2009 - Springer</div><div class="gs_rs">Human and Computer do not speak the same language. This is one of the challenging <br>problems to those working on interface of human and computers. This paper is an effort to <br>summarize the approaches which brings humans and computers a bit closer in terms of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:l3I3zbNzFgAJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6319665566937751&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'l3I3zbNzFgAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:329"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB71" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW71"><a href="http://mcg.ict.ac.cn/download/pdf/paper/2008/17.PERSONALIZED%20EVENT-BASED%20NEWS%20VIDEO%20RETRIEVAL%20WITH%20DYNAMIC%20USER-LOG.pdf" class=yC86><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mcg.ict.ac.cn/download/pdf/paper/2008/17.PERSONALIZED%20EVENT-BASED%20NEWS%20VIDEO%20RETRIEVAL%20WITH%20DYNAMIC%20USER-LOG.pdf" class=yC85>Ming Li1, 2, 3 Yantao Zheng1 Shi-Yong Neo1 Xiangdong Wang2 Sheng Tang2 Shou-Xun Lin2 School of Computing, NUS 2Key Laboratory of Intelligent  &hellip;</a></h3><div class="gs_a">X Wang, S Tang, SX Lin - mcg.ict.ac.cn</div><div class="gs_rs">ABSTRACT Personalization especially in the domain of information retrieval is essentially <br>important, as users might pose the same query even when they are searching for different <br>information. It is thus necessary to create a retrieval engine which takes into consideration <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'0rhu1D5UbJ8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md71', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md71" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:0rhu1D5UbJ8J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:328"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB72" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW72"><a href="https://scholarworks.iu.edu/dspace/bitstream/handle/2022/7666/umi-indiana-1772.pdf?sequence=1" class=yC88><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iu.edu</span><span class="gs_ggsS">iu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=GFXaoaPlZ20C&amp;oi=fnd&amp;pg=PA1&amp;ots=EtZbw4efdz&amp;sig=KKY7uccORdoUGVfb6kptXHlHT6k" class=yC87>A domain-centric approach to designing user interfaces of video retrieval systems</a></h3><div class="gs_a">DE Albertson - 2007 - books.google.com</div><div class="gs_rs">User-and task-centric efforts in video information retrieval (IR) research are needed because <br>current experiments are showing few significant results. It is our belief that unsatisfactory <br>results in video IR can be partially attributed to the overemphasis on technologically-<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lQrc5--3vbcJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13239940720554150549&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'lQrc5--3vbcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:327"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB73" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW73"><a href="http://doras.dcu.ie/17158/1/1500000020.pdf" class=yC8A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dcu.ie</span><span class="gs_ggsS">dcu.ie <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://doras.dcu.ie/17158/1/1500000020.pdf" class=yC89>Spoken content retrieval: A survey of techniques and technologies</a></h3><div class="gs_a"><a href="/citations?user=eIiM958AAAAJ&amp;hl=en&amp;oi=sra">M Larson</a>, <a href="/citations?user=YJuN_H8AAAAJ&amp;hl=en&amp;oi=sra">GJF Jones</a> - Foundations and Trends in Information  &hellip;, 2012 - doras.dcu.ie</div><div class="gs_rs">Speech media, that is, digital audio and video containing spoken content, has blossomed in <br>recent years. Large collections are accruing on the Internet as well as in private and <br>enterprise settings. This growth has motivated extensive research on techniques and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1438600154426638707&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=80">Cited by 3</a> <a href="/scholar?q=related:cwFs6jfv9hMJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1438600154426638707&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'cwFs6jfv9hMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md73', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md73" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:cwFs6jfv9hMJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:326"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB74" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW74"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/16051/thesis-sub.pdf?sequence=1" class=yC8C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/16051" class=yC8B>combining multimodal external resources for event-based news video retrieval and question answering</a></h3><div class="gs_a">NEOSHI YONG - 2008 - scholarbank.nus.edu</div><div class="gs_rs">The ever-increasing amount of multimedia data available online creates an urgent need on <br>how to index these information and support effective retrieval by users. In recent years, we <br>observe the gradual shift from performing retrieval solely based on analyzing one media <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:h5EV9TPOgHUJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8466994022007017863&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'h5EV9TPOgHUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:325"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> A Cloud-based Collaborative Video Story Authoring and Sharing Platform</h3><div class="gs_a">C Wang, <a href="/citations?user=Y_Y3fVEAAAAJ&amp;hl=en&amp;oi=sra">R Ranjan</a>, <a href="/citations?user=y6m820wAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a>, <a href="/citations?user=PlZO-l8AAAAJ&amp;hl=en&amp;oi=sra">K Mitra</a>, S Saha, M Meng&hellip;</div><div class="gs_fl"><a onclick="return gs_ocit(event,'ZQ51-Rcr_p8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:324"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB76" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW76"><a href="http://www.aquaphoenix.com/presentation/thesisproposal/paper.pdf" class=yC8E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aquaphoenix.com</span><span class="gs_ggsS">aquaphoenix.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.aquaphoenix.com/presentation/thesisproposal/paper.pdf" class=yC8D>Semantic Multi-modal Analysis, Structuring, and Visualization for Candid Personal Interaction Videos</a></h3><div class="gs_a">A Haubold - 2009 - aquaphoenix.com</div><div class="gs_rs">Abstract Videos are rich in multimedia content and semantics, which should be used by <br>video browsers to better present the audio-visual information to the viewer. Ubiquitous video <br>players allow for content to be scanned linearly, rarely providing summaries or methods <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:IxsW_WiEfP4J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18337677369203366691&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'IxsW_WiEfP4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md76', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md76" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:IxsW_WiEfP4J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:323"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB77" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW77"><a href="http://tel.archives-ouvertes.fr/docs/00/12/27/87/PDF/guironnet.pdf" class=yC90><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://tel.archives-ouvertes.fr/docs/00/12/27/87/PDF/guironnet.pdf" class=yC8F>Titre: MÃTHODES DE RÃSUMÃ DE VIDÃO Ã PARTIR D&#39;INFORMATIONS BAS NIVEAU, DU MOUVEMENT DE CAMÃRA OU DE L&#39;ATTENTION VISUELLE</a></h3><div class="gs_a">M GUIRONNET - 2007 - tel.archives-ouvertes.fr</div><div class="gs_rs">Durant cette derniÃ¨re dÃ©cennie, la technologie Â«numÃ©riqueÂ» a rÃ©volutionnÃ© les moyens de <br>communication avec l&#39;arrivÃ©e entre autres des tÃ©lÃ©phones portables, de l&#39;internet Ã  haut <br>dÃ©bit et de la tÃ©lÃ©vision numÃ©rique. L&#39;explosion des moyens de communication a conduit <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:aKEiBgoxw14J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6828355379146170728&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'aKEiBgoxw14J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md77', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md77" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:aKEiBgoxw14J:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:322"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/E6068580N62HV082.pdf" class=yC91>Web-based semantic analysis of chinese news video</a></h3><div class="gs_a">H Feng, Z Pang, K Qiu, G Song - Advances in Multimedia Information  &hellip;, 2006 - Springer</div><div class="gs_rs">The semantic analysis of the Chinese news video with the help of World Wide Web is <br>proposed. First, we segment the news video into a series of story units. Second, we extract <br>the key phrases from the corresponding ASR transcript of news story, and optimize the key <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:18RgCcsU9DIJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/53/10/RN198328003.html?source=googlescholar" class="gs_nph" class=yC92>BL Direct</a> <a href="/scholar?cluster=3671582458482050263&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'18RgCcsU9DIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:321"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB79" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW79"><a href="http://www.ime.usp.br/~yw/2004/mac5701i/planos/christian_plan.pdf" class=yC94><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from usp.br</span><span class="gs_ggsS">usp.br <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ime.usp.br/~yw/2004/mac5701i/planos/christian_plan.pdf" class=yC93>Plano de Estudos: AplicaÃ§Ã£o para a recuperaÃ§Ã£o de vÃ­deos indexados por conceitos</a></h3><div class="gs_a"><a href="/citations?user=_NmB4gIAAAAJ&amp;hl=en&amp;oi=sra">CDP Trillo</a> - ime.usp.br</div><div class="gs_rs">A pesquisa a ser desenvolvida durante o semestre, Ã© orientada ao desenvolvimento de uma <br>aplicaÃ§Ã£o de recuperaÃ§Ã£o de informaÃ§Ã£o baseada em consultas em linguagem natural. A <br>recuperaÃ§Ã£o de informaÃ§Ã£o serÃ¡ feita de um banco de dados de vÃ­deos, os quais sÃ£o <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Ufbq9B0GZWoJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Ufbq9B0GZWoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md79', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md79" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Ufbq9B0GZWoJ:scholar.google.com/&amp;hl=en&amp;num=80&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
