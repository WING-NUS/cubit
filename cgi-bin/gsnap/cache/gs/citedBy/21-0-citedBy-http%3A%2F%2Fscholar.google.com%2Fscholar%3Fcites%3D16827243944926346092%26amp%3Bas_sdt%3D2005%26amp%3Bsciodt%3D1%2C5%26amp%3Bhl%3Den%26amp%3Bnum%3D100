Total results = 21
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.119.3200&amp;rep=rep1&amp;type=pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432643" class=yC0>LyricAlly: Automatic synchronization of textual lyrics to acoustic music signals</a></h3><div class="gs_a"><a href="/citations?user=aNVcd3EAAAAJ&amp;hl=en&amp;oi=sra">MY Kan</a>, Y Wang, D Iskandar, TL New&hellip; - Audio, Speech, and  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present LyricAlly, a prototype that automatically aligns acoustic musical signals <br>with their corresponding textual lyrics, in a manner similar to manually-aligned karaoke. We <br>tackle this problem based on a multimodal approach, using an appropriate pairing of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6994750038097962320&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 27</a> <a href="/scholar?q=related:UKV6kRlYEmEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/47/0A/RN222855432.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=6994750038097962320&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'UKV6kRlYEmEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="https://wikis.utexas.edu/download/attachments/4588896/may_acmmm08.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utexas.edu</span><span class="gs_ggsS">utexas.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459359.1459382" class=yC3>Combination of audio and lyrics features for genre classification in digital audio collections</a></h3><div class="gs_a"><a href="/citations?user=AXBXw6sAAAAJ&amp;hl=en&amp;oi=sra">R Mayer</a>, <a href="/citations?user=OiFfgYEAAAAJ&amp;hl=en&amp;oi=sra">R Neumayer</a>, A Rauber - Proceedings of the 16th ACM  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In many areas multimedia technology has made its way into mainstream. In the <br>case of digital audio this is manifested in numerous online music stores having turned into <br>profitable businesses. The widespread user adaption of digital audio both on home <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16724162633624228390&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 16</a> <a href="/scholar?q=related:JpIIj04pGOgJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16724162633624228390&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'JpIIj04pGOgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.2653&amp;rep=rep1&amp;type=pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.2653&amp;rep=rep1&amp;type=pdf" class=yC5>Relationships between lyrics and melody in popular music</a></h3><div class="gs_a"><a href="/citations?user=Q37xBqwAAAAJ&amp;hl=en&amp;oi=sra">E Nichols</a>, <a href="/citations?user=j03zZgcAAAAJ&amp;hl=en&amp;oi=sra">D Morris</a>, S Basu, C Raphael - Proceedings of the 10&#39;th  &hellip;, 2009 - Citeseer</div><div class="gs_rs">ABSTRACT Composers of popular music weave lyrics, melody, and instrumentation <br>together to create a consistent and compelling emotional scene. The relationships among <br>these elements are critical to musical communication, and understanding the statistics <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15915262251703357463&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 5</a> <a href="/scholar?q=related:F1D2-7Fe3twJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15915262251703357463&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 22 versions</a> <a onclick="return gs_ocit(event,'F1D2-7Fe3twJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md2', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md2" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:F1D2-7Fe3twJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.6633&amp;rep=rep1&amp;type=pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.6633&amp;rep=rep1&amp;type=pdf" class=yC7>Multi-modal Analysis of Music: A large-scale Evaluation</a></h3><div class="gs_a"><a href="/citations?user=AXBXw6sAAAAJ&amp;hl=en&amp;oi=sra">R Mayer</a>, <a href="/citations?user=OiFfgYEAAAAJ&amp;hl=en&amp;oi=sra">R Neumayer</a> - Proc. WEMIS, 2009 - Citeseer</div><div class="gs_rs">AbstractâMultimedia data by definition comprises several different types of content <br>modalities. Music specifically inherits eg audio at its core, text in the form of lyrics, images by <br>means of album covers, or video in the form of music videos. Yet, in many Music <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5555999944125956649&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 3</a> <a href="/scholar?q=related:KcK3IYPgGk0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5555999944125956649&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'KcK3IYPgGk0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:KcK3IYPgGk0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://staff.aist.go.jp/h.fujihara/pdf/waspaa2009_fujihara.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5346497" class=yC9>A novel framework for recognizing phonemes of singing voice in polyphonic music</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a>, <a href="/citations?user=8cX3pewAAAAJ&amp;hl=en&amp;oi=sra">HG Okuno</a> - Applications of Signal  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A novel method is described that can be used to recognize the phoneme of a <br>singing voice (vocal) in polyphonic music. Though we focus on the voiced phoneme in this <br>paper, this method is design to concurrently recognize other elements of a singing voice <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2904691984875507359&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 3</a> <a href="/scholar?q=related:n85tdPiJTygJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2904691984875507359&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'n85tdPiJTygJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://staff.aist.go.jp/m.goto/PAPER/JASJ200810goto.pdf" class=yCB>æ­å£°æå ±å¦çã®æè¿ã®ç ç©¶</a></h3><div class="gs_a">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å° - æ¥æ¬é³é¿å­¦ä¼èª, 2008 - staff.aist.go.jp</div><div class="gs_rs">â Recent studies on singing information processing. ââ Masataka Goto, Takeshi Saitou, Tomoyasu <br>Nakano and Hiromasa Fujihara (National Institute of Ad- vanced Industrial Science and Technology <br>(AIST), Tsukuba, 305â8568) e-mail: m.goto@aist.go.jp  </div><div class="gs_fl"><a href="/scholar?cites=3835003938146142193&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 6</a> <a href="/scholar?q=related:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3835003938146142193&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'8cdsVcOrODUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md5', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md5" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:8cdsVcOrODUJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://144.206.159.178/FT/CONF/16408942/16408945.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 144.206.159.178</span><span class="gs_ggsS">144.206.159.178 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/data/Conferences/SPIEP/17645/682004_1.pdf" class=yCD>Enriching text with images and colored light</a></h3><div class="gs_a">D Sekulovski, G Geleijnse&hellip; - &hellip;  of the is&amp;t/spie  &hellip;, 2008 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">ABSTRACT We present an unsupervised method to enrich textual applications with relevant <br>images and colors. The images are collected by querying large image repositories and <br>subsequently the colors are computed using image processing. A prototype system based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7761141948238539376&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 4</a> <a href="/scholar?q=related:cCokTHsdtWsJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7761141948238539376&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'cCokTHsdtWsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/j181l048303427k1.pdf" class=yCF>Multimodal Aspects of Music Retrieval: Audio, Song Lyricsâand Beyond?</a></h3><div class="gs_a"><a href="/citations?user=AXBXw6sAAAAJ&amp;hl=en&amp;oi=sra">R Mayer</a>, A Rauber - Advances in Music Information Retrieval, 2010 - Springer</div><div class="gs_rs">Music retrieval is predominantly seen as a problem to be tackled in the acoustic domain. <br>With the exception of symbolic music retrieval and score-based systems, which form rather <br>separate sub-disciplines on their own, most approaches to retrieve recordings of music by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14250391750993582317&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 3</a> <a href="/scholar?q=related:7RghtMuPw8UJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14250391750993582317&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'7RghtMuPw8UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://staff.aist.go.jp/m.goto/PAPER/IEEETASLP201201mauch.pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5876304" class=yC10>Integrating Additional Chord Information into HMM-Based Lyrics-to-Audio Alignment</a></h3><div class="gs_a"><a href="/citations?user=_gfIN8AAAAAJ&amp;hl=en&amp;oi=sra">M Mauch</a>, H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Audio, Speech, and Language  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Aligning lyrics to audio has a wide range of applications such as the automatic <br>generation of karaoke scores, song-browsing by lyrics, and the generation of audio <br>thumbnails. Existing methods are restricted to using only lyrics and match them to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3269594826642557923&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 3</a> <a href="/scholar?q=related:4wt8-SvvXy0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3269594826642557923&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'4wt8-SvvXy0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Singing Phoneme Class Detection In Polyphonic Music Recordings</h3><div class="gs_a">V Ourania - 2008 - magistrska naloga, Univerza  &hellip;</div><div class="gs_fl"><a href="/scholar?cites=11653457682537027000&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:uJWqBAVmuaEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11653457682537027000&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'uJWqBAVmuaEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.dse.nl/~gijsg/AmbiSys-GeleijnseEtAl.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dse.nl</span><span class="gs_ggsS">dse.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1363164" class=yC12>Enriching music with synchronized lyrics, images and colored lights</a></h3><div class="gs_a">G Geleijnse, D Sekulovski, <a href="/citations?user=OweuZLoAAAAJ&amp;hl=en&amp;oi=sra">J Korst</a>, S Pauws&hellip; - Proceedings of the 1st  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract We present a method to synchronize popular music with its lyrics at the stanza level. <br>First we apply an algorithm to segment audio content into harmonically similar and/or <br>contrasting progressions, ie the stanzas. We map the stanzas found to a sequence of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1472177553128121368&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:GGSBRK85bhQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1472177553128121368&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'GGSBRK85bhQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1823753" class=yC14>Word level automatic alignment of music and lyrics using vocal synthesis</a></h3><div class="gs_a">NC Maddage, <a href="/citations?user=jnU62sUAAAAJ&amp;hl=en&amp;oi=sra">KC Sim</a>, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - ACM Transactions on Multimedia  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract We propose a signal-based approach instead of the commonly used model-based <br>approach, to automatically align vocal music with text lyrics at the word level. In this <br>approach, we use a text-to-speech system to synthesize the singing voice according to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16739146216492474140&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:HJ9bzctkTegJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16739146216492474140&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'HJ9bzctkTegJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://eden.dei.uc.pt/~hroliv/pubs/GoncaloOliveiraMScThesis2007.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uc.pt</span><span class="gs_ggsS">uc.pt <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://eden.dei.uc.pt/~hroliv/pubs/GoncaloOliveiraMScThesis2007.pdf" class=yC15>GeraÃ§ao de texto com base em ritmo</a></h3><div class="gs_a">HRG Oliveira - 2007 - eden.dei.uc.pt</div><div class="gs_rs">Resumo Esta tese introduz o problema da geraÃ§Ã£o de texto baseado em ritmo, mais <br>precisamente a geraÃ§Ã£o de uma letra de acordo com uma dada melodia. Numa primeira <br>fase a base para o sistema foi preparada. Foram implementados algoritmos para obter a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=116389058122914482&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:spooUTt_nQEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=116389058122914482&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'spooUTt_nQEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md12', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md12" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:spooUTt_nQEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://staff.aist.go.jp/m.goto/PAPER/JSSSTCS200902goto.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://japanlinkcenter.org/JST.JSTAGE/jssst/26.1_4?from=Google" class=yC17>é³æ¥½ã»é³å£°ã®é³é¿ä¿¡å·ã®èªè­ã»çè§£ç ç©¶ã®åå</a></h3><div class="gs_a">å¾è¤çå­ï¼ ç·æ¹æ·³ - ã³ã³ãã¥ã¼ã¿ ã½ããã¦ã§ã¢, 2009 - J-STAGE</div><div class="gs_rs">æ¬è§£èª¬è«æã§ã¯, è¿å¹´ç®è¦ã¾ããç ç©¶ãé²å±ããé³æ¥½ã®é³é¿ä¿¡å·ã®èªè­ã»çè§£ã¨, <br>é·å¹´ã®ç ç©¶ã«ããæ§è½ãåä¸ããé³å£°ã®é³é¿ä¿¡å·ã®èªè­ã»çè§£ã«ã¤ãã¦, ãã®ç ç©¶ã®ç¾ç¶ãç´¹ä»<br>ãã. ã¤ã³ã¿ãã§ã¼ã¹ãã¤ã³ã¿ã©ã¯ã·ã§ã³ã«é¢é£ããå­¦ä¼ã«ããã¦ã, é³æ¥½ã»é³å£°ã«é¢ããç ç©¶ææã<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13656507651163753345&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:gQsFfFqphb0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13656507651163753345&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'gQsFfFqphb0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf" class=yC19>A repetition-based framework for lyric alignment in popular songs</a></h3><div class="gs_a">LM Thang, KANM Yen - National University of Singapore  &hellip;, 2007 - w.comp.nus.edu</div><div class="gs_rs">ABSTRACT We examine the problem of automatically aligning acoustic musical audio and <br>textual lyric in popular songs. Existing works have tackled the problem using <br>computationally-expensive audio processing techniques, resulting in solutions unsuitable <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17581284984694044119&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 27 versions</a> <a onclick="return gs_ocit(event,'1_WYP4tF_fMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md14', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md14" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://drops.dagstuhl.de/opus/volltexte/2012/3464/pdf/3.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dagstuhl.de</span><span class="gs_ggsS">dagstuhl.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://drops.dagstuhl.de/opus/volltexte/2012/3464/" class=yC1B>Lyrics-to-Audio Alignment and its Application}}</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Multimodal Music Processing} - drops.dagstuhl.de</div><div class="gs_rs">Abstract Automatic lyrics-to-audio alignment techniques have been drawing attention in the <br>last years and various studies have been made in this field. The objective of lyrics-to-audio <br>alignment is to estimate a temporal relationship between lyrics and musical audio signals <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:pR0KaNFrpC0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3288872175025135013&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'pR0KaNFrpC0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202011/pdfs/0000365.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5946416" class=yC1D>Concurrent estimation of singing voice F0 and phonemes by using spectral envelopes estimated from polyphonic music</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Acoustics, Speech and Signal Processing  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The scarcity of available multi-track recordings constitutes a severe constraint on <br>the training of probabilistic models for voice extraction from polyphonic music. We propose a <br>novel training method to estimate a spectral envelope of a singing voice that makes it <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:PRpLckBrSYIJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9388152822770113085&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'PRpLckBrSYIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/13382/thesis_21_amendment_2.pdf?sequence=1" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/13382" class=yC1F>REFINING MUSIC SIGNAL TO LYRIC TEXT SYNCHRONIZATION FROM LINE-LEVEL TO SYLLABLE-LEVEL BY CONSTRAINING DYNAMIC TIME WARPING  &hellip;</a></h3><div class="gs_a">D ISKANDAR - 2007 - scholarbank.nus.sg</div><div class="gs_rs">The problem we consider in this thesis is synchronization between lyric text and the <br>corresponding singing voice recording. We limit the singing to the pop genre to make the <br>problem manageable. The recordings we consider in this problem are those we can <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:q87ftenzmuwJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17049207524468706987&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'q87ftenzmuwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2043615" class=yC21>Beat space segmentation and octave scale cepstral feature for sung language recognition in pop music</a></h3><div class="gs_a">NC Maddage, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - ACM Transactions on Multimedia Computing,  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Sung language recognition relies on both effective feature extraction and acoustic <br>modeling. In this paper, we study rhythm based music segmentation with the frame size <br>being the duration of the smallest note in the music, as opposed to fixed length <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:PDNw-UBaogIJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'PDNw-UBaogIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://staff.aist.go.jp/m.goto/PAPER/ASJ200909fujihara.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://staff.aist.go.jp/m.goto/PAPER/ASJ200909fujihara.pdf" class=yC22>æ¥½æ²ä¸­ã®æ­å£°ã®åºæ¬å¨æ³¢æ°ã¨é³ç´ ãåææ¨å®å¯è½ãªãã¬ã¼ã ã¯ã¼ã¯</a></h3><div class="gs_a">è¤åå¼å°ï¼ å¾è¤çå­ï¼ å¥¥ä¹å(äº¬å¤§ - staff.aist.go.jp</div><div class="gs_rs">é³æ¥½ã¯, ç£æ¥­çã«ãæåçã«ãéè¦ãªã³ã³ãã³ãã§ãã, ãã®ä¸­ã§ãæ­å£°ã¯éè¦ãªå½¹å²ãæããã¦<br>ãã. æ¬ç¨¿ã§ã¯, æ··åé³ä¸­ã®æ­å£°ã®æ­è© (é³ç´ ) ã¨åºæ¬å¨æ³¢æ° (F0) ãåæã«èªè­ããããã®ææ³, <br>WPST (Weighted composition of Probabilistic Spectral Template) æ³ãææ¡ã, F0 æ¨å®ã¨<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:uGdvSfHJOboJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'uGdvSfHJOboJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:uGdvSfHJOboJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007goto.pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://staff.aist.go.jp/m.goto/PAPER/SIGMUS201007goto.pdf" class=yC24>æ­å£°æå ±å¦ç: æ­å£°ãå¯¾è±¡ã¨ããé³æ¥½æå ±å¦ç</a></h3><div class="gs_a">å¾è¤çå­ï¼ é½è¤æ¯ï¼ ä¸­éå«éï¼ è¤åå¼å° - 2010 - staff.aist.go.jp</div><div class="gs_rs">æ¬ç¨¿ã§ã¯,ãæ­å£°æå ±å¦çã ã¨åä»ããæ°ããç ç©¶é åã«ãããæãã®ç ç©¶äºä¾ãç´¹ä»ãã. <br>ããã¯æ­å£°ã«å¯¾ããé³æ¥½æå ±å¦çã§ãã, ãã®ç ç©¶å¯¾è±¡ã¯å¤å²ã«æ¸¡ãã, æ¬ç¨¿ã§ã¯, <br>æ­å£°çè§£ã·ã¹ãã , æ­å£°ã«åºã¥ãé³æ¥½æå ±æ¤ç´¢ã·ã¹ãã , æ­å£°åæã·ã¹ãã ã®ä¸ã¤ã®éè¦ãª<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:uzd8aAf29qoJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12319304342396745659&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'uzd8aAf29qoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md20', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md20" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:uzd8aAf29qoJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
