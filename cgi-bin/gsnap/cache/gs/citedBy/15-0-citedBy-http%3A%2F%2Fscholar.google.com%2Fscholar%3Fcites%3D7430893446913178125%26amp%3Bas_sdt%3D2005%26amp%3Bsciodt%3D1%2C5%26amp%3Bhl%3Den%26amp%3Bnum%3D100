Total results = 15
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://orbit-dtu-dk.cvt.dk/services/downloadRegister/3841539/Meng.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cvt.dk</span><span class="gs_ggsS">cvt.dk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4244528" class=yC0>Temporal feature integration for music genre classification</a></h3><div class="gs_a">A Meng, <a href="/citations?user=nLCcgUgAAAAJ&amp;hl=en&amp;oi=sra">P Ahrendt</a>, <a href="/citations?user=6Y_3isIAAAAJ&amp;hl=en&amp;oi=sra">J Larsen</a>&hellip; - Audio, Speech, and  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Temporal feature integration is the process of combining all the feature vectors in a <br>time window into a single feature vector in order to capture the relevant temporal information <br>in the window. The mean and variance along the temporal dimension are often used for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8687253143269348899&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 71</a> <a href="/scholar?q=related:IxZ37rRSj3gJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3E/56/RN211839634.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=8687253143269348899&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'IxZ37rRSj3gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://users.cis.fiu.edu/~lli003/Music/cla/39.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fiu.edu</span><span class="gs_ggsS">fiu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1416349" class=yC3>Improving music genre classification by short time feature integration</a></h3><div class="gs_a">A Meng, <a href="/citations?user=nLCcgUgAAAAJ&amp;hl=en&amp;oi=sra">P Ahrendt</a>, <a href="/citations?user=6Y_3isIAAAAJ&amp;hl=en&amp;oi=sra">J Larsen</a> - Acoustics, Speech, and Signal  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Many different short-time features, using time windows of 10-30 ms, have been <br>proposed for music segmentation, retrieval and genre classification. However, often the <br>available time frame of the music to make the actual decision or comparison (the decision <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4197415667870435693&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 56</a> <a href="/scholar?q=related:bYkTok83QDoJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4197415667870435693&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 24 versions</a> <a onclick="return gs_ocit(event,'bYkTok83QDoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/D002215180712083.pdf" class=yC5>Machine-learning based classification of speech and music</a></h3><div class="gs_a">MKS Khan, WG Al-Khatib - Multimedia Systems, 2006 - Springer</div><div class="gs_rs">Abstract The need to classify audio into categories such as speech or music is an important <br>aspect of many multimedia document retrieval systems. In this paper, we investigate audio <br>features that have not been previously used in music-speech classification, such as the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14707290853251444735&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 17</a> <a href="/scholar?q=related:_29nxBvLGswJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0A/05/RN192229120.html?source=googlescholar" class="gs_nph" class=yC6>BL Direct</a> <a href="/scholar?cluster=14707290853251444735&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'_29nxBvLGswJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://publik.tuwien.ac.at/files/PubDat_186351.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tuwien.ac.at</span><span class="gs_ggsS">tuwien.ac.at <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0065245810780037" class=yC7>Features for content-based audio retrieval</a></h3><div class="gs_a">D MitroviÄ, M Zeppelzauer, <a href="/citations?user=AvNBy7MAAAAJ&amp;hl=en&amp;oi=sra">C Breiteneder</a> - Advances in computers, 2010 - Elsevier</div><div class="gs_rs">Abstract Today, a large number of audio features exists in audio retrieval for different <br>purposes, such as automatic speech recognition, music information retrieval, audio <br>segmentation, and environmental sound retrieval. The goal of this chapter is to review <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10438406953689636585&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 14</a> <a href="/scholar?q=related:6V6jv9Gr3JAJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10438406953689636585&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'6V6jv9Gr3JAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://upcommons.upc.edu/e-prints/bitstream/2117/12069/1/Interspeech2010_Taras_cameraready.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from upc.edu</span><span class="gs_ggsS">upc.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://upcommons.upc.edu/handle/2117/12069" class=yC9>A fast one-pass-training feature selection technique for GMM-based acoustic event detection with audio-visual data</a></h3><div class="gs_a">T Butko, C Nadeu CamprubÃ­ - 2011 - upcommons.upc.edu</div><div class="gs_rs">Acoustic event detection becomes a difficult task, even for a small number of events, in <br>scenarios where events are produced rather spontaneously and often overlap in time. In this <br>work, we aim to improve the detection rate by means of feature selection. Using a one-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6047690933112956074&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 2</a> <a href="/scholar?q=related:qmSMk9y27VMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6047690933112956074&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'qmSMk9y27VMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://d-nb.info/1003440045/34" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from d-nb.info</span><span class="gs_ggsS">d-nb.info <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://d-nb.info/1003440045/34" class=yCB>Etablierung einer telemedizinisch gestÃ¼tzten bioakustischen Hypertonie-Therapie mittels Virtual Lab</a></h3><div class="gs_a">P Friedrich - 2010 - d-nb.info</div><div class="gs_rs">Kurzfassung Ziel der Arbeit war es, ein virtuelles telemedizinisches Labor, das Virtual Lab, <br>als Entwicklungs-und Erprobungsplattform fÃ¼r personalisierte und individualisierte <br>Therapien zu realisieren. Gleichzeitig wurde mit dieser Entwicklungsumgebung ein neuer <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5987776735571131922&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 2</a> <a onclick="return gs_ocit(event,'Er4QZjfbGFMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md5', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md5" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Er4QZjfbGFMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:Er4QZjfbGFMJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=11142299502916222181&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://research.kfu.edu.sa/doc_files/654.doc" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kfu.edu.sa</span><span class="gs_ggsS">kfu.edu.sa <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://research.kfu.edu.sa/doc_files/654.doc" class=yCD>Automatic Classification of Speech &amp; Music in Digitized Audio</a></h3><div class="gs_a">MKS Khan - 2005 - research.kfu.edu.sa</div><div class="gs_rs">Page 1. Automatic Classification of Speech &amp; Music in Digitized Audio by Muhammad Kashif<br>Saeed Khan A Thesis Presented to the DEANSHIP OF GRADUATE STUDIES In Partial<br>Fulfillment of the Requirements for the Degree MASTER OF SCIENCE IN <b>...</b> </div><div class="gs_fl"><a href="/scholar?cites=16123181065875075333&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 1</a> <a href="/scholar?q=related:BbHJUMELwd8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16123181065875075333&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'BbHJUMELwd8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md6', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md6" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:BbHJUMELwd8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://lsas2008.dke-research.de/conftool/uploads/158/10-Sturm20121212.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dke-research.de</span><span class="gs_ggsS">dke-research.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://lsas2008.dke-research.de/conftool/uploads/158/10-Sturm20121212.pdf" class=yCF>A survey of evaluation in music genre recognition</a></h3><div class="gs_a"><a href="/citations?user=KdeYIvMAAAAJ&amp;hl=en&amp;oi=sra">BL Sturm</a> - Proc. Adaptive Multimedia Retrieval.  &hellip;, 2012 - lsas2008.dke-research.de</div><div class="gs_rs">Abstract. Much work is focused upon music genre recognition (MGR) from audio recordings, <br>symbolic data, and other modalities. While reviews have been written of some of this work <br>before, no survey has been made of the approaches to evaluating approaches to MGR. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4253992352053677712&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 1</a> <a onclick="return gs_ocit(event,'kE5jbYE3CTsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:kE5jbYE3CTsJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.eurasip.org/Proceedings/Eusipco/Eusipco2010/Contents/papers/1569292741.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurasip.org</span><span class="gs_ggsS">eurasip.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.eurasip.org/Proceedings/Eusipco/Eusipco2010/Contents/papers/1569292741.pdf" class=yC11>Speech Detection On Broadcast Audio</a></h3><div class="gs_a">Ã Zubari, EC Ozan, BO Acar, T Ciloglu&hellip; - European Signal  &hellip;, 2010 - eurasip.org</div><div class="gs_rs">ABSTRACT Speech boundary detection contributes to performance of speech based <br>applications such as speech recognition and speaker recognition. Speech boundary <br>detector implemented in this study works on broadcast audio as a pre-processor module <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9448658243530233344&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 1</a> <a href="/scholar?q=related:AJavgZxgIIMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9448658243530233344&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'AJavgZxgIIMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:AJavgZxgIIMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> MASTER OF APPLIED SCIENCE</h3><div class="gs_a">C Behrens</div><div class="gs_fl"><a href="/scholar?q=related:q5UQEPTzelMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'q5UQEPTzelMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://140.133.6.46/ETD-db/ETD-search/view_etd?URN=etd-0910110-032811" class=yC13>The Study of Integrating Temporal and Non-Temporal Features to Classify Music Genre</a></h3><div class="gs_a">S Wu - 2010 - 140.133.6.46</div><div class="gs_rs">Abstract This paper proposes a hierarchical music genre classification method integrating <br>both temporal and non-temporal features of music. First of all, music genres are devided into <br>two primary classes based on the difference of global timbre. Then, the concept of audio <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:hhLVHj9zpQkJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'hhLVHj9zpQkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:hhLVHj9zpQkJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/V468J6584081L8L3.pdf" class=yC14>On the Classification of Acoustic Sequences for Intervention in Essential Hypertension</a></h3><div class="gs_a">P Friedrich, T Kohler, B Wolf - World Congress on Medical Physics and  &hellip;, 2010 - Springer</div><div class="gs_rs">Acoustic signals are able to modulate the human metabolic and central-nervous functions <br>and evoke physiological effects. The anti-hypertensive effect of certain iterative sound <br>patterns, as a possible intervention in essential hypertension, has been examined in <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:2SXzckNwG4sJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10023728830636828121&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'2SXzckNwG4sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Singing Voice Detection in Western Popular Music</h3><div class="gs_a">W Yuansheng</div><div class="gs_fl"><a href="/scholar?q=related:dx-cdoqIEO4J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'dx-cdoqIEO4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://repository.tudelft.nl/assets/uuid:51d9ddff-4ff1-494d-8070-c246b073e3d4/ewi_smits_2008.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tudelft.nl</span><span class="gs_ggsS">tudelft.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://repository.tudelft.nl/assets/uuid:51d9ddff-4ff1-494d-8070-c246b073e3d4/ewi_smits_2008.pdf" class=yC15>Interactive learning for video content analysis</a></h3><div class="gs_a">EAP Smits - 2008 - repository.tudelft.nl</div><div class="gs_rs">This thesis is submitted partial fulfilment of the requirements for the degree of Master of <br>Science in Media and Knowledge Engineering. It is preceded by a research assignment <br>under the title âInteractive learning for video content analysis: state of the artâ. This <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:TQNrgkDGfNgJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15599561189672813389&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'TQNrgkDGfNgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md13', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md13" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TQNrgkDGfNgJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://www.tdx.cat/bitstream/handle/10803/32176/TTB1%201.pdf.txt?sequence=2" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[TXT]</span> from tdx.cat</span><span class="gs_ggsS">tdx.cat <span class=gs_ctg2>[TXT]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.tdx.cat/handle/10803/32176" class=yC17>Feature selection for multimodal: acoustic Event detection</a></h3><div class="gs_a">T Butko - 2011 - tdx.cat</div><div class="gs_rs">The detection of the Acoustic Events (AEs) naturally produced in a meeting room may help <br>to describe the human and social activity. The automatic description of interactions between <br>humans and environment can be useful for providing: implicit assistance to the people <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7VjguCfGFUEJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4689872460883974381&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'7VjguCfGFUEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
