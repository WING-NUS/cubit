Total results = 69
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.163.9627&amp;rep=rep1&amp;type=pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1618491" class=yC0>Fast motion deblurring</a></h3><div class="gs_a"><a href="/citations?user=JcmBvtUAAAAJ&amp;hl=en&amp;oi=sra">S Cho</a>, S Lee - ACM Transactions on Graphics (TOG), 2009 - dl.acm.org</div><div class="gs_rs">Abstract This paper presents a fast deblurring method that produces a deblurring result from <br>a single image of moderate size in a few seconds. We accelerate both latent image <br>estimation and kernel estimation in an iterative deblurring process by introducing a novel <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1643245671790425753&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 98</a> <a href="/scholar?q=related:mTbEcTv7zRYJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1643245671790425753&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'mTbEcTv7zRYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="ftp://128.97.4.251/pub/camreport/cam09-61.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 128.97.4.251</span><span class="gs_ggsS">128.97.4.251 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5206743" class=yC2>Blind motion deblurring from a single image using sparse approximation</a></h3><div class="gs_a"><a href="/citations?user=Mo4v5iwAAAAJ&amp;hl=en&amp;oi=sra">JF Cai</a>, H Ji, C Liu, <a href="/citations?user=985QGhAAAAAJ&amp;hl=en&amp;oi=sra">Z Shen</a> - Computer Vision and Pattern  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Restoring a clear image from a single motion-blurred image due to camera shake <br>has long been a challenging problem in digital imaging. Existing blind deblurring techniques <br>either only remove simple motion blurring, or need user interactions to work on more <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6443524090430787064&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 78</a> <a href="/scholar?q=related:-I3MXwX_a1kJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6443524090430787064&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'-I3MXwX_a1kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8139130&amp;id=-qUJAgAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC4>Image sensor with improved light sensitivity</a></h3><div class="gs_a">JT Compton, JF Hamilton Jr - US Patent 8,139,130, 2012 - Google Patents</div><div class="gs_rs">An image sensor for capturing a color image is disclosed having a two-dimensional array <br>having first and second groups of pixels wherein pixels from the first group of pixels have <br>narrower spectral photoresponses than pixels from the second group of pixels and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13571122539268633452&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 60</a> <a href="/scholar?q=related:bHNn7AtQVrwJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13571122539268633452&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'bHNn7AtQVrwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1778767" class=yC5>Image deblurring using inertial measurement sensors</a></h3><div class="gs_a"><a href="/citations?user=aSJthdEAAAAJ&amp;hl=en&amp;oi=sra">N Joshi</a>, SB Kang, <a href="/citations?user=ZeJjFQMAAAAJ&amp;hl=en&amp;oi=sra">CL Zitnick</a>, <a href="/citations?user=SkbvqqgAAAAJ&amp;hl=en&amp;oi=sra">R Szeliski</a> - ACM Transactions on Graphics &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract We present a deblurring algorithm that uses a hardware attachment coupled with a <br>natural image prior to deblur images from consumer cameras. Our approach uses a <br>combination of inexpensive gyroscopes and accelerometers in an energy optimization <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6348215758193602680&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 67</a> <a href="/scholar?q=related:eLCqCJlkGVgJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6348215758193602680&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'eLCqCJlkGVgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://repository.ust.hk/dspace/bitstream/1783.1/3201/1/dualdeblur_cvpr08_lowres.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ust.hk</span><span class="gs_ggsS">ust.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4587830" class=yC6>Robust dual motion deblurring</a></h3><div class="gs_a">J Chen, <a href="/citations?user=k9TsUVsAAAAJ&amp;hl=en&amp;oi=sra">L Yuan</a>, <a href="/citations?user=EWfpM74AAAAJ&amp;hl=en&amp;oi=sra">CK Tang</a>&hellip; - Computer Vision and  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a robust algorithm to deblur two consecutively captured blurred <br>photos from camera shaking. Previous dual motion deblurring algorithms succeeded in <br>small and simple motion blur and are very sensitive to noise. We develop a robust <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12756327856197567665&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 49</a> <a href="/scholar?q=related:sYQTa5mUB7EJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12756327856197567665&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'sYQTa5mUB7EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPATAPP11191538&amp;id=1g-ZAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC8>Processing color and panchromatic pixels</a></h3><div class="gs_a">JF Hamilton, JT Compton - US Patent App. 11/191,538, 2005 - Google Patents</div><div class="gs_rs">A method for forming a final digital color image includes capturing an image using an image <br>sensor having panchromatic pixels and color pixels corresponding to at least two color <br>photoresponses; providing from the captured image a digital panchromatic image and an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9978877784313750327&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 30</a> <a href="/scholar?q=related:N-MOd3kYfIoJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9978877784313750327&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'N-MOd3kYfIoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5674049" class=yC9>Richardson-lucy deblurring for scenes under a projective motion path</a></h3><div class="gs_a"><a href="/citations?user=nFhLmFkAAAAJ&amp;hl=en&amp;oi=sra">YW Tai</a>, <a href="/citations?user=XhyKVFMAAAAJ&amp;hl=en&amp;oi=sra">P Tan</a>, <a href="/citations?user=Gv1QGSMAAAAJ&amp;hl=en&amp;oi=sra">MS Brown</a> - Pattern Analysis and Machine  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper addresses how to model and correct image blur that arises when a <br>camera undergoes ego motion while observing a distant scene. In particular, we discuss <br>how the blurred image can be modeled as an integration of the clear scene under a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6756811280393294026&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 40</a> <a href="/scholar?q=related:ytAxdg8ExV0J:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6756811280393294026&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'ytAxdg8ExV0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.1660&amp;rep=rep1&amp;type=pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/y07467510533568u.pdf" class=yCA>Single image deblurring using motion density functions</a></h3><div class="gs_a">A Gupta, <a href="/citations?user=aSJthdEAAAAJ&amp;hl=en&amp;oi=sra">N Joshi</a>, <a href="/citations?user=ZeJjFQMAAAAJ&amp;hl=en&amp;oi=sra">C Lawrence Zitnick</a>, <a href="/citations?user=YAtwLpwAAAAJ&amp;hl=en&amp;oi=sra">M Cohen</a>&hellip; - Computer VisionâECCV  &hellip;, 2010 - Springer</div><div class="gs_rs">We present a novel single image deblurring method to estimate spatially non-uniform blur <br>that results from camera shake. We use existing spatially invariant deconvolution methods in <br>a local and robust way to compute initial estimates of the latent image. The camera motion <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17207611097480113166&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 34</a> <a href="/scholar?q=related:Dixfkh-3ze4J:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17207611097480113166&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'Dixfkh-3ze4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://koasas.kaist.ac.kr/bitstream/10203/24885/1/Correction%20of%20Spatially%20Varing_201006_TPAMI.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4912214" class=yCC>Correction of spatially varying image and video motion blur using a hybrid camera</a></h3><div class="gs_a"><a href="/citations?user=nFhLmFkAAAAJ&amp;hl=en&amp;oi=sra">YW Tai</a>, <a href="/citations?user=D1bVs1AAAAAJ&amp;hl=en&amp;oi=sra">H Du</a>, <a href="/citations?user=Gv1QGSMAAAAJ&amp;hl=en&amp;oi=sra">MS Brown</a>, S Lin - Pattern Analysis and Machine  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We describe a novel approach to reduce spatially varying motion blur in video and <br>images using a hybrid camera system. A hybrid camera is a standard video camera that is <br>coupled with an auxiliary low-resolution camera sharing the same optical path but <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15473773355804711062&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 26</a> <a href="/scholar?q=related:ltikL-LivdYJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15473773355804711062&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 21 versions</a> <a onclick="return gs_ocit(event,'ltikL-LivdYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.9855&amp;rep=rep1&amp;type=pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5539935" class=yCE>Coded exposure imaging for projective motion deblurring</a></h3><div class="gs_a"><a href="/citations?user=nFhLmFkAAAAJ&amp;hl=en&amp;oi=sra">YW Tai</a>, N Kong, S Lin, SY Shin - Computer Vision and Pattern  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We propose a method for deblurring of spatially variant object motion. A principal <br>challenge of this problem is how to estimate the point spread function (PSF) of the spatially <br>variant blur. Based on the projective motion blur model of, we present a blur estimation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=888212621617048567&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 20</a> <a href="/scholar?q=related:96uAZKyQUwwJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=888212621617048567&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'96uAZKyQUwwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5539941" class=yC10>Robust flash deblurring</a></h3><div class="gs_a"><a href="/citations?user=td18GeEAAAAJ&amp;hl=en&amp;oi=sra">S Zhuo</a>, <a href="/citations?user=em0P5ugAAAAJ&amp;hl=en&amp;oi=sra">D Guo</a>, <a href="/citations?user=AdEsZwsAAAAJ&amp;hl=en&amp;oi=sra">T Sim</a> - Computer Vision and Pattern  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Motion blur due to camera shake is an annoying yet common problem in low-light <br>photography. In this paper, we propose a novel method to recover a sharp image from a pair <br>of motion blurred and flash images, consecutively captured using a hand-held camera. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8528192891037245943&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 15</a> <a href="/scholar?q=related:92FJBkA6WnYJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8528192891037245943&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'92FJBkA6WnYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://research.microsoft.com/en-us/um/people/cohen/enhancing_spacetime.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from microsoft.com</span><span class="gs_ggsS">microsoft.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5559006" class=yC11>Enhancing and experiencing spacetime resolution with videos and stills</a></h3><div class="gs_a">A Gupta, P Bhat, M Dontcheva&hellip; - &hellip;  (ICCP), 2009 IEEE  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present solutions for enhancing the spatial and/or temporal resolution of videos. <br>Our algorithm targets the emerging consumer-level hybrid cameras that can simultaneously <br>capture video and high-resolution stills. Our technique produces a high spacetime <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15519023857626824965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 15</a> <a href="/scholar?q=related:BcGopvmlXtcJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15519023857626824965&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'BcGopvmlXtcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:BcGopvmlXtcJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=17952607505900594057&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="ftp://ftp.math.ucla.edu/pub/camreport/cam09-60.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucla.edu</span><span class="gs_ggsS">ucla.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5206711" class=yC13>High-quality curvelet-based motion deblurring from an image pair</a></h3><div class="gs_a"><a href="/citations?user=Mo4v5iwAAAAJ&amp;hl=en&amp;oi=sra">JF Cai</a>, H Ji, C Liu, <a href="/citations?user=985QGhAAAAAJ&amp;hl=en&amp;oi=sra">Z Shen</a> - Computer Vision and Pattern  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract One promising approach to remove motion deblurring is to recover one clear image <br>using an image pair. Existing dual-image methods require an accurate image alignment <br>between the image pair, which could be very challenging even with the help of user <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15293485005972408709&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 17</a> <a href="/scholar?q=related:hYm_ppVfPdQJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15293485005972408709&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'hYm_ppVfPdQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://dspace.mit.edu/openaccess-disseminate/1721.1/62818" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mit.edu</span><span class="gs_ggsS">mit.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5585100" class=yC15>Motion blur removal with orthogonal parabolic exposures</a></h3><div class="gs_a"><a href="/citations?user=XHJ8AGMAAAAJ&amp;hl=en&amp;oi=sra">TS Cho</a>, <a href="/citations?user=9aw_QGAAAAAJ&amp;hl=en&amp;oi=sra">A Levin</a>, <a href="/citations?user=NJ9c4ygAAAAJ&amp;hl=en&amp;oi=sra">F Durand</a>&hellip; - &hellip;  (ICCP), 2010 IEEE  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Object movement during exposure generates blur. Removing blur is challenging <br>because one has to estimate the motion blur, which can spatially vary over the image. Even <br>if the motion is successfully identified, blur removal can be unstable because the blur <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15919342223985787135&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 14</a> <a href="/scholar?q=related:_8CMjmjd7NwJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15919342223985787135&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'_8CMjmjd7NwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://www.ims.tuwien.ac.at/media/documents/publications/1291.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tuwien.ac.at</span><span class="gs_ggsS">tuwien.ac.at <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5539894" class=yC17>A spatially varying PSF-based prior for alpha matting</a></h3><div class="gs_a"><a href="/citations?user=5D0_pjcAAAAJ&amp;hl=en&amp;oi=sra">C Rhemann</a>, <a href="/citations?user=8iOtuUwAAAAJ&amp;hl=en&amp;oi=sra">C Rother</a>, <a href="/citations?user=3pyzQQ8AAAAJ&amp;hl=en&amp;oi=sra">P Kohli</a>&hellip; - Computer Vision and  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper we considerably improve on a state-of-the-art alpha matting approach <br>by incorporating a new prior which is based on the image formation process. In particular, <br>we model the prior probability of an alpha matte as the convolution of a high-resolution <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17098749557330963893&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 13</a> <a href="/scholar?q=related:tW0UHCH2Su0J:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17098749557330963893&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'tW0UHCH2Su0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPATAPP12480820&amp;id=oWrlAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC19>Interpolation for four-channel color filter array</a></h3><div class="gs_a">JE Adams, M Kumar, BH Pillman&hellip; - US Patent App. 12/ &hellip;, 2009 - Google Patents</div><div class="gs_rs">A method is described for forming a full-color output image from a color filter array image <br>comprising capturing an image using an image sensor including panchromatic pixels and <br>color pixels having at least two different color responses, the pixels being arranged in a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15477636809737346105&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 10</a> <a href="/scholar?q=related:OZznpqycy9YJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15477636809737346105&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'OZznpqycy9YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.cs.cornell.edu/home/~dph/papers/deblur.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cornell.edu</span><span class="gs_ggsS">cornell.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5539938" class=yC1A>Generating sharp panoramas from motion-blurred videos</a></h3><div class="gs_a"><a href="/citations?user=2vaDEYAAAAAJ&amp;hl=en&amp;oi=sra">Y Li</a>, SB Kang, <a href="/citations?user=aSJthdEAAAAJ&amp;hl=en&amp;oi=sra">N Joshi</a>, SM Seitz&hellip; - Computer Vision and  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we show how to generate a sharp panorama from a set of motion-<br>blurred video frames. Our technique is based on joint global motion estimation and multi-<br>frame deblurring. It also automatically computes the duty cycle of the video, namely the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6951025334533181395&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 9</a> <a href="/scholar?q=related:00-XsbYAd2AJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6951025334533181395&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'00-XsbYAd2AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5206625" class=yC1C>Removing partial blur in a single image</a></h3><div class="gs_a"><a href="/citations?user=75aprCAAAAAJ&amp;hl=en&amp;oi=sra">S Dai</a>, <a href="/citations?user=zAlz89wAAAAJ&amp;hl=en&amp;oi=sra">Y Wu</a> - Computer Vision and Pattern Recognition, 2009.  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Removing image partial blur is of great practical importance. However, as existing <br>recovery techniques usually assume a one-layer clear image model, they can not <br>characterize the actual generation process of partial blurs. In this paper, a two-layer image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1667179453333576174&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 9</a> <a href="/scholar?q=related:7hnHNOECIxcJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1667179453333576174&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'7hnHNOECIxcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://www.eecis.udel.edu/~ding/files/ECCV10.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from udel.edu</span><span class="gs_ggsS">udel.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/532616702850X574.pdf" class=yC1D>Analysis of motion blur with a flutter shutter camera for non-linear motion</a></h3><div class="gs_a">Y Ding, <a href="/citations?user=z7vO3-MAAAAJ&amp;hl=en&amp;oi=sra">S McCloskey</a>, <a href="/citations?user=R9L_AfQAAAAJ&amp;hl=en&amp;oi=sra">J Yu</a> - Computer VisionâECCV 2010, 2010 - Springer</div><div class="gs_rs">Motion blurs confound many computer vision problems. The fluttered shutter (FS) camera [1] <br>tackles the motion deblurring problem by emulating invertible broadband blur kernels. <br>However, existing FS methods assume known constant velocity motions, eg, via user <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14517912554599145566&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 5</a> <a href="/scholar?q=related:XuAQmYz8eckJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14517912554599145566&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'XuAQmYz8eckJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://www.hci.iis.u-tokyo.ac.jp/~ysato/papers/Shimano-ACCV09.pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/Q3N332312JT47173.pdf" class=yC1F>Video temporal super-resolution based on self-similarity</a></h3><div class="gs_a">M Shimano, <a href="/citations?user=P1FZwkAAAAAJ&amp;hl=en&amp;oi=sra">T Okabe</a>, <a href="/citations?user=gtfbzYwAAAAJ&amp;hl=en&amp;oi=sra">I Sato</a>, <a href="/citations?user=VEUW-qIAAAAJ&amp;hl=en&amp;oi=sra">Y Sato</a> - Computer VisionâACCV 2010, 2011 - Springer</div><div class="gs_rs">We propose a method for making temporal super-resolution video from a single video by <br>exploiting the self-similarity that exists in the spatio-temporal domain of videos. Temporal <br>super-resolution is inherently ill-posed problem because there are an infinite number of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5173690412710401359&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 5</a> <a href="/scholar?q=related:T5Vy2wSkzEcJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5173690412710401359&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'T5Vy2wSkzEcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0030399210000022" class=yC21>An improved RichardsonâLucy algorithm based on local prior</a></h3><div class="gs_a">W Yongpan, F Huajun, X Zhihai, L Qi&hellip; - Optics &amp; Laser  &hellip;, 2010 - Elsevier</div><div class="gs_rs">Ringing is one of the most common disturbing artifacts in image deconvolution. With a totally <br>known kernel, the standard RichardsonâLucy (RL) algorithm succeeds in many motion <br>deblurring processes, but the resulting images still contain visible ringing. When the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6017931371796410091&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 6</a> <a href="/scholar?q=related:666awrL8g1MJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6017931371796410091&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'666awrL8g1MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://www.gris.informatik.tu-darmstadt.de/~sroth/pubs/cvpr11schmidt.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tu-darmstadt.de</span><span class="gs_ggsS">tu-darmstadt.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995653" class=yC22>Bayesian deblurring with integrated noise estimation</a></h3><div class="gs_a"><a href="/citations?user=Lmh0gWkAAAAJ&amp;hl=en&amp;oi=sra">U Schmidt</a>, K Schelten, <a href="/citations?user=0yDoR0AAAAAJ&amp;hl=en&amp;oi=sra">S Roth</a> - Computer Vision and Pattern  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Conventional non-blind image deblurring algorithms involve natural image priors <br>and maximum a-posteriori (MAP) estimation. As a consequence of MAP estimation, separate <br>pre-processing steps such as noise estimation and training of the regularization <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=446290360769556389&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 5</a> <a href="/scholar?q=related:pa9Z9LeKMQYJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=446290360769556389&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'pa9Z9LeKMQYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://www.ee.iitm.ac.in/~raju/conf/c68.pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iitm.ac.in</span><span class="gs_ggsS">iitm.ac.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5543835" class=yC24>Unscented transformation for depth from motion-blur in videos</a></h3><div class="gs_a">C Paramanand, AN Rajagopalan - Computer Vision and  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In images and videos of a 3D scene, blur due to camera shake can be a source of <br>depth information. Our objective is to find the shape of the scene from its motion-blurred <br>observations without having to restore the original image. In this paper, we pose depth <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12823974865084504105&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 4</a> <a href="/scholar?q=related:KVjmCTLp97EJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12823974865084504105&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'KVjmCTLp97EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.1465&amp;rep=rep1&amp;type=pdf" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.1465&amp;rep=rep1&amp;type=pdf" class=yC26>Variational optical flow from alternate exposure images</a></h3><div class="gs_a">A Sellent, <a href="/citations?user=4fN_h5QAAAAJ&amp;hl=en&amp;oi=sra">M Eisemann</a>, <a href="/citations?user=r6HOvnoAAAAJ&amp;hl=en&amp;oi=sra">B GoldlÃ¼cke</a>, <a href="/citations?user=FwNaHxQAAAAJ&amp;hl=en&amp;oi=sra">T Pock</a>&hellip; - Proc. VMV, 2009 - Citeseer</div><div class="gs_rs">Abstract Traditional optic flow algorithms rely on consecutive short-exposure images. In <br>contrast, longexposed images contain integrated motion information directly in form of <br>motion blur. In this paper, we show how the additional information provided by a long <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8807221072193427986&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 3</a> <a href="/scholar?q=related:EppsCueIOXoJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'EppsCueIOXoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md23', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md23" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:EppsCueIOXoJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5661787" class=yC28>Motion field estimation from alternate exposure images</a></h3><div class="gs_a">A Sellent, <a href="/citations?user=4fN_h5QAAAAJ&amp;hl=en&amp;oi=sra">M Eisemann</a>, <a href="/citations?user=r6HOvnoAAAAJ&amp;hl=en&amp;oi=sra">B Goldlucke</a>&hellip; - Pattern Analysis and  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Traditional optical flow algorithms rely on consecutive short-exposed images. In this <br>work, we make use of an additional long-exposed image for motion field estimation. Long-<br>exposed images integrate motion information directly in the form of motion-blur. With this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10812481301067215024&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 3</a> <a href="/scholar?q=related:sDCq0nSmDZYJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10812481301067215024&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'sDCq0nSmDZYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6072265" class=yC29>Robust image deblurring with an inaccurate blur kernel</a></h3><div class="gs_a">H Ji, K Wang - Image Processing, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Most existing nonblind image deblurring methods assume that the blur kernel is <br>free of error. However, it is often unavoidable in practice that the input blur kernel is <br>erroneous to some extent. Sometimes, the error could be severe, eg, for images degraded <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7806765353175325716&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 2</a> <a href="/scholar?q=related:FJzXCLozV2wJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7806765353175325716&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'FJzXCLozV2wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8119435&amp;id=LLkEAgAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC2A>Wafer level processing for backside illuminated image sensors</a></h3><div class="gs_a">FT Brady - US Patent 8,119,435, 2012 - Google Patents</div><div class="gs_rs">A backside illuminated image sensor comprises a sensor layer having a plurality of <br>photosensitive elements of a pixel array, an oxide layer adjacent a backside surface of the <br>sensor layer, and at least one dielectric layer adjacent a frontside surface of the sensor <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8887872123642934327&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 2</a> <a href="/scholar?q=related:N3TVQpsQWHsJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8887872123642934327&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'N3TVQpsQWHsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/63668526768x127k.pdf" class=yC2B>Image reconstruction for high-sensitivity imaging by using combined long/short exposure type single-chip image sensor</a></h3><div class="gs_a">S Ugawa, T Azuma, T Imagawa, Y Okada - Computer VisionâACCV 2010, 2011 - Springer</div><div class="gs_rs">We propose a image reconstruction method and a sensor for high-sensitivity imaging using <br>long-term exposed green pixels over several frames. As a result of extending the exposure <br>time of green pixels, motion blur increases. We use motion information detected from high-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7632163871704984037&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 2</a> <a href="/scholar?q=related:5amLuJrk6mkJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7632163871704984037&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'5amLuJrk6mkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://yuwing.kaist.ac.kr/papers/tr_motionmatting.pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5766000" class=yC2C>Motion regularization for matting motion blurred objects</a></h3><div class="gs_a"><a href="/citations?user=uZU1uIkAAAAJ&amp;hl=en&amp;oi=sra">HT Lin</a>, <a href="/citations?user=nFhLmFkAAAAJ&amp;hl=en&amp;oi=sra">YW Tai</a>, <a href="/citations?user=Gv1QGSMAAAAJ&amp;hl=en&amp;oi=sra">MS Brown</a> - Pattern Analysis and Machine  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper addresses the problem of matting motion blurred objects from a single <br>image. Existing single image matting methods are designed to extract static objects that <br>have fractional pixel occupancy. This arises because the physical scene object has a finer <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11809057006402897030&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 4</a> <a href="/scholar?q=related:hoT7vscy4qMJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11809057006402897030&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'hoT7vscy4qMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://bmvc10.dcs.aber.ac.uk/proc/conference/paper73/paper73.pdf" class=yC2F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aber.ac.uk</span><span class="gs_ggsS">aber.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://bmvc10.dcs.aber.ac.uk/proc/conference/paper73/paper73.pdf" class=yC2E>Inferring image transformation and structure from motion-blurred images</a></h3><div class="gs_a">P Chandramouli&hellip; - Proceedings of the British  &hellip;, 2010 - bmvc10.dcs.aber.ac.uk</div><div class="gs_rs">Abstract This paper deals with the problem of estimating structure of 3D scenes and image <br>transformations from observations that are blurred due to unconstrained camera motion. <br>Initially, we consider a fronto-parallel planar scene and relate the reference image of the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9804736087039038306&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 2</a> <a href="/scholar?q=related:YlNqAIZrEYgJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9804736087039038306&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'YlNqAIZrEYgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md29', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md29" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:YlNqAIZrEYgJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8068153&amp;id=Ebv-AQAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC30>Producing full-color image using CFA image</a></h3><div class="gs_a">M Kumar, JE Adams Jr - US Patent 8,068,153, 2011 - Google Patents</div><div class="gs_rs">A method of forming a full-color output image using a color filter array image having a <br>plurality of color channels and a panchromatic channel, comprising capturing a color filter <br>array image having a plurality of color channels and a panchromatic channel, wherein the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6917907758131397853&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 1</a> <a href="/scholar?q=related:3URXEHRYAWAJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6917907758131397853&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'3URXEHRYAWAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2001589" class=yC31>PSO based motion deblurring for single image</a></h3><div class="gs_a">C Song, H Zhao, W Jing, H Zhu - &hellip;  of the 13th annual conference on  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract This paper addresses the issue of non-uniform motion deblurring due to hand <br>shake for a single photograph. The main difficulty of spatially variant motion deblurring is <br>that the deconvolution algorithm can not directly be used to estimate the blur kernel as the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6593174382442975995&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 2</a> <a href="/scholar?q=related:-xK4Lyupf1sJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'-xK4Lyupf1sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6128756" class=yC32>Real Time High-Sensitivity Imaging for Home Surveillance System by Using Combined Long/Short Exposure</a></h3><div class="gs_a">S Sato, Y Okada, T Azuma - Digital Image Computing  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In the future, a comprehensive home surveillance system could be constructed <br>using a network of pre-existing cameras found in household products. To be incorporated <br>into household products, the camera must be small. In addition, for surveillance cameras, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12095548433896530574&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 1</a> <a href="/scholar?q=related:jmLvNTcF3KcJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12095548433896530574&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'jmLvNTcF3KcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://www.hci.iis.u-tokyo.ac.jp/~okabe/matsui-CVA10.pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/P216M541541TR028.pdf" class=yC33>Image Enhancement of Low-light Scenes with Near-infrared Flash Images</a></h3><div class="gs_a">S Matsui, <a href="/citations?user=P1FZwkAAAAAJ&amp;hl=en&amp;oi=sra">T Okabe</a>, M Shimano, <a href="/citations?user=VEUW-qIAAAAJ&amp;hl=en&amp;oi=sra">Y Sato</a> - Computer VisionâACCV 2009, 2010 - Springer</div><div class="gs_rs">Abstract. We present a novel technique for enhancing an image captured in low light by <br>using near-infrared flash images. The main idea is to combine a color image with near-<br>infrared flash images captured at the same time without causing any interference with the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10306338644572142037&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 1</a> <a href="/scholar?q=related:1XmaVGR4B48J:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10306338644572142037&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'1XmaVGR4B48J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5646713" class=yC35>A deburring technique for large scale motion blur images using a hybrid camera</a></h3><div class="gs_a">S Xu, H Liang, D Tu, G Li - Image and Signal Processing (CISP) &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract If a camera moves fast in low light environments, large scale motion blur is induced. <br>Algorithms for deblurring motion blur images mostly fail miserably when applied to large <br>scale motion blur images. The motion path of the camera is complex, and the estimation of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12984760751308180506&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 2</a> <a href="/scholar?q=related:GowoaxsjM7QJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'GowoaxsjM7QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/L70411842GW7218J.pdf" class=yC36>Two algorithms for motion estimation from alternate exposure images</a></h3><div class="gs_a">A Sellent, <a href="/citations?user=4fN_h5QAAAAJ&amp;hl=en&amp;oi=sra">M Eisemann</a>, <a href="/citations?user=st_GV1QAAAAJ&amp;hl=en&amp;oi=sra">M Magnor</a> - Video Processing and Computational  &hellip;, 2011 - Springer</div><div class="gs_rs">Most algorithms for dense 2D motion estimation assume pairs of images that are acquired <br>with an idealized, infinitively short exposure time. In this work we compare two approaches <br>that use an additional, motion-blurred image of a scene to estimate highly accurate, dense <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8421016945207578490&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 1</a> <a href="/scholar?q=related:eo-kkkt23XQJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8421016945207578490&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'eo-kkkt23XQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="http://dev.ipol.im/~tendero/these_11_compressed.pdf" class=yC38><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ipol.im</span><span class="gs_ggsS">ipol.im <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://dev.ipol.im/~tendero/these_11_compressed.pdf" class=yC37>Mathematical theory of the flutter shutter</a></h3><div class="gs_a">Y TENDERO - 2012 - dev.ipol.im</div><div class="gs_rs">RÃ©sumÃ© Cette thÃ¨se apporte des solutions thÃ©oriques et pratiques Ã  deux problÃ¨mes <br>soulevÃ©s par la photographie numÃ©rique en prÃ©sence de mouvement, et par la <br>photographie infrarouge. La photographie d&#39;objets en mouvement semblait ne pouvoir se <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9674913492171921593&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 1</a> <a href="/scholar?q=related:uZiKrY8yRIYJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9674913492171921593&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'uZiKrY8yRIYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md36', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md36" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:uZiKrY8yRIYJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://yuwing.kaist.ac.kr/papers/cvpr12_deblurCRF.pdf" class=yC3A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6247654" class=yC39>Nonlinear camera response functions and image deblurring</a></h3><div class="gs_a">S Kim, <a href="/citations?user=nFhLmFkAAAAJ&amp;hl=en&amp;oi=sra">YW Tai</a>, <a href="/citations?user=1F2czKYAAAAJ&amp;hl=en&amp;oi=sra">SJ Kim</a>, <a href="/citations?user=Gv1QGSMAAAAJ&amp;hl=en&amp;oi=sra">MS Brown</a>&hellip; - Computer Vision and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper investigates the role that nonlinear camera response functions (CRFs) <br>have on image deblurring. In particular, we show how nonlinear CRFs can cause a spatially <br>invariant blur to behave as a spatially varying blur. This can result in noticeable ringing <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16737363147650588224&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 1</a> <a href="/scholar?q=related:QF5tvBoPR-gJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16737363147650588224&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'QF5tvBoPR-gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0031320311004808" class=yC3B>Image deblurring with matrix regression and gradient evolution</a></h3><div class="gs_a"><a href="/citations?user=0ggsACEAAAAJ&amp;hl=en&amp;oi=sra">S Xiang</a>, G Meng, Y Wang, C Pan, C Zhang - Pattern Recognition, 2011 - Elsevier</div><div class="gs_rs">This paper presents a supervised learning algorithm for image deblurring. The task is <br>addressed into the conceptual framework of matrix regression and gradient evolution. <br>Specifically, given pairs of blurred image patches and their corresponding clear ones, an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5702036419959689813&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 3</a> <a href="/scholar?q=related:VZIJzOmzIU8J:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5702036419959689813&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'VZIJzOmzIU8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB39" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW39"><a href="http://users.soe.ucsc.edu/~milanfar/publications/journal/MotionDeblurringFinal.pdf" class=yC3D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucsc.edu</span><span class="gs_ggsS">ucsc.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5738336" class=yC3C>Removing Motion Blur With SpaceâTime Processing</a></h3><div class="gs_a">H Takeda, <a href="/citations?user=iGzDl8IAAAAJ&amp;hl=en&amp;oi=sra">P Milanfar</a> - Image Processing, IEEE Transactions  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Although spatial deblurring is relatively well understood by assuming that the blur <br>kernel is shift invariant, motion blur is not so when we attempt to deconvolve on a frame-by-<br>frame basis: this is because, in general, videos include complex, multilayer transitions. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8410740395951817120&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 2</a> <a href="/scholar?q=related:oIG8_tPzuHQJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8410740395951817120&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'oIG8_tPzuHQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB40" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW40"><a href="http://www.cse.ust.hk/~psander/pg2012proc/pg/pdf/v31i7pp2183-2192.pdf" class=yC3F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ust.hk</span><span class="gs_ggsS">ust.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cse.ust.hk/~psander/pg2012proc/pg/pdf/v31i7pp2183-2192.pdf" class=yC3E>Registration Based Non-uniform Motion Deblurring</a></h3><div class="gs_a"><a href="/citations?user=JcmBvtUAAAAJ&amp;hl=en&amp;oi=sra">S Cho</a>, H Cho, <a href="/citations?user=nFhLmFkAAAAJ&amp;hl=en&amp;oi=sra">YW Tai</a>, S Lee - Computer Graphics Forum, 2012 - cse.ust.hk</div><div class="gs_rs">Abstract This paper proposes an algorithm which uses image registration to estimate a non-<br>uniform motion blur point spread function (PSF) caused by camera shake. Our study is <br>based on a motion blur model which models blur effects of camera shakes using a set of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3855146302801996626&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 1</a> <a href="/scholar?q=related:UmusDyM7gDUJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3855146302801996626&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'UmusDyM7gDUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S092523121200104X" class=yC40>Video-based non-uniform object motion blur estimation and deblurring</a></h3><div class="gs_a">X Deng, Y Shen, M Song, D Tao, J Bu, C Chen - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Motion deblurring is a challenging problem in computer vision. Most previous blind <br>deblurring approaches usually assume that the Point Spread Function (PSF) is spatially <br>invariant. However, non-uniform motions exist ubiquitously and cannot be handled <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12107508654918928055&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 1</a> <a href="/scholar?q=related:twZpK_mCBqgJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12107508654918928055&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'twZpK_mCBqgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5523878" class=yC41>Inter-Frame Information Transfer Via Projection Onto Convex Set for Video Deblurring</a></h3><div class="gs_a"><a href="/citations?user=zFHvcz8AAAAJ&amp;hl=en&amp;oi=sra">Y Huang</a>, N Fan - Selected Topics in Signal Processing, IEEE  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We study the problem of interpolating blurred frames from their adjacent sharp <br>frames using optical flow for video camera anti-shake deblurring post-processing. A new <br>method is proposed that alternatively projects the resulting image onto two convex sets: <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13905072164809046938&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 1</a> <a href="/scholar?q=related:mnudL3a9-MAJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13905072164809046938&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'mnudL3a9-MAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8237831&amp;id=pgAhAgAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC42>Four-channel color filter array interpolation</a></h3><div class="gs_a">JE Adams Jr, M Kumar, BH Pillman&hellip; - US Patent  &hellip;, 2012 - Google Patents</div><div class="gs_rs">A method of forming a full-color output image from a color filter array image having a plurality <br>of color pixels having at least two different color responses and panchromatic pixels, <br>comprising capturing a color filter array image using an image sensor including <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:tdO4L-c4cZkJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11056681125760521141&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'tdO4L-c4cZkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://dualdeblur.googlecode.com/svn-history/r87/trunk/dualdeblur/reports/deblur-november/deblur-november.pdf" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from googlecode.com</span><span class="gs_ggsS">googlecode.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://dualdeblur.googlecode.com/svn-history/r87/trunk/dualdeblur/reports/deblur-november/deblur-november.pdf" class=yC43>Image Deblurring With Two Images</a></h3><div class="gs_a">S Hua, W Lei - 2008 - dualdeblur.googlecode.com</div><div class="gs_rs">Deblurring is the process of finding I when given B. If kernel K is known, the problem becomes <br>well-posed and is named non-blind deconvolution. In the opposite, when the kernel K is <br>unknown, the problem is ill-posed and named blind deconvolution. Suppose that the <b> ...</b> </div><div class="gs_fl"><a href="/scholar?q=related:gBFnRPO-WhoJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1899040144924021120&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'gBFnRPO-WhoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md44', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md44" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:gBFnRPO-WhoJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=MHPhrxEaOfcC&amp;oi=fnd&amp;pg=PA25&amp;ots=o159Mruf2j&amp;sig=7q_kQuwdDmPDBq5vytWLa9Ct5sg" class=yC45>Fundamentals of Image Restoration</a></h3><div class="gs_a"><a href="/citations?user=ctk0e5wAAAAJ&amp;hl=en&amp;oi=sra">BK GUNTURK</a> - Image Restoration: Fundamentals and Advances, 2012 - books.google.com</div><div class="gs_rs">In many imaging applications, the measured image is a degraded version of the true (or <br>original) image that ideally represents the scene. The degradation may be due to (1) <br>atmospheric distortions (including turbulence and aerosol scattering),(2) optical <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:BZBCXRyIu7cJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'BZBCXRyIu7cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8194296&amp;id=eIMdAgAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC46>Image sensor with improved light sensitivity</a></h3><div class="gs_a">JT Compton, JF Hamilton Jr, TE DeWeese - US Patent 8,194,296, 2012 - Google Patents</div><div class="gs_rs">A system for capturing a color image, includes a two-dimensional array having first and <br>second groups of pixels, pixels from the first group of pixels have narrower spectral <br>photoresponses than pixels from the second group of pixels and the first group of pixels <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:PzE6XpHwZecJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16673997702550925631&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'PzE6XpHwZecJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8203633&amp;id=hgseAgAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC47>Four-channel color filter array pattern</a></h3><div class="gs_a">JE Adams Jr, M Kumar, BH Pillman&hellip; - US Patent  &hellip;, 2012 - Google Patents</div><div class="gs_rs">An image sensor for capturing a color image comprising a two dimensional array of light-<br>sensitive pixels including panchromatic pixels and color pixels having at least two different <br>color responses, the pixels being arranged in a repeating pattern having a square minimal <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lhRuaNjxoD0J:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4440815144354583702&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'lhRuaNjxoD0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8224082&amp;id=Gu0fAgAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC48>CFA image with synthetic panchromatic image</a></h3><div class="gs_a">M Kumar, JE Adams Jr, BH Pillman - US Patent 8,224,082, 2012 - Google Patents</div><div class="gs_rs">A method for forming a final digital color image with reduced motion blur including of a <br>processor for providing images having panchromatic pixels and color pixels corresponding <br>to at least two color photo responses, interpolating between the panchromatic pixels and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7XQfdtG2j4kJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9912342315611944173&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'7XQfdtG2j4kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8125546&amp;id=xfMEAgAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC49>Color filter array pattern having four-channels</a></h3><div class="gs_a">JE Adams Jr, M Kumar, BH Pillman&hellip; - US Patent  &hellip;, 2012 - Google Patents</div><div class="gs_rs">An image sensor for capturing a color image comprising a two dimensional array of light-<br>sensitive pixels including panchromatic pixels and color pixels having at least three different <br>color responses, the pixels being arranged in a rectangular minimal repeating unit having <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:yArAfB5cjtsJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15820683827010865864&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'yArAfB5cjtsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB50" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW50"><a href="http://diglib.eg.org/EG/DL/CGF/volume30/issue7;internal&amp;action=action.digitallibrary.issuebibtex" class=yC4B><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from eg.org</span><span class="gs_ggsS">eg.org <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2011.02057.x/full" class=yC4A>Motion Deblurring from a Single Image using Circular Sensor Motion</a></h3><div class="gs_a">Y Bando, <a href="/citations?user=8UC5dY4AAAAJ&amp;hl=en&amp;oi=sra">BY Chen</a>, T Nishita - Computer Graphics Forum, 2011 - Wiley Online Library</div><div class="gs_rs">Abstract Image blur caused by object motion attenuates high frequency content of images, <br>making post-capture deblurring an ill-posed problem. The recoverable frequency band <br>quickly becomes narrower for faster object motion as high frequencies are severely <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:aGI2pgognVoJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6529410264867431016&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'aGI2pgognVoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8045024&amp;id=F0n5AQAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC4C>Producing full-color image with reduced motion blur</a></h3><div class="gs_a">M Kumar, JE Adams Jr - US Patent 8,045,024, 2011 - Google Patents</div><div class="gs_rs">A method of forming a full-color output image using a color filter array image having a <br>plurality of color channels and a panchromatic channel, comprising capturing a color filter <br>array image having a plurality of color channels and a panchromatic channel, wherein the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:VMQtq21VUmMJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7156876687356838996&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'VMQtq21VUmMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB52" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW52"><a href="http://icpr2010.org/pdfs/icpr2010_ThBCT9.8.pdf" class=yC4E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from icpr2010.org</span><span class="gs_ggsS">icpr2010.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://icpr2010.org/pdfs/icpr2010_ThBCT9.8.pdf" class=yC4D>Progressive MAP-Based Deconvolution with Pixel-Dependent Gaussian Prior</a></h3><div class="gs_a"><a href="/citations?user=HwZgCf8AAAAJ&amp;hl=en&amp;oi=sra">M Tanaka</a>, T Kanda, <a href="/citations?user=z1G7lrYAAAAJ&amp;hl=en&amp;oi=sra">M Okutomi</a> - 2010 International Conference On  &hellip;, 2010 - icpr2010.org</div><div class="gs_rs">Abstract A deconvolution is a fundamental technique and used in various vision <br>applications. A maximum a posteriori estimation is known as a powerful tool. In this paper, <br>we propose a progressive MAP-based deconvolution algorithm with a pixel dependent <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:GrL7rcV2_3sJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8934990777125941786&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'GrL7rcV2_3sJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md52', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md52" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:GrL7rcV2_3sJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5563836" class=yC4F>Panoramic aerial camera image motion measurement using a hybrid system</a></h3><div class="gs_a">G Li, P Jia - &hellip;  and Information Technology (ICCSIT), 2010 3rd  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Image motion compensation is significantly important for aerial camera photograph. <br>To execute the compensation, we must determine the image motion exactly. This paper <br>proposes a method of real-time image motion measurement for panoramic aerial cameras <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:YgJSvwUhcB8J:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'YgJSvwUhcB8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB54" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW54"><a href="https://eng.ucmerced.edu/people/zhu/BMVC12.pdf" class=yC51><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucmerced.edu</span><span class="gs_ggsS">ucmerced.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://eng.ucmerced.edu/people/zhu/BMVC12.pdf" class=yC50>Fast Non-uniform Deblurring using Constrained Camera Pose Subspace</a></h3><div class="gs_a">Z Hu, <a href="/citations?user=p9-ohHsAAAAJ&amp;hl=en&amp;oi=sra">MH Yang</a> - eng.ucmerced.edu</div><div class="gs_rs">Abstract Camera shake during exposure time often results in non-uniform blur across the <br>entire image. Recent algorithms model the non-uniform blurry image as a linear combination <br>of images observed by the camera at discretized poses, and focus on estimating the time <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3t6swzXHH8cJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14348405971555770078&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'3t6swzXHH8cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md54', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md54" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:3t6swzXHH8cJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB55" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW55"><a href="http://tel.archives-ouvertes.fr/docs/00/75/24/09/PDF/Tendero2012.pdf" class=yC53><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://tel.archives-ouvertes.fr/tel-00752409/" class=yC52>ThÃ©orie mathÃ©matique du Flutter Shutter: ses paradoxes et leur solution</a></h3><div class="gs_a">Y Tendero - 2012 - tel.archives-ouvertes.fr</div><div class="gs_rs">RÃ©sumÃ© Cette thÃ¨se apporte des solutions thÃ©oriques et pratiques Ã  deux problÃ¨mes <br>soulevÃ©s par la photographie numÃ©rique en prÃ©sence de mouvement, et par la <br>photographie infrarouge. La photographie d&#39;objets en mouvement semblait ne pouvoir se <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'cMikydsELEwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5711083" class=yC54>Motion estimation method for blurred videos and application of deblurring with spatially varying blur kernels</a></h3><div class="gs_a">XC He, T Luo, SC Yuk, KP Chow&hellip; - &hellip;  (ICCIT), 2010 5th  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Optical flow methods, such as Lucas-Kanade and Horn-Schunck algorithms, are <br>popular in motion estimation. However, they fall short on accuracy when they are applied to <br>blurred videos. Some people utilize hybrid camera system to get a low resolution image to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:tZQV7i42eCIJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2483794769685615797&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'tZQV7i42eCIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB57" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW57"><a href="http://www.cse.cuhk.edu.hk/~leojia/papers/depth_deblur_iccp12.pdf" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cuhk.edu.hk</span><span class="gs_ggsS">cuhk.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6215220" class=yC55>Depth-aware motion deblurring</a></h3><div class="gs_a"><a href="/citations?user=Go9TaC4AAAAJ&amp;hl=en&amp;oi=sra">L Xu</a>, <a href="/citations?user=XPAkzTEAAAAJ&amp;hl=en&amp;oi=sra">J Jia</a> - &hellip;  (ICCP), 2012 IEEE International Conference on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Motion deblurring from images that are captured in a scene with depth variation <br>needs to estimate spatially-varying point spread functions (PSFs). We tackle this <br>problemwith a stereopsis configuration, using depth information to help blur removal. We <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3056916472690576511&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=69">Cited by 1</a> <a href="/scholar?q=related:fwzD7lVZbCoJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3056916472690576511&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'fwzD7lVZbCoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1345570" class=yC57>Real-time computational camera system for high-sensitivity imaging by using combined long/short exposure</a></h3><div class="gs_a">S Sato, Y Okada, T Azuma - IS&amp;T/SPIE  &hellip;, 2012 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract In this study, we realize high-resolution (4K-format), small-size (1.43 x 1.43 Î¼m pixel <br>pitch size with a single imager) and high-sensitivity (four times higher sensitivity as <br>compared to conventional imagers) video camera system. Our proposed system is the real <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Nftb78Oe2GsJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7771135721399384885&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Nftb78Oe2GsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6161744" class=yC58>Blur-free high-sensitivity imaging system utilizing combined long/short exposure green pixels</a></h3><div class="gs_a">S Sato, Y Okada, T Azuma - Consumer Electronics (ICCE),  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In previous work, we have proposed a high-sensitivity imaging system based on the <br>long exposure for all the G (green) pixels, in which the inherent blur was suppressed with <br>using both the regularization method and motion information from blur-free short-term <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:JsZA9GGHwv4J:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'JsZA9GGHwv4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=760631" class=yC59>Method to detect and calculate motion blur kernel</a></h3><div class="gs_a">J Wu, H Feng, Z Xu, Q Li, Z Fu - 5th  &hellip;, 2010 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract Motion during camera&#39;s exposure time causes image blur, we call it motion blur. <br>According to the linear system theory, if we can find the blur kernel which has the same <br>meaning of point spread function, the blurred image can be restored by the blur kernel <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lMWg-WCF0PwJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18217207144265074068&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'lMWg-WCF0PwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB61" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW61"><a href="http://www.ibe.kagoshima-u.ac.jp/~cgv/papers/2010/1-(1)-6-accv2010_deblur_final.pdf" class=yC5B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kagoshima-u.ac.jp</span><span class="gs_ggsS">kagoshima-u.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/56063643R764X701.pdf" class=yC5A>Video deblurring and super-resolution technique for multiple moving objects</a></h3><div class="gs_a">T Yamaguchi, H Fukuda, R Furukawa&hellip; - Computer VisionâACCV  &hellip;, 2011 - Springer</div><div class="gs_rs">Video camera is now commonly used and demand of capturing a single frame from video <br>sequence is increasing. Since resolution of video camera is usually lower than digital <br>camera and video data usually contains a many motion blur in the sequence, simple <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Dgju_-oOCcgJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14414068484854908942&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'Dgju_-oOCcgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ri"><h3 class="gs_rt"><a href="http://search.ieice.org/bin/pdf_link.php?fname=j94-d_8_1376&amp;category=D&amp;lang=J&amp;year=2011&amp;abst=" class=yC5C>èªå·±ç¸ä¼¼æ§ã«åºã¥ãé«æéåè§£è½æ åã®çæ</a></h3><div class="gs_a">å³¶éç¾ä¿å­ï¼ å²¡é¨å­å¼ï¼ ä½è¤ãã¾ãï¼ ä½è¤æ´ä¸ - search.ieice.org</div><div class="gs_rs">ããã¾ã æ¬è«æã§ã¯, æ åã«ãããèªå·±ç¸ä¼¼æ§ãå©ç¨ãããã¨ã«ãã, ä¸ã¤ã®å¥åæ åãã, 1 <br>ãã¬ã¼ã ãæéæ¹åã«åè§£ãã¦é²åæéãç­ãã, ãã¬ã¼ã ã¬ã¼ããå¢å ããé«æéåè§£è½åææ³ã<br>ææ¡ãã. æ¬æ¥, åä¸ã®ä½æéåè§£è½ãã¬ã¼ã ãæ§æã§ããè¤æ°ã®é«æéåè§£è½ãã¬ã¼ã ã®<b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'YcewQgJu0BgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB63" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW63"><a href="http://www.eshukan.com/upfiles/jwz/20120407222527359.pdf" class=yC5E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eshukan.com</span><span class="gs_ggsS">eshukan.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.eshukan.com/upfiles/jwz/20120407222527359.pdf" class=yC5D>è®¡ç®æå½±ç»¼è¿°</a></h3><div class="gs_a">å¾æ å¥ï¼ æ¶ä¸¹ï¼ æå½è¾ï¼ å¼ å - è®¡ç®æºåºç¨ç ç©¶, 2010 - eshukan.com</div><div class="gs_rs">æè¦: éçè®¡ç®æº, ä¼ æå¨ç­åè¿ææ¯çåå±, ä¸ç§æ°çæå½±ææ¯âââè®¡ç®æå½±éæ¸å½¢æ. <br>è®¡ç®æå½±æ¯å©ç¨è®¡ç®æºè½¯ä»¶æ¹æ³ç»åç°ä»£ä¼ æå¨åç°ä»£åå­¦ç­ææ¯åé åºæ°åæå½±è®¾å¤ä»¥å<br>ç¸å³åºç¨çç»¼åææ¯, å®çªç ´äºä¼ ç»æ°ç æå½±ææ¯é¾ä»¥è§£å³çç§ç§é¾é¢, æä¸ºæå½±ææ¯çå<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:UTmscnOPmyoJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3070205296991025489&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'UTmscnOPmyoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md63', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md63" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:UTmscnOPmyoJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ri"><h3 class="gs_rt"><a href="http://d.wanfangdata.com.cn/periodical_xxwxjsjxt201209040.aspx" class=yC5F>ä¸ç§å¤åè¾¨çæ··åç¸æºç³»ç»æ å®æ¹æ³</a></h3><div class="gs_a">å¾æ å¥ï¼ å¼ åï¼ æ¶ä¸¹ï¼ æå½è¾ - å°åå¾®åè®¡ç®æºç³»ç», 2012 - ä¸æ¹æ°æ®èµæºç³»ç»</div><div class="gs_rs">æè¦: éå¯¹é«éä½åè¾¨çç¸æºåé«åè¾¨çç¸æºææçæ··åç¸æºç³»ç», æåºåºäºç©ºé´çº¦æçå¤åè¾¨<br>çç¸æºæ å®æ¹æ³. éç¨ç©ºé´è·ç¦»å°ºåº¦ä½ä¸ºç»ä¸ååæ ç³»ä¹é´çåº¦éåä½, å»ºç«åç¸æºåå¤åæ°<br>æ¨¡ååå¤ç¸æºç¸å¯¹å¤é¨åæ°æ¨¡å. éç¨ååç¸æºæ å®åé¨åæ°åå¤ç¸æºèåæ å®ç¸å¯¹å¤é¨åæ°<b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'YNFXhoqcLjQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB65" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW65"><a href="http://repository.dl.itc.u-tokyo.ac.jp/dspace/bitstream/2261/28052/1/matsui.pdf" class=yC61><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> <a href="http://repository.dl.itc.u-tokyo.ac.jp/dspace/handle/2261/28052" class=yC60>è¿èµ¤å¤ç»åãç¨ããä½ç§åº¦ã·ã¼ã³ã«ãããç»è³ªæ¹å</a></h3><div class="gs_a">æ¾äºå£®ä» - 2009 - repository.dl.itc.u-tokyo.ac.jp</div><div class="gs_rs">æ¦ è¦æ¬è«æã§ã¯, ä½ç§åº¦ã·ã¼ã³ãæ®å½±ããç»åã®ç»è³ªæ¹åãç®çã¨ã, è¿èµ¤å¤ãã©ãã·ã¥ãç§å°ãã¦<br>åæã«æ®å½±ãããã«ã©ã¼ç»åã¨è¿èµ¤å¤ç»åãç¨ãããã¨ã«ãã, ã«ã©ã¼ç»åä¸­ã«å«ã¾ãããã¤ãºã¨<br>ãã©ã¼ãå¹æçã«é¤å»ããææ³ãææ¡ãã. ä½ç§åº¦ç»åã®ãã¤ãºé¤å»ã«ã¤ãã¦, å¾æ¥ã§ã¯<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:K8PPoE7Gx5MJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'K8PPoE7Gx5MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/97390x/201208/42776906.html" class=yC62>è¾¹ç¼åºåçº¦æå¼å¯¼çè¿å¨æ¨¡ç³å¾åå¤å</a></h3><div class="gs_a">æ®µæ±æ°¸ï¼ å­é«å³°ï¼ åä¸æï¼ æ½æ¥æ´ª - è®¡ç®æºè¾å©è®¾è®¡ä¸å¾å½¢å­¦å­¦æ¥, 2012 - cqvip.com</div><div class="gs_rs">ç¸æºå¨ææç©ä½æ¶, ç©ä½è¿å¨æç¸æºæå¨ä¼å¯¼è´å¾åæ¨¡ç³. éå¯¹æ­¤é®é¢, æåºä¸ç§è¾¹ç¼åºåçº¦æ<br>å¼å¯¼çè¿å¨æ¨¡ç³å¾åå¤åç®æ³. éè¿åæå¾åä¸­é¶è·è¾¹ç¼åºåä¸è¿å¨æ¨¡ç³æ ¸ä¹é´çåå¨å³ç³», <br>å»ºç«äºä¸ç§æ°çè¾¹ç¼åºåçº¦æ; å¯¹è¯¥çº¦æè¿ä¸æ­¥ç»åRANSAC ç®æ³, å¯ææå°å¼å¯¼è¿å¨æ¨¡ç³æ ¸<b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'3fCc1L6KVncJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:333"><div class="gs_ri"><h3 class="gs_rt"><a href="http://japanlinkcenter.org/JST.JSTAGE/ieejeiss/130.1561?from=Google" class=yC63>é«ç²¾ç´°åç»æ®å½±ã®ããã®è¤åã»ã³ãµã«ã¡ã©</a></h3><div class="gs_a">é·åä¸ï¼ ç¥åè¯å¸ï¼ å²©äºåéï¼ è°·åç°æ­£å½¦ - é»æ°å­¦ä¼è«æèª C (é»å­ã»æå ±ã» &hellip;, 2010 - J-STAGE</div><div class="gs_rs">A resolution of camera has been drastically improved under a current request for high-<br>quality digital images. For example, digital still camera has several mega pixels. Although a <br>video camera has the higher frame-rate, the resolution of a video camera is lower than <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:4l6F7GiTj4cJ:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9768188195643940578&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'4l6F7GiTj4cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:332"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB68" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW68"><a href="http://www.ibe.kagoshima-u.ac.jp/~cgv/papers/2010/3-(2)-16-prmu_deblur_final2.pdf" class=yC65><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kagoshima-u.ac.jp</span><span class="gs_ggsS">kagoshima-u.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ibe.kagoshima-u.ac.jp/~cgv/papers/2010/3-(2)-16-prmu_deblur_final2.pdf" class=yC64>æã¶ããããªæ åããã®è¶è§£åå¦çææ³ã®ææ¡</a></h3><div class="gs_a">å±±å£æçï¼ ç¦ç°æ äººï¼ å¤å·äº®ï¼ å·å´æ´&hellip; - é»å­æå ±éä¿¡å­¦ä¼æè¡ &hellip;, 2010 - ibe.kagoshima-u.ac.jp</div><div class="gs_rs">Abstract Since resolution of video camera is usually lower than digital camera and video <br>data usually contains a motion blur in the sequence, simple frame capture can produce only <br>low quality image. In this paper, we propose a method to restore a sharp and high-<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:wIuZ_Vu5b78J:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13794447988406979520&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'wIuZ_Vu5b78J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md68', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md68" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:wIuZ_Vu5b78J:scholar.google.com/&amp;hl=en&amp;num=69&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
