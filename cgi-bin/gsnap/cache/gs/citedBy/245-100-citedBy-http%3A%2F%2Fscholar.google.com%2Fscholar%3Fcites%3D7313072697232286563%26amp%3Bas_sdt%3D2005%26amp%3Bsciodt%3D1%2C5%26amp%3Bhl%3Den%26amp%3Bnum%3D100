<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB100" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW100"><a href="http://www.ci.i.u-tokyo.ac.jp/~nakayama/pdf/nakayama_PhDen_1.0.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ci.i.u-tokyo.ac.jp/~nakayama/pdf/nakayama_PhDen_1.0.pdf" class=yC0>Linear Distance Metric Learning for Large-scale Generic Image Recognition</a></h3><div class="gs_a">H Nakayama - 2011 - ci.iu-tokyo.ac.jp</div><div class="gs_rs">Abstract Generic image recognition is a technique that enables computers to recognize <br>unconstrained real-world images and describe their content in a natural language. It is <br>known to be an extremely difficult problem due to the wide variety of targets and the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4422308382856783015&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:pyRpNwsyXz0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4422308382856783015&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'pyRpNwsyXz0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md100', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md100" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pyRpNwsyXz0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB101" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW101"><a href="http://www.comp.nus.edu.sg/~atung/publication/hashfile.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5767837" class=yC2>Hashfile: An efficient index structure for multimedia data</a></h3><div class="gs_a">D Zhang, <a href="/citations?user=XcaEg-cAAAAJ&amp;hl=en&amp;oi=sra">D Agrawal</a>, G Chen&hellip; - Data Engineering (ICDE &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Nearest neighbor (NN) search in high dimensional space is an essential query in <br>many multimedia retrieval applications. Due to the curse of dimensionality, existing index <br>structures might perform even worse than a simple sequential scan of data when <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15899888069438374610&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:0oJYO_W_p9wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15899888069438374610&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'0oJYO_W_p9wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB102" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW102"><a href="http://people.cs.clemson.edu/~jzwang/1301863/mm2012/p19-sang.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2393358" class=yC4>Right buddy makes the difference: An early exploration of social relation analysis in multimedia applications</a></h3><div class="gs_a"><a href="/citations?user=u6ivSjgAAAAJ&amp;hl=en&amp;oi=sra">J Sang</a>, C Xu - Proceedings of the 20th ACM international conference  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Social media is becoming popular these days, where user necessarily interacts with <br>each other to form social networks. Influence network, as one special case of social network, <br>has been recognized as significantly impacting social activities and user decisions. We <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12703279017938959809&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a onclick="return gs_ocit(event,'wb3sQvYcS7AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB103" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW103"><a href="http://ima.ac.uk/papers/Hao2012d.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ima.ac.uk</span><span class="gs_ggsS">ima.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ima.ac.uk/papers/Hao2012d.pdf" class=yC6>Fast Semantic Image Retrieval Based on Random Forest</a></h3><div class="gs_a">H Fu, <a href="/citations?user=pHkKtyMAAAAJ&amp;hl=en&amp;oi=sra">G Qiu</a> - ACM MM, 2012 - ima.ac.uk</div><div class="gs_rs">ABSTRACT This paper introduces random forest as a computational and data structure <br>paradigm for fusing low-level visual features and high-level semantic concepts for image <br>retrieval. We use visual features to split the tree nodes and use the image labels to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6324048158802542345&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:CX_jT0uIw1cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'CX_jT0uIw1cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md103', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md103" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:CX_jT0uIw1cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB104" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW104"><a href="http://160.40.50.4/iti/files/document/publications/Nikolopoulos_SocialMediaModeling_2011.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 160.40.50.4</span><span class="gs_ggsS">160.40.50.4 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/H677486M722341J1.pdf" class=yC8>Combining multi-modal features for social media analysis</a></h3><div class="gs_a">S Nikolopoulos, E Giannakidou, <a href="/citations?user=Nr7smP8AAAAJ&amp;hl=en&amp;oi=sra">I Kompatsiaris</a>&hellip; - Social Media Modeling  &hellip;, 2011 - Springer</div><div class="gs_rs">In this chapter we discuss methods for efficiently modeling the diverse information carried by <br>social media. The problem is viewed as a multi-modal analysis process where specialized <br>techniques are used to overcome the obstacles arising from the heterogeneity of data. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17357154929829405975&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:F-XmT3IA4fAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17357154929829405975&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'F-XmT3IA4fAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2071958" class=yCA>Transfer tagging from image to video</a></h3><div class="gs_a"><a href="/citations?user=PVv2xDYAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, <a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, Z Huang, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a> - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Nowadays massive amount of web video datum has been emerging on the Internet. <br>To achieve an effective and efficient video retrieval, it is critical to automatically assign <br>semantic keywords to the videos via content analysis. However, most of the existing video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=340631156113727724&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:7OCWUzsqugQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=340631156113727724&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'7OCWUzsqugQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/52307107G17H8518.pdf" class=yCB>On vocabulary size in bag-of-visual-words representation</a></h3><div class="gs_a"><a href="/citations?user=QwSTX6EAAAAJ&amp;hl=en&amp;oi=sra">J Hou</a>, J Kang, N Qi - Advances in Multimedia Information Processing-PCM &hellip;, 2010 - Springer</div><div class="gs_rs">Bag-of-visual-words is a popular image representation that produces high matching <br>accuracy and efficiency. While vocabulary size impacts on matching accuracy, existing <br>research usually selects the vocabulary size empirically. Research on representative local <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8778422733697627619&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:47VCWvc403kJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8778422733697627619&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'47VCWvc403kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB107" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW107"><a href="http://arxiv.org/pdf/1107.2859" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S016516841200148X" class=yCC>Label-specific training set construction from web resource for image annotation</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, C Zhao, TS Chua, R Jain - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract Recently many research efforts have been devoted to image annotation by <br>leveraging on the associated tags/keywords of web images as training labels. A key issue to <br>resolve is the relatively low accuracy of the tags. In this paper, we propose a novel semi-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15877522534659742940&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:3FynX59KWNwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15877522534659742940&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'3FynX59KWNwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB108" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW108"><a href="http://www.ami-lab.org/uploads/Publications/Journal/WP4/1_Hidden-concept%20Driven%20Multi-label%20Image%20Annotation%20and%20Label%20Ranking.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ami-lab.org</span><span class="gs_ggsS">ami-lab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6035980" class=yCE>Hidden-Concept Driven Multilabel Image Annotation and Label Ranking</a></h3><div class="gs_a"><a href="/citations?user=lDppvmoAAAAJ&amp;hl=en&amp;oi=sra">BK Bao</a>, T Li, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a> - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Conventional semisupervised image annotation algorithms usually propagate <br>labels predominantly via holistic similarities over image representations and do not fully <br>consider the label locality, inter-label similarity, and intra-label diversity among multilabel <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10764899804631215676&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:PC63YVabZJUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10764899804631215676&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'PC63YVabZJUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB109" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW109"><a href="http://www.ifp.illinois.edu/~cao4/papers/MINet_charu_chapter_2010.pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from illinois.edu</span><span class="gs_ggsS">illinois.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/v07440308727n270.pdf" class=yC10>Multimedia Information Networks in Social Media</a></h3><div class="gs_a"><a href="/citations?user=S-hBSfIAAAAJ&amp;hl=en&amp;oi=sra">L Cao</a>, <a href="/citations?user=Nut-uvoAAAAJ&amp;hl=en&amp;oi=sra">GJ Qi</a>, SF Tsai, MH Tsai, AD Pozo&hellip; - Social Network Data  &hellip;, 2011 - Springer</div><div class="gs_rs">The popularity of personal digital cameras and online photo/video sharing community has <br>lead to an explosion of multimedia information. Unlike traditional multimedia data, many new <br>multimedia datasets are organized in a structural way, incorporating rich information such <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2044083149590388455&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:516yZssKXhwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2044083149590388455&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'516yZssKXhwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB110" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW110"><a href="http://c2inet.sce.ntu.edu.sg/ivor/publication/HAIOKL.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6413856" class=yC12>Handling ambiguity via input-output kernel learning</a></h3><div class="gs_a">X Xu, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">IW Tsang</a>, D Xu - Data Mining (ICDM), 2012 IEEE 12th  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Data ambiguities exist in many data mining and machine learning applications such <br>as text categorization and image retrieval. For instance, it is generally beneficial to utilize the <br>ambiguous unlabeled documents to learn a more robust classifier for text categorization <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6693061042128013713&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a onclick="return gs_ocit(event,'kckLdI6H4lwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB111" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW111"><a href="http://newdesign.aclweb.org/anthology/C/C10/C10-2149.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aclweb.org</span><span class="gs_ggsS">aclweb.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1944715" class=yC14>Automatic generation of semantic fields for annotating web images</a></h3><div class="gs_a">G Wang, TS Chua, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, YC Wang - Proceedings of the 23rd  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract The overwhelming amounts of multimedia contents have triggered the need for <br>automatically detecting the semantic concepts within the media contents. With the <br>development of photo sharing websites such as Flickr, we are able to obtain millions of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16309016145247876043&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:yzOcdcZDVeIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16309016145247876043&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'yzOcdcZDVeIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB112" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW112"><a href="http://vireo.cs.cityu.edu.hk/papers/tmm12-zhushiai.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6166364" class=yC16>Sampling and Ontologically Pooling Web Images for Visual Concept Learning</a></h3><div class="gs_a">S Zhu, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a> - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Sufficient training examples are essential for effective learning of semantic visual <br>concepts. In practice, however, acquiring noise-free training examples has always been <br>expensive. Recently the rapid popularization of social media websites, such as Flickr, has <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12515950008825682404&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:5BmafjiWsa0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12515950008825682404&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'5BmafjiWsa0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB113" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW113"><a href="http://www.ee.columbia.edu/ln/dvmm/publications/12/ICMR_CompactHashing.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2324819" class=yC18>Compact hashing for mixed image-keyword query over multi-label images</a></h3><div class="gs_a"><a href="/citations?user=8VY7ZDcAAAAJ&amp;hl=en&amp;oi=sra">X Liu</a>, <a href="/citations?user=Fqqx4HsAAAAJ&amp;hl=en&amp;oi=sra">Y Mu</a>, B Lang, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Proceedings of the 2nd ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Recently locality-sensitive hashing (LSH) algorithms have attracted much attention <br>owing to its empirical success and theoretic guarantee in large-scale visual search. In this <br>paper we address the new topic of hashing with multi-label data, in which images in the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11767331050278839082&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:KmNLhj71TaMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11767331050278839082&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'KmNLhj71TaMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/L8P720LQHX33645R.pdf" class=yC1A>Multi-label image annotation by structural grouping sparsity</a></h3><div class="gs_a"><a href="/citations?user=t4283loAAAAJ&amp;hl=en&amp;oi=sra">Y Han</a>, F Wu, Y Zhuang - Social Media Modeling and Computing, 2011 - Springer</div><div class="gs_rs">We can obtain high-dimensional heterogeneous features from real-world images on photo-<br>sharing website, for an example Flickr. Those features are implemented to describe their <br>various aspects of visual characteristics, such as color, texture and shape etc. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4662306904707760298&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:qgD1iG3Xs0AJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4662306904707760298&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'qgD1iG3Xs0AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB115" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW115"><a href="http://www.ifp.illinois.edu/~qi4/papers/2011_CVPR_LSSVM-gjqi.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from illinois.edu</span><span class="gs_ggsS">illinois.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995378" class=yC1B>Locality-sensitive support vector machine by exploring local correlation and global regularization</a></h3><div class="gs_a"><a href="/citations?user=Nut-uvoAAAAJ&amp;hl=en&amp;oi=sra">GJ Qi</a>, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, T Huang - Computer Vision and Pattern  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Local classifiers have obtained great success in classification task due to its <br>powerful discriminating ability on local regions. However, most of them still have restricted <br>generalization in twofold:(1) each local classifier is sensitive to noise in local regions <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5470998965449023510&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:FmAejpLk7EsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5470998965449023510&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'FmAejpLk7EsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/A553175X25512365.pdf" class=yC1D>Stable multi-label boosting for image annotation with structural feature selection</a></h3><div class="gs_a">YT Zhuang, <a href="/citations?user=t4283loAAAAJ&amp;hl=en&amp;oi=sra">YH Han</a>, F Wu, <a href="/citations?user=r5jS8eYAAAAJ&amp;hl=en&amp;oi=sra">JC Yang</a> - Science China Information Sciences, 2011 - Springer</div><div class="gs_rs">Abstract Automatic annotating images with appropriate multiple tags are very important to <br>image retrieval and image understanding. We can obtain high-dimensional heterogenous <br>visual features from real-world images to describe their various aspects of visual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9440242766506050577&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:EUQLk8d6AoMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9440242766506050577&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'EUQLk8d6AoMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1878142" class=yC1E>Concept detector refinement using social videos</a></h3><div class="gs_a"><a href="/citations?user=-BFEdeMAAAAJ&amp;hl=en&amp;oi=sra">X Liu</a>, <a href="/citations?user=KdPSGmAAAAAJ&amp;hl=en&amp;oi=sra">B Huet</a> - Proceedings of the international workshop on Very- &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract The explosion of social video sharing sites gives new challenges on video search <br>and indexing techniques. Because of the concept diversity in social videos, it is very hard to <br>build a well annotated dataset that provides good coverage over the whole meaning of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18083707735018173218&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:IvO9l1889voJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18083707735018173218&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'IvO9l1889voJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB118" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW118"><a href="http://www.cse.ust.hk/~dyyeung/paper/pdf/yeung.kdd2012b.pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ust.hk</span><span class="gs_ggsS">ust.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cse.ust.hk/~dyyeung/paper/pdf/yeung.kdd2012b.pdf" class=yC1F>A probabilistic model for multimodal hash function learning</a></h3><div class="gs_a"><a href="/citations?user=j88o8jQAAAAJ&amp;hl=en&amp;oi=sra">Y Zhen</a>, <a href="/citations?user=nEsOOx8AAAAJ&amp;hl=en&amp;oi=sra">DY Yeung</a> - Proceedings of the ACM SIGKDD International  &hellip;, 2012 - cse.ust.hk</div><div class="gs_rs">ABSTRACT In recent years, both hashing-based similarity search and multimodal similarity <br>search have aroused much research interest in the data mining and other communities. <br>While hashing-based similarity search seeks to address the scalability issue, multimodal <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2932619473867265690&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:mlZAsd_BsigJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2932619473867265690&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'mlZAsd_BsigJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md118', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md118" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:mlZAsd_BsigJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6129508" class=yC21>Image Annotation by InputâOutput Structural Grouping Sparsity</a></h3><div class="gs_a"><a href="/citations?user=t4283loAAAAJ&amp;hl=en&amp;oi=sra">Y Han</a>, F Wu, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, Y Zhuang - Image Processing, IEEE  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic image annotation (AIA) is very important to image retrieval and image <br>understanding. Two key issues in AIA are explored in detail in this paper, ie, structured <br>visual feature selection and the implementation of hierarchical correlated structures <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14977813615922221440&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:gJENuSDi288J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14977813615922221440&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'gJENuSDi288J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2324850" class=yC22>Geo-based automatic image annotation</a></h3><div class="gs_a"><a href="/citations?user=lIZZTW0AAAAJ&amp;hl=en&amp;oi=sra">HM Sergieh</a>, G Gianini, M DÃ¶ller, <a href="/citations?user=WnWkNTsAAAAJ&amp;hl=en&amp;oi=sra">H Kosch</a>&hellip; - Proceedings of the 2nd  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract A huge number of user-tagged images are daily uploaded to the web. Recently, a <br>growing number of those images are also geotagged. These provide new opportunities for <br>solutions to automatically tag images so that efficient image management and retrieval <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9798581986867066980&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:ZLjA5WaO-4cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'ZLjA5WaO-4cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB121" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW121"><a href="http://ro.uow.edu.au/cgi/viewcontent.cgi?article=4222&amp;context=theses" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uow.edu.au</span><span class="gs_ggsS">uow.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ro.uow.edu.au/theses/3222/" class=yC23>Robust content-based image retrieval of multi-example queries</a></h3><div class="gs_a"><a href="/citations?user=QmsjV8QAAAAJ&amp;hl=en&amp;oi=sra">J Zhang</a> - 2011 - ro.uow.edu.au</div><div class="gs_rs">Abstract This thesis investigates three major issues in the active field of content-based <br>image retrieval (CBIR), which are feature aggregation for similarity measure, robust <br>contentbased image retrieval and retrieval model by incorporating background knowledge.</div><div class="gs_fl"><a href="/scholar?cites=17615136502393770643&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:k5Iy51GJdfQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'k5Iy51GJdfQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB122" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW122"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ZhaZJ_J03_Semantic-Gap-Oriented%20Active%20Learning%20for%20Multilable%20Image%20Annotation.pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6111295" class=yC25>Semantic-Gap-Oriented Active Learning for Multilabel Image Annotation</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, ZJ Zha, D Tao, TS Chua - Image Processing, IEEE  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract User interaction is an effective way to handle the semantic gap problem in image <br>annotation. To minimize user effort in the interactions, many active learning methods were <br>proposed. These methods treat the semantic concepts individually or correlatively. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12951969610158265254&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:plMj67yjvrMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12951969610158265254&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'plMj67yjvrMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S002002551200223X" class=yC27>Visual query processing for efficient image retrieval using a SOM-based filter-refinement scheme</a></h3><div class="gs_a">Z Yu, HS Wong, J You, G Han - Information Sciences, 2012 - Elsevier</div><div class="gs_rs">Visual query processing is one of the new issues for content-based image retrieval. In this <br>paper, we propose (i) a filter-refinement scheme based on a modified form of the self-<br>organizing map and (ii) a new interactive approach for similarity matching in image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5694670573192370115&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:w1cA9LaIB08J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5694670573192370115&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'w1cA9LaIB08J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB124" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW124"><a href="http://vireo.cs.cityu.edu.hk/papers/hktan_icmr11.pdf" class=yC29><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://vireo.cs.cityu.edu.hk/papers/hktan_icmr11.pdf" class=yC28>Fusing heterogeneous modalities for video and image re-ranking</a></h3><div class="gs_a">H Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">C Ngo</a> - Proc. ICMR, 2011 - vireo.cs.cityu.edu.hk</div><div class="gs_rs">ABSTRACT Multimedia documents in popular image and video sharing websites such as <br>Flickr and Youtube are heterogeneous documents with diverse ways of representations and <br>rich user-supplied information. In this paper, we investigate how the agreement among <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10174193331251927437&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:jVlr-e3-MY0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10174193331251927437&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'jVlr-e3-MY0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md124', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md124" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jVlr-e3-MY0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB125" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW125"><a href="http://c2inet.sce.ntu.edu.sg/ivor/publication/HLapSc.pdf" class=yC2B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6165311" class=yC2A>Laplacian Sparse Coding, Hypergraph Laplacian Sparse Coding, and Applications</a></h3><div class="gs_a"><a href="/citations?user=fe-1v0MAAAAJ&amp;hl=en&amp;oi=sra">S Gao</a>, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">I Tsang</a>, <a href="/citations?user=Eeolw80AAAAJ&amp;hl=en&amp;oi=sra">L Chia</a> - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Sparse coding exhibits good performance in many computer vision applications. <br>However, due to the overcomplete codebook and the independent coding process, the <br>locality and the similarity among the instances to be encoded are lost. To preserve such <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16333137899786300&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:PJCVAucGOgAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16333137899786300&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'PJCVAucGOgAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168412002484" class=yC2C>Asymmetric propagation based batch mode active learning for image retrieval</a></h3><div class="gs_a">B Niu, <a href="/citations?user=o8PT69EAAAAJ&amp;hl=en&amp;oi=sra">J Cheng</a>, X Bai, H Lu - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract Relevance feedback is an effective approach to improve the performance of image <br>retrieval by leveraging the labeling of human. In order to alleviate the burden of labeling, <br>active learning method has been introduced to select the most informative samples for <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:8XPkAkyV9HEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'8XPkAkyV9HEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB127" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW127"><a href="http://c2inet.sce.ntu.edu.sg/ivor/publication/TMM2012.pdf" class=yC2E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6151164" class=yC2D>Tag-Based Image Retrieval Improved by Augmented Features and Group-Based Refinement</a></h3><div class="gs_a"><a href="/citations?user=5lBah4AAAAAJ&amp;hl=en&amp;oi=sra">L Chen</a>, D Xu, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">IW Tsang</a>, <a href="/citations?user=CcbnBvgAAAAJ&amp;hl=en&amp;oi=sra">J Luo</a> - Multimedia, IEEE Transactions  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose a new tag-based image retrieval framework to improve <br>the retrieval performance of a group of related personal images captured by the same user <br>within a short period of an event by leveraging millions of training web images and their <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2250062933330550862&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:TuBwB1HUOR8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2250062933330550862&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'TuBwB1HUOR8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2382380" class=yC2F>Tag ranking by propagating relevance over tag and image graphs</a></h3><div class="gs_a">M Li, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, H Li, C Zhao - &hellip;  of the 4th International Conference on  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we explore the problem of tag ranking by propagating relevance over <br>community-contributed images and their associated tags. To rank the tags more accurately, <br>we propose a novel tag ranking scheme through a two-stage graph-based relevance <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'edO4DW-1rSsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB129" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW129"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202012/pdfs/0002373.pdf" class=yC31><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6288392" class=yC30>A cross-modal approach for extracting semantic relationships of concepts from an image database</a></h3><div class="gs_a">M Katsurai, T Ogawa&hellip; - Acoustics, Speech and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a cross-modal approach for extracting semantic relationships <br>of concepts from an image database. First, canonical correlation analysis (CCA) is used to <br>capture the cross-modal correlations between visual features and tag features in the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Ivh2GA_fgpwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11277821672817096738&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'Ivh2GA_fgpwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S092523121200149X" class=yC32>Query difficulty estimation for image retrieval</a></h3><div class="gs_a">Y Li, <a href="/citations?user=gsTrHoMAAAAJ&amp;hl=en&amp;oi=sra">B Geng</a>, <a href="/citations?user=cvgKxDQAAAAJ&amp;hl=en&amp;oi=sra">L Yang</a>, C Xu, W Bian - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Query difficulty estimation predicts the performance of the search result of the given query. It <br>is a powerful tool for multimedia retrieval and receives increasing attention. It can guide the <br>pseudo relevance feedback to rerank the image search results and re-write the query by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16754922410140218058&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:yrKjHylxhegJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16754922410140218058&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'yrKjHylxhegJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB131" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW131"><a href="http://c2inet.sce.ntu.edu.sg/ivor/publication/MLLperf.pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6378455" class=yC33>Objective-guided Image Annotation</a></h3><div class="gs_a">Q Mao, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">IWH Tsang</a>, <a href="/citations?user=fe-1v0MAAAAJ&amp;hl=en&amp;oi=sra">S Gao</a> - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic image annotation, which is usually formulated as a multi-label <br>classification problem, is one of major tools to enhance the semantic understanding of web <br>images. Many multimedia applications (eg, tag-based image retrieval) can greatly benefit <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'b5dAJc4Grw0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://japanlinkcenter.org/DN/JST.JSTAGE/ipsjtrans/5.214?from=Google" class=yC35>Improving Content-based Social Image Retrieval Based on an Image-tag Relationship Model</a></h3><div class="gs_a">J Li, Q Ma, Y Asano, M Yoshikawa - IPSJ Online Transactions, 2012 - japanlinkcenter.org</div><div class="gs_rs">With the recent rapid growth of social image hosting websites, such as Flickr, it is easier to <br>construct a large database with social tagged images. We propose an unsupervised <br>approach for automatic ranking social images to improve content-based social image <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'yhiOJTpW_QEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/M88M1H7226712281.pdf" class=yC36>Auto-grouped sparse representation for visual analysis</a></h3><div class="gs_a"><a href="/citations?user=Q8iay0gAAAAJ&amp;hl=en&amp;oi=sra">J Feng</a>, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">X Yuan</a>, Z Wang, H Xu, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a> - Computer VisionâECCV 2012, 2012 - Springer</div><div class="gs_rs">In this work, we investigate how to automatically uncover the underlying group structure of a <br>feature vector such that each group characterizes certain object-specific patterns, eg, visual <br>pattern or motion trajectories from one object. By mining the group structure, we can <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'--dCJw4JxU8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB134" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW134"><a href="http://www.cse.msu.edu/~liuxm/publication/Feng_Xiao_Zhuang_Liu_ACCV2012_aumfs.pdf" class=yC38><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from msu.edu</span><span class="gs_ggsS">msu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cse.msu.edu/~liuxm/publication/Feng_Xiao_Zhuang_Liu_ACCV2012_aumfs.pdf" class=yC37>Adaptive Unsupervised Multi-View Feature Selection for Visual Concept Recognition</a></h3><div class="gs_a">Y Feng, J Xiao, Y Zhuang, <a href="/citations?user=Bii0w1oAAAAJ&amp;hl=en&amp;oi=sra">X Liu</a> - cse.msu.edu</div><div class="gs_rs">Abstract. To reveal and leverage the correlated and complemental information between <br>different views, a great amount of multi-view learning algorithms have been proposed in <br>recent years. However, unsupervised feature selection in multiview learning is still a <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'yPbhKIGB4owJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md134', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md134" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:yPbhKIGB4owJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6135507" class=yC39>Event Driven Web Video Summarization by Tag Localization and Key-Shot Identification</a></h3><div class="gs_a"><a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, G Li, ZJ Zha, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - &hellip; , IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the explosive growth of web videos on the Internet, it becomes challenging to <br>efficiently browse hundreds or even thousands of videos. When searching an event query, <br>users are often bewildered by the vast quantity of web videos returned by search engines. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17159319909387631227&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:e3pWKosmIu4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'e3pWKosmIu4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB136" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW136"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ZhaZJ_C02_Locally%20Regressive%20G-Optimal%20Design%20for%20Image%20Retrieval.pdf" class=yC3B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1992055" class=yC3A>Locally regressive G-optimal design for image retrieval</a></h3><div class="gs_a">ZJ Zha, YT Zheng, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, F Chang&hellip; - Proceedings of the 1st  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Content Based Image Retrieval (CBIR) has attracted increasing attention from both <br>academia and industry. Relevance Feedback is one of the most effective techniques to <br>bridge the semantic gap in CBIR. One of the key research problems related to relevance <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MdyaLWAb31kJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6475924889077996593&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MdyaLWAb31kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6408281" class=yC3C>Bootstrapping Visual Categorization with Relevant Negatives</a></h3><div class="gs_a">X Li, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">C Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>, <a href="/citations?user=63P8arUAAAAJ&amp;hl=en&amp;oi=sra">D Koelma</a>, <a href="/citations?user=03svEqcAAAAJ&amp;hl=en&amp;oi=sra">A Smeulders</a> - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Learning classifiers for many visual concepts is important for image categorization <br>and retrieval. As a classifier tends to misclassify negative examples which are visually <br>similar to positive ones, inclusion of such misclassified and thus relevant negatives should <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'OO1LLgkmnhAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/l204120636264836.pdf" class=yC3D>Content-based image retrieval by integrating color and texture features</a></h3><div class="gs_a">XY Wang, BB Zhang, HY Yang - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract Content-based image retrieval (CBIR) has been an active research topic in the last <br>decade. Feature extraction and representation is one of the most important issues in the <br>CBIR. In this paper, we propose a content-based image retrieval method based on an <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gVdnN5_Q_V8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'gVdnN5_Q_V8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB139" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW139"><a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.b4b91eee52c04039.4368616e6472696b613130547269706172746974652e706466.pdf" class=yC3F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iiit.ac.in</span><span class="gs_ggsS">iiit.ac.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.b4b91eee52c04039.4368616e6472696b613130547269706172746974652e706466.pdf" class=yC3E>Tripartite Graph Models for Multi Modal Image Retrieval</a></h3><div class="gs_a">C Pulla, <a href="/citations?user=U9dH-DoAAAAJ&amp;hl=en&amp;oi=sra">CV Jawahar</a> - Proceedings of the British Machine Vision  &hellip; - web2py.iiit.ac.in</div><div class="gs_rs">Abstract Most of the traditional image retrieval methods use either low level visual features <br>or embedded text for representation and indexing. In recent years, there has been significant <br>interest in combining these two different modalities for effective retrieval. In this paper, we <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1xwCOMxVpUwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5522914853641133271&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'1xwCOMxVpUwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md139', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md139" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:1xwCOMxVpUwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB140" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW140"><a href="http://users.ece.gatech.edu/~jkim/ipdps2011-diao.pdf" class=yC41><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gatech.edu</span><span class="gs_ggsS">gatech.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6012809" class=yC40>Large-Scale Semantic Concept Detection on Manycore Platforms for Multimedia Mining</a></h3><div class="gs_a">M Diao, <a href="/citations?user=4m8b4aEAAAAJ&amp;hl=en&amp;oi=sra">C Nicopoulos</a>, J Kim - Parallel &amp; Distributed Processing &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Media mining, the extraction of meaningful knowledge from multimedia content has <br>become a major application and poses significant computational challenges in today&#39;s <br>platforms. Media mining applications contain many sophisticated algorithms that include <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:WnPdOIjM7BMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1435747266666001242&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'WnPdOIjM7BMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB141" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW141"><a href="https://qmro.qmul.ac.uk/jspui/bitstream/123456789/3148/1/NIKOLOPOULOSSemanticMultimedia2012.pdf" class=yC43><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from qmul.ac.uk</span><span class="gs_ggsS">qmul.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://qmro.qmul.ac.uk/jspui/handle/123456789/3148" class=yC42>Semantic multimedia analysis using knowledge and context</a></h3><div class="gs_a">S Nikolopoulos - 2012 - qmro.qmul.ac.uk</div><div class="gs_rs">The difficulty of semantic multimedia analysis can be attributed to the extended diversity in <br>form and appearance exhibited by the majority of semantic concepts and the difficulty to <br>express them using a finite number of patterns. In meeting this challenge there has been a <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'l4ZFOpGvpecJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><a href="http://inderscience.metapress.com/index/F681174TL7473015.pdf" class=yC44>Cross-media retrieval: state-of-the-art and open issues</a></h3><div class="gs_a">J Liu, C Xu, H Lu - International Journal of Multimedia Intelligence and  &hellip;, 2010 - Inderscience</div><div class="gs_rs">Cross-media retrieval is able to provide retrieval results with similar semantics but different <br>media to the query. Since cross-media retrieval complements currently popular text/content-<br>based retrieval which provides retrieval results with the same media to the query, it <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3ha4PHVMgPsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'3ha4PHVMgPsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB143" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW143"><a href="http://www.jdl.ac.cn/doc/2011/2011122914252062927_xianming_acmmm11pdf.pdf" class=yC46><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jdl.ac.cn</span><span class="gs_ggsS">jdl.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072355" class=yC45>Learning heterogeneous data for hierarchical web video classification</a></h3><div class="gs_a"><a href="/citations?user=697UEEIAAAAJ&amp;hl=en&amp;oi=sra">X Liu</a>, H Yao, <a href="/citations?user=lRSD7PQAAAAJ&amp;hl=en&amp;oi=sra">R Ji</a>, P Xu, <a href="/citations?user=KPMK3B4AAAAJ&amp;hl=en&amp;oi=sra">X Sun</a>, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Web videos such as YouTube are hard to obtain sufficient precisely labeled training <br>data and analyze due to the complex ontology. To deal with these problems, we present a <br>hierarchical web video classification framework by learning heterogeneous web data, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jezPPdA4NfwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18173494338364894349&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'jezPPdA4NfwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212001609" class=yC47>Collaborative visual modeling for automatic image annotation via sparse model coding</a></h3><div class="gs_a">M Wang, F Li, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a> - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Building visual models provides an important way to detect visual concepts from images. <br>However, due to the problems of visual diversity and uncertainty, the estimation based on <br>these models is not satisfactory. The visual relatedness among visual models is ignored <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:64sTPjJAVpEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10472628568030677995&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'64sTPjJAVpEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6193443" class=yC48>Quantitative Characterization of Semantic Gaps for Learning Complexity Estimation and Inference Model Selection</a></h3><div class="gs_a">J Fan, <a href="/citations?user=QLLFowsAAAAJ&amp;hl=en&amp;oi=sra">X He</a>, R Jain, N Zhou, J Peng - 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract n this paper, a novel data-driven algorithm is developed for achieving quantitative <br>characterization of the semantic gaps directly in the visual feature space, where the visual <br>feature space is the com-mon space for concept classifier training and automatic concept <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:bHs2QYrmAYYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'bHs2QYrmAYYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6156448" class=yC49>User-Aware Image Tag Refinement via Ternary Semantic Analysis</a></h3><div class="gs_a"><a href="/citations?user=u6ivSjgAAAAJ&amp;hl=en&amp;oi=sra">J Sang</a>, C Xu, J Liu - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Large-scale user contributed images with tags are easily available on photo <br>sharing websites. However, the noisy or incomplete correspondence between the images <br>and tags prohibits them from being leveraged for precise image retrieval and effective <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=647506676532295279&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:b7ZmRORn_AgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=647506676532295279&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'b7ZmRORn_AgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2390880" class=yC4A>Towards data-driven estimation of image tag relevance using visually similar and dissimilar folksonomy images</a></h3><div class="gs_a">S Lee, <a href="/citations?user=VpjWb7wAAAAJ&amp;hl=en&amp;oi=sra">W De Neve</a>, YM Ro - &hellip;  of the 2012 international workshop on  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Given that the presence of non-relevant tags in an image folksonomy hampers the <br>effective organization and retrieval of images, this paper discusses a novel technique for <br>estimating the relevance of user-supplied tags with respect to the content of a seed image. <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'TS--RUR6UGMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB148" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW148"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ICCV11.pdf" class=yC4C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6126323" class=yC4B>Multi-label visual classification with label exclusive context</a></h3><div class="gs_a">X Chen, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">XT Yuan</a>, Q Chen, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - Computer Vision (ICCV &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We introduce in this paper a novel approach to multi-label image classification <br>which incorporates a new type of context-label exclusive context-with linear representation <br>and classification. Given a set of exclusive label groups that describe the negative <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2422096281759318535&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:BwIPRrwDnSEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2422096281759318535&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'BwIPRrwDnSEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB149" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW149"><a href="http://www.yugangjiang.info/publication/mm11_pooling.pdf" class=yC4E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from yugangjiang.info</span><span class="gs_ggsS">yugangjiang.info <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2071934" class=yC4D>On the pooling of positive examples with ontology for visual concept learning</a></h3><div class="gs_a">S Zhu, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a> - Proceedings of the 19th ACM international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract A common obstacle in effective learning of visual concept classifiers is the scarcity <br>of positive training examples due to expensive labeling cost. This paper explores the <br>sampling of weakly tagged web images for concept learning without human assistance. In <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2075649445082088822&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:dsVaTCwwzhwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2075649445082088822&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'dsVaTCwwzhwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB150" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW150"><a href="http://arxiv.org/pdf/1212.4522" class=yC50><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://arxiv.org/abs/1212.4522" class=yC4F>A Multi-View Embedding Space for Modeling Internet Images, Tags, and their Semantics</a></h3><div class="gs_a">Y Gong, Q Ke, M Isard, <a href="/citations?user=xKOEaRoAAAAJ&amp;hl=en&amp;oi=sra">S Lazebnik</a> - arXiv preprint arXiv:1212.4522, 2012 - arxiv.org</div><div class="gs_rs">Abstract: This paper investigates the problem of modeling Internet images and associated <br>text or tags for tasks such as image-to-image search, tag-to-image search, and image-to-tag <br>search (image annotation). We start with canonical correlation analysis (CCA), a popular <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'eYSwTPZ-yhQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231213000192" class=yC51>Jing Liu, Yifan Zhang, Zechao Li, Hanqing Lu</a></h3><div class="gs_a">J Liu, Y Zhang, Z Li, H Lu - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract With the permeation of Web 2.0, large-scale user contributed images with tags are <br>easily available on social websites. However, the noisy or incomplete correspondence <br>between images and tags prohibit us from precise image retrieval and effective <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'mhd7TfI-GV8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S092523121200906X" class=yC52>Hybrid Image Summarization by Hypergraph Partition</a></h3><div class="gs_a">M Li, C Zhao, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a> - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract The objectiveof hybrid image summarization is selecting a few visual exemplars <br>and semantic exemplars of a large-scale image collection and organizing them to represent <br>the collection. In this paper, we present a framework for hybrid image summarization in <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'ldtgTpHIVcYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396328" class=yC53>Image tag re-ranking by coupled probability transition</a></h3><div class="gs_a">J Xiao, W Zhou, X Li, M Wang, Q Tian - Proceedings of the 20th ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract The large amount of user-tagged images on social networks is helpful to facilitate <br>image management and image search. However, many tags are weakly relevant or <br>irrelevant to the visual content, resulting in unsatisfactory performance in tag related <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'zbfQT4zNS74J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB154" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW154"><a href="http://www.asci.tudelft.nl/media/proceedings_asci_conference_2010/asci2010_submission_4.pdf" class=yC55><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tudelft.nl</span><span class="gs_ggsS">tudelft.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.asci.tudelft.nl/media/proceedings_asci_conference_2010/asci2010_submission_4.pdf" class=yC54>Combining Multi-Feature Tag Relevance Learning for Social Image Retrieval</a></h3><div class="gs_a">X Li, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - asci.tudelft.nl</div><div class="gs_rs">ABSTRACT Interpreting the relevance of a user-contributed tag with respect to the visual <br>content of an image is an emerging problem in social image retrieval. In the literature this <br>problem is tackled by analyzing the correlation between tags and images represented by <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:LQqyUH4q6f4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'LQqyUH4q6f4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md154', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md154" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:LQqyUH4q6f4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB155" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW155"><a href="http://libque.cityu.edu.hk/bitstream/2031/6211/1/abstract.html" class=yC57><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://libque.cityu.edu.hk/handle/2031/6211" class=yC56>Video hyperlinking for multimedia search</a></h3><div class="gs_a">HK Tan - 2010 - libque.cityu.edu.hk</div><div class="gs_rs">ï»¿ With the spread of Web 2.0, web videos have become prevalent online. There is a growing <br>need for effective modeling and organization of video data to facilitate browsing or retrieval. <br>Meanwhile, the modeling of web pages through hyperlink graph has seen tremendous <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gk0oVGtrZrQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12999195483169115522&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'gk0oVGtrZrQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md155', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md155" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:gk0oVGtrZrQJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=16418393280767177700&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/P683840235086106.pdf" class=yC58>Improving image tags by exploiting web search results</a></h3><div class="gs_a">X Zhang, Z Li, W Chao - Multimedia Tools and Applications, 2011 - Springer</div><div class="gs_rs">Abstract Automatic image tagging automatically assigns image with semantic keywords <br>called tags, which significantly facilitates image search and organization. Most of present <br>image tagging approaches are constrained by the training model learned from the training <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:bJ_eVAvtPIwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'bJ_eVAvtPIwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB157" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW157"><a href="http://www.ieeeprojects.yavum.com/ieee2012basepaper/Learn%20to%20Personalized%20Image%20Search%20from%20the%20Photo%20Sharing%20Websites.pdf" class=yC5A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from yavum.com</span><span class="gs_ggsS">yavum.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6111487" class=yC59>Learn to Personalized Image Search From the Photo Sharing Websites</a></h3><div class="gs_a"><a href="/citations?user=u6ivSjgAAAAJ&amp;hl=en&amp;oi=sra">J Sang</a>, C Xu, D Lu - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Increasingly developed social sharing websites like Flickr and Youtube allow users <br>to create, share, annotate, and comment medias. The large-scale user-generated metadata <br>not only facilitate users in sharing and organizing multimedia content, but provide useful <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14042515156176223035&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:OzvsVCIJ4cIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14042515156176223035&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'OzvsVCIJ4cIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/E188714864865608.pdf" class=yC5B>Ranking content-based social images search results with social tags</a></h3><div class="gs_a">J Li, Q Ma, Y Asano, M Yoshikawa - Information Retrieval Technology, 2011 - Springer</div><div class="gs_rs">With the recent rapid growth of social image hosting websites, such as Flickr, it is easier to <br>construct a large database with tagged images. Social tags have been proven to be effective <br>for providing keyword-based image retrieval and widely used on these websites, but <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:RfP0VB2YzYYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9713587223054840645&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'RfP0VB2YzYYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB159" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW159"><a href="http://people.cs.clemson.edu/~jzwang/1301863/mm2012/p509-wu.pdf" class=yC5D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2393419" class=yC5C>Annotating web images using NOVA: NOn-conVex group spArsity</a></h3><div class="gs_a">F Wu, Y Yuan, <a href="/citations?user=uOJH_AEAAAAJ&amp;hl=en&amp;oi=sra">Y Rui</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, Y Zhuang - Proceedings of the 20th ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract As image feature vector is large, selecting the right features plays a fundamental <br>role in Web image annotation. Most existing approaches are either based on individual <br>feature selection, which leads to local optima, or using a convex penalty, which leads to <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'C6cdV2hTPTAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB160" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW160"><a href="http://www.riunet.upv.es/bitstream/handle/10251/17923/tfm_guigarfr.pdf?sequence=1" class=yC5F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from upv.es</span><span class="gs_ggsS">upv.es <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.riunet.upv.es/handle/10251/17923" class=yC5E>Similarity-Preserving Binary Hashing for Image Retrieval in large databases</a></h3><div class="gs_a">G GARCÃA FRANCO - 2012 - riunet.upv.es</div><div class="gs_rs">[ES] Las tÃ©cnicas de hashing se han vuelto muy populares a la hora de resolver problemas <br>de recuperaciÃ³n de imÃ¡genes basada en contenido en grandes bases de datos porque <br>permiten representar los vectores de caracterÃ­sticas utilizando cÃ³digos binarios <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'_iFNWwLLXC4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2324811" class=yC60>Watching and talking: media content as social nexus</a></h3><div class="gs_a">DA Shamma, L Kennedy, <a href="/citations?user=wZB07xAAAAAJ&amp;hl=en&amp;oi=sra">EF Churchill</a> - Proceedings of the 2nd ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract New multimedia applications, such as community-created video repositories and <br>tools for synchronous sharing, have revolutionized the ways that media is watched and <br>shared. Effective instrumentation of these applications can enable researchers and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:o-7iX4OrlyUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'o-7iX4OrlyUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB162" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW162"><a href="http://www.willfulwreckords.com/GinsuScience/CVPR2012/data/papers/169_P2A-19.pdf" class=yC62><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from willfulwreckords.com</span><span class="gs_ggsS">willfulwreckords.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6247819" class=yC61>Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach</a></h3><div class="gs_a"><a href="/citations?user=inRIcS0AAAAJ&amp;hl=en&amp;oi=sra">L Duan</a>, D Xu, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Computer Vision and Pattern  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Recent work has demonstrated the effectiveness of domain adaptation methods for <br>computer vision applications. In this work, we propose a new multiple source domain <br>adaptation method called Domain Selection Machine (DSM) for event recognition in <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:OutiZOca180J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14832353478891989818&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'OutiZOca180J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0045790612001838" class=yC63>Real web community based automatic image annotation</a></h3><div class="gs_a">X Ke, S Li, G Chen - Computers &amp; Electrical Engineering, 2012 - Elsevier</div><div class="gs_rs">Abstract Automatic image annotation is a challenging problem in pattern recognition. Large <br>annotation databases are difficult to build, and the existing models can only label small <br>scale of image sets. In order to solve the problems with the annotation of large databases, <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'MYQvZ7nl5tcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212001555" class=yC64>Constructing visual tag dictionary by mining community-contributed media corpus</a></h3><div class="gs_a"><a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=g2gAY_0AAAAJ&amp;hl=en&amp;oi=sra">K Yang</a> - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Visual-word based image representation has shown effectiveness in a wide variety of <br>applications such as categorization, annotation and search. By detecting keypoints in <br>images and treating their patterns as visual words, an image can be represented as a bag <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:6wChc0Kj9PgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17939142721526628587&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'6wChc0Kj9PgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6182401" class=yC65>Image tag refinement using tag semantic and visual similarity</a></h3><div class="gs_a">W Cheng, X Wang - Computer Science and Network  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Social tagging on online websites provides users interfaces of describing resources <br>with their own tags, and vast user-provided image tags facilitate image retrieval and <br>management. However, these tags are often not related to the actual image content, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1aMPdDijHyUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'1aMPdDijHyUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB166" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW166"><a href="http://people.cs.clemson.edu/~jzwang/1301863/mm2012/p479-qi.pdf" class=yC67><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2393347.2393416" class=yC66>Multi-view learning from imperfect tagging</a></h3><div class="gs_a">Z Qi, M Yang, ZM Zhang, Z Zhang - Proceedings of the 20th ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In many real-world applications, tagging is imperfect: incomplete, inconsistent, and <br>error-prone. Solutions to this problem will generate societal and technical impacts. In this <br>paper, we investigate this arguably new problem: learning from imperfect tagging. We <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'RwEYdGGu9KUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:333"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2339617" class=yC68>Inductive multi-task learning with multiple view data</a></h3><div class="gs_a">J Zhang, J Huan - Proceedings of the 18th ACM SIGKDD international  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In many real-world applications, it is becoming common to have data extracted from <br>multiple diverse sources, known as&quot; multi-view&quot; data. Multi-view learning (MVL) has been <br>widely studied in many applications, but existing MVL methods learn a single task <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3uvNdqFIHdUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'3uvNdqFIHdUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:332"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168412002204" class=yC69>Multimedia encyclopedia construction by mining web knowledge</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, ZJ Zha, Y Gao, TS Chua, X Wu - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract In recent years, we have witnessed the blooming of Web 2.0 content such as <br>Wikipedia, Flickr and YouTube, etc. How might we benefit from such rich media resources <br>available on the internet? This paper presents a novel concept called Mediapedia, a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:-PXgd9tRpOoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'-PXgd9tRpOoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:331"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB169" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW169"><a href="http://www.cais.ntu.edu.sg/~axsun/paper/sun_icmr12.pdf" class=yC6B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2324808" class=yC6A>Content is still king: the effect of neighbor voting schemes on tag relevance for social image retrieval</a></h3><div class="gs_a"><a href="/citations?user=mW33ltsAAAAJ&amp;hl=en&amp;oi=sra">BQ Truong</a>, <a href="/citations?user=wyKGVKUAAAAJ&amp;hl=en&amp;oi=sra">A Sun</a>, <a href="/citations?user=o0F3sqEAAAAJ&amp;hl=en&amp;oi=sra">SS Bhowmick</a> - Proceedings of the 2nd ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Tags associated with social images are valuable information source for superior tag-<br>based image retrieval (TagIR) experiences. One of the key issues in TagIR is to learn the <br>effectiveness of a tag in describing the visual content of its annotated image, also known <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14775811415916518753&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:YZ3deTI6Ds0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14775811415916518753&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'YZ3deTI6Ds0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:330"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2333120" class=yC6C>Assistive tagging: A survey of multimedia tagging with human-computer joint exploration</a></h3><div class="gs_a"><a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=V9W87PYAAAAJ&amp;hl=en&amp;oi=sra">B Ni</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, TS Chua - ACM Computing Surveys (CSUR), 2012 - dl.acm.org</div><div class="gs_rs">Abstract Along with the explosive growth of multimedia data, automatic multimedia tagging <br>has attracted great interest of various research communities, such as computer vision, <br>multimedia, and information retrieval. However, despite the great progress achieved in the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9840991510067465628&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:nM00e6M5kogJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'nM00e6M5kogJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:329"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168412001661" class=yC6D>Social Image Tagging Using Graph-based Reinforcement on Multi-type Interrelated Objects</a></h3><div class="gs_a">X Zhang, X Zhao, Z Li, J Xia, R Jain, W Chao - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract Social image tagging is becoming increasingly popular with the development of <br>social website, where images are annotated with arbitrary keywords called tags. Most of <br>present image tagging approaches are mainly based on the visual similarity or mapping <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10188512964859572982&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:9rLqe4_eZI0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'9rLqe4_eZI0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:328"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6137246" class=yC6E>Tag Clustering and Refinement on Semantic Unity Graph</a></h3><div class="gs_a"><a href="/citations?user=yQks8w4AAAAJ&amp;hl=en&amp;oi=sra">Y Liu</a>, F Wu, <a href="/citations?user=vCoh6tYAAAAJ&amp;hl=en&amp;oi=sra">Y Zhang</a>, <a href="/citations?user=VUN-9cQAAAAJ&amp;hl=en&amp;oi=sra">J Shao</a>&hellip; - Data Mining (ICDM),  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Recently, there has been extensive research towards the user-provided tags on <br>photo sharing websites which can greatly facilitate image retrieval and management. <br>However, due to the arbitrariness of the tagging activities, these tags are often imprecise <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12112815277408788979&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:8yHgf1FdGagJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12112815277408788979&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'8yHgf1FdGagJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:327"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1937750" class=yC6F>Annotate Wikipedia with Flickr images: concepts and case study</a></h3><div class="gs_a">J Xiao, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - Proceedings of the Second International Conference  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Wikipedia, as an open editable resource, provides reliable knowledge and <br>taxonomy. Contrast to the rich literal information, Wikipedia is lack of visual illustrations, like <br>images and animations. Can we visually annotate Wikipedia concept and provide <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:SoIPgUyuSfkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17963080232349958730&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'SoIPgUyuSfkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:326"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB174" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW174"><a href="http://arxiv.org/pdf/1207.1522" class=yC71><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://arxiv.org/abs/1207.1522" class=yC70>Multimodal similarity-preserving hashing</a></h3><div class="gs_a">J Masci, <a href="/citations?user=UU3N6-UAAAAJ&amp;hl=en&amp;oi=sra">MM Bronstein</a>, AA Bronstein&hellip; - arXiv preprint arXiv: &hellip;, 2012 - arxiv.org</div><div class="gs_rs">Abstract: We introduce an efficient computational framework for hashing data belonging to <br>multiple modalities into a single representation space where they become mutually <br>comparable. The proposed approach is based on a novel coupled siamese neural <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:PHn0hdPitAQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=339145270072998204&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'PHn0hdPitAQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:325"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/5K3K747738N8740W.pdf" class=yC72>Image annotation by sparse logistic regression</a></h3><div class="gs_a">S He, J Jia - Advances in Multimedia Information Processing-PCM  &hellip;, 2011 - Springer</div><div class="gs_rs">Image annotation aims at finding suitable multiple tags for unlabeled images. Image <br>annotation could be taken as a process of modeling the relationships between images and <br>annotated key words. In this paper, we utilize sparse logistic regression to encode the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7407359879534091927&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:lx4ch5A6zGYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7407359879534091927&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'lx4ch5A6zGYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:324"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB176" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW176"><a href="http://www.lifl.fr/~urruty/publis/icme2011.pdf" class=yC74><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lifl.fr</span><span class="gs_ggsS">lifl.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6011867" class=yC73>A semantically significant visual representation for social image retrieval</a></h3><div class="gs_a">I El Sayad, J Martinet, T Urruty&hellip; - Multimedia and Expo &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Having effective methods to access the desired images is essential nowadays with <br>the availability of a huge amount of digital images. We propose a higher-level visual <br>representation that enhances the traditional part-based Bag of Visual Words (BOW) <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ExXMkHomkrsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13515907739549439251&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ExXMkHomkrsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:323"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/U2413311423U2722.pdf" class=yC75>Automatic tagging by exploring tag information capability and correlation</a></h3><div class="gs_a">X Zhang, Z Huang, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, <a href="/citations?user=PVv2xDYAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, Z Li - World Wide Web, 2012 - Springer</div><div class="gs_rs">Abstract Automatic tagging can automatically label images and videos with semantic tags to <br>significantly facilitate multimedia search and organization. However, most of existing tagging <br>algorithms often don&#39;t differentiate between tags used to describe visual content, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1763491691324278466&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:woa8kVkueRgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1763491691324278466&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'woa8kVkueRgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:322"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/n21118436x784101.pdf" class=yC76>Combining global and local matching of multiple features for precise item image retrieval</a></h3><div class="gs_a">H Li, X Wang, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, C Zhao - Multimedia Systems - Springer</div><div class="gs_rs">Abstract With the fast-growing of online shopping services, there are millions even billions of <br>commercial item images available on the Internet. How to effectively leverage visual search <br>method to find the items of users&#39; interests is an important yet challenging task. Besides <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2778197965817864700&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:_GFJlFQkjiYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'_GFJlFQkjiYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:321"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB179" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW179"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.188.6683&amp;rep=rep1&amp;type=pdf" class=yC78><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.188.6683&amp;rep=rep1&amp;type=pdf" class=yC77>Automatic Image Tagging via Category Label and Web Data</a></h3><div class="gs_a">ZW ShenghuaGao, IWHT Liang-TienChia - 2010 - Citeseer</div><div class="gs_rs">ABSTRACT Image tagging is an important technique for the image content understanding <br>and text based image processing. Given a selection of images, how to tag these images <br>efficiently and effectively is an interesting problem. In this paper, a novel semi-auto image <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:0oPTlD8uzbAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12739889771515708370&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'0oPTlD8uzbAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md179', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md179" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:0oPTlD8uzbAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:320"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB180" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW180"><a href="http://arxiv.org/pdf/1209.2295" class=yC7A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://arxiv.org/abs/1209.2295" class=yC79>Multimodal diffusion geometry by joint diagonalization of Laplacians</a></h3><div class="gs_a"><a href="/citations?user=wJdbXnYAAAAJ&amp;hl=en&amp;oi=sra">D Eynard</a>, K Glashoff, <a href="/citations?user=UU3N6-UAAAAJ&amp;hl=en&amp;oi=sra">MM Bronstein</a>&hellip; - arXiv preprint arXiv: &hellip;, 2012 - arxiv.org</div><div class="gs_rs">Abstract: We construct an extension of diffusion geometry to multiple modalities through joint <br>approximate diagonalization of Laplacian matrices. This naturally extends classical data <br>analysis tools based on spectral geometry, such as diffusion maps and spectral clustering. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:poVam7HVvuMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16410789050976994726&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'poVam7HVvuMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:319"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB181" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW181"><a href="https://dspace.ist.utl.pt/bitstream/2295/1275683/1/dissertacao.pdf" class=yC7C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utl.pt</span><span class="gs_ggsS">utl.pt <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://dspace.ist.utl.pt/bitstream/2295/1275683/1/dissertacao.pdf" class=yC7B>NarBIR-Narrative based image repository</a></h3><div class="gs_a">FAV da Silva - 2012 - dspace.ist.utl.pt</div><div class="gs_rs">Abstract With the popularization of digital photography, a great need for storage and <br>organization of photographs was created. One of the most popular organization methods <br>used nowadays is tagging. This method consists on classifying resources based on <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'L130m7BVjBoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md181', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md181" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:L130m7BVjBoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:318"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6413738" class=yC7D>Simultaneously Combining Multi-view Multi-label Learning with Maximum Margin Classification</a></h3><div class="gs_a">Z Fang, ZM Zhang - Data Mining (ICDM), 2012 IEEE 12th  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Multiple feature views arise in various important data classification scenarios. <br>However, finding a consensus feature view from multiple feature views for a classifier is still <br>a challenging task. We present a new classification framework using the multi-label <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'GpoWnWTbF5EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:317"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB183" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW183"><a href="http://www.postech.ac.kr/~seungjin/publications/icdm12_KangYS.pdf" class=yC7F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from postech.ac.kr</span><span class="gs_ggsS">postech.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6413830" class=yC7E>Deep Learning to Hash with Multiple Representations</a></h3><div class="gs_a">Y Kang, S Kim, S Choi - Data Mining (ICDM), 2012 IEEE 12th  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Hashing seeks an embedding of high-dimensional objects into a similarity-<br>preserving low-dimensional Hamming space such that similar objects are indexed by binary <br>codes with small Hamming distances. A variety of hashing methods have been developed<b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'W8qrnzDd7MwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:316"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB184" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW184"><a href="http://www.yugangjiang.info/publication/TMM_queryadaptivesearch.pdf" class=yC81><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from yugangjiang.info</span><span class="gs_ggsS">yugangjiang.info <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.yugangjiang.info/publication/TMM_queryadaptivesearch.pdf" class=yC80>Query-Adaptive Image Search with Hash Codes</a></h3><div class="gs_a"><a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a>, <a href="/citations?user=i0KkESEAAAAJ&amp;hl=en&amp;oi=sra">J Wang</a>, X Xue, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - 2010 - yugangjiang.info</div><div class="gs_rs">AbstractâScalable image search based on visual similarity has been an active topic of <br>research in recent years. Stateof-the-art solutions often use hashing methods to embed <br>highdimensional image features into Hamming space, where search can be performed in <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:4-PKnwBqHzwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'4-PKnwBqHzwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md184', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md184" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:4-PKnwBqHzwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:315"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB185" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW185"><a href="http://vireo.cs.cityu.edu.hk/papers/TIP12_Context.pdf" class=yC83><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6153060" class=yC82>Fast Semantic Diffusion for Large-Scale Context-Based Image and Video Annotation</a></h3><div class="gs_a"><a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a>, Q Dai, <a href="/citations?user=i0KkESEAAAAJ&amp;hl=en&amp;oi=sra">J Wang</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>&hellip; - Image Processing,  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Exploring context information for visual recognition has recently received significant <br>research attention. This paper proposes a novel and highly efficient approach, which is <br>named semantic diffusion, to utilize semantic context for large-scale image and video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15721518152466683232&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:YNWhp3YNLtoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15721518152466683232&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'YNWhp3YNLtoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:314"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6209403" class=yC84>Sparse Unsupervised Dimensionality Reduction for Multiple View Data</a></h3><div class="gs_a"><a href="/citations?user=t4283loAAAAJ&amp;hl=en&amp;oi=sra">Y Han</a>, F Wu, D Tao, <a href="/citations?user=VUN-9cQAAAAJ&amp;hl=en&amp;oi=sra">J Shao</a>, Y Zhuang, J Jiang - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Different kinds of high-dimensional visual features can be extracted from a single <br>image. Images can thus be treated as multiple view data when taking each type of extracted <br>high-dimensional visual feature as a particular understanding of images. In this paper, we <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:OKvapz4nJhwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'OKvapz4nJhwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:313"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB187" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW187"><a href="http://www.comp.nus.edu.sg/~TANCL/publications/c2012/ictai2012_web.pdf" class=yC86><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.comp.nus.edu.sg/~TANCL/publications/c2012/ictai2012_web.pdf" class=yC85>Web Image Organization and Object Discovery by Actively Creating Visual Clusters through Crowdsourcing</a></h3><div class="gs_a">Q Chen, G Wang, <a href="/citations?user=nmFSOaEAAAAJ&amp;hl=en&amp;oi=sra">CL Tan</a> - comp.nus.edu.sg</div><div class="gs_rs">AbstractâIn this paper, we propose to organize web images by actively creating visual <br>clusters via crowdsourcing. We develop a two-phase framework to efficiently and effectively <br>combine computers and a large number of human workers to build high quality visual <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'jhMlqY98NSUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md187', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md187" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jhMlqY98NSUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:312"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB188" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW188"><a href="http://oro.open.ac.uk/32088/1/magalhaes-rueger-2010-mtapp-preprint.pdf" class=yC88><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from open.ac.uk</span><span class="gs_ggsS">open.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/pdf/10.1007/s11042-010-0558-3" class=yC87>Using manual and automated annotations to search images by semantic similarity</a></h3><div class="gs_a"><a href="/citations?user=P6fBAXMAAAAJ&amp;hl=en&amp;oi=sra">J MagalhÃ£es</a>, <a href="/citations?user=oFU5A7sAAAAJ&amp;hl=en&amp;oi=sra">S RÃ¼ger</a> - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract Finding semantically similar images is a problem that relies on image annotations <br>manually assigned by amateurs or professionals, or automatically computed by some <br>algorithm using low-level image features. These image annotations create a keyword <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gvj7q4pGVc4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14867867306238408834&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'gvj7q4pGVc4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:311"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB189" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW189"><a href="http://sites.google.com/site/luozhipi/EstimatingPosesofWorldsPhotoswithGeographicMetadata_luo.pdf" class=yC8A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/YR3KX062436V756U.pdf" class=yC89>Estimating Poses of World&#39;s Photos with Geographic Metadata</a></h3><div class="gs_a"><a href="/citations?user=0UkdiUT1ooUC&amp;hl=en&amp;oi=sra">Z Luo</a>, H Li, J Tang, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, TS Chua - Advances in Multimedia Modeling, 2010 - Springer</div><div class="gs_rs">Abstract. Users can explore the world by viewing place related photos on Google Maps. One <br>possible way is to take the nearby photos for viewing. However, for a given geo-location, <br>many photos with view directions not pointing to the desired regions are returned by that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9406749074418054949&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:JTfsr3F8i4IJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9406749074418054949&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'JTfsr3F8i4IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:310"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB190" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW190"><a href="http://www.jdl.ac.cn/doc/2011/201112291652871233_p2077-shuhuiwangcikm2011.pdf" class=yC8C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jdl.ac.cn</span><span class="gs_ggsS">jdl.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2063894" class=yC8B>Efficient lp-norm multiple feature metric learning for image categorization</a></h3><div class="gs_a">S Wang, Q Huang, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - Proceedings of the 20th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Previous metric learning approaches are only able to learn the metric based on <br>single concatenated multivariate feature representation. However, for many real world <br>problems with multiple feature representation such as image categorization, the model <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:HsowtuVnNUgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5203179180798298654&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'HsowtuVnNUgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:309"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB191" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW191"><a href="http://repository.tudelft.nl/assets/uuid:7db29bc0-7b51-4655-bf8b-de7d95d42e12/20.pdf" class=yC8E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tudelft.nl</span><span class="gs_ggsS">tudelft.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://repository.tudelft.nl/assets/uuid:7db29bc0-7b51-4655-bf8b-de7d95d42e12/20.pdf" class=yC8D>Automatic ground-truth image generation from user tags</a></h3><div class="gs_a">T Tsirelis, A Delopoulos - 2011 - repository.tudelft.nl</div><div class="gs_rs">ABSTRACT Automatic selection of ground-truth images is very important for the training of <br>image classifiers, like the ones used in concept-based image retrieval. For this purpose, we <br>propose a method which collects a sufficient number of groundtruth images based on their <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:TEMlupW-qQcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=552181979612005196&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'TEMlupW-qQcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md191', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md191" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TEMlupW-qQcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:308"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212006984" class=yC8F>Nonlinear matrix factorization with unified embedding for social tag relevance learning</a></h3><div class="gs_a">Z Li, J Liu, H Lu - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Abstract With the proliferation of social images, social image tagging is an essential issue for <br>text-based social image retrieval. However, the original tags annotated by web users are <br>always noisy, irrelevant and incomplete to interpret the image visual contents. In this <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'dfqYv1IpuOYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:307"><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=j0vy1nxh2AwC&amp;oi=fnd&amp;pg=PA3&amp;ots=I_0Wh2sDcH&amp;sig=Tt5cMrPDaa8KUIJhJtgaD0hXRk4" class=yC90>Image Re-Emotionalizing</a></h3><div class="gs_a">M Xu, <a href="/citations?user=V9W87PYAAAAJ&amp;hl=en&amp;oi=sra">B Ni</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a> - The Era of Interactive Media, 2012 - books.google.com</div><div class="gs_rs">Abstract In this work, we develop a novel system for synthesizing user specified emotional <br>affection onto arbitrary input images. To tackle the subjectivity and complexity issue of the <br>image affection generation process, we propose a learning framework which discovers <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:oy-SwDEcHBcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'oy-SwDEcHBcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:306"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB194" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW194"><a href="http://www.cmlab.csie.ntu.edu.tw/~zenic/Download/ICME2012/Conference/data/4711a055.pdf" class=yC92><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6298374" class=yC91>Media Lifecycle and Content Analysis in Social Media Communities</a></h3><div class="gs_a"><a href="/citations?user=u0xUDSoAAAAJ&amp;hl=en&amp;oi=sra">L Xie</a>, <a href="/citations?user=Z962IGQAAAAJ&amp;hl=en&amp;oi=sra">H Sundaram</a> - Multimedia and Expo (ICME), 2012 IEEE  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper examines the role of content analysis in media-rich online communities. <br>We highlight changes in the multimedia generation and consumption process that has <br>occurred the past decade, and discuss several new angles this has brought to multimedia <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:GORNwd9NdqMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11778687498850264088&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'GORNwd9NdqMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:305"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB195" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW195"><a href="http://www.nlpr.ia.ac.cn/2011papers/gjhy/gh96.pdf" class=yC94><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ia.ac.cn</span><span class="gs_ggsS">ia.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2071956" class=yC93>Exploiting user information for image tag refinement</a></h3><div class="gs_a"><a href="/citations?user=u6ivSjgAAAAJ&amp;hl=en&amp;oi=sra">J Sang</a>, J Liu, C Xu - Proceedings of the 19th ACM international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Photo sharing websites allow users to describe images with freely chosen tags. The <br>user-generated tags not only facilitate the users in sharing and organizing images, but also <br>provide large scale meaningful data for image retrieval and management. Extensive <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:wXaQxD3QQQAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18524837195577025&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'wXaQxD3QQQAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:304"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB196" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW196"><a href="https://www.ideals.illinois.edu/bitstream/handle/2142/26213/liangliang_cao.pdf?sequence=1" class=yC96><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from illinois.edu</span><span class="gs_ggsS">illinois.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://www.ideals.illinois.edu/handle/2142/26213" class=yC95>Heterogeneous Feature Fusion for Visual Recognition</a></h3><div class="gs_a"><a href="/citations?user=S-hBSfIAAAAJ&amp;hl=en&amp;oi=sra">L Cao</a> - 2011 - ideals.illinois.edu</div><div class="gs_rs">Abstract: In the past decade, the popularity of the Internet and digital cameras has led to a <br>flourishing of images and videos. Surveillance videos are increasing explosively with the <br>huge amounts of surveillance cameras. Compared with traditional datasets in computer <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:e42Rxe0f5xgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1794438082612792699&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'e42Rxe0f5xgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:303"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB197" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW197"><a href="http://www.postech.ac.kr/~seungjin/publications/icdm12_YunJM.pdf" class=yC98><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from postech.ac.kr</span><span class="gs_ggsS">postech.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6413731" class=yC97>Hashing with Generalized NystrÃ¶m Approximation</a></h3><div class="gs_a">JM Yun, S Kim, S Choi - Data Mining (ICDM), 2012 IEEE 12th  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Hashing, which involves learning binary codes to embed high-dimensional data <br>into a similarity-preserving low-dimensional Hamming space, is often formulated as linear <br>dimensionality reduction followed by binary quantization. Linear dimensionality reduction, <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'43JVxpzHpL0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:302"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/F6174484310V2KKJ.pdf" class=yC99>Fusing Heterogeneous Information for Social Image Retrieval</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a> - Web-Age Information Management, 2012 - Springer</div><div class="gs_rs">Given that millions of images are uploaded to the social web in a single day, how to <br>effectively retrieve such increasing amounts of unstructured data becomes crucial. While <br>current social platforms find images on the base of user-contributed tags, the tags are <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lpWrx2u_2doJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'lpWrx2u_2doJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:301"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/R84721R4L26735M1.pdf" class=yC9A>Image tagging by exploiting feature correlation</a></h3><div class="gs_a">X Zhang, Z Li - Digital Libraries: For Cultural Heritage, Knowledge  &hellip;, 2011 - Springer</div><div class="gs_rs">Image tagging is a task that automatically assigns the query image with semantic keywords <br>called tags. Since tags and image visual content are represented in different feature space, <br>how to merge the multiple features by their correlation to tag the query image is an <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:-snUxyTEfqEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11636954149422418426&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'-snUxyTEfqEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
