Total results = 55
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://137.189.35.203/WebUI/PhotoQualityEvaluation/downloads/eccv08_Photo%20and%20Video%20Quality%20Evaluation.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.189.35.203</span><span class="gs_ggsS">137.189.35.203 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/dv202m2j7q855688.pdf" class=yC0>Photo and video quality evaluation: Focusing on the subject</a></h3><div class="gs_a">Y Luo, <a href="/citations?user=qpBtpGsAAAAJ&amp;hl=en&amp;oi=sra">X Tang</a> - Computer VisionâECCV 2008, 2008 - Springer</div><div class="gs_rs">Traditionally, distinguishing between high quality professional photos and low quality <br>amateurish photos is a human task. To automatically assess the quality of a photo that is <br>consistent with humans perception is a challenging topic in computer vision. Various <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11267046775442129932&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 90</a> <a href="/scholar?q=related:DETwzViXXJwJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11267046775442129932&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'DETwzViXXJwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://nguyendangbinh.org/Proceedings/CHI/2007/docs/p61.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nguyendangbinh.org</span><span class="gs_ggsS">nguyendangbinh.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1240634" class=yC2>Understanding videowork</a></h3><div class="gs_a"><a href="/citations?user=fHDEDM0AAAAJ&amp;hl=en&amp;oi=sra">D Kirk</a>, <a href="/citations?user=3UlxG6UAAAAJ&amp;hl=en&amp;oi=sra">A Sellen</a>, R Harper, K Wood - Proceedings of the SIGCHI  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract In this paper we elucidate the patterns of behavior of home movie makers through a <br>study of 12 families and a separate focus group of 7 teenagers. Analogous to a similar study <br>of photowork [13], the goal is to provide a deeper understanding of what people currently <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2870057364301773262&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 71</a> <a href="/scholar?q=related:znkzdPd91CcJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/1F/4B/RN211396660.html?source=googlescholar" class="gs_nph" class=yC4>BL Direct</a> <a href="/scholar?cluster=2870057364301773262&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'znkzdPd91CcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=957013.957121" class=yC5>AVE: automated home video editing</a></h3><div class="gs_a"><a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, L Lu, HJ Zhang - Proceedings of the eleventh ACM international &hellip;, 2003 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we present a system that automates home video editing. This system <br>automatically extracts a set of highlight segments from a set of raw home videos and aligns <br>them with user supplied incidental music based on the content of the video and incidental <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7922238345244625324&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 64</a> <a href="/scholar?q=related:rMlIGs1x8W0J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7922238345244625324&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'rMlIGs1x8W0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1294950" class=yC6>Optimization-based automated home video editing system</a></h3><div class="gs_a"><a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, L Lu, HJ Zhang - Circuits and Systems for Video  &hellip;, 2004 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present an optimization-based system that automates home video <br>editing. This system automatically selects suitable or desirable highlight segments from a set <br>of raw home videos and aligns them with a given piece of incidental music to create an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3289167985097022929&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 62</a> <a href="/scholar?q=related:0Z1LDtt4pS0J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/49/21/RN149970251.html?source=googlescholar" class="gs_nph" class=yC7>BL Direct</a> <a href="/scholar?cluster=3289167985097022929&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'0Z1LDtt4pS0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://ntur.lib.ntu.edu.tw/retrieve/169496/22.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1180653" class=yC8>Tiling slideshow</a></h3><div class="gs_a">JC Chen, <a href="/citations?user=DcltNjQAAAAJ&amp;hl=en&amp;oi=sra">WT Chu</a>, JH Kuo, CY Weng&hellip; - Proceedings of the 14th  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract This paper presents a new medium, called tiling slideshow, to display photos in a <br>tile-like manner, coordinating with the pace of background music. In contrast to the <br>conventional photo slideshow, multiple photos that have similar characteristics are well <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10445939200003255474&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 52</a> <a href="/scholar?q=related:sqyFgVtu95AJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10445939200003255474&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 18 versions</a> <a onclick="return gs_ocit(event,'sqyFgVtu95AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://mll.csie.ntu.edu.tw/internal/PUC2005_mProducer_camera.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1236594" class=yCA>Point-of-capture archiving and editing of personal experiences from a mobile device</a></h3><div class="gs_a">CI Wu, C James Teng, <a href="/citations?user=LdNIR90AAAAJ&amp;hl=en&amp;oi=sra">YC Chen</a>, TY Lin&hellip; - Personal and  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Personal experience computing is an emerging research area in computing support <br>for capturing, archiving, and editing. This paper presents our design, implementation, and <br>evaluation of a mobile authoring tool called mProducer that enables everyday users to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15157145489679960582&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 29</a> <a href="/scholar?q=related:BspJUYT_WNIJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/50/09/RN205846808.html?source=googlescholar" class="gs_nph" class=yCC>BL Direct</a> <a href="/scholar?cluster=15157145489679960582&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 29 versions</a> <a onclick="return gs_ocit(event,'BspJUYT_WNIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4220728" class=yCD>Home video visual quality assessment with spatiotemporal factors</a></h3><div class="gs_a"><a href="/citations?user=7Yq4wf4AAAAJ&amp;hl=en&amp;oi=sra">T Mei</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, CZ Zhu, HQ Zhou&hellip; - Circuits and Systems for  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Compared with the video programs taken by professionals, home videos are <br>always with low quality content resulted from non-professional capture skills. In this paper, <br>we present a novel spatiotemporal quality assessment scheme in terms of low-level <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1358275560725304021&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 27</a> <a href="/scholar?q=related:1VIgEG2Q2RIJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/55/59/RN210060611.html?source=googlescholar" class="gs_nph" class=yCE>BL Direct</a> <a href="/scholar?cluster=1358275560725304021&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'1VIgEG2Q2RIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.132.4237&amp;rep=rep1&amp;type=pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4032616" class=yCF>Modeling and mining of users&#39; capture intention for home videos</a></h3><div class="gs_a"><a href="/citations?user=7Yq4wf4AAAAJ&amp;hl=en&amp;oi=sra">T Mei</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, HQ Zhou, <a href="/citations?user=2sQYtYwAAAAJ&amp;hl=en&amp;oi=sra">S Li</a> - Multimedia, IEEE Transactions  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the rapid adoption of consumer digital video recorders and an increase of <br>home video data, content analysis has become an interesting and key research issue to <br>provide personalized experiences and services for both camcorder users and viewers. In <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5333293531725944749&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 24</a> <a href="/scholar?q=related:rddpyTWqA0oJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0C/4C/RN200866354.html?source=googlescholar" class="gs_nph" class=yC11>BL Direct</a> <a href="/scholar?cluster=5333293531725944749&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'rddpyTWqA0oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1404882" class=yC12>Re-cinematography: Improving the camerawork of casual video</a></h3><div class="gs_a">ML Gleicher, F Liu - ACM Transactions on Multimedia Computing,  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract This article presents an approach to postprocessing casually captured videos to <br>improve apparent camera movement. Re-cinematography transforms each frame of a video <br>such that the video better follows cinematic conventions. The approach breaks a video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14460635668313473318&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 22</a> <a href="/scholar?q=related:JgErTIZ_rsgJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14460635668313473318&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'JgErTIZ_rsgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://kusu.comp.nus.edu/proceedings/icme05/defevent/papers/cr1147.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1521399" class=yC13>Video quality classification based home video segmentation</a></h3><div class="gs_a">S Wu, YF Ma, HJ Zhang - &hellip;  and Expo, 2005. ICME 2005. IEEE  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Home videos often have some abnormal camera motions, such as camera shaking <br>and irregular camera motions, which cause the degradation of visual quality. To remove bad <br>quality segments and automatic stabilize shaky ones are necessary steps for home video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16461667931535768016&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 14</a> <a href="/scholar?q=related:0PmgHcSXc-QJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16461667931535768016&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'0PmgHcSXc-QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://csce.uark.edu/~jgauch/library/Video/Achanta.2006.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uark.edu</span><span class="gs_ggsS">uark.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1580433" class=yC15>Modeling intent for home video repurposing</a></h3><div class="gs_a"><a href="/citations?user=lc2HaZwAAAAJ&amp;hl=en&amp;oi=sra">RSV Achanta</a>, WQ Yan, MS Kankanhalli - Multimedia, IEEE, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Amateur home videos rarely convey intent effectively, primarily because of the <br>limitations of conventional consumer-quality video cameras and the difficulties of video <br>postprocessing. The authors describe a general approach for video-intent delivery based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4206118216265038784&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 13</a> <a href="/scholar?q=related:wPvGkzsiXzoJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4F/49/RN183745838.html?source=googlescholar" class="gs_nph" class=yC17>BL Direct</a> <a href="/scholar?cluster=4206118216265038784&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'wPvGkzsiXzoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://kusu.comp.nus.edu/proceedings/mm05/docs/mm439.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101149.1101246" class=yC18>Spatio-temporal quality assessment for home videos</a></h3><div class="gs_a"><a href="/citations?user=7Yq4wf4AAAAJ&amp;hl=en&amp;oi=sra">T Mei</a>, CZ Zhu, HQ Zhou, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a> - Proceedings of the 13th annual ACM  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract Compared with the video programs taken by professionals, home videos are <br>always with low-quality content resulted from lack of professional capture skills. In this paper, <br>we present a novel spatio-temporal quality assessment scheme in terms of low-level <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17767443617586613351&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 12</a> <a href="/scholar?q=related:ZwwIkdWjkvYJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17767443617586613351&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'ZwwIkdWjkvYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="https://webspace.utexas.edu/akm798/research/akm_eccv_sep10.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utexas.edu</span><span class="gs_ggsS">utexas.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/G21X63481841608W.pdf" class=yC1A>Towards computational models of the visual aesthetic appeal of consumer videos</a></h3><div class="gs_a"><a href="/citations?user=t9eduncAAAAJ&amp;hl=en&amp;oi=sra">A Moorthy</a>, P Obrador, N Oliver - Computer VisionâECCV 2010, 2010 - Springer</div><div class="gs_rs">In this paper, we tackle the problem of characterizing the aesthetic appeal of consumer <br>videos and automatically classifying them into high or low aesthetic appeal. First, we <br>conduct a controlled user study to collect ratings on the aesthetic value of 160 consumer <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2432116076044645029&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 13</a> <a href="/scholar?q=related:pXIoba-cwCEJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2432116076044645029&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'pXIoba-cwCEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://pages.cs.wisc.edu/~fliu/papers/acmmm08.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from wisc.edu</span><span class="gs_ggsS">wisc.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459404" class=yC1C>Discovering panoramas in web videos</a></h3><div class="gs_a">F Liu, Y Hu, ML Gleicher - Proceedings of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract While methods for stitching panoramas have been successful given proper source <br>images, providing these source images still remains a burden. In this paper, we present a <br>method to discover panoramic source images within widely available web videos. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16654337235864356568&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 10</a> <a href="/scholar?q=related:2IZBQXoXIOcJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16654337235864356568&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'2IZBQXoXIOcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICICS_PCM%202003/PDFFILES/PAPERS/2C13P0906.PDF" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1292638" class=yC1E>Adaptive monitoring for video surveillance</a></h3><div class="gs_a"><a href="/citations?user=wIE1tY4AAAAJ&amp;hl=en&amp;oi=sra">J Wang</a>, WQ Yan, MS Kankanhalli&hellip; - &hellip; , 2003 and Fourth  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Adaptability is one of the key issues in the important area of surveillance systems. <br>Based on attention and sensor samples, the experiential sampling technique provides a <br>general framework for analyzing video data. In this paper, we present a scheme for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5994628161929859462&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 10</a> <a href="/scholar?q=related:htFhaI0yMVMJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5994628161929859462&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 31 versions</a> <a onclick="return gs_ocit(event,'htFhaI0yMVMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICME%202003/pdfs/0100097.pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1220863" class=yC20>Colorizing infrared home videos</a></h3><div class="gs_a">WQ Yan, MS Kankanhalli - Multimedia and Expo, 2003. ICME&#39; &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A color video always conveys more vivid sentiments than a grayscale one. <br>Obtaining a grayscale video from a color video is almost trivial but the converse is known to <br>be hard. Nowadays, digital camcorders come equipped with an infrared device for night <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16417441424250828236&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 10</a> <a href="/scholar?q=related:zPVA9f131uMJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16417441424250828236&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'zPVA9f131uMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1246955" class=yC22>A systemic framework of camera motion analysis for home video</a></h3><div class="gs_a">DJ Lan, YF Ma, HJ Zhang - Image Processing, 2003. ICIP 2003. &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A key factor that results in low quality home video is abnormal camera motion, such <br>as camera shaking and irregular camera motion. Those unwanted camera movements make <br>video content analysis more difficult than that in high quality video, because these <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12000427938983843821&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 7</a> <a href="/scholar?q=related:7RfOJ6EViqYJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12000427938983843821&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'7RfOJ6EViqYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://ntur.lib.ntu.edu.tw/dspace/bitstream/246246/20060927122847164327/1/mum_2004.pdf" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1052400" class=yC23>Design and evaluation of mProducer: a mobile authoring tool for personal experience computing</a></h3><div class="gs_a"><a href="/citations?user=GnoD6-gAAAAJ&amp;hl=en&amp;oi=sra">CMJ Teng</a>, CI Wu, <a href="/citations?user=LdNIR90AAAAJ&amp;hl=en&amp;oi=sra">YC Chen</a>, <a href="/citations?user=aV5Bk7MAAAAJ&amp;hl=en&amp;oi=sra">H Chu</a>&hellip; - Proceedings of the 3rd  &hellip;, 2004 - dl.acm.org</div><div class="gs_rs">Abstract Personal experience computing is about computing support for recording, storing, <br>retrieving, editing, analyzing, and sharing of personal experiences. In this paper, we present <br>our design, implementation and evaluation of a mobile authoring tool called mProducer. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6173757555918946606&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 7</a> <a href="/scholar?q=related:LsHugcmXrVUJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6173757555918946606&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'LsHugcmXrVUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.125&amp;rep=rep1&amp;type=pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.125&amp;rep=rep1&amp;type=pdf" class=yC25>Home-video content analysis for MTV-style video generation</a></h3><div class="gs_a">SH Lee, CH Yeh, <a href="/citations?user=81d60okAAAAJ&amp;hl=en&amp;oi=sra">CCJ Kuo</a> - Proc. SPIE, 2005 - Citeseer</div><div class="gs_rs">Abstract Intelligent video pre-processing and authoring techniques that facilitate people to <br>create MTV-style music video clips are investigated in this research. First, we present an <br>automatic approach to detect and remove bad shots often occurring in home video, such <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8064772632502498968&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 5</a> <a href="/scholar?q=related:mHaa8vrT628J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8064772632502498968&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'mHaa8vrT628J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md18', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md18" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:mHaa8vrT628J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> <a href="http://en.scientificcommons.org/35119566" class=yC27>FPGA Prototyping of a Watermarking Algorithm for MPEG-4</a></h3><div class="gs_rs"><a href="https://support.google.com/websearch/bin/answer.py?answer=45449&hl=en">This site may harm your computer.</a></div><div class="gs_a">W Cai - 2007 - en.scientificcommons.org</div><div class="gs_fl"><a href="/scholar?cites=1104579298415268525&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 4</a> <a href="/scholar?q=related:rdJKBAVBVA8J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1104579298415268525&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'rdJKBAVBVA8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:rdJKBAVBVA8J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a>  <a href="/scholar?q=info:rdJKBAVBVA8J:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=17486255302385058113&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1689105" class=yC28>Video Stabilization by Multi-Trajectory Mapping and Smoothing</a></h3><div class="gs_a">S Wu, Z Ren - &hellip;  and Signal Processing, 2005 Fifth International  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract To remove visually annoying vibrations caused by jittery camera from video is a <br>very challenging issue. This paper proposed a video stabilization approach based on multi-<br>trajectory mapping and smoothing. First, the estimated affine parameters in consecutive <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8532217829695976372&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:tHNIFemGaHYJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'tHNIFemGaHYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.2127&amp;rep=rep1&amp;type=pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/U512Q5X0081517M1.pdf" class=yC29>Analogies based video editing</a></h3><div class="gs_a">WQ Yan, MS Kankanhalli, <a href="/citations?user=wIE1tY4AAAAJ&amp;hl=en&amp;oi=sra">J Wang</a> - Multimedia Systems, 2005 - Springer</div><div class="gs_rs">A well-produced video always creates a strong impression on the viewer. However, due to <br>the limitations of the camera, the ambient conditions or the skills of the videographer, the <br>quality of captured videos sometimes falls short of one&#39;s expectations. On the other hand, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13469578948090295803&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 4</a> <a href="/scholar?q=related:-3UAO7CO7boJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/33/03/RN180039385.html?source=googlescholar" class="gs_nph" class=yC2B>BL Direct</a> <a href="/scholar?cluster=13469578948090295803&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'-3UAO7CO7boJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.64.4317&amp;rep=rep1&amp;type=pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1198307" class=yC2C>Multimedia simplification for optimized MMS synthesis</a></h3><div class="gs_a">WQ Yan, MS Kankanhalli - ACM Transactions on Multimedia Computing &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract We propose a novel transcoding technique called multimedia simplification which is <br>based on experiential sampling. Multimedia simplification helps optimize the synthesis of <br>MMS (multimedia messaging service) messages for mobile phones. Transcoding is useful <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6272608690979916157&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:fcni6F7IDFcJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6272608690979916157&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'fcni6F7IDFcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://www.ijiee.org/papers/183-X050.pdf" class=yC2F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ijiee.org</span><span class="gs_ggsS">ijiee.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ijiee.org/papers/183-X050.pdf" class=yC2E>Interaction Model for Emotive Video Production</a></h3><div class="gs_a">H Mitarai, A Yoshitaka - Proc. of International Conference on Future  &hellip;, 2010 - ijiee.org</div><div class="gs_rs">AbstractâVideo images are capable of expressing various kinds of information; however <br>special knowledge and techniques are required for authoring quality video content. In order <br>to represent impressions, proper camerawork is required for delivering understandable <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1665930922786360504&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:uAwFCVmTHhcJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'uAwFCVmTHhcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md23', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md23" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:uAwFCVmTHhcJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/92586a/200803/26789084.html" class=yC30>ä¸ç§é²æ£æ§çæå¨è§é¢ç¨³åç®æ³</a></h3><div class="gs_a">é»äºåï¼ ç¦å»ºå½¬ï¼ å¶é½ç¥¥ï¼ éå¿å½ - åçµå­. æ¿å, 2008 - cqvip.com</div><div class="gs_rs">è®¾è®¡äºä¸ç§éç¨äºç§»å¨æåè®¾å¤è·åçè§é¢åºåçåºäºç¹å¾å¹éçé²æ£æ§ä¸ç¨³å®è§é¢ç¨³åç®æ³, <br>ç®æ³é¦åå°äº®åº¦èªéåºæ¨¡åèå¥ä¼ ç»çKLT æ¹æ³ä¸­ä»¥å®ç°é²æ£æ§çç¹å¾å¹é, <br>èååºäºç¹å¾è¯¯å·®åæåè¿å¨ä¸è´æ§ååå¯¹ç¹å¾éåè¿è¡æææ§éªè¯ä»¥æé«ç¹å¾çå¯é æ§, å¹¶<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12622016207616994823&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 2</a> <a href="/scholar?q=related:B-471N1oKq8J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12622016207616994823&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'B-471N1oKq8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICICS_PCM%202003/PDFFILES/PAPERS/1A11P0911.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1292401" class=yC31>Information-integration approach to designing digital video albums</a></h3><div class="gs_a">C Madhwacharyula, W Jun, Y Weiqi, J Yi&hellip; - &hellip; , 2003 and Fourth  &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present the architecture of the DVA (digital video album) system. <br>Information-integration is the key principle utilized in this system to allow for content-based <br>indexing, intuitive access and retrieval of digital video. Our implementation of the system <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4720112419584736435&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 1</a> <a href="/scholar?q=related:s_xBxTw1gUEJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4720112419584736435&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'s_xBxTw1gUEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.1122&amp;rep=rep1&amp;type=pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/pn3430677t4406l4.pdf" class=yC33>A cross-modal approach for karaoke artifacts correction</a></h3><div class="gs_a">WQ Yan, MS Kankanhalli - Multimedia Tools and Applications, 2008 - Springer</div><div class="gs_rs">Abstract Karaoke singing is a popular form of entertainment in several parts of the world. <br>Since this genre of performance attracts amateurs, the singing often has artifacts related to <br>scale, tempo, and synchrony. We have developed an approach to correct these artifacts <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12356163693489506090&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 1</a> <a href="/scholar?q=related:KkcF2WnpeasJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/19/4E/RN232981854.html?source=googlescholar" class="gs_nph" class=yC35>BL Direct</a> <a href="/scholar?cluster=12356163693489506090&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'KkcF2WnpeasJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6123340" class=yC36>Shooting Assistance by Recognizing User&#39;s Camera Manipulation for Intelligible Video Production</a></h3><div class="gs_a">H Mitarai, A Yoshitaka - Multimedia (ISM), 2011 IEEE  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We propose a system which achieves cooperative capture assistance by camera <br>manipulation recognition. Based on an experimental result on inexperienced users, the <br>incremental interaction model, which the system and a user cooperatively shoot, was <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:v3DHBNnmR2gJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7514228323053433023&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'v3DHBNnmR2gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/V0713534KR13G768.pdf" class=yC37>Automatic Home Video Editing</a></h3><div class="gs_a"><a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, HJ Zhang - Multimedia Content Analysis, 2009 - Springer</div><div class="gs_rs">In this chapter, we present an optimization-based system for editing home video in an <br>automatic manner. This system automatically selects highlight segments from a set of raw <br>home videos and aligns them with a given piece of incidental music to create an edited <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:vBliDpMw5Q8J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1145375088395295164&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'vBliDpMw5Q8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.7697&amp;rep=rep1&amp;type=pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.7697&amp;rep=rep1&amp;type=pdf" class=yC38>Analogies Based Video Editing</a></h3><div class="gs_a">MS Kankanhalli - Citeseer</div><div class="gs_rs">Abstract A well-produced video always creates a strong impression on the viewer. However <br>due to the limitations of the camera, the ambient conditions or the skills of the videographer, <br>the quality of captured videos sometimes falls short of one&#39;s expectations. On the other <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:kBtpFa3WfAcJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=539542094254513040&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'kBtpFa3WfAcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md29', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md29" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:kBtpFa3WfAcJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2037436" class=yC3A>Visual reporting in time-critical work: exploring video use in emergency response</a></h3><div class="gs_a"><a href="/citations?user=J3YPdZIAAAAJ&amp;hl=en&amp;oi=sra">F Bergstrand</a>, <a href="/citations?user=Z12whP4AAAAJ&amp;hl=en&amp;oi=sra">J Landgren</a> - &hellip;  of the 13th International Conference on  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract This paper reports on an explorative project aimed to study the use of live video <br>technology in emergency response work. The initial stage of the project aimed at enabling <br>an emergency response organization with live video capabilities. The study covered the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17484198974900317763&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 1</a> <a href="/scholar?q=related:Q64uGVVapPIJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17484198974900317763&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'Q64uGVVapPIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6012226" class=yC3B>Interactive video cam system for emotive video production</a></h3><div class="gs_a">H Mitarai, A Yoshitaka - Multimedia and Expo (ICME), 2011  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We propose a system which achieves cooperative capture assistance based on an <br>experimental result and the incremental interaction model which the system and a user <br>cooperatively create a shot. The system complements user&#39;s lack of cinematographic <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:V5owIoQk-mAJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6987937921746639447&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'V5owIoQk-mAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6405521" class=yC3C>Development of Video Shooting Assistant System for Better Expression of Affective Information</a></h3><div class="gs_a">H Mitarai, A Yoshitaka - Knowledge, Information and Creativity  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Authoring quality video contents is difficult since proper camerawork and editing are <br>required for delivering nonverbal information which appropriately represents user&#39;s <br>expressive intentions. Based on an experiment on non-expert users, we proposed the <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'rw_DIh8KchsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/lh7413485m406rv4.pdf" class=yC3D>Cross-Modal Approach for Karaoke Artifacts Correction</a></h3><div class="gs_a">WQ Yan, MS Kankanhalli - &hellip;  of Multimedia for Digital Entertainment and  &hellip;, 2009 - Springer</div><div class="gs_rs">In this chapter, we combine adaptive sampling in conjunction with video analogies (VA) to <br>correct the audio stream in the karaoke environment k= k (t): k (t)=(U (t), K (t)), t Ã (ts, te) Îº=\ <br>left {Îº (t):\ kappa (t)=\ left (U (t),\ K (t)\ right),\ t â\ left (t _ s,{t _ e\ right)\ right\} where ts and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:oYqzO5N08cYJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14335367264607636129&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'oYqzO5N08cYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://www.iis.sinica.edu.tw/~hhyeh/vqa/yang_icassp2011.pdf" class=yC3F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sinica.edu.tw</span><span class="gs_ggsS">sinica.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5946616" class=yC3E>Video aesthetic quality assessment by combining semantically independent and dependent features</a></h3><div class="gs_a">CY Yang, HH Yeh, <a href="/citations?user=WKk6fIQAAAAJ&amp;hl=en&amp;oi=sra">CS Chen</a> - Acoustics, Speech and Signal  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper aims to accomplish the work of assessing the aesthetic quality of a <br>video. Unlike previous assessing works focusing mainly on the extraction of aesthetic <br>features in a film, we further study the features, discover their semantic property on videos <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:QzPIVVDGDdAJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14991856782925116227&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'QzPIVVDGDdAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Fundamentals of Media Security</h3><div class="gs_a">WQ Yan, J Weir - 2010 - WeiQi Yan, Jonathan Weir &amp; Ventus  &hellip;</div><div class="gs_fl"><a href="/scholar?q=related:AREtVtYBdcMJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14084165429751517441&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'AREtVtYBdcMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="http://digital.library.unt.edu/ark:/67531/metadc3695/m2/1/high_res_d/thesis.pdf" class=yC41><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unt.edu</span><span class="gs_ggsS">unt.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://digital.library.unt.edu/ark:/67531/metadc3695/m2/1/high_res_d/thesis.pdf" class=yC40>FPGA PROTOTYPING OF A WATERMARKING ALGORITHM FOR MPEG-4 Wei Cai, BE</a></h3><div class="gs_a"><a href="/citations?user=nvAKagcAAAAJ&amp;hl=en&amp;oi=sra">E Kougianos</a> - 2007 - digital.library.unt.edu</div><div class="gs_rs">In the immediate future, multimedia product distribution through the Internet will become <br>main stream. However, it can also have the side effect of unauthorized duplication and <br>distribution of multimedia products. That effect could be a critical challenge to the legal <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MMX8c0TXjTEJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3570746768578364720&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MMX8c0TXjTEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md36', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md36" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:MMX8c0TXjTEJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=852467" class=yC42>MTV-style home video generation via tempo analysis</a></h3><div class="gs_a">SH Lee, CH Yeh, <a href="/citations?user=81d60okAAAAJ&amp;hl=en&amp;oi=sra">CCJ Kuo</a> - Optics East, 2004 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract Intelligent video pre-processing and authoring techniques that facilitate people to <br>create MTV-style music video clips are investigated in this research. First, we present an <br>automatic approach to detect and remove bad shots often occurring in home video, such <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:CCnVd_tka2YJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7380103445586716936&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'CCnVd_tka2YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://vipl.ict.ac.cn/sites/default/files/papers/files/2010_ACMMM_qqxu_Memory%20Matrix_%20A%20Novel%20User%20Experience%20for%20Home%20Video.pdf" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874115" class=yC43>Memory matrix: a novel user experience for home video</a></h3><div class="gs_a">Q Xu, Z Wu, G Li, L Qin, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, Q Huang - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Nowadays, various efforts have sprung up aiming to automatically analyze home <br>videos and provide users satisfactory experiences. In this paper, we present a novel user <br>experience for home video called Memory Matrix, which could facilitate users to re-<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:WCY5gCm6OzAJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3475576224854124120&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'WCY5gCm6OzAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB39" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW39"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.8281&amp;rep=rep1&amp;type=pdf" class=yC46><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.8281&amp;rep=rep1&amp;type=pdf" class=yC45>é¨èº«å½±çç·¨è¼¯ç³»çµ±</a></h3><div class="gs_a">å³å¿ ç© - Citeseer</div><div class="gs_rs">ä¸­ææè¦å¨é»è¦è¼å©ï¤¿è£½, å²å­, ç·¨è¼¯, ä»¥åï§ç¨ï¥©ä½åäººç¶ï¦ä¸. åäººç¶ï¦è¨ç®æ¯ã§åæ°èç<br>ç ç©¶ï¦´å. éé ç ç©¶åç¾æåçå°ï¨åç·¨è¼¯å·¥å·ç¨±ä½ mProducer çè¨­è¨, å¯¦ä½, ä»¥åé©è­. <br>å®è½åè¨±æ¥å¸¸ä½¿ç¨èå¨å·æï¤¿å½±åè½çï¨åè£ç½®ä¸ï¤¿è£½å½±çå¾, ï§·å³ä¸ææï¥¡çå®æå£ç¸®ä»¥å<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jE38gi0mb9UJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15379553229406489996&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'jE38gi0mb9UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md39', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md39" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jE38gi0mb9UJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> STUDENTENSUPPORT. BE</h3><div class="gs_a">W YAN, J WEIR, LKS AUF</div><div class="gs_fl"><a href="/scholar?q=related:Dqk1jcCPctEJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15092283358437550350&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'Dqk1jcCPctEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB41" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW41"><a href="http://144.206.159.178/ft/CONF/16428103/16428149.pdf" class=yC48><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 144.206.159.178</span><span class="gs_ggsS">144.206.159.178 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://144.206.159.178/ft/CONF/16428103/16428149.pdf" class=yC47>Shaking video stabilization with content completion</a></h3><div class="gs_a">Y Peng, <a href="/citations?user=tjEfgsEAAAAJ&amp;hl=en&amp;oi=sra">Q Ye</a>, Y Liu, J Jiao - &hellip;  of SPIE, the International Society for  &hellip;, 2009 - 144.206.159.178</div><div class="gs_rs">ABSTRACT A new stabilization algorithm to counterbalance the shaking motion in a video <br>based on classical Kandade-Lucas-Tomasi (KLT) method is presented in this paper. Feature <br>points are evaluated with law of large numbers and clustering algorithm to reduce the side <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9II6lby0CtsJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15783626566123815668&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'9II6lby0CtsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md41', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md41" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:9II6lby0CtsJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/V525L535221103M1.pdf" class=yC49>Video Quality Analysis for Concert Video Mashup Generation</a></h3><div class="gs_a">P Shrestha, H Weda, M Barbieri, P de With - Advanced Concepts for  &hellip;, 2010 - Springer</div><div class="gs_rs">Videos recorded by the audience in a concert provide natural and lively views from different <br>angles. However, such recordings are generally incomplete and suffer from low signal <br>quality due to poor lighting conditions and use of hand-held cameras. It is our objective to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:SwJ0mGARKicJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'SwJ0mGARKicJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> VENTUS. DK</h3><div class="gs_a">W YAN, J WEIR</div><div class="gs_fl"><a href="/scholar?q=related:nL_voShiBdEJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'nL_voShiBdEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/data/Conferences/SPIEP/67607/83990L.pdf" class=yC4A>Adaptive smoothing in real-time image stabilization</a></h3><div class="gs_a"><a href="/citations?user=3tCNLDsAAAAJ&amp;hl=en&amp;oi=sra">S Wu</a>, DC Zhang, Y Zhang&hellip; - Proc. of SPIE  &hellip;, 2012 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">ABSTRACT When using the conventional fixed smoothing factor to display the stabilized <br>video, we have the issue of large undefined black border regions (BBR) when camera is fast <br>panning and zooming. To minimize the size of BBR and also provide smooth visualization <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:o3XKpGKnWBYJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1610220908898317731&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'o3XKpGKnWBYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB45" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW45"><a href="http://www.eurasip.org/Proceedings/Eusipco/Eusipco2011/papers/1569424041.pdf" class=yC4C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurasip.org</span><span class="gs_ggsS">eurasip.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.eurasip.org/Proceedings/Eusipco/Eusipco2011/papers/1569424041.pdf" class=yC4B>Impact of Excitation Frequency on Short-Term Recording Synchronisation and Confidence Estimation</a></h3><div class="gs_a">D Korchagin - Proceedings European Signal Processing Conference  &hellip;, 2011 - eurasip.org</div><div class="gs_rs">ABSTRACT In this paper, we present the results of a study on excitation frequency impact on <br>short-term recording synchronisation and confidence estimation for multisource audiovisual <br>data, recorded by different personal capturing devices during social events. The core of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:5ovwqoKMUvMJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17533230792148290534&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'5ovwqoKMUvMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md45', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md45" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:5ovwqoKMUvMJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB46" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW46"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.114.83&amp;rep=rep1&amp;type=pdf" class=yC4E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.114.83&amp;rep=rep1&amp;type=pdf" class=yC4D>CONTENT PARSING OF HOME VIDEOS BY MOTION ANALYSIS</a></h3><div class="gs_a">åºæ¼éåçå®¶åº­è¦é »å§å®¹åæ - 2005 - Citeseer</div><div class="gs_rs">ABSTRACT Due to the increasing use of hand-held camcorders, an explosion of home <br>video data is already underway. Home videos, by nature, are unedited, unstructured and <br>lack of story-line. They contain unrestricted content domain which usually mixes together <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:e8zOrOdBV_gJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17894844107672439931&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'e8zOrOdBV_gJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md46', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md46" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:e8zOrOdBV_gJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> BOOKBOON. COM</h3><div class="gs_a">W YAN, J WEIR</div><div class="gs_fl"><a href="/scholar?q=related:ygZFwB_L5psJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11233889657752454858&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'ygZFwB_L5psJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://research.microsoft.com/en-us/groups/sds/videowork-final.pdf" class=yC50><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from microsoft.com</span><span class="gs_ggsS">microsoft.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://research.microsoft.com/en-us/groups/sds/videowork-final.pdf" class=yC4F>Understanding Videowork</a></h3><div class="gs_a">D KirkÂ¹, <a href="/citations?user=3UlxG6UAAAAJ&amp;hl=en&amp;oi=sra">A Sellen</a>, R Harper, K Wood - research.microsoft.com</div><div class="gs_rs">ABSTRACT In this paper we elucidate the patterns of behavior of home movie makers <br>through a study of 12 families and a separate focus group of 7 teenagers. Analogous to a <br>similar study of photowork [13], the goal is to provide a deeper understanding of what <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jKrYySOWSyQJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2615349089073146508&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'jKrYySOWSyQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md48', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md48" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jKrYySOWSyQJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://prada-research.net/~svetha/publications/2006/conferences/adams_greenhill_venkatesh_acmmm06.pdf" class=yC52><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from prada-research.net</span><span class="gs_ggsS">prada-research.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1180790" class=yC51>Browsing personal media archives with spatial context using panoramas</a></h3><div class="gs_a"><a href="/citations?user=kbcVlyAAAAAJ&amp;hl=en&amp;oi=sra">B Adams</a>, S Greenhill, <a href="/citations?user=AEkRUQcAAAAJ&amp;hl=en&amp;oi=sra">S Venkatesh</a> - Proceedings of the 14th annual  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract This paper presents novel techniques for using panoramas as spatial context to <br>enhance browsing of personal media archives. This context, scenes where frequent media <br>capture takes place, is present in the disparate photos and videos, but not leveraged by <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:a7VvyuqHOTcJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3979361188246959467&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'a7VvyuqHOTcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB50" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW50"><a href="http://www.tdx.cat/bitstream/handle/10803/33293/TPOE1de1.pdf.txt?sequence=2" class=yC54><span class="gs_ggsL"><span class=gs_ctg2>[TXT]</span> from tdx.cat</span><span class="gs_ggsS">tdx.cat <span class=gs_ctg2>[TXT]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.tdx.cat/handle/10803/33293" class=yC53>Media aesthetics based multimedia storytelling.</a></h3><div class="gs_a">P Obrador Espinosa - 2011 - tdx.cat</div><div class="gs_rs">Since the earliest of times, humans have been interested in recording their life experiences, <br>for future reference and for storytelling purposes. This task of recording experiences--ie, both <br>image and video capture--has never before in history been as easy as it is today. This is <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:EQE_2iYGCLcJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13188798272694583569&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'EQE_2iYGCLcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://207.21.18.5/publications/FXPAL-PR-10-582.pdf" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 207.21.18.5</span><span class="gs_ggsS">207.21.18.5 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874034" class=yC55>NudgeCam: Toward targeted, higher quality media capture</a></h3><div class="gs_a">S Carter, <a href="/citations?user=RWmuEhkAAAAJ&amp;hl=en&amp;oi=sra">J Adcock</a>, J Doherty, S Branham - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract NudgeCam is a mobile application that can help users capture more relevant, <br>higher quality media. To guide users to capture media more relevant to a particular project, <br>third-party template creators can show users media that demonstrates relevant content <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:sQf89sL6lSwJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3212749624473028529&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'sQf89sL6lSwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB52" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW52"><a href="http://pi4.informatik.uni-mannheim.de/~kopf/publications/2006/Kopf_2006d.pdf" class=yC58><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-mannheim.de</span><span class="gs_ggsS">uni-mannheim.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://pi4.informatik.uni-mannheim.de/~kopf/publications/2006/Kopf_2006d.pdf" class=yC57>Verbesserung der QualitÃ¤t von historischen Filmen</a></h3><div class="gs_a">S Kopf, M Knaus - 2006 - pi4.informatik.uni-mannheim.de</div><div class="gs_rs">Zusammenfassung Die UNESCO hat schon frÃ¼h erkannt, dass historische Filme einen <br>wichtigen Baustein in der Erhaltung des kulturellen Erbes darstellen. Durch eine <br>Digitalisierung kÃ¶nnen diese fÃ¼r die Zukunft erhalten werden, ohne dass Filme durch <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:QmaauCLwBEAJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4613075950250911298&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'QmaauCLwBEAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md52', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md52" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:QmaauCLwBEAJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:QmaauCLwBEAJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=15587563684571403239&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/91997x/201107/37094961.html" class=yC59>è§é¢ç¨³åææ¯ç»¼è¿°</a></h3><div class="gs_a">éå¯ç«ï¼ å®å©ï¼ ä½æ¾ç - çµè§ææ¯, 2011 - cqvip.com</div><div class="gs_rs">æçç»´æ®: æçå¸æ·; æçæ¶èå¤¹; æçä½å; æçç²¾éè¾. è´­ç©è½¦; åå¼ä¸­å¿; å®¢æä¸­å¿. é¦é¡µ |<br>ç¥è¯ç»çºªäºº | æåå¤§å¨ | å­¦èç©ºé´ | å­¦æ¯æºæ | ç­ç¹ä¸é¢ | å­¦æ¯è®ºå | è®ºæåè¡¨ |<br>EIåISTPä¼è®® | å½éä¼è®® | æè²å¹è®­. é«çº§æç´¢ | ä¸ä¸æ£ç´¢ | æåæç¨¿. <b>...</b> </div><div class="gs_fl"><a href="/scholar?q=related:grtJ5zRlif0J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18269244641298201474&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'grtJ5zRlif0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> STUDENTENSUPPORT. NL</h3><div class="gs_a">W YAN, J WEIR</div><div class="gs_fl"><a href="/scholar?q=related:UN3X_foQH1oJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6493927857878195536&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'UN3X_foQH1oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
