Total results = 7
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://mmas.comp.nus.edu.sg/63140030.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/j80g848m551677u7.pdf" class=yC0>An eye fixation database for saliency detection in images</a></h3><div class="gs_a"><a href="/citations?user=mUvcmRsAAAAJ&amp;hl=en&amp;oi=sra">S Ramanathan</a>, <a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, <a href="/citations?user=fOmROdkAAAAJ&amp;hl=en&amp;oi=sra">N Sebe</a>, M Kankanhalli&hellip; - Computer VisionâECCV  &hellip;, 2010 - Springer</div><div class="gs_rs">To learn the preferential visual attention given by humans to specific image content, we <br>present NUSEF-an eye fixation database compiled from a pool of 758 images and 75 <br>subjects. Eye fixations are an excellent modality to learn semantics-driven human <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5254088566672117251&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7">Cited by 22</a> <a href="/scholar?q=related:A87uyrZF6kgJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5254088566672117251&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'A87uyrZF6kgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.elec.qmul.ac.uk/people/stefan/theses/thesis-kraisak-final.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from qmul.ac.uk</span><span class="gs_ggsS">qmul.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.elec.qmul.ac.uk/people/stefan/theses/thesis-kraisak-final.pdf" class=yC2>Multi-modal multi-semantic image retrieval</a></h3><div class="gs_a"><a href="/citations?user=DZ3WuzUAAAAJ&amp;hl=en&amp;oi=sra">K Kesorn</a> - 2010 - elec.qmul.ac.uk</div><div class="gs_rs">There are billions of images on the World Wide Web (WWW), which are accessed by many <br>millions of users globally. The continued rapid growth in digital visualisation makes it <br>increasingly difficult to find, organise, access, and maintain users&#39; visual information. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15629014515943679507&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7">Cited by 3</a> <a href="/scholar?q=related:EyY_bOVp5dgJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15629014515943679507&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'EyY_bOVp5dgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:EyY_bOVp5dgJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/205015J07UJ7271X.pdf" class=yC4>Visual vocabulary optimization with spatial context for image annotation and classification</a></h3><div class="gs_a">Z Yang, Y Peng, J Xiao - Advances in Multimedia Modeling, 2012 - Springer</div><div class="gs_rs">In this paper, we propose a new approach of visual vocabulary optimization with spatial <br>context, which contains important spatial information that has not been fully exploited. The <br>novelty of our method mainly lies in two aspects: when spatial information is considered, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11691644846610930656&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7">Cited by 1</a> <a href="/scholar?q=related:4FtFAgsRQaIJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11691644846610930656&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'4FtFAgsRQaIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://eeeweba.ntu.edu.sg/computervision/Research%20Papers/2012/Mining%20Visual%20Collocation%20Patterns%20via%20Self-Supervised%20%20Subspace%20Learning.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6095381" class=yC5>Mining Visual Collocation Patterns via Self-Supervised Subspace Learning</a></h3><div class="gs_a">J Yuan, Y Wu - Systems, Man, and Cybernetics, Part B:  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Traditional text data mining techniques are not directly applicable to image data <br>which contain spatial information and are characterized by high-dimensional visual features. <br>It is not a trivial task to discover meaningful visual patterns from images because the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11909232773069790307&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7">Cited by 1</a> <a href="/scholar?q=related:Y9xD-hsYRqUJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11909232773069790307&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'Y9xD-hsYRqUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0957417411004295" class=yC7>Visual content representation using semantically similar visual words</a></h3><div class="gs_a"><a href="/citations?user=DZ3WuzUAAAAJ&amp;hl=en&amp;oi=sra">K Kesorn</a>, S Chimlek, S Poslad&hellip; - Expert Systems with  &hellip;, 2011 - Elsevier</div><div class="gs_rs">Local feature analysis of visual content, namely using Scale Invariant Feature Transform <br>(SIFT) descriptors, have been deployed in the &#39;bag-of-visual words&#39; model (BVW) as an <br>effective method to represent visual content information and to enhance its classification <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:BiwwePoTHuMJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16365540062390135814&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'BiwwePoTHuMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6035787" class=yC8>An Enhanced Bag-of-Visual Word Vector Space Model to Represent Visual Content in Athletics Images</a></h3><div class="gs_a"><a href="/citations?user=DZ3WuzUAAAAJ&amp;hl=en&amp;oi=sra">K Kesorn</a>, S Poslad - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Images that have a different visual appearance may be semantically related using a <br>higher level conceptualization. However, image classification and retrieval systems tend to <br>rely only on the low-level visual structure within images. This paper presents a framework <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1421296692416773097&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=7">Cited by 2</a> <a href="/scholar?q=related:6Xvwt891uRMJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1421296692416773097&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'6Xvwt891uRMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5582604" class=yC9>Semantically similar visual words discovery to facilitate visual invariance</a></h3><div class="gs_a">S Chimlek, <a href="/citations?user=DZ3WuzUAAAAJ&amp;hl=en&amp;oi=sra">K Kesorn</a>, P Piamsa-Nga&hellip; - Multimedia and Expo ( &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A major limitation of many image classification and retrieval systems is that they <br>only rely on the visual structure within images. However, images that have a different visual <br>appearance may be semantically related at a higher level conceptualization. This paper <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:X30J98TmvuIJ:scholar.google.com/&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16338750231779310943&amp;hl=en&amp;num=7&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'X30J98TmvuIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
