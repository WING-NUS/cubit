Total results = 17
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.cvl.iis.u-tokyo.ac.jp/~siratori/pub/EG2006shiratori.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2006.00964.x/full" class=yC0>DancingâtoâMusic Character Animation</a></h3><div class="gs_a"><a href="/citations?user=YvS3QpkAAAAJ&amp;hl=en&amp;oi=sra">T Shiratori</a>, A Nakazawa, K Ikeuchi - Computer Graphics Forum, 2006 - Wiley Online Library</div><div class="gs_rs">Abstract In computer graphics, considerable research has been conducted on realistic <br>human motion synthesis. However, most research does not consider human emotional <br>aspects, which often strongly affect human motion. This paper presents a new approach <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=633537909590779081&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 44</a> <a href="/scholar?q=related:yeTmRF_HyggJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/03/4F/RN205284173.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=633537909590779081&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'yeTmRF_HyggJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.mybytes.de/papers/moerchen06modelling.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mybytes.de</span><span class="gs_ggsS">mybytes.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1561266" class=yC3>Modeling timbre distance with temporal statistics from polyphonic music</a></h3><div class="gs_a">F Morchen, A Ultsch, M Thies&hellip; - Audio, Speech, and  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Timbre distance and similarity are expressions of the phenomenon that some music <br>appears similar while other songs sound very different to us. The notion of genre is often <br>used to categorize music, but songs from a single genre do not necessarily sound similar <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12008273152375151403&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 39</a> <a href="/scholar?q=related:K1suPc_0paYJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/35/4D/RN182970360.html?source=googlescholar" class="gs_nph" class=yC5>BL Direct</a> <a href="/scholar?cluster=12008273152375151403&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'K1suPc_0paYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://gigapaper.ir/Articles/Most_Downloaded_Papers_from_all_IEEE_Journals/Audio_Speech_and_Language_Pr/Audio_Signal_Representations_for_Indexing_in_the_Transform_Domain-gze.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gigapaper.ir</span><span class="gs_ggsS">gigapaper.ir <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5410060" class=yC6>Audio signal representations for indexing in the transform domain</a></h3><div class="gs_a">E Ravelli, G Richard, L Daudet - Audio, Speech, and Language  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Indexing audio signals directly in the transform domain can potentially save a <br>significant amount of computation when working on a large database of signals stored in a <br>lossy compression format, without having to fully decode the signals. Here, we show that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=702377783790948675&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 15</a> <a href="/scholar?q=related:Q1H8fN9YvwkJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=702377783790948675&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'Q1H8fN9YvwkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://hal.archives-ouvertes.fr/docs/00/60/43/89/PDF/caetano_segmentation_DAFx2010.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/hal-00604389/" class=yC8>Automatic Segmentation of the Temporal Evolution of Isolated Acoustic Musical Instrument Sounds Using Spectro-Temporal Cues</a></h3><div class="gs_a">M Freitas Caetano, JJ Burred&hellip; - Proc. of the 13th Int.  &hellip;, 2010 - hal.archives-ouvertes.fr</div><div class="gs_rs">ABSTRACT The automatic segmentation of isolated musical instrument sounds according to <br>the temporal evolution is not a trivial task. It requires a model capable of capturing regions <br>such as the attack, decay, sustain and release accurately for many types of instruments <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5312439173596346363&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 6</a> <a href="/scholar?q=related:-3cUKUiTuUkJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5312439173596346363&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'-3cUKUiTuUkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://cjc.ict.ac.cn/quanwenjiansuo/2007-05/zyb.zip" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cjc.ict.ac.cn/quanwenjiansuo/2007-05/zyb.zip" class=yCA>åºäºåå®¹çé³é¢ä¸é³ä¹åæç»¼è¿° [J]</a></h3><div class="gs_a">å¼ ä¸å½¬ï¼ å¨æ°ï¼ è¾¹èç¥ºï¼ é­å - è®¡ç®æºå­¦æ¥, 2007 - cjc.ict.ac.cn</div><div class="gs_rs">æè¦æºå¨å¬è§åæ¬ä¸å¤§ç ç©¶é¢å: è¯­é³ä¿¡å·å¤çä¸è¯å«, ä¸è¬é³é¢ä¿¡å·åæ, åºäºåå®¹çé³ä¹ä¿¡å·<br>åæ. å¶ä¸­, è¯­é³ä¿¡å·å¤çä¸è¯å«æ©å·²æä¸ºä¸ä¸ªä¼ ç»çç ç©¶ç­ç¹. éçä¿¡æ¯ç§å­¦ä¸ææ¯çè¿éåå±<br>, åºäºåå®¹çé³é¢ä¸é³ä¹ä¿¡å·åæä¹éæ¸æä¸ºä¸ä¸ªæ°çç ç©¶ç­ç¹, è¿å å¹´æ¥åå¾äºå¤§éç ç©¶<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3219106275905615902&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 14</a> <a href="/scholar?q=related:HriVNRqQrCwJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3219106275905615902&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'HriVNRqQrCwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:HriVNRqQrCwJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://homepage.fudan.edu.cn/weili/files/2011/06/LW-2010SIGIRFull.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fudan.edu.cn</span><span class="gs_ggsS">fudan.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1835554" class=yCC>Robust audio identification for mp3 popular music</a></h3><div class="gs_a">W Li, Y Liu, X Xue - Proceedings of the 33rd international ACM SIGIR  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Audio identification via fingerprint has been an active research field with wide <br>applications for years. Many technical papers were published and commercial software <br>systems were also employed. However, most of these previously reported methods work <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16960941287960898160&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 7</a> <a href="/scholar?q=related:cGoqDT1eYesJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16960941287960898160&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'cGoqDT1eYesJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4023803" class=yCE>Music motive extraction through hanson intervallic analysis</a></h3><div class="gs_a">JF Serrano, JM Inesta - Computing, 2006. CIC&#39;06. 15th  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Music motive extraction is an important concept to consider in music information <br>retrieval. Among the possible applications are the creations of music databases that need of <br>indexing tools and access in a dynamic way, copyright management and plagiarism <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9570264436716232082&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 2</a> <a href="/scholar?q=related:kl0Gvcxo0IQJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9570264436716232082&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'kl0Gvcxo0IQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://search.ieice.org/bin/gbpdf.php?category=D&amp;lang=J&amp;year=2007&amp;fname=j90-d_8_2242&amp;abst=" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ieice.org</span><span class="gs_ggsS">ieice.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://search.ieice.org/bin/gbpdf.php?category=D&amp;lang=J&amp;year=2007&amp;fname=j90-d_8_2242&amp;abst=" class=yCF>é³æ¥½ç¹å¾´ãèæ®ããèè¸åä½ã®èªåçæ</a></h3><div class="gs_a">ç½é³¥è²´äº®ï¼ ä¸­æ¾¤ç¯¤å¿ï¼ æ± ååå² - é»å­æå ±éä¿¡å­¦ä¼è«æèª D, 2007 - search.ieice.org</div><div class="gs_rs">ããã¾ã è¿å¹´ã³ã³ãã¥ã¼ã¿ã°ã©ãã£ãã¯ã¹ã®åéã§ã¯, èªç¶ãªã­ã£ã©ã¯ã¿ã®ã¢ãã¡ã¼ã·ã§ã³ãçæãã<br>ææ³ãæ°å¤ãææ¡ããã¦ãã¦ãã. ããã, äººéã®æ¯èãã®å°è±¡ãå¤§ããå·¦å³ããè¡¨ç¾ãèæ®ããææ³<br>ã¯ã»ã¨ãã©ææ¡ããã¦ããªã. ããã§æ¬è«æã§ã¯, è¡¨ç¾ãéè¦ãªè¦å ã¨ãªãèè¸åä½ãå¯¾è±¡ã¨ãã¦, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4660265276715245284&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 3</a> <a href="/scholar?q=related:5B6z-ZOWrEAJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4660265276715245284&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'5B6z-ZOWrEAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:5B6z-ZOWrEAJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://cjc.ict.ac.cn/eng/qwjse/view.asp?id=2381" class=yC11>A Review of Content-Based Audio and Music Analysis</a></h3><div class="gs_a">YB Zhang, J Zhou, ZQ Bian, J Guo - CHINESE JOURNAL OF  &hellip;, 2007 - cjc.ict.ac.cn</div><div class="gs_rs">Abstract Machine hearing includes three fields: Speech signal processing and recognition, <br>general audio signal processing, and content-based music analysis. Speech signal <br>processing and recognition has been a traditional research field for many years. There are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11924188369424478068&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 1</a> <a href="/scholar?q=related:dO9jGiU6e6UJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/38/5D/RN212307498.html?source=googlescholar" class="gs_nph" class=yC12>BL Direct</a> <a href="/scholar?cluster=11924188369424478068&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'dO9jGiU6e6UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:dO9jGiU6e6UJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5521945" class=yC13>Automatic scene change detection for composed speech and music sound under low snr in compressed domain</a></h3><div class="gs_a">X Yu, C Li, X Xu, S Yang, W Wan - Wireless Mobile and  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the amount of MP3 compressed data increasing, automatic scene change <br>detection is becoming more and more important. Several studies have proposed some <br>interesting approaches. However, none of these techniques analyze the audio signals in a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8733476929448536007&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 1</a> <a href="/scholar?q=related:x2fZ0f6KM3kJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8733476929448536007&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'x2fZ0f6KM3kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.cvl.iis.u-tokyo.ac.jp/papers/all/743.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cvl.iis.u-tokyo.ac.jp/papers/all/743.pdf" class=yC14>è¦³å¯ã«åºã¥ãé³æ¥½ããã³ã¢ã¼ã·ã§ã³ã­ã£ããã£ãã¼ã¿ããã®èè¸åä½çæææ³</a></h3><div class="gs_a">ä¸­æ¾¤ç¯¤å¿ï¼ ç½é³¥è²´äº®ï¼ æ± ååå² - ç»åã®èªè­ã»çè§£ã·ã³ãã¸ã¦ã  ( &hellip;, 2005 - cvl.iis.u-tokyo.ac.jp</div><div class="gs_rs">ããã¾ã æ¬è«æã¯, é³æ¥½ãå¥åããã¨, ããã«åã£ãã­ã£ã©ã¯ã¿ã¼ã¢ãã¡ã¼ã·ã§ã³ãçæããææ³ã<br>ææ¡ãã¦ãã. æ¬ææ³ã¯ã¢ã¼ã·ã§ã³ã­ã£ããã£ãã¼ã¿ã®è¦³å¯ã«åºã¥ãåä½è§£æ, é³æ¥½è§£æ, <br>ããã³è§£æã«åºã¥ãåä½çæææ³ã«ãã£ã¦æ§æããã. åä½è§£æé¨åã§ã¯, ã¢ã¼ã·ã§ã³ã­ã£ããã£ã¼<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5205889945280204894&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 2</a> <a href="/scholar?q=related:XlA0wFIJP0gJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'XlA0wFIJP0gJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:XlA0wFIJP0gJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Music-Driven Dance Synthesis by Multimodal Dance Performance Analysis</h3><div class="gs_a">Y Demir - 2008 - KoÃ§ University</div><div class="gs_fl"><a href="/scholar?q=related:7dn-VyF6fckJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'7dn-VyF6fckJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5301778" class=yC16>Audio segmentation in AAC domain for content analysis</a></h3><div class="gs_a">R Zhu, H Ai, R Hu - &hellip;  and Mobile Computing, 2009. WiCom&#39;09.  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We focus the attention on the audio scene segmentation in AAC domain for audio-<br>based multimedia indexing and retrieval applications. In particular, a MFCC extraction <br>method is proposed, which is adaptive to the window switch in AAC encoding process, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:kSUFcO8CYngJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8674499059668821393&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'kSUFcO8CYngJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8013229&amp;id=c0jtAQAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC17>Automatic creation of thumbnails for music videos</a></h3><div class="gs_a">C Xu - US Patent 8,013,229, 2011 - Google Patents</div><div class="gs_rs">There is provided a method for automatically creating a music video thumbnail (50) from a <br>music video signal (12). The music video signal is separated into a music signal (16) and a <br>video signal (18). The music signal is analysed by detecting similarity regions and the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:joE1fTcSfmEJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7025072498277712270&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'joE1fTcSfmEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5784292" class=yC18>Fast Audio Feature Extraction From Compressed Audio Data</a></h3><div class="gs_a"><a href="/citations?user=tPlneaIAAAAJ&amp;hl=en&amp;oi=sra">G Schuller</a>, M Gruhne, T Friedrich - Selected Topics in Signal  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We describe an efficient system, which directly extracts features from compressed <br>audio material. It consists of a time-frequency conversion method and a feature extraction <br>algorithm. The conversion method provides the feature extraction algorithm with a suitable <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:bJNfz_MGBFMJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5981913849280828268&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'bJNfz_MGBFMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/95389x/1997008/2608268.html" class=yC19>ä½¿ç¨åçè°±ç¸åçå®æ¶èååæ¢ç¸å³å¨ä½å¤ç®æ æ£æµ</a></h3><div class="gs_a">é»ç®çï¼ èµè¹å¯ - ä¸­å½æ¿å, 1997 - cqvip.com</div><div class="gs_rs">æåºä¸ç§ä½¿ç¨åçè°±ç¸åçå®æ¶èååæ¢ç¸å³å¨ä½ç®æ æ£æµ. ä½¿ç¨è¿ç§æ¹æ³, <br>è¾å¥é¢æä¸¤å¥èåå¾å. å¨ç¬¬äºèåå¾åä¸­, åèå¾åæ¯å¯¹æ¯åº¦åè½¬ç. ä½¿è¡å°åæ å¯¹è¿ä¸¤èå<br>åçè°±ä½ç¸åå¤ç, è¿ç§æ¹æ³å¯å¤§å¤§æå¶è¾åºé¢çç´æµé¡¹åä¸ç¸è¦çä¼ªç¸å³å³°, å¢å¼ºç¸å³å³°ç<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:EutYCy41UGsJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7732739032070286098&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'EutYCy41UGsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.joca.cn/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=11396" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from joca.cn</span><span class="gs_ggsS">joca.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.joca.cn/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=11396" class=yC1A>MP3 åç¼©åä¸­è¯­é³åå²çç ç©¶ä¸å®ç°</a></h3><div class="gs_a">å¸¸è¾½è±«ï¼ å¤¯å°æ¸ï¼ ä¸æºæ ¹ï¼ ææè²ï¼ è®¸éªç¼ - Journal of ComputeT  &hellip;, 2009 - joca.cn</div><div class="gs_rs">æè¦: éå¯¹è¯´è¯äººæ¹åç¹æ£æµé®é¢, å¨MP3 æ ¼å¼ä¸ç¨æ¹è¿åBIC ç®æ³å®ç°äºå¤è¯èæ¹åç¹çæ£æµ<br>o æ ¹æ®éåç¼©åä¸­MFCC çæ±åè¿ç¨, æåºäºä¸ç§å¨åç¼©åMP3 æ ¼å¼ä¸å©ç¨MDCT <br>ç³»æ°è®¡ç®MFCC ç¹å¾åæ°çæ°æ¹æ³o å¨æ­¤åºç¡ä¸, ä½¿ç¨æ¹è¿åBIC æ¹åç¹æ£æµç®æ³æ£æµè¯´è¯äºº<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9OVIkVtEC2wJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7785391540910220788&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'9OVIkVtEC2wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
