Total results = 21
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.6804&amp;rep=rep1&amp;type=pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/data/Conferences/SPIEP/22452/244_1.pdf" class=yC0>Discovery and fusion of salient multimodal features toward news story segmentation</a></h3><div class="gs_a"><a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">W Hsu</a>, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a>, <a href="/citations?user=-YFevSIAAAAJ&amp;hl=en&amp;oi=sra">CW Huang</a>&hellip; - Proceedings &hellip;, 2004 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">ABSTRACT In this paper, we present our new results in news video story segmentation and <br>classification in the context of TRECVID video retrieval benchmarking event 2003. We <br>applied and extended the Maximum Entropy statistical model to effectively fuse diverse <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1064727941698658781&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 59</a> <a href="/scholar?q=related:3aFh-2usxg4J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/43/03/RN146760544.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=1064727941698658781&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 18 versions</a> <a onclick="return gs_ocit(event,'3aFh-2usxg4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://dro.deakin.edu.au/eserv/DU:30023258/tjondronegoro-contentbased-2005.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from deakin.edu.au</span><span class="gs_ggsS">deakin.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101362" class=yC3>Content-based video indexing for sports applications using integrated multi-modal approach</a></h3><div class="gs_a"><a href="/citations?user=_OblDDgAAAAJ&amp;hl=en&amp;oi=sra">D Tjondronegoro</a>, <a href="/citations?user=Vt5edEkAAAAJ&amp;hl=en&amp;oi=sra">YPP Chen</a>, B Pham - Proceedings of the 13th annual  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract To sustain an ongoing rapid growth of video information, there is an emerging <br>demand for a sophisticated content-based video indexing system. However, current video <br>indexing solutions are still immature and lack of any standard. This doctoral consists of a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6490305720990925273&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 24</a> <a href="/scholar?q=related:2YFZh6oyEloJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6490305720990925273&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 18 versions</a> <a onclick="return gs_ocit(event,'2YFZh6oyEloJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:2YFZh6oyEloJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=15416151159179791011&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://scholarbank.nus.edu.sg/bitstream/handle/10635/14865/ChaisornL.pdf?sequence=1" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu.sg/handle/10635/14865" class=yC5>A Hierarchical Multi-Modal approach to story segmentation in news video</a></h3><div class="gs_a">L Chaisorn - 2005 - scholarbank.nus.edu.sg</div><div class="gs_rs">This research presents a multi-modal two-level framework for news story segmentation. We <br>divide our system into two levels: shot level that classifies the input video shots into one of <br>the predefined categories using a hybrid of heuristic and learning based approaches; and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7879365373768026534&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 7</a> <a href="/scholar?q=related:psn0jg8hWW0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7879365373768026534&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'psn0jg8hWW0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1352017" class=yC7>A scalable and extensible segment-event-object-based sports video retrieval system</a></h3><div class="gs_a"><a href="/citations?user=_OblDDgAAAAJ&amp;hl=en&amp;oi=sra">D Tjondronegoro</a>, <a href="/citations?user=Vt5edEkAAAAJ&amp;hl=en&amp;oi=sra">YPP Chen</a>, A Joly - ACM Transactions on Multimedia  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Sport video data is growing rapidly as a result of the maturing digital technologies <br>that support digital video capture, faster data processing, and large storage. However,(1) <br>semi-automatic content extraction and annotation,(2) scalable indexing model, and (3) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9110262179265373003&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 6</a> <a href="/scholar?q=related:S-PJDS8nbn4J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9110262179265373003&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'S-PJDS8nbn4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/90287a/200711/25873702.html" class=yC8>æ°é»è§é¢æäºåååå²ææ¯ç»¼è¿°</a></h3><div class="gs_a">åä¸­ï¼ å¼ æ¥ç°ï¼ èè²æº - ä¸­å½å¾è±¡å¾å½¢å­¦æ¥, 2007 - cqvip.com</div><div class="gs_rs">æ°é»è§é¢çæäºåååå²ä¸è¬éç¨ç»è®¡å­¦æèä¿¡æ¯æ²¦çæ¹æ³, å°æ°é»èç®åå²æä¸ç³»åæåèª<br>ä¸»é¢åå®¹çæäºåå. è¿äºåååæ çæ¯è§é¢æµçé«å±è¯­ä¹, æ¯å»ºç«è§é¢ç´¢å¼çæä½³å±æ¬¡. <br>è¯¥æå¯¹è¿ä¸ææ¯è¿è¡äºç»¼è¿°, å°ç°ææ¹æ³æ ¹æ®å©ç¨ä¿¡æ¯çè§åº¦åä¸º3 ç±»: åæ¨¡æçåå²æ¹æ³, å¤<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14723120262307448603&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 6</a> <a href="/scholar?q=related:GwcxRN8HU8wJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14723120262307448603&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'GwcxRN8HU8wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://eprints.qut.edu.au/archive/00004940/01/4940_1.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from qut.edu.au</span><span class="gs_ggsS">qut.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://eprints.qut.edu.au/4940/" class=yC9>Extraction and classification of self-consumable sport video highlights using generic HMM</a></h3><div class="gs_a"><a href="/citations?user=_OblDDgAAAAJ&amp;hl=en&amp;oi=sra">DW Tjondronegoro</a>, <a href="/citations?user=Vt5edEkAAAAJ&amp;hl=en&amp;oi=sra">YPP Chen</a>, BL Pham - 2005 - eprints.qut.edu.au</div><div class="gs_rs">This paper aims to automatically extract and classify self-consumable sport video highlights. <br>For this purpose, we will emphasize the benefits of using play-break sequences as the <br>effective inputs for HMM-based classifier. HMM is used to model the stochastic pattern of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3787580828061743828&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:1DqzFrIwkDQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3787580828061743828&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'1DqzFrIwkDQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5365789" class=yCB>News Video Story Segmentation Based on NaÃ¯ve Bayes Model</a></h3><div class="gs_a">W Jianping, P Tianqiang&hellip; - &hellip;  Computation, 2009. ICNC&#39; &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Story boundary detection is the foundation of content based news video retrieval. In <br>this paper, Naive Bayes Model, which has been successfully used in multi-modal feature <br>fusion, is implemented in news video story segmentation. Firstly, we get candidate <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16960127135758115259&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 2</a> <a href="/scholar?q=related:u2FTeMV5XusJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16960127135758115259&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'u2FTeMV5XusJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.ntu.edu.sg/home/dclyu/paperfiles/J02_2008_DSS_dcLyu.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0167923607001303" class=yCC>Cross-lingual audio-to-text alignment for multimedia content management</a></h3><div class="gs_a">DC Lyu, RY Lyu, YC Chiang, <a href="/citations?user=1ZO4t_AAAAAJ&amp;hl=en&amp;oi=sra">CN Hsu</a> - Decision Support Systems, 2008 - Elsevier</div><div class="gs_rs">This paper addresses a content management problem in situations where we have a <br>collection of spoken documents in audio stream format in one language and a collection of <br>related text documents in another. In our case, we have a huge digital archive of audio <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9789402137456041562&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:Wl6EpmDx2ocJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9789402137456041562&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'Wl6EpmDx2ocJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://spiedigitallibrary.org/data/Conferences/SPIEP/20859/67904G_1.pdf" class=yCE>News video story segmentation method using fusion of audio-visual features</a></h3><div class="gs_a">J Wen, P Zeng, X Luan, Y Xie - Proceedings of SPIE, the  &hellip;, 2007 - spiedigitallibrary.org</div><div class="gs_rs">ABSTRACT News story segmentation is an important aspect for news video analysis. This <br>paper presents a method for news video story segmentation. Different form prior works, <br>which base on visual features transform, the proposed technique uses audio features as <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5576160282545253440&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:QGzAszuAYk0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/37/3F/RN223327057.html?source=googlescholar" class="gs_nph" class=yCF>BL Direct</a> <a href="/scholar?cluster=5576160282545253440&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'QGzAszuAYk0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://poseidon.csd.auth.gr/papers/PUBLISHED/CONFERENCE/pdf/Cherif07a.pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from auth.gr</span><span class="gs_ggsS">auth.gr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4555491" class=yC10>Shot type identification of movie content</a></h3><div class="gs_a">I Cherif, V Solachidis, <a href="/citations?user=lWmGADwAAAAJ&amp;hl=en&amp;oi=sra">I Pitas</a> - Signal Processing and Its  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper aims at providing a quantitative description of shot types commonly used <br>in movie productions. Only qualitative descriptions are available in the literature and even <br>these are subject to various naming conventions. A vocabulary is fixed and human body-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17938268653832913038&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:jkA8tUyI8fgJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17938268653832913038&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'jkA8tUyI8fgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://homepage.cs.latrobe.edu.au/ypchen/My_Papers_eCopies/knowledgebasedsys.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from latrobe.edu.au</span><span class="gs_ggsS">latrobe.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0950705106000943" class=yC12>Using object and trajectory analysis to facilitate indexing and retrieval of video</a></h3><div class="gs_a">C Lopez, <a href="/citations?user=Vt5edEkAAAAJ&amp;hl=en&amp;oi=sra">YPP Chen</a> - Knowledge-based systems, 2006 - Elsevier</div><div class="gs_rs">This paper aims to show that by using low level feature extraction, motion and object <br>identifying and tracking methods, features can be extracted and indexed for efficient and <br>effective retrieval for video; such as an awards ceremony video. Video scene/shot analysis <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12969737400345881824&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:4Oy28XLD_bMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12969737400345881824&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'4Oy28XLD_bMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://www.ecice06.com/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=10836" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ecice06.com</span><span class="gs_ggsS">ecice06.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ecice06.com/CN/article/downloadArticleFile.do?attachType=PDF&amp;id=10836" class=yC14>åºäºæ´ç´ è´å¶æ¯æ¨¡åçæ°é»æäºåå²æ¹æ³</a></h3><div class="gs_a">å½­å¤©å¼ºï¼ æå¼¼ç¨ - Computer Engineering, 2009 - ecice06.com</div><div class="gs_rs">æè¦: æåºä¸ç§åºäºæ´ç´ è´å¶æ¯æ¨¡åçæ°é»è§é¢æäºåå²æ¹æ³. éè¿å¯¹æ°é»è§é¢è¿è¡éå¤´æ£æµ, <br>è·å¾åéæäºè¾¹çç¹, ä»åéè¾¹çç¹å¨å´éå¤´æåå¤æ¨¡æä¸­çº§ç¹å¾, å½¢æå±æ§éåä½ä¸ºè¾å¥, <br>åºç¨æ´ç´ è´å¶æ¯æ¨¡åå¯¹åéè¾¹çç¹è¿è¡åç±»åå¯¹ç»æè¿è¡åå¤ç, å¾å°æ°é»æäº. å®éªç»æ<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2641775582126664554&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:auv1LOV4qSQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2641775582126664554&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'auv1LOV4qSQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:auv1LOV4qSQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/96163x/200905/31591405.html" class=yC16>åºäºç´æ¹å¾åäº¤åçµçææ¬å¾åè¯å«æ¹æ³</a></h3><div class="gs_a">é­å¿åï¼ å½­å¤©å¼ºï¼ æå¼¼ç¨ - æ°æ®ééä¸å¤ç, 2009 - cqvip.com</div><div class="gs_rs">ä»äººçè®¤ç¥ç¹æ§åºå, æåºäºä¸ç§åºäºäº¤åçµçææ¬å¾åè¯å«æ¹æ³. é¦å, æåå¾ååå®çå¹³æ»<br>å¾åç»åèµ·æ¥ææå¹¿ä¹å¾å, å¹¿ä¹å¾åçç´æ¹å¾ç§°ä¸ºå¹¿ä¹ç´æ¹å¾; å¶æ¬¡, å©ç¨å¹¿ä¹ç´æ¹å¾è®¡ç®å¹¿ä¹<br>å¾åäºå¼åååçäº¤åçµ, å¾å°å¾åæä½³äºå¼åé­å¼; æå, ç±æä½³äºå¼åéå¼ä¸çäº¤åçµåè®¾<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2442337262577139450&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:-j6c3czs5CEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2442337262577139450&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'-j6c3czs5CEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://homepage.cs.latrobe.edu.au/ypchen/My_Papers_eCopies/DianChenJolyFinal_ACMTOMMCAP.doc" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[DOC]</span> from latrobe.edu.au</span><span class="gs_ggsS">latrobe.edu.au <span class=gs_ctg2>[DOC]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[DOC]</span><span class="gs_ct2">[DOC]</span></span> <a href="http://homepage.cs.latrobe.edu.au/ypchen/My_Papers_eCopies/DianChenJolyFinal_ACMTOMMCAP.doc" class=yC17>Dian tjondronegoro Queensland University of Technology, Australia Yi-Ping Phoebe Chen Deakin University, Australia and</a></h3><div class="gs_a">A Joly - homepage.cs.latrobe.edu.au</div><div class="gs_rs">Sport video data is growing rapidly as a result of the maturing digital technologies that <br>support digital video capture, faster data processing, and large storage. However,(1) semi-<br>automatic content extraction and annotation,(2) scalable indexing model, and (3) effective <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:tYr2H-mjM2wJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'tYr2H-mjM2wJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md13', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md13" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:tYr2H-mjM2wJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://eprints.qut.edu.au/archive/00002199/01/PhDThesis_Tjondronegoro.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from qut.edu.au</span><span class="gs_ggsS">qut.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://eprints.qut.edu.au/archive/00002199" class=yC19>PhD Thesis:&quot; Content-based Video Indexing for Sports Applications using Multi-modal approach&quot;</a></h3><div class="gs_a"><a href="/citations?user=_OblDDgAAAAJ&amp;hl=en&amp;oi=sra">DW Tjondronegoro</a> - 2005 - eprints.qut.edu.au</div><div class="gs_rs">Triggered by technology innovations, there has been a huge increase in the utilization of <br>video, as one of the most preferred types of media due to its content richness, for many <br>significant applications. To sustain an ongoing rapid growth of video information, there is <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:k7wHTZYfh10J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6739390097781144723&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'k7wHTZYfh10J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www3.ntu.edu.sg/home/dclyu/paperfiles/10_2005_taai_dcLyu.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www3.ntu.edu.sg/home/dclyu/paperfiles/10_2005_taai_dcLyu.pdf" class=yC1B>A Bi-lingual TV News-to-Document Indexing System</a></h3><div class="gs_a">DC Lyu, RY Lyu, Y Chiang, <a href="/citations?user=1ZO4t_AAAAAJ&amp;hl=en&amp;oi=sra">CN Hsu</a> - ntu.edu.sg</div><div class="gs_rs">ABSTRACT Based on a bi-lingual speech recognition engine and a parallel text alignment <br>technique, we initially developed an audio search engine for indexing bilingual broadcast <br>news to documents found on the World Wide Web (WWW). First, the automatic audio <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:do2Hrw1e56gJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12170799930864536950&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'do2Hrw1e56gJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md15', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md15" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:do2Hrw1e56gJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=828045" class=yC1D>Video shot classification with concept detection</a></h3><div class="gs_a">Z Ji, Y Su - International Symposium on  &hellip;, 2007 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract It is a challenging work to classify video shots into a predefined genre set according <br>to their semantic contents, which is helpful to video indexing, summarization and retrieval. <br>This research proposes a novel shot classification algorithm with concept detection for <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:nnCXtltYwrsJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/35/4B/RN223377322.html?source=googlescholar" class="gs_nph" class=yC1E>BL Direct</a> <a href="/scholar?cluster=13529473381503037598&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'nnCXtltYwrsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://theses.gla.ac.uk/2132/01/2010hopfgartner1phd.pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gla.ac.uk</span><span class="gs_ggsS">gla.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://theses.gla.ac.uk/2132/01/2010hopfgartner1phd.pdf" class=yC1F>Personalized Video Retrieval: Application of Implicit Feedback and Semantic User Profiles</a></h3><div class="gs_a"><a href="/citations?user=tiASHnwAAAAJ&amp;hl=en&amp;oi=sra">F Hopfgartner</a> - SIGIR Forum, 2010 - theses.gla.ac.uk</div><div class="gs_rs">Abstract A challenging problem in the user profiling domain is to create profiles of users of <br>retrieval systems. This problem even exacerbates in the multimedia domain. Due to the <br>Semantic Gap, the difference between low-level data representation of videos and the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:vXrMzuDouqsJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12374458978393684669&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'vXrMzuDouqsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md17', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md17" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:vXrMzuDouqsJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://www.dl.kuis.kyoto-u.ac.jp/papers/2004/doc/mthesis-hayashi.pdf" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kyoto-u.ac.jp</span><span class="gs_ggsS">kyoto-u.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.dl.kuis.kyoto-u.ac.jp/papers/2004/doc/mthesis-hayashi.pdf" class=yC21>ÃÃÃ Â¹Ã ÃÃÃ ÃÃÃ Ã Ã ÃÃÃÃ Ã Ã ÃÃ Ã Ã Ã ÃÃ ÃÃÃ ÃÃ Ã Ã Ã ÃÃ ÃÃÃ ÃÃ</a></h3><div class="gs_a">H HAYASHI - dl.kuis.kyoto-u.ac.jp</div><div class="gs_rs">To discover topic changes in news articles, relations between articles must be analyzed. <br>Most of existing approaches analyze the relations between articles at the level of keyword <br>frequency. Moreover, they trace changes of words themselves included in articles, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:q7ZT3frZsGMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'q7ZT3frZsGMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md18', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md18" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:q7ZT3frZsGMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Discovery and Fusion of Salient Multi-modal Features towards News Story Segmentation</h3><div class="gs_a">G Iyengar</div><div class="gs_fl"><a href="/scholar?q=related:BxCJ50-8boUJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'BxCJ50-8boUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> First Draft und Diskussionsgrundlage</h3><div class="gs_a">B Baumann, C Meinel, H Sack, C Willems&hellip; - 2009</div><div class="gs_fl"><a href="/scholar?q=related:tUZEi6_TG98J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'tUZEi6_TG98J')" href="#" class="gs_nph">Cite</a></div></div></div>
