Total results = 6
<div class="gs_r" style="z-index:400"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396484" class=yC0>Analyzing social media via event facets</a></h3><div class="gs_a">Z Wang, P Cui, <a href="/citations?user=u0xUDSoAAAAJ&amp;hl=en&amp;oi=sra">L Xie</a>, <a href="/citations?user=LpKK_lkAAAAJ&amp;hl=en&amp;oi=sra">H Chen</a>, W Zhu&hellip; - Proceedings of the 20th  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Microblog is a prominent information platform for sharing experiences, discussing <br>current events, and exchanging ideas. Many events are first reported in social media, and <br>increasing amounts of rich-media content are associated with the posts, making them <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'QfyiZKcuNAEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168412003738" class=yC1>Accurate off-line query expansion for large-scale mobile visual search</a></h3><div class="gs_a">K Gao, Y Zhang, D Zhang, S Lin - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract Mobile visual search is a new class of applications that use images taken by <br>camera phone to initiate search queries. It is a very challenging task mainly because of <br>image affine transformations caused by viewpoints changes, and motion blur due to hand <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'jGqqcFaDOSwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231213000234" class=yC2>Jing Zhang, Lei Sui, Li Zhuo, Zhenwei Li, Yuncong Yang</a></h3><div class="gs_a">J Zhang, L Sui, L Zhuo, Z Li, Y Yang - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract Bag-of-words (BoW) model has been widely used in pornographic images <br>recognition and filtering. Most of existing methods create BoW from images with scale-<br>invariant feature transform (SIFT) descriptor in the pixel domain. These methods require <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'UDxciQ8T6uMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv12.papers/ntt.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv12.papers/ntt.pdf" class=yC3>TRECVid 2012 Semantic Video Concept Detection by NTT-MD-DUT</a></h3><div class="gs_a">Y Sun, K Sudo, Y Taniguchi, H Li, L Yi, Y Guan - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT In this paper, we describe the TRECVid 2012 video concept detection system <br>first developed at the NTT Media Intelligence Laboratories in collaboration with Dalian <br>University of Technology. For this year&#39;s task, we adopted a subspace partition based <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'uABxxg6aogcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:uABxxg6aogcJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://lshao.staff.shef.ac.uk/pub/MotionStructure_TCSVT2012.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from shef.ac.uk</span><span class="gs_ggsS">shef.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://lshao.staff.shef.ac.uk/pub/MotionStructure_TCSVT2012.pdf" class=yC5>Embedding Motion and Structure Features for Action Recognition</a></h3><div class="gs_a">X Zhen, L Shao, D Tao, X Li - 2012 - lshao.staff.shef.ac.uk</div><div class="gs_rs">AbstractâWe propose a novel method to model human actions by explicitly coding motion <br>and structure features that are separately extracted from video sequences. Firstly, the motion <br>template (one feature map) is applied to encode the motion information and image planes <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'GQ8X2SbYPwwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:GQ8X2SbYPwwJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/91690x/201227/43276680.html" class=yC7>å¤è§é¢æè¦ææ¯: æ¹æ³, åºç¨åææ</a></h3><div class="gs_a">åä¸­ï¼ èè²æºï¼ åºå½¦ä¼ - è®¡ç®æºå·¥ç¨ä¸åºç¨, 2012 - cqvip.com</div><div class="gs_rs">å¤è§é¢æè¦ææ¯è¿å¹´æ¥åå°äºå½åå¤å­¦èçå¹¿æ³å³æ³¨, å®æ¯æéè¿å¯¹è§é¢ç»æååå®¹çåæ, <br>ä»å¤ä¸ªç¸å³è§é¢æä»¶ä¸­æååºææä¹çé¨å, å°å®ä»¬ä»¥ä¸å®æ¹å¼è¿è¡ç»å, å½¢æç®æ´ç, <br>è½å¤ååè¡¨ç°è¯­ä¹åå®¹çæ¦è¦, ç®çæ¯æä¾å¿«æ·çæµè§åæ¥è¯¢æå¡. ç®åè¯¥ææ¯è¿å¤äºèµ·æ­¥<b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'JlHoS-uMJQMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
