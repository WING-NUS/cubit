Total results = 4
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/harish-ISM2011.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6123364" class=yC0>Affective video summarization and story board generation using pupillary dilation and eye gaze</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, K Yadati, M Kankanhalli&hellip; - Multimedia (ISM), 2011  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We propose a semi-automated, eye-gaze based method for affective analysis of <br>videos. Pupillary Dilation (PD) is introduced as a valuable behavioural signal for <br>assessment of subject arousal and engagement. We use PD information for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7986207911324922454&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=4">Cited by 1</a> <a href="/scholar?q=related:VrLvWMi11G4J:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7986207911324922454&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'VrLvWMi11G4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.comp.nus.edu.sg/~harishk/homepage_material/research_description.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.comp.nus.edu.sg/~harishk/homepage_material/research_description.pdf" class=yC2>Brief summary of work done during PhD</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a> - comp.nus.edu.sg</div><div class="gs_rs">The focus of my PhD thesis has been to get a better understanding of visual perception and <br>attention as people interact with digital images and video. My first problem was on finding <br>how low level global and local information in images influence category discrimination <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:wlNrPDqYTssJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'wlNrPDqYTssJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:wlNrPDqYTssJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/harish-mm11.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072406" class=yC4>Eye-tracking methodology and applications to images and video</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, M Kankanhalli - Proceedings of the 19th ACM international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Our tutorial introduces eye-tracking as an exciting, non-intrusive method of <br>capturing user attention during human interaction with digital images and videos. We <br>believe eye-gaze can play a valuable role in understanding and processing (a) huge <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9cC-iDePvZoJ:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11150225721119391989&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'9cC-iDePvZoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://137.132.14.55/handle/10635/33304" class=yC6>Human Visual Perception, study and applications to understanding Images and Videos</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a> - 2011 - 137.132.14.55</div><div class="gs_rs">Assessing whether a photograph is interesting, or spotting people in conversation or <br>important objects in an images and videos, are visual tasks that we humans do effortlessly <br>and in a robust manner. In this thesis I first explore and quantify how humans distinguish <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jna1s1LyMH0J:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9020976490639357582&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'jna1s1LyMH0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jna1s1LyMH0J:scholar.google.com/&amp;hl=en&amp;num=4&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
