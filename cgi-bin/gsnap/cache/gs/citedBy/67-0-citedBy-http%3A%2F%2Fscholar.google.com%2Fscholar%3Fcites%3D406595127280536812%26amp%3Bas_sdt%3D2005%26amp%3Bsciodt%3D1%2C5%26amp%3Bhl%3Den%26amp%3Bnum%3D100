Total results = 67
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://kusu.comp.nus.edu/proceedings/mm09/wsmc/p1.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631137" class=yC0>Visual tag dictionary: interpreting tags with visual words</a></h3><div class="gs_a"><a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=g2gAY_0AAAAJ&amp;hl=en&amp;oi=sra">K Yang</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, HJ Zhang - &hellip;  of the 1st workshop on Web- &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Visual-word based image representation has shown effectiveness in a wide variety <br>of applications such as categorization, annotation and search. By detecting keypoints in <br>images and treating their patterns as visual words, an image can be represented as a bag <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13684631048843383814&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 22</a> <a href="/scholar?q=related:BlSXRG-T6b0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13684631048843383814&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'BlSXRG-T6b0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://137.132.145.151/lms/sites/default/files/mm10-richang.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874013" class=yC2>Dynamic captioning: video accessibility enhancement for hearing impairment</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, M Xu, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, TS Chua - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract There are more than 66 million people suÂ® ering from hearing impairment and this <br>disability brings them diÂ±culty in the video content understanding due to the loss of audio <br>information. If scripts are available, captioning technology can help them in a certain <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14295702409738921748&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 25</a> <a href="/scholar?q=related:FCcvi5mJZMYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14295702409738921748&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'FCcvi5mJZMYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.eurecom.fr/~troncy/Publications/Troncy-icmr11.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurecom.fr</span><span class="gs_ggsS">eurecom.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1992054" class=yC4>Finding media illustrating events</a></h3><div class="gs_a"><a href="/citations?user=-BFEdeMAAAAJ&amp;hl=en&amp;oi=sra">X Liu</a>, <a href="/citations?user=1BxhcigAAAAJ&amp;hl=en&amp;oi=sra">R Troncy</a>, <a href="/citations?user=KdPSGmAAAAAJ&amp;hl=en&amp;oi=sra">B Huet</a> - Proceedings of the 1st ACM International  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract We present a method combining semantic inferencing and visual analysis for <br>finding automatically media (photos and videos) illustrating events. We report on <br>experiments validating our heuristic for mining media sharing platforms and large event <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4393278562019656302&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 20</a> <a href="/scholar?q=related:bkKFWJMP-DwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4393278562019656302&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'bkKFWJMP-DwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://lms.comp.nus.edu.sg/papers/media/2010/tist-jinhui.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1899418" class=yC6>Image annotation by k NN-sparse graph-based label propagation over noisily tagged web images</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, TS Chua, <a href="/citations?user=Nut-uvoAAAAJ&amp;hl=en&amp;oi=sra">GJ Qi</a>&hellip; - ACM Transactions on  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract In this article, we exploit the problem of annotating a large-scale image corpus by <br>label propagation over noisily tagged web images. To annotate the images more accurately, <br>we propose a novel kNN-sparse graph-based semi-supervised learning approach for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13238107845729306771&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 28</a> <a href="/scholar?q=related:k8gBe_I0t7cJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13238107845729306771&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'k8gBe_I0t7cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://lms.comp.nus.edu.sg/papers/media/2010/mm10-gaoyue.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1873970" class=yC8>W2go: a travel guidance system by automatic landmark ranking</a></h3><div class="gs_a">Y Gao, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, Q Dai, TS Chua&hellip; - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we present a travel guidance system W2Go (Where to Go), which can <br>automatically recognize and rank the landmarks for travellers. In this system, a novel <br>Automatic Landmark Ranking (ALR) method is proposed by utilizing the tag and geo-tag <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14138977723431265510&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 20</a> <a href="/scholar?q=related:5iRI9FO9N8QJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14138977723431265510&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'5iRI9FO9N8QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.cais.ntu.edu.sg/~assourav/papers/TagRep-MM-2010.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874029" class=yCA>Quantifying tag representativeness of visual content of social images</a></h3><div class="gs_a"><a href="/citations?user=wyKGVKUAAAAJ&amp;hl=en&amp;oi=sra">A Sun</a>, <a href="/citations?user=o0F3sqEAAAAJ&amp;hl=en&amp;oi=sra">SS Bhowmick</a> - Proceedings of the international conference on  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Social tags describe images from many aspects including the visual content <br>observable from the images, the context and usage of images, user opinions and others. Not <br>all tags are therefore useful for image search and are appropriate for tag recommendation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9270399511018333765&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 12</a> <a href="/scholar?q=related:ReYzFD0Tp4AJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9270399511018333765&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ReYzFD0Tp4AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.cs.clemson.edu/~jzwang/1201863/mm2010/p35-chen.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1873959" class=yCC>Efficient large-scale image annotation by probabilistic collaborative multi-label propagation</a></h3><div class="gs_a">X Chen, <a href="/citations?user=Fqqx4HsAAAAJ&amp;hl=en&amp;oi=sra">Y Mu</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, TS Chua - Proceedings of the international  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Annotating large-scale image corpus requires huge amount of human efforts and is <br>thus generally unaffordable, which directly motivates recent development of semi-<br>supervised or active annotation methods. In this paper we revisit this notoriously <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17532080644981185834&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 17</a> <a href="/scholar?q=related:Kt2aKXV2TvMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17532080644981185834&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'Kt2aKXV2TvMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.ee.columbia.edu/~yjiang/publication/civr10_sampling.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816051" class=yCE>On the sampling of web images for learning visual concept classifiers</a></h3><div class="gs_a">S Zhu, G Wang, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a> - Proceedings of the ACM  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Visual concept learning often requires a large set of training images. In practice, <br>nevertheless, acquiring noise-free training labels with sufficient positive examples is always <br>expensive. A plausible solution for training data collection is by sampling the largely <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11972773823535266513&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 16</a> <a href="/scholar?q=related:0Trqp1vWJ6YJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11972773823535266513&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'0Trqp1vWJ6YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://lms.comp.nus.edu.sg/papers/media/2010/mmm10-richang.pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/925p41335j22184n.pdf" class=yC10>Mediapedia: Mining web knowledge to construct multimedia encyclopedia</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, ZJ Zha, <a href="/citations?user=0UkdiUT1ooUC&amp;hl=en&amp;oi=sra">Z Luo</a>, TS Chua - Advances in Multimedia  &hellip;, 2010 - Springer</div><div class="gs_rs">Abstract. In recent years, we have witnessed the blooming of Web 2.0 content such as <br>Wikipedia, Flickr and YouTube, etc. How might we benefit from such rich media resources <br>available on the internet? This paper presents a novel concept called Mediapedia, a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14978197780089934046&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 13</a> <a href="/scholar?q=related:3hDO6YU_3c8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14978197780089934046&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'3hDO6YU_3c8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://infolab.stanford.edu/~wangz/project/imsearch/review/MTA/neela.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from stanford.edu</span><span class="gs_ggsS">stanford.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/07NJ448H48433W7P.pdf" class=yC12>Automatic image semantic interpretation using social action and tagging data</a></h3><div class="gs_a">N Sawant, <a href="/citations?user=4Nmf18IAAAAJ&amp;hl=en&amp;oi=sra">J Li</a>, <a href="/citations?user=inVzWAcAAAAJ&amp;hl=en&amp;oi=sra">JZ Wang</a> - Multimedia Tools and Applications, 2011 - Springer</div><div class="gs_rs">Abstract The plethora of social actions and annotations (tags, comments, ratings) from online <br>media sharing Websites and collaborative games have induced a paradigm shift in the <br>research on image semantic interpretation. Social inputs with their added context <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4265122887020118349&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 16</a> <a href="/scholar?q=related:TQE5DKvCMDsJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4265122887020118349&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'TQE5DKvCMDsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://comminfo.rutgers.edu/~mor/publications/NaamanMTAP10socialMultimedia.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from rutgers.edu</span><span class="gs_ggsS">rutgers.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/V836432078K27014.pdf" class=yC14>Social multimedia: highlighting opportunities for search and mining of multimedia data in social media applications</a></h3><div class="gs_a"><a href="/citations?user=IeqjwlIAAAAJ&amp;hl=en&amp;oi=sra">M Naaman</a> - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract In recent years, various Web-based sharing and community services such as Flickr <br>and YouTube have made a vast and rapidly growing amount of multimedia content available <br>online. Uploaded by individual participants, content in these immense pools of content is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10786183381739155064&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 13</a> <a href="/scholar?q=related:eK7IVqM4sJUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10786183381739155064&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'eK7IVqM4sJUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/civr10-hongrichang.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816055" class=yC16>Exploring large scale data for multimedia QA: an initial study</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, G Li, L Nie, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, TS Chua - Proceedings of the ACM  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract With the explosive growth of multimedia contents on the internet, multimedia search <br>has become more and more important. However, users are often bewildered by the vast <br>quantity of information content returned by the search engines. In this scenario, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4416393598065709541&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 8</a> <a href="/scholar?q=related:5REEEZQuSj0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4416393598065709541&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'5REEEZQuSj0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://archive.itee.uq.edu.au/~uqyyan10/papers/wwwj.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uq.edu.au</span><span class="gs_ggsS">uq.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/732M8G373RV73181.pdf" class=yC18>Mining multi-tag association for image tagging</a></h3><div class="gs_a"><a href="/citations?user=PVv2xDYAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, Z Huang, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, <a href="/citations?user=y6m820wAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a> - World Wide Web, 2011 - Springer</div><div class="gs_rs">Abstract Automatic media tagging plays a critical role in modern tag-based media retrieval <br>systems. Existing tagging schemes mostly perform tag assignment based on community <br>contributed media resources, where the tags are provided by users interactively. However, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14467407038118961623&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 12</a> <a href="/scholar?q=related:1yERsAyOxsgJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14467407038118961623&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'1yERsAyOxsgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://sites.google.com/site/kuiyuanyang/diversityMMM.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/03h02628295005wj.pdf" class=yC1A>Social image search with diverse relevance ranking</a></h3><div class="gs_a"><a href="/citations?user=g2gAY_0AAAAJ&amp;hl=en&amp;oi=sra">K Yang</a>, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, HJ Zhang - Advances in Multimedia Modeling, 2010 - Springer</div><div class="gs_rs">Abstract. Recent years have witnessed the success of many online social media websites, <br>which allow users to create and share media information as well as describe the media <br>content with tags. However, the existing ranking approaches for tag-based image search <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13041490641532690681&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 6</a> <a href="/scholar?q=related:-SAjRqSu_LQJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13041490641532690681&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'-SAjRqSu_LQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://www.user.tu-berlin.de/wojwoj/pdf/KawWACV11.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tu-berlin.de</span><span class="gs_ggsS">tu-berlin.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5711531" class=yC1C>Multi-modal visual concept classification of images via Markov random walk over tags</a></h3><div class="gs_a">M Kawanabe, <a href="/citations?user=5B8CTlEAAAAJ&amp;hl=en&amp;oi=sra">A Binder</a>, C Muller&hellip; - &hellip;  of Computer Vision ( &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic annotation of images is a challenging task in computer vision because of <br>âsemantic gapâ between highlevel visual concepts and image appearances. Therefore, user <br>tags attached to images can provide further information to bridge the gap, even though <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5644255625755417378&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 6</a> <a href="/scholar?q=related:ItumSpZsVE4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5644255625755417378&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ItumSpZsVE4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www.cais.ntu.edu.sg/~assourav/papers/JASIST-TagIR-2011.pdf" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21659/full" class=yC1E>Tagâbased social image retrieval: An empirical evaluation</a></h3><div class="gs_a"><a href="/citations?user=wyKGVKUAAAAJ&amp;hl=en&amp;oi=sra">A Sun</a>, <a href="/citations?user=o0F3sqEAAAAJ&amp;hl=en&amp;oi=sra">SS Bhowmick</a>, N Nguyen&hellip; - Journal of the  &hellip;, 2011 - Wiley Online Library</div><div class="gs_rs">Abstract Tags associated with social images are valuable information source for superior <br>image search and retrieval experiences. Although various heuristics are valuable to boost <br>tag-based search for images, there is a lack of general framework to study the impact of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12821041757159329039&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 5</a> <a href="/scholar?q=related:D43uoYx97bEJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12821041757159329039&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'D43uoYx97bEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://137.132.145.151/lms/sites/default/files/mm10-jinhui.pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874139" class=yC20>One person labels one million images</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, Q Chen, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, TS Chua, R Jain - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Targeting the same objective of alleviating the manual work as automatic <br>annotation, in this paper, we propose a novel framework with minimal human effort to <br>manually annotate a large-scale image corpus. In this framework, a dynamic multi-scale <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18179452320848807319&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 5</a> <a href="/scholar?q=related:lwVYI5FjSvwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18179452320848807319&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'lwVYI5FjSvwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1992042" class=yC22>Active learning through notes data in flickr: an effortless training data acquisition approach for object localization</a></h3><div class="gs_a"><a href="/citations?user=7hogLBYAAAAJ&amp;hl=en&amp;oi=sra">L Zhang</a>, J Ma, C Cui, P Li - Proceedings of the 1st ACM International  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Most of the state-of-the-art systems for object localization rely on supervised <br>machine learning techniques, and are thus limited by the lack of labeled training data. In this <br>paper, our motivation is to provide training dataset for object localization effectively and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18332098546666447452&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:XCKCJYCyaP4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'XCKCJYCyaP4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/R830386W66L11531.pdf" class=yC23>Web video retagging</a></h3><div class="gs_a">Z Chen, J Cao, T Xia, Y Song, Y Zhang, J Li - Multimedia Tools and  &hellip;, 2011 - Springer</div><div class="gs_rs">Abstract Tags associated with web videos play a crucial role in organizing and accessing <br>large-scale video collections. However, the raw tag list (RawL) is usually incomplete, <br>imprecise and unranked, which reduces the usability of tags. Meanwhile, compared with <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13722599603110798911&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:P-ZVMqJ3cL4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13722599603110798911&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'P-ZVMqJ3cL4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://charuaggarwal.net/2010_TPAMI_revised_v2.pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from charuaggarwal.net</span><span class="gs_ggsS">charuaggarwal.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6035718" class=yC24>Exploring context and content links in social media: A latent space method</a></h3><div class="gs_a"><a href="/citations?user=Nut-uvoAAAAJ&amp;hl=en&amp;oi=sra">GJ Qi</a>, <a href="/citations?user=x_wsduUAAAAJ&amp;hl=en&amp;oi=sra">C Aggarwal</a>, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, <a href="/citations?user=z7GCqT4AAAAJ&amp;hl=en&amp;oi=sra">H Ji</a>&hellip; - Pattern Analysis and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Social media networks contain both content and context-specific information. Most <br>existing methods work with either of the two for the purpose of multimedia mining and <br>retrieval. In reality, both content and context information are rich sources of information for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=456745240989403318&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 5</a> <a href="/scholar?q=related:tggGcmCvVgYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=456745240989403318&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'tggGcmCvVgYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://www.idiap.ch/~gatica/publications/NegoescuGatica-book10.pdf" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from idiap.ch</span><span class="gs_ggsS">idiap.ch <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.idiap.ch/~gatica/publications/NegoescuGatica-book10.pdf" class=yC26>Internet multimedia search and mining</a></h3><div class="gs_a"><a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>, TS Chua - Internet Multimedia Search and Mining, 2010 - idiap.ch</div><div class="gs_rs">Abstract: We present in this chapter a review of current work that leverages on large online <br>social networks&#39; meta-information, in particular Flickr Groups. We briefly present this hugely <br>successful feature in Flickr and discuss the various ways in which metadata stemming <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5011184445285179392&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:AKAv3bNNi0UJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5011184445285179392&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'AKAv3bNNi0UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md20', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md20" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:AKAv3bNNi0UJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/MM2011.pdf" class=yC29><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072334" class=yC28>Towards multi-semantic image annotation with graph regularized exclusive group lasso</a></h3><div class="gs_a">X Chen, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">X Yuan</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, J Tang, <a href="/citations?user=uOJH_AEAAAAJ&amp;hl=en&amp;oi=sra">Y Rui</a>&hellip; - Proceedings of the 19th  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract To bridge the semantic gap between low level feature and human perception, most <br>of the existing algorithms aim mainly at annotating images with concepts coming from only <br>one semantic space, eg cognitive or affective. The naive combination of the outputs from <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11933754482419175968&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:IBJSWXk2naUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11933754482419175968&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'IBJSWXk2naUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2037681" class=yC2A>Video accessibility enhancement for hearing-impaired users</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">XT Yuan</a>, M Xu, J Jiang&hellip; - ACM Transactions on  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract There are more than 66 million people suffering from hearing impairment and this <br>disability brings them difficulty in video content understanding due to the loss of audio <br>information. If the scripts are available, captioning technology can help them in a certain <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9071236764054744351&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:H23ygsaB430J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'H23ygsaB430J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874332" class=yC2B>Towards a universal detector by mining concepts with small semantic gaps</a></h3><div class="gs_a"><a href="/citations?user=Q8iay0gAAAAJ&amp;hl=en&amp;oi=sra">J Feng</a>, Y Zheng, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a> - Proceedings of the international conference on &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Can we have a universal detector that could recognize unseen objects with no <br>training exemplars available? Such a detector is so desirable, as there are hundreds of <br>thousands of object concepts in human vocabulary but few available labeled image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11345425452790866222&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 2</a> <a href="/scholar?q=related:LmFGs1YMc50J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11345425452790866222&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'LmFGs1YMc50J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5583296" class=yC2C>Image tag refinement along the &#39;what&#39;dimension using tag categorization and neighbor voting</a></h3><div class="gs_a">S Lee, <a href="/citations?user=VpjWb7wAAAAJ&amp;hl=en&amp;oi=sra">W De Neve</a>, YM Ro - Multimedia and Expo (ICME), 2010 &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Online sharing of images is increasingly becoming popular, resulting in the <br>availability of vast collections of user-contributed images that have been annotated with user-<br>supplied tags. However, user-supplied tags are often not related to the actual image <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14333672889554803712&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 2</a> <a href="/scholar?q=related:AJyPzIxv68YJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14333672889554803712&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'AJyPzIxv68YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5673752" class=yC2D>Multi-modality transfer based on multi-graph optimization for domain adaptive video concept annotation</a></h3><div class="gs_a">S Xu, S Tang, Y Zhang, J Li - Image and Video Technology ( &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Multi-modality, the unique and important property of video data, is typically ignored <br>in existing video adaptation processes. To solve this problem, we propose a novel <br>approach, named multi-modality transfer based on multi-graph optimization (MMT-MGO) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16379685269125004665&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:ef3kDvhUUOMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16379685269125004665&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'ef3kDvhUUOMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/pdf/10.1007/s11042-010-0567-2" class=yC2E>Social image annotation via cross-domain subspace learning</a></h3><div class="gs_a"><a href="/citations?user=eAJfUeIAAAAJ&amp;hl=en&amp;oi=sra">S Si</a>, D Tao, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, KP Chan - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract In recent years, cross-domain learning algorithms have attracted much attention to <br>solve labeled data insufficient problem. However, these cross-domain learning algorithms <br>cannot be applied for subspace learning, which plays a key role in multimedia processing. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6024443664525864448&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:AK5qEZgfm1MJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6024443664525864448&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'AK5qEZgfm1MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/u4103h782u560305.pdf" class=yC2F>Graph-based multi-space semantic correlation propagation for video retrieval</a></h3><div class="gs_a">B Feng, J Cao, X Bao, L Bao, Y Zhang, S Lin&hellip; - The Visual Computer, 2011 - Springer</div><div class="gs_rs">Abstract By introducing the concept detection results to the retrieval process, concept-based <br>video retrieval (CBVR) has been successfully used for semantic content-based video <br>retrieval application. However, how to select and fuse the appropriate concepts for a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4196193837562780924&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:_NycGBDgOzoJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4196193837562780924&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'_NycGBDgOzoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396336" class=yC30>Query expansion enhancement by fast binary matching</a></h3><div class="gs_a">X Li, W Zhou, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, Q Tian - Proceedings of the 20th ACM international &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Query expansion has been successfully employed to improve the performance of <br>image retrieval system. It usually expands the original query based on the information from <br>top ranked images. However, it may fail when some of the top ranked images are false <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13307178778663768119&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a onclick="return gs_ocit(event,'N9DeQJiYrLgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://arxiv.org/pdf/1107.2859" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S016516841200148X" class=yC31>Label-specific training set construction from web resource for image annotation</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, C Zhao, TS Chua, R Jain - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract Recently many research efforts have been devoted to image annotation by <br>leveraging on the associated tags/keywords of web images as training labels. A key issue to <br>resolve is the relatively low accuracy of the tags. In this paper, we propose a novel semi-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15877522534659742940&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:3FynX59KWNwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15877522534659742940&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'3FynX59KWNwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://www.ifp.illinois.edu/~cao4/papers/MINet_charu_chapter_2010.pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from illinois.edu</span><span class="gs_ggsS">illinois.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/v07440308727n270.pdf" class=yC33>Multimedia Information Networks in Social Media</a></h3><div class="gs_a"><a href="/citations?user=S-hBSfIAAAAJ&amp;hl=en&amp;oi=sra">L Cao</a>, <a href="/citations?user=Nut-uvoAAAAJ&amp;hl=en&amp;oi=sra">GJ Qi</a>, SF Tsai, MH Tsai, AD Pozo&hellip; - Social Network Data  &hellip;, 2011 - Springer</div><div class="gs_rs">The popularity of personal digital cameras and online photo/video sharing community has <br>lead to an explosion of multimedia information. Unlike traditional multimedia data, many new <br>multimedia datasets are organized in a structural way, incorporating rich information such <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2044083149590388455&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:516yZssKXhwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2044083149590388455&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'516yZssKXhwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320312000983" class=yC35>Supervised Sparse Patch Coding Towards Misalignment-Robust Face Recognition</a></h3><div class="gs_a">C Lang, S Feng, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">X Yuan</a> - Journal of Visual Communication and Image  &hellip;, 2012 - Elsevier</div><div class="gs_rs">Abstract We address the challenging problem of face recognition under the scenarios where <br>both training and test data are possibly contaminated with spatial misalignments. A <br>supervised sparse coding framework is developed in this paper towards a practical <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=528312835727278598&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:BsLNl7nxVAcJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=528312835727278598&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'BsLNl7nxVAcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://www.eecs.qmul.ac.uk/~tmh/papers/fu2012attribsocial.pdf" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from qmul.ac.uk</span><span class="gs_ggsS">qmul.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.eecs.qmul.ac.uk/~tmh/papers/fu2012attribsocial.pdf" class=yC36>Attribute learning for understanding unstructured social activity</a></h3><div class="gs_a">Y Fu, <a href="/citations?user=nHhtvqkAAAAJ&amp;hl=en&amp;oi=sra">T Hospedales</a>, <a href="/citations?user=MeS5d4gAAAAJ&amp;hl=en&amp;oi=sra">T Xiang</a>, <a href="/citations?user=Nhi0I8cAAAAJ&amp;hl=en&amp;oi=sra">S Gong</a> - European Conference on  &hellip;, 2012 - eecs.qmul.ac.uk</div><div class="gs_rs">Abstract. The rapid development of social video sharing platforms has created a huge <br>demand for automatic video classification and annotation techniques, in particular for videos <br>containing social activities of a group of people (eg YouTube video of a wedding reception<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3673206268454347093&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:VaH1waPZ-TIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3673206268454347093&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'VaH1waPZ-TIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md32', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md32" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:VaH1waPZ-TIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ZhaZJ_J03_Semantic-Gap-Oriented%20Active%20Learning%20for%20Multilable%20Image%20Annotation.pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6111295" class=yC38>Semantic-Gap-Oriented Active Learning for Multilabel Image Annotation</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, ZJ Zha, D Tao, TS Chua - Image Processing, IEEE  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract User interaction is an effective way to handle the semantic gap problem in image <br>annotation. To minimize user effort in the interactions, many active learning methods were <br>proposed. These methods treat the semantic concepts individually or correlatively. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12951969610158265254&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 5</a> <a href="/scholar?q=related:plMj67yjvrMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12951969610158265254&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'plMj67yjvrMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212009022" class=yC3A>Kai Jiang, Huagang Yin, Peng Wang, Nenghai Yu</a></h3><div class="gs_a"><a href="/citations?user=nz-Li8kAAAAJ&amp;hl=en&amp;oi=sra">K Jiang</a>, H Yin, P Wang, N Yu - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract This paper proposed a method that fully exploits contextual information of geo-<br>tagged web photos to recommend tourism attractions to a user according to his personal <br>interest and current time and location. The proposed method first detects tourism <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'47hWBsoEwdcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0306457312000945" class=yC3B>Multifaceted conceptual image indexing on the world wide web</a></h3><div class="gs_a">F Fauzi, M Belkhatir - Information Processing &amp; Management, 2012 - Elsevier</div><div class="gs_rs">In this paper, we describe a user-centered design of an automated multifaceted concept-<br>based indexing framework which analyzes the semantics of the Web image contextual <br>information and classifies it into five broad semantic concept facets: signal, object, abstract<b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'1C0mDJ9wcyoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2382380" class=yC3C>Tag ranking by propagating relevance over tag and image graphs</a></h3><div class="gs_a">M Li, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, H Li, C Zhao - &hellip;  of the 4th International Conference on  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we explore the problem of tag ranking by propagating relevance over <br>community-contributed images and their associated tags. To rank the tags more accurately, <br>we propose a novel tag ranking scheme through a two-stage graph-based relevance <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'edO4DW-1rSsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.worldscientific.com/doi/abs/10.1142/S0218001411008944" class=yC3D>VIDEO RETRIEVAL VIA LEARNING COLLABORATIVE SEMANTIC DISTANCE</a></h3><div class="gs_a">S Zhu, Z Liang, J XIAOYUAN - International Journal of Pattern  &hellip;, 2011 - World Scientific</div><div class="gs_rs">Graph-based semi-supervised learning approaches have been proven effective and efficient <br>in solving the problem of the inefficiency of labeled data in many real-world application <br>areas, such as video annotation. However, the pairwise similarity metric, a significant <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:I_zYHEfFiI0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10198618265397820451&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'I_zYHEfFiI0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212001518" class=yC3E>Exploring multi-modality structure for cross domain adaptation in video concept annotation</a></h3><div class="gs_a">S Xu, S Tang, Y Zhang, J Li, YT Zheng - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Domain adaptive video concept detection and annotation has recently received significant <br>attention, but in existing video adaptation processes, all the features are treated as one <br>modality, while multi-modalities, the unique and important property of video data, is <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:02zCLL48PksJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5421837788893048019&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'02zCLL48PksJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Video Annotation via Learning with Spatio-Temporal Correlation</h3><div class="gs_a">S Zhu, Z Liang, Y Liu</div><div class="gs_fl"><a href="/scholar?q=related:aLAINKA8-nwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'aLAINKA8-nwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2382688" class=yC3F>Automatic annotation of tagged content using predefined semantic concepts</a></h3><div class="gs_a">MG Manzato, <a href="/citations?user=6lc68RgAAAAJ&amp;hl=en&amp;oi=sra">R Goularte</a> - Proceedings of the 18th Brazilian symposium  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract User tags are an important source of information that can be used to gather <br>semantic data about the content, reducing the semantic gap and the restrictive domain of <br>automatic indexing approaches. In this paper, we propose an automatic technique for <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'3KgeQqL2a44J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB41" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW41"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ICCV11.pdf" class=yC41><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6126323" class=yC40>Multi-label visual classification with label exclusive context</a></h3><div class="gs_a">X Chen, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">XT Yuan</a>, Q Chen, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - Computer Vision (ICCV &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We introduce in this paper a novel approach to multi-label image classification <br>which incorporates a new type of context-label exclusive context-with linear representation <br>and classification. Given a set of exclusive label groups that describe the negative <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2422096281759318535&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 2</a> <a href="/scholar?q=related:BwIPRrwDnSEJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2422096281759318535&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'BwIPRrwDnSEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231213000192" class=yC42>Jing Liu, Yifan Zhang, Zechao Li, Hanqing Lu</a></h3><div class="gs_a">J Liu, Y Zhang, Z Li, H Lu - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract With the permeation of Web 2.0, large-scale user contributed images with tags are <br>easily available on social websites. However, the noisy or incomplete correspondence <br>between images and tags prohibit us from precise image retrieval and effective <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'mhd7TfI-GV8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB43" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW43"><a href="http://www.ieeeprojects.yavum.com/ieee2012basepaper/Learn%20to%20Personalized%20Image%20Search%20from%20the%20Photo%20Sharing%20Websites.pdf" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from yavum.com</span><span class="gs_ggsS">yavum.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6111487" class=yC43>Learn to Personalized Image Search From the Photo Sharing Websites</a></h3><div class="gs_a"><a href="/citations?user=u6ivSjgAAAAJ&amp;hl=en&amp;oi=sra">J Sang</a>, C Xu, D Lu - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Increasingly developed social sharing websites like Flickr and Youtube allow users <br>to create, share, annotate, and comment medias. The large-scale user-generated metadata <br>not only facilitate users in sharing and organizing multimedia content, but provide useful <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14042515156176223035&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 2</a> <a href="/scholar?q=related:OzvsVCIJ4cIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14042515156176223035&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'OzvsVCIJ4cIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6185680" class=yC45>Learning Hierarchical Semantic Description via Mixed-norm Regularization for Image Understanding</a></h3><div class="gs_a">L Li, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, Q Huang - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper proposes a new perspective-Vicept representation to solve the problem <br>of visual polysemia and concept polymorphism in the large scale semantic image <br>understanding. Vicept characterizes the membership probability distribution between <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2321977605125072398&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:DmbkWVRSOSAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'DmbkWVRSOSAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6092497" class=yC46>Query Difficulty Prediction for Web Image Search</a></h3><div class="gs_a">X Tian, Y Lu, <a href="/citations?user=cvgKxDQAAAAJ&amp;hl=en&amp;oi=sra">L Yang</a> - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Image search plays an important role in our daily life. Given a query, the image <br>search engine is to retrieve images related to it. However, different queries have different <br>search difficulty levels. For some queries, they are easy to be retrieved (the search engine <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:AD64Z0noKUEJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'AD64Z0noKUEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6301770" class=yC47>The TFC Model: Tensor Factorization and Tag Clustering for Item Recommendation in Social Tagging Systems</a></h3><div class="gs_a"><a href="/citations?user=0aMmyHgAAAAJ&amp;hl=en&amp;oi=sra">D Rafailidis</a>, P Daras - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, a novel Tensor Factorization and tag Clustering (TFC) model is <br>presented for item recommendation in social tagging systems. The TFC model consists of <br>three distinctive steps, in each of which important innovative elements are proposed. More <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:bIk9aU-TW0MJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'bIk9aU-TW0MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320312000272" class=yC48>Visual attention modeling based on short-term environmental adaption</a></h3><div class="gs_a"><a href="/citations?user=KPMK3B4AAAAJ&amp;hl=en&amp;oi=sra">X Sun</a>, H Yao, <a href="/citations?user=lRSD7PQAAAAJ&amp;hl=en&amp;oi=sra">R Ji</a> - Journal of Visual Communication and Image  &hellip;, 2012 - Elsevier</div><div class="gs_rs">Abstract Visual attention modeling is crucial for interpreting the structure and functionality of <br>human vision system. A typical computational model of visual attention includes two basic <br>elements: visual representation and saliency measurement. Most existing models left two <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:m9FdbKhISpAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'m9FdbKhISpAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168412003738" class=yC49>Accurate off-line query expansion for large-scale mobile visual search</a></h3><div class="gs_a">K Gao, Y Zhang, D Zhang, S Lin - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract Mobile visual search is a new class of applications that use images taken by <br>camera phone to initiate search queries. It is a very challenging task mainly because of <br>image affine transformations caused by viewpoints changes, and motion blur due to hand <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'jGqqcFaDOSwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168412002204" class=yC4A>Multimedia encyclopedia construction by mining web knowledge</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, ZJ Zha, Y Gao, TS Chua, X Wu - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract In recent years, we have witnessed the blooming of Web 2.0 content such as <br>Wikipedia, Flickr and YouTube, etc. How might we benefit from such rich media resources <br>available on the internet? This paper presents a novel concept called Mediapedia, a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:-PXgd9tRpOoJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'-PXgd9tRpOoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072470" class=yC4B>Capturing a great photo via learning from community-contributed photo collections</a></h3><div class="gs_a">H Li, L Yi, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, X Wang - Proceedings of the 19th ACM international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract We present a novel system, named PhotoReference, to help users, especially <br>amateur photographers, to interactively learn to improve photograph skills by leveraging the <br>available web image collections. The idea behind is based on the observation that the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:dB5ii56TgxcJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'dB5ii56TgxcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/n21118436x784101.pdf" class=yC4C>Combining global and local matching of multiple features for precise item image retrieval</a></h3><div class="gs_a">H Li, X Wang, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, C Zhao - Multimedia Systems - Springer</div><div class="gs_rs">Abstract With the fast-growing of online shopping services, there are millions even billions of <br>commercial item images available on the Internet. How to effectively leverage visual search <br>method to find the items of users&#39; interests is an important yet challenging task. Besides <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2778197965817864700&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:_GFJlFQkjiYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'_GFJlFQkjiYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396355" class=yC4D>Geometric context-preserving progressive transmission in mobile visual search</a></h3><div class="gs_a">J Xia, K Gao, D Zhang, Z Mao - &hellip;  of the 20th ACM international conference &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Progressive transmission is very effective to reduce retrieval latency in mobile <br>visual search. However, the acceleration effects of existing progressive transmission <br>strategies are often limited because of the neglect of geometric information in the query <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'IuJSsL8ZZQEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv12.papers/ntt.pdf" class=yC4F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv12.papers/ntt.pdf" class=yC4E>TRECVid 2012 Semantic Video Concept Detection by NTT-MD-DUT</a></h3><div class="gs_a">Y Sun, K Sudo, Y Taniguchi, H Li, L Yi, Y Guan - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT In this paper, we describe the TRECVid 2012 video concept detection system <br>first developed at the NTT Media Intelligence Laboratories in collaboration with Dalian <br>University of Technology. For this year&#39;s task, we adopted a subspace partition based <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'uABxxg6aogcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md53', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md53" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:uABxxg6aogcJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB54" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW54"><a href="http://www.cse.iitb.ac.in/~comad/2011/images/papers/research/1_bw_comad2011_submission_6_automated%20content%20labeling%20using%20context%20in%20email_n.pdf" class=yC51><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iitb.ac.in</span><span class="gs_ggsS">iitb.ac.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cse.iitb.ac.in/~comad/2011/images/papers/research/1_bw_comad2011_submission_6_automated%20content%20labeling%20using%20context%20in%20email_n.pdf" class=yC50>Automated Content Labeling using Context in Email</a></h3><div class="gs_a">A Raghuveer - cse.iitb.ac.in</div><div class="gs_rs">Abstract Through a recent survey, we observe that a significant percentage of people still <br>share photos through email. When an user composes an email with an attachment, he/she <br>most likely talks about the attachment in the body of the email. In this paper, we develop a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:X9uky70sFagJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12111635966559837023&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'X9uky70sFagJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md54', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md54" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:X9uky70sFagJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396346" class=yC52>Towards relevance and saliency ranking of image tags</a></h3><div class="gs_a">S Feng, C Lang, B Li - Proceedings of the 20th ACM international  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Social image tag ranking has emerged as an important research topic recently due <br>to its potential application on web image search. This paper presents an adaptive all-season <br>tag ranking algorithm which can handle the images with and without distinct object (s) <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'kikuzFQC9noJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231213000180" class=yC53>Han Wang, Wei Liang, Xinxiao Wu, Peng Teng</a></h3><div class="gs_a">H Wang, W Liang, <a href="/citations?user=iZHC_EQAAAAJ&amp;hl=en&amp;oi=sra">X Wu</a>, P Teng - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract In this paper, we propose a novel method for scene image retrieval in which the <br>semantic meaning of an image and a new low-level feature are combined. The fluid nature <br>of scene images makes learning semantics essential in our retrieval task. Compared to a <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'xnuZzvZ4j0MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320312000387" class=yC54>Image annotation by semi-supervised cross-domain learning with group sparsity</a></h3><div class="gs_a">Y Yuan, F Wu, <a href="/citations?user=VUN-9cQAAAAJ&amp;hl=en&amp;oi=sra">J Shao</a>, Y Zhuang - Journal of Visual Communication and  &hellip;, 2012 - Elsevier</div><div class="gs_rs">Abstract With the explosive growth of multimedia data in the web, multi-label image <br>annotation has been attracted more and more attention. Although the amount of available <br>data is large and growing, the number of labeled data is quite small. This paper proposes <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:99PNz5Umyn0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'99PNz5Umyn0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB58" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW58"><a href="http://ylu.cc/cikm2012.pdf" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ylu.cc</span><span class="gs_ggsS">ylu.cc <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2398532" class=yC55>Semantic context learning with large-scale weakly-labeled image set</a></h3><div class="gs_a">Y Lu, W Zhang, K Zhang, X Xue - Proceedings of the 21st ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract There are a large number of images available on the web; meanwhile, only a <br>subset of web images can be labeled by professionals because manual annotation is time-<br>consuming and labor-intensive. Although we can now use the collaborative image tagging <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'GkUs5GLH-eIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231213000143" class=yC57>Liansheng Zhuang, Haoyuan Gao, Jiebo Luo, Zhouchen Lin</a></h3><div class="gs_a">L Zhuang, H Gao, <a href="/citations?user=CcbnBvgAAAAJ&amp;hl=en&amp;oi=sra">J Luo</a>, <a href="/citations?user=TanjFwoAAAAJ&amp;hl=en&amp;oi=sra">Z Lin</a> - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract Topic model is a popular tool for visual concept learning. Most topic models are <br>either unsupervised or fully supervised. In this paper, to take advantage of both limited <br>labeled training images and rich unlabeled images, we proposes a novel regularized <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'d3625usrxKwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB60" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW60"><a href="http://ylu.cc/mm11.pdf" class=yC59><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ylu.cc</span><span class="gs_ggsS">ylu.cc <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2071970" class=yC58>Automatic image annotation with weakly labeled dataset</a></h3><div class="gs_a">W Zhang, Y Lu, X Xue, J Fan - Proceedings of the 19th ACM international &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract It is very attractive to exploit weakly-labeled image dataset for multi-label annotation <br>applications. In our paper the meaning of the terminology weakly labeled is threefold: i) only <br>a small subset of the available images are labeled; ii) even for the labeled image, the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4370472051892388543&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:v4LR6SwJpzwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4370472051892388543&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'v4LR6SwJpzwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0957417412004824" class=yC5A>Towards a universal detector by mining concepts with small semantic gaps</a></h3><div class="gs_a">C Lang, <a href="/citations?user=Q8iay0gAAAAJ&amp;hl=en&amp;oi=sra">J Feng</a>, Y Zheng - Expert Systems with Applications, 2012 - Elsevier</div><div class="gs_rs">Can we have a universal detector that could visually recognize unseen objects with no <br>training exemplars available? Such a detector is so desirable, as there are hundreds of <br>thousands of object concepts in human vocabulary but few labeled image examples <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:cWPc7fxEgDwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4359560292407731057&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'cWPc7fxEgDwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6146437" class=yC5B>Long-Term Incremental Web-Supervised Learning of Visual Concepts via Random Savannas</a></h3><div class="gs_a">R Ewerth, K Ballafkir, M Muhling&hellip; - Multimedia, IEEE  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The idea of using image and video data available in the World-Wide Web (WWW) <br>as training data for classifier construction has received some attention in the past few years. <br>In this paper, we present a novel incremental and scalable web-supervised learning <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:SWn48dJdyzkJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'SWn48dJdyzkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB63" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW63"><a href="http://dspace.library.uvic.ca:8080/bitstream/handle/1828/2967/DarrenMinifie_MastersThesis_July_14_2010.pdf?sequence=1" class=yC5D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uvic.ca</span><span class="gs_ggsS">uvic.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dspace.library.uvic.ca:8080/handle/1828/2967" class=yC5C>On Mobile Accessibility: Learning from our Desktop Ancestors</a></h3><div class="gs_a">D Minifie - 2010 - dspace.library.uvic.ca</div><div class="gs_rs">The art and science of creating effective mobile software has become significantly more <br>important since on-the-go computing has taken centre stage in users&#39; daily lives. This area of <br>application development introduces many new constraints not commonly found in <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:M02v9V0KXP4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'M02v9V0KXP4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB64" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW64"><a href="http://arxiv.org/pdf/1110.3109" class=yC5F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://arxiv.org/abs/1110.3109" class=yC5E>Robust Image Analysis by L1-Norm Semi-supervised Learning</a></h3><div class="gs_a"><a href="/citations?user=OUXS8doAAAAJ&amp;hl=en&amp;oi=sra">Z Lu</a>, Y Peng - arXiv preprint arXiv:1110.3109, 2011 - arxiv.org</div><div class="gs_rs">Abstract: This paper presents a novel L1-norm semi-supervised learning algorithm for robust <br>image analysis by giving new L1-norm formulation of Laplacian regularization which is the <br>key step of graph-based semi-supervised learning. Since our L1-norm Laplacian <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:n_8Y-DGmxZ0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11368675567912681375&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'n_8Y-DGmxZ0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396331" class=yC60>Social tag alignment with image regions by sparse reconstructions</a></h3><div class="gs_a">Y Liu, J Liu, Z Li, B Niu, H Lu - Proceedings of the 20th ACM international &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract How to align social tags with image regions without additional human intervention <br>is a challenging but a valuable task since it can provide more detailed image semantic <br>information and improve the accuracy of image retrieval. To this end, we propose a novel <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'4Ds4-ISvCIgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ri"><h3 class="gs_rt"><a href="http://japanlinkcenter.org/JST.JSTAGE/iieej/39.924?from=Google" class=yC61>6-12 ç»åãã¼ã¿ãã¼ã¹</a></h3><div class="gs_a">å¤ç°æè£ï¼ å è¤ä¿ä¸ - ç»åé»å­å­¦ä¼èª, 2010 - J-STAGE</div><div class="gs_rs">ç»åãã¼ã¿ãã¼ã¹ã»ç»åæ¤ç´¢ã¯,ãç»åããæ½åºãããç¹å¾´éãã­ã¼ã¨ãã¦ç»åãç®¡çã, <br>ãããæ¤ç´¢ããææ³ã ã¨,ãç»åã«ä»ä¸ãããä»å çãªæå ±ãã­ã¼ã¨ãã¦ç»åãç®¡çã, <br>ãããæ¤ç´¢ããææ³ã ãåºç¤æè¡ã¨ãã¦ãã, ãã®ä¸¡èããã¾ãèåãããã¨, ã¤ã¾ã, ã¤ã³ã¿ã¼ãã£<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:mTyj5VH0RKcJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12053027135332498585&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'mTyj5VH0RKcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
