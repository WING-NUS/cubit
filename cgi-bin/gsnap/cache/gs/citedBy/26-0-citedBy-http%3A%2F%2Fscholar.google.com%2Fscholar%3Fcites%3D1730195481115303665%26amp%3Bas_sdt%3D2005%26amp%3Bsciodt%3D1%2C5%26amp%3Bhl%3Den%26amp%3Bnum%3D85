Total results = 26
<div class="gs_r" style="z-index:400"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168409002898" class=yC0>Pitch-frequency histogram-based music information retrieval for Turkish music</a></h3><div class="gs_a">AC Gedik, B Bozkurt - Signal Processing, 2010 - Elsevier</div><div class="gs_rs">This study reviews the use of pitch histograms in music information retrieval studies for <br>western and non-western music. The problems in applying the pitch-class histogram-based <br>methods developed for western music to non-western music and specifically to Turkish <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4553613874426066507&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 27</a> <a href="/scholar?q=related:S7Z0VbGvMT8J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4553613874426066507&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'S7Z0VbGvMT8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://perso.telecom-paristech.fr/~grichard/Publications/ICASSP-10_Joder.pdf" class=yC2><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from telecom-paristech.fr</span><span class="gs_ggsS">telecom-paristech.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5495784" class=yC1>A comparative study of tonal acoustic features for a symbolic level music-to-score alignment</a></h3><div class="gs_a">C Joder, S Essid, G Richard - Acoustics Speech and Signal  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper we review the acoustic features used for music-to-score alignment and <br>study their influence on the performance in a challenging alignment task, where the audio <br>data is polyphonic and may contain percussion. Furthermore, as we aim at using âreal <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5814509981742082263&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 14</a> <a href="/scholar?q=related:1yizlAVKsVAJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5814509981742082263&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'1yizlAVKsVAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://publik.tuwien.ac.at/files/PubDat_186351.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tuwien.ac.at</span><span class="gs_ggsS">tuwien.ac.at <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0065245810780037" class=yC3>Features for content-based audio retrieval</a></h3><div class="gs_a">D MitroviÄ, M Zeppelzauer, <a href="/citations?user=AvNBy7MAAAAJ&amp;hl=en&amp;oi=sra">C Breiteneder</a> - Advances in computers, 2010 - Elsevier</div><div class="gs_rs">Abstract Today, a large number of audio features exists in audio retrieval for different <br>purposes, such as automatic speech recognition, music information retrieval, audio <br>segmentation, and environmental sound retrieval. The goal of this chapter is to review <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10438406953689636585&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 14</a> <a href="/scholar?q=related:6V6jv9Gr3JAJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10438406953689636585&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'6V6jv9Gr3JAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://perso.telecom-paristech.fr/~grichard/Publications/Joder_TASLP2011.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from telecom-paristech.fr</span><span class="gs_ggsS">telecom-paristech.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5752828" class=yC5>A conditional random field framework for robust and scalable audio-to-score matching</a></h3><div class="gs_a">C Joder, S Essid, G Richard - Audio, Speech, and Language  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we introduce the use of conditional random fields (CRFs) for the audio-<br>to-score alignment task. This framework encompasses the statistical models which are used <br>in the literature and allows for more flexible dependency structures. In particular, it allows <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13792815992584799983&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 14</a> <a href="/scholar?q=related:7wZYWhHtab8J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13792815992584799983&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'7wZYWhHtab8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://hal.archives-ouvertes.fr/docs/00/51/14/52/PDF/Papadopoulos_LocalKey_DAFX_2009.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/hal-00511452/" class=yC7>Local key estimation based on harmonic and metric structures</a></h3><div class="gs_a">H Papadopoulos, G Peeters - Proceedings of the  &hellip;, 2009 - hal.archives-ouvertes.fr</div><div class="gs_rs">ABSTRACT In this paper, we present a method for estimating the local keys of an audio <br>signal. We propose to address the problem of local key finding by investigating the possible <br>combination and extension of different previous proposed global key estimation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15872313996596424563&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 9</a> <a href="/scholar?q=related:cy-XGnzJRdwJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15872313996596424563&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'cy-XGnzJRdwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202010/ISMIR_2010_papers/ismir2010-9.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202010/ISMIR_2010_papers/ismir2010-9.pdf" class=yC9>An improved hierarchical approach for music-to-symbolic score alignment</a></h3><div class="gs_a">C Joder, S Essid, G Richard - Proc. of ISMIR, 2010 - mirlab.org</div><div class="gs_rs">ABSTRACT We present an efficient approach for an off-line alignment of a symbolic score to <br>a recording of the same piece, using a statistical model. A hidden state model is built from <br>the score, which allows for the use of two different kinds of features, namely chroma <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3980229672021482352&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 6</a> <a href="/scholar?q=related:cFtPbcydPDcJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3980229672021482352&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'cFtPbcydPDcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md5', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md5" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:cFtPbcydPDcJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.cs.fiu.edu/~lli003/Music/clu/5.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fiu.edu</span><span class="gs_ggsS">fiu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA319&amp;ots=oDOSrFexe7&amp;sig=s67zZOnwtgcbIMC2kk2VmZr2EMk" class=yCB>Clustering music recordings by their keys</a></h3><div class="gs_a">Y Liu, Y Wang, A Shenoy, WH Tsai&hellip; - Proceedings of the  &hellip;, 2008 - books.google.com</div><div class="gs_rs">ABSTRACT Music key, a high level feature of musical audio, is an effective tool for structural <br>analysis of musical works. This paper presents a novel unsupervised approach for clustering <br>music recordings by their keys. Based on chroma-based features extracted from acoustic <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16035653830951345082&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 3</a> <a href="/scholar?q=related:upv83jIWit4J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16035653830951345082&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 24 versions</a> <a onclick="return gs_ocit(event,'upv83jIWit4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://hal.inria.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from inria.fr</span><span class="gs_ggsS">inria.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://hal.inria.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf" class=yCD>Joint estimation of musical content information from an audio signal</a></h3><div class="gs_a">H Papadopoulos - 2010 - hal.inria.fr</div><div class="gs_rs">RÃ©sumÃ© Dans cette these, nous nous intÃ©ressons au probleme de l&#39;extraction automatique <br>d&#39;informations de contenu d&#39;un signal audio de musique. La plupart des travaux existants <br>abordent ce probleme en considÃ©rant les attributs musicaux de maniere indÃ©pendante les <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5504565459161893914&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 4</a> <a href="/scholar?q=related:Gsw65B4lZEwJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Gsw65B4lZEwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Gsw65B4lZEwJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:Gsw65B4lZEwJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=14696771562731948953&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.ifs.tuwien.ac.at/mir/pub/VZENZ_Thesis_ChordDetection.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tuwien.ac.at</span><span class="gs_ggsS">tuwien.ac.at <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ifs.tuwien.ac.at/mir/pub/VZENZ_Thesis_ChordDetection.pdf" class=yCF>Automatic Chord Detection in Polyphonic Audio Data</a></h3><div class="gs_a">V ZENZ - 2007 - ifs.tuwien.ac.at</div><div class="gs_rs">Abstract Automatic analysis of digital audio data has a long tradition. Many tasks that <br>humans solve easily, like distinguishing the constituting instrument in polyphonic audio or <br>the recognition of rhythm or harmonies are still not solved for computers. Especially the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7865519449673115977&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 1</a> <a href="/scholar?q=related:SSmxI0TwJ20J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7865519449673115977&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'SSmxI0TwJ20J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:SSmxI0TwJ20J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://www.ntu.edu.sg/home/aseschng/SpeechTechWeb/members/Conformation_theses/WangLei_ConfirmationReport_Final.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ntu.edu.sg/home/aseschng/SpeechTechWeb/members/Conformation_theses/WangLei_ConfirmationReport_Final.pdf" class=yC11>Unsupervised Techniques for Audio Content Analysis and Summarization</a></h3><div class="gs_a">W Lei - 2008 - ntu.edu.sg</div><div class="gs_rs">Abstract This thesis explores unsupervised algorithms for broadcast audio analysis and <br>summarization. For this work, the audio content analysis and summarization task will be to <br>extract semantic structures from audio databases and organize them into a hierarchical <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7070979980582480431&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 1</a> <a href="/scholar?q=related:L2qEJNQqIWIJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7070979980582480431&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'L2qEJNQqIWIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:L2qEJNQqIWIJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://140.124.72.88/lab/ICASSP2011/pdfs/0000397.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 140.124.72.88</span><span class="gs_ggsS">140.124.72.88 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5946424" class=yC13>Hidden Discrete Tempo Model: A tempo-aware timing model for audio-to-score alignment</a></h3><div class="gs_a">C Joder, S Essid, G Richard - Acoustics, Speech and Signal  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present the Hidden Discrete Tempo Model, an effective Dynamic <br>Bayesian Network for audio to score matching. Its main feature is an explicit modeling of <br>tempo, which directly in fluences the timing model of the musical performance. Thanks to a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11752353636382794061&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 2</a> <a href="/scholar?q=related:TeFdm12_GKMJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11752353636382794061&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'TeFdm12_GKMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5201119" class=yC15>Key detection through pitch class distribution model and ANN</a></h3><div class="gs_a">J Sun, <a href="/citations?user=y-NYC0sAAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, L Lei - Digital Signal Processing, 2009 16th  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Being one of the most well-known high-level music content concepts, music key <br>reveals an important theoretical feature for Western music in music structure analysis, music <br>note and chord transcription and music mood comprehension. Key finding becomes a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16638621702957738344&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 1</a> <a href="/scholar?q=related:aKnmnEhC6OYJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16638621702957738344&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'aKnmnEhC6OYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5537762" class=yC16>Audio onset detection using energy-based and pitch-based processing</a></h3><div class="gs_a">HL Tan, Y Zhu, L Chaisorn&hellip; - Circuits and Systems ( &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Leveraging on the strength of energy-based processing for transient detection and <br>pitch-based processing for softer onsets detection, we present a system that combines both <br>energy and pitch cues for detecting onsets from different instrument categories. Given an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10563072987584255051&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 1</a> <a href="/scholar?q=related:S9R7qeqSl5IJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'S9R7qeqSl5IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://perso.telecom-paristech.fr/~grichard/Publications/Joder_ACMM2010.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from telecom-paristech.fr</span><span class="gs_ggsS">telecom-paristech.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874100" class=yC17>A conditional random field viewpoint of symbolic audio-to-score matching</a></h3><div class="gs_a">C Joder, S Essid, G Richard - Proceedings of the international  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract We present a new approach of symbolic audio-to-score alignment, with the use of <br>Conditional Random Fields (CRFs). Unlike Hidden Markov Models, these graphical models <br>allow the calculation of state conditional probabilities to be made on the basis of several <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3856715124112058410&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 1</a> <a href="/scholar?q=related:KmiTvPjNhTUJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3856715124112058410&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'KmiTvPjNhTUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://music-ir.org/mirex/results/2009/abs/TZC.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from music-ir.org</span><span class="gs_ggsS">music-ir.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://music-ir.org/mirex/results/2009/abs/TZC.pdf" class=yC19>An Energy-based and Pitch-based Approach to Audio Onset Detection</a></h3><div class="gs_a">HL Tan, Y Zhu, L Chaisorn - &hellip;  (2009), onset detection contest.(Cited on  &hellip;, 2009 - music-ir.org</div><div class="gs_rs">ABSTRACT This paper presents an approach for audio onset detection submitted to MIREX <br>2009. The technique utilizes information on the general characteristics of the notes for onset <br>categorization, as well as integrates energy-based and pitch-based detection results.</div><div class="gs_fl"><a href="/scholar?cites=11684914782836418676&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 1</a> <a href="/scholar?q=related:dOis7RUoKaIJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11684914782836418676&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'dOis7RUoKaIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md14', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md14" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:dOis7RUoKaIJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://wwwiti.cs.uni-magdeburg.de/~stober/publ/mtap2012adaptiveMIR.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-magdeburg.de</span><span class="gs_ggsS">uni-magdeburg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/76x78554q0526132.pdf" class=yC1B>Adaptive music retrievalâa state of the art</a></h3><div class="gs_a">S Stober, <a href="/citations?user=LuMoBX0AAAAJ&amp;hl=en&amp;oi=sra">A NÃ¼rnberger</a> - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract With the development of more and more sophisticated Music Information Retrieval <br>approaches, aspects of adaptivity are becoming an increasingly important research topic. <br>Even though, adaptive techniques have already found their way into Music Information <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MvLbFPg5nPEJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17409853997172126258&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MvLbFPg5nPEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://hal.archives-ouvertes.fr/docs/00/65/57/81/PDF/PapadopoulosPeeters_LcalKey_IEEE_2011.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6074928" class=yC1D>Local Key Estimation from an Audio Signal Relying on Harmonic and Metrical Structures</a></h3><div class="gs_a">H Papadopoulos, G Peeters - Audio, Speech, and Language  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present a method for estimating the progression of musical key <br>from an audio signal. We address the problem of local key finding by investigating the <br>possible combination and extension of different previously proposed approaches for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6634530592485010227&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 1</a> <a href="/scholar?q=related:M-uHMGyWElwJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6634530592485010227&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'M-uHMGyWElwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://www.music-ir.org/mirex/abstracts/2010/TZC1.pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from music-ir.org</span><span class="gs_ggsS">music-ir.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.music-ir.org/mirex/abstracts/2010/TZC1.pdf" class=yC1F>MIREX 2010 Audio Onset Detection</a></h3><div class="gs_a">HL Tan, Y Zhu, L Chaisorn - 2010 - music-ir.org</div><div class="gs_rs">ABSTRACT This paper presents an approach for the Audio Onset Detection task [1], which is <br>submitted to MIREX 2010. In MIREX 2009, we presented our approach that utilizes <br>information on the general characteristics of the notes for onset categorization, as well as <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:sjCHQlFPE_cJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17803660962352804018&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'sjCHQlFPE_cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md17', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md17" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:sjCHQlFPE_cJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://www.mmk.ei.tum.de/~jod/publis/Joder_WASPAA2011.pdf" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tum.de</span><span class="gs_ggsS">tum.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6082330" class=yC21>Optimizing the mapping from a symbolic to an audio representation for music-to-score alignment</a></h3><div class="gs_a">C Joder, S Essid, G Richard - &hellip; of Signal Processing to Audio and &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A key processing step in music-to-score alignment systems is the estimation of the <br>intantaneous match between an audio observation and the score. We here propose a <br>general formulation of this matching measure, using a linear transformation from the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10181427204325312900&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 1</a> <a href="/scholar?q=related:hAlPTBmyS40J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10181427204325312900&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'hAlPTBmyS40J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> <a href="https://www.dora.dmu.ac.uk/handle/2086/1126" class=yC23>Emotionally expressive music based interaction language for social robots</a></h3><div class="gs_a"><a href="/citations?user=giHcUJgAAAAJ&amp;hl=en&amp;oi=sra">A Ayesh</a> - 2009 - dora.dmu.ac.uk</div><div class="gs_fl"><a href="/scholar?q=related:bczDhdO2Il4J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6783185008371944557&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'bczDhdO2Il4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:bczDhdO2Il4J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/13438/FengSY.pdf?sequence=1" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/13438" class=yC24>Efficient and robust audio fingerprinting</a></h3><div class="gs_a">F SHUYU - 2007 - scholarbank.nus.edu</div><div class="gs_rs">As the amount of music data in multimedia databases increases rapidly, there are strong <br>needs to investigate and develop content-based music information retrieval systems in order <br>to support effective and efficient analysis, retrieval and management for music data. This <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ma51jv-FocAJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13880522859185811097&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'ma51jv-FocAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://iospress.metapress.com/index/YP2146512036R874.pdf" class=yC26>A music key detection method based on pitch class distribution theory</a></h3><div class="gs_a">J Sun, <a href="/citations?user=y-NYC0sAAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, L Ma - International Journal of Knowledge-based and  &hellip;, 2011 - IOS Press</div><div class="gs_rs">As one of the most well-known music elements, music key, which reveals an important <br>feature in music transcription, structure analysis and mood comprehension, is always an <br>essential theoretical construct of music. As a result, key finding is becoming a popular <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14850064247040401659&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=26">Cited by 1</a> <a href="/scholar?q=related:-2CS8MAGFs4J:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14850064247040401659&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'-2CS8MAGFs4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://documents.irevues.inist.fr/bitstream/handle/2042/28959/joder_407.pdf.txt?sequence=2" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[TXT]</span> from inist.fr</span><span class="gs_ggsS">inist.fr <span class=gs_ctg2>[TXT]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://documents.irevues.inist.fr/handle/2042/28959" class=yC27>Etude des descripteurs acoustiques pour l&#39;alignement temporel audio-sur-partition musicale</a></h3><div class="gs_a">C Joder, S Essid, G Richard - XXIIe colloque GRETSI ( &hellip;, 2009 - documents.irevues.inist.fr</div><div class="gs_rs">Dans cet article, nous comparons l&#39;influence des descripteurs acoustiques utilisÃ©s dans les <br>systÃ¨mes d&#39;alignement temporel musique/partition, pour une tÃ¢che oÃ¹ la musique peut Ãªtre <br>polyphonique avec percussions. DiffÃ©rentes reprÃ©sentations de l&#39;Ã©tat de l&#39;art sont <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:4P2DJM47YJEJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10475438489825705440&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'4P2DJM47YJEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://tel.archives-ouvertes.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://tel.archives-ouvertes.fr/tel-00548952/" class=yC29>Estimation conjointe d&#39;information de contenu musical d&#39;un signal audio</a></h3><div class="gs_a">H Papadopoulos - 2010 - tel.archives-ouvertes.fr</div><div class="gs_rs">RÃ©sumÃ© Dans cette these, nous nous intÃ©ressons au probleme de l&#39;extraction automatique <br>d&#39;informations de contenu d&#39;un signal audio de musique. La plupart des travaux existants <br>abordent ce probleme en considÃ©rant les attributs musicaux de maniere indÃ©pendante les <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:B-SObITZT2wJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7804695842036573191&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'B-SObITZT2wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://rua.ua.es/dspace/bitstream/10045/18326/1/Tesis_Pertusa.pdf" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ua.es</span><span class="gs_ggsS">ua.es <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dialnet.unirioja.es/servlet/tesis?codigo=20907" class=yC2B>Computationally efficient methods for polyphonic music transcription</a></h3><div class="gs_a">A Pertusa-IbÃ¡Ã±ez - 2010 - dialnet.unirioja.es</div><div class="gs_rs">Resumen: Automatic music transcription is a music information retrieval (MIR) task which <br>involves many different disciplines, such as audio signal processing, machine learning, <br>computer science, psychoacoustics and music perception, music theory, and music <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:2JJri16MFxAJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1159549766765089496&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'2JJri16MFxAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md24', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md24" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:2JJri16MFxAJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://www.mmk.ei.tum.de/~jod/publis/Joder_CORESA2010.pdf" class=yC2E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tum.de</span><span class="gs_ggsS">tum.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.mmk.ei.tum.de/~jod/publis/Joder_CORESA2010.pdf" class=yC2D>Approche hiÃ©rarchique pour un alignement musique-sur-partition efficace.</a></h3><div class="gs_a">C Joder, S Essid, G Richard - mmk.ei.tum.de</div><div class="gs_rs">RÃ©sumÃ© Dans le cadre du probleme d&#39;alignement audio-surpartition, nous utilisons un <br>modelea Ã©tats cachÃ©s pour modÃ©liser l&#39;Ã©volution du contenu du signal sonore en rapport <br>avec la partition. Nous proposons dans cet article une mÃ©thode hiÃ©rarchique de rÃ©duction <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:WbdHwrNLK4QJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9523789072435754841&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'WbdHwrNLK4QJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md25', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md25" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:WbdHwrNLK4QJ:scholar.google.com/&amp;hl=en&amp;num=26&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
