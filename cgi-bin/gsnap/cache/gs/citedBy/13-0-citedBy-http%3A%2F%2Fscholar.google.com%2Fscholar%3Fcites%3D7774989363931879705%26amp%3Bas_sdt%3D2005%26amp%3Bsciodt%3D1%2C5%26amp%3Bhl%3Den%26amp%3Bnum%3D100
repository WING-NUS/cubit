Total results = 13
<div class="gs_r" style="z-index:400"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=ds6xRLdlmJgC&amp;oi=fnd&amp;pg=PR9&amp;ots=jvk_hKhgWd&amp;sig=TPpQYXW6Ben_ZIxVcoYsbAGcExM" class=yC0>Content-based image and video retrieval</a></h3><div class="gs_a"><a href="/citations?user=ZgWULzYAAAAJ&amp;hl=en&amp;oi=sra">O Marques</a>, B Furht - 2002 - books.google.com</div><div class="gs_rs">The amount of audiovisual information available in digital format has grown exponentially in <br>recent years. Gigabytes of new images, audio and video clips are generated and stored <br>everyday. Most audiovisual content can be accessed through the Internet, which is a very <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8181352041348610094&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 68</a> <a href="/scholar?q=related:LjgAv1UAinEJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8181352041348610094&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'LjgAv1UAinEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md0', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md0" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:LjgAv1UAinEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=5138427890560917326&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/trecvid04.pdf" class=yC2><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/trecvid04.pdf" class=yC1>TRECVID 2004 search and feature extraction task by NUS PRIS</a></h3><div class="gs_a">TS Chua, SY Neo, KY Li, G Wang, R Shi&hellip; - Proceedings of the  &hellip;, 2004 - 137.132.145.151</div><div class="gs_rs">ABSTRACT This paper describes the details of our systems for feature extraction and search <br>tasks of TRECVID-2004. For feature extraction, we emphasize the use of visual auto-concept <br>annotation technique, with the fusion of text and specialized detectors, to induce concepts <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4598184418179278710&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 60</a> <a href="/scholar?q=related:dudPuV0I0D8J:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4598184418179278710&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'dudPuV0I0D8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:dudPuV0I0D8J:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.cse.fau.edu/~oge/pdf/MUSE_MTAP_2002.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fau.edu</span><span class="gs_ggsS">fau.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/3PG9LURE73FHWE85.pdf" class=yC3>MUSE: A content-based image search and retrieval system using relevance feedback</a></h3><div class="gs_a"><a href="/citations?user=ZgWULzYAAAAJ&amp;hl=en&amp;oi=sra">O Marques</a>, B Furht - Multimedia Tools and Applications, 2002 - Springer</div><div class="gs_rs">The field of Content-Based Visual Information Retrieval (CBVIR) has experienced <br>tremendous growth in the recent years and many research groups are currently working on <br>solutions to the problem of finding a desired image or video clip in a huge archive without <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6184843981334136965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 27</a> <a href="/scholar?q=related:hUiiEtX61FUJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/16/49/RN115204002.html?source=googlescholar" class="gs_nph" class=yC5>BL Direct</a> <a href="/scholar?cluster=6184843981334136965&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'hUiiEtX61FUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://infolab.usc.edu/DocsDemos/ei2000.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from usc.edu</span><span class="gs_ggsS">usc.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Soft query in image retrieval systems</h3><div class="gs_a"><a href="/citations?user=jEdhxGMAAAAJ&amp;hl=en&amp;oi=sra">C Shahabi</a>, YS Chen - Proceedings SPIE internet imaging (EI14), electronic  &hellip;, 2000</div><div class="gs_fl"><a href="/scholar?cites=5647139983826478748&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 14</a> <a href="/scholar?q=related:nObhPOWrXk4J:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5647139983826478748&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'nObhPOWrXk4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0167865505001947" class=yC7>Content-based audio retrieval with relevance feedback</a></h3><div class="gs_a"><a href="/citations?user=HqpBhJ8AAAAJ&amp;hl=en&amp;oi=sra">C Wan</a>, M Liu - Pattern recognition letters, 2006 - Elsevier</div><div class="gs_rs">In this paper, we have proposed two relevance feedback algorithms for content-based audio <br>retrieval. One is a modified version of a technique used for image retrieval with positive <br>feedback; another is based on a constrained optimization concept. Experiments show that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2158277673548705531&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 7</a> <a href="/scholar?q=related:--6G6hu-8x0J:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2158277673548705531&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'--6G6hu-8x0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://igitur-archive.library.uu.nl/dissertations/2005-0228-003047/full.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uu.nl</span><span class="gs_ggsS">uu.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://igitur-archive.library.uu.nl/dissertations/2005-0228-003047/full.pdf" class=yC8>Shape decomposition and retrieval</a></h3><div class="gs_a">M Tanase-Avatavului - University of Utrecht, 2005 - igitur-archive.library.uu.nl</div><div class="gs_rs">This dissertation represents the results of research carried out by the author at Utrecht <br>University, Institute of Information and Computing Sciences, in the group Geometry, Imaging <br>and Virtual Environments. This research was mainly funded by the NWO (the Dutch <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16781189167600899430&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 3</a> <a href="/scholar?q=related:ZhV8K6PC4ugJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16781189167600899430&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'ZhV8K6PC4ugJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md5', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md5" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:ZhV8K6PC4ugJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://users.rsise.anu.edu.au/~wanglei/My_papers/Weighting_Retrieval_PAA.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from anu.edu.au</span><span class="gs_ggsS">anu.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/185M3VW3WQMJYEXJ.pdf" class=yCA>A dynamic sub-vector weighting scheme for image retrieval with relevance feedback</a></h3><div class="gs_a">L Wang, KL Chan - Pattern Analysis &amp; Applications, 2003 - Springer</div><div class="gs_rs">In image retrieval with relevance feedback, feature components obtained from low-level <br>descriptors are often weighted to reflect the high-level concepts and a user&#39;s subjective <br>perception embodied in the images labelled by the user in the feedback. However, the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14037777979409925242&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 2</a> <a href="/scholar?q=related:eoSHarI00MIJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/05/4B/RN142608800.html?source=googlescholar" class="gs_nph" class=yCC>BL Direct</a> <a href="/scholar?cluster=14037777979409925242&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'eoSHarI00MIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.6418&amp;rep=rep1&amp;type=pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.6418&amp;rep=rep1&amp;type=pdf" class=yCD>Relevance Feedback in Content-Based Image Retrieval</a></h3><div class="gs_a">VN Gudivada - International Journal of Computer Science  &hellip;, 2010 - Citeseer</div><div class="gs_rs">Abstract Content-Based Image Retrieval (CBIR) systems are required to effectively harness <br>information from ubiquitous image collections. Despite intense research efforts by the <br>multidisciplinary CBIR community since early 1990s, apparently there is a mismatch <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9299656211777907456&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 1</a> <a href="/scholar?q=related:AOtGjA0ED4EJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9299656211777907456&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'AOtGjA0ED4EJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:AOtGjA0ED4EJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/k8y2wjqyk9yynjen.pdf" class=yCF>A note on learning automata based schemes for adaptation of BP parameters</a></h3><div class="gs_a"><a href="/citations?user=n2OALakAAAAJ&amp;hl=en&amp;oi=sra">M Meybodi</a>, H Beigy - &hellip;  Data Engineering and Automated LearningâIDEAL &hellip;, 2000 - Springer</div><div class="gs_rs">Backpropagation is often used as the learning algorithm in layered-structure neural <br>networks, because of its efficiency. However, backpropagation is not free from problems. <br>The learning process sometimes gets trapped in a local minimum and the network cannot <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3312170398155859684&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 1</a> <a href="/scholar?q=related:5OaksW0x9y0J:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4F/63/RN089349862.html?source=googlescholar" class="gs_nph" class=yC10>BL Direct</a> <a href="/scholar?cluster=3312170398155859684&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'5OaksW0x9y0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/qt606td6jx9nnyye.pdf" class=yC11>Bayesian Learning for Image Retrieva Using Multiple Features</a></h3><div class="gs_a"><a href="/citations?user=HwIAqgIAAAAJ&amp;hl=en&amp;oi=sra">L Wang</a>, K Chan - Intelligent Data Engineering and Automated Learning &hellip;, 2000 - Springer</div><div class="gs_rs">Image retrieval using multiple features often uses explicit weights that represent the <br>importance of the features in their similarity metrics. In this paper, a novel retrieval method <br>based on Bayesian Learning is presented. Instead of giving every feature a weight <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7548697367210518806&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=13">Cited by 1</a> <a href="/scholar?q=related:FmmCwUJcwmgJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/01/00/RN089350335.html?source=googlescholar" class="gs_nph" class=yC12>BL Direct</a> <a href="/scholar?cluster=7548697367210518806&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'FmmCwUJcwmgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.scholarbank.nus.edu.sg/bitstream/handle/10635/14783/thesis_body%20(huamin).pdf?sequence=2" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.scholarbank.nus.edu.sg/handle/10635/14783" class=yC13>Auto-annotation of multimedia contents: Theory and application</a></h3><div class="gs_a">F HUAMIN - 2005 - scholarbank.nus.edu.sg</div><div class="gs_rs">In this thesis, we propose a learning-based framework for auto-annotation of multimedia <br>contents. The framework is open and is designed to incorporate different base learners, <br>including the single-view machine learning (traditional) and the bootstrapping approaches<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3raxSNrVELEJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12758932877839808222&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'3raxSNrVELEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Semantic Retrieval by Color, Shape and Spatial Relationships: A Study with Color Logo Retrieval via Object-oriented FrameworkÂ· Wâ eOÃ¿rÃ© b_â rÃ®TzzÃ´Ã² &hellip;</h3><div class="gs_a">CK Tung</div><div class="gs_fl"><a href="/scholar?q=related:UymvULQ2IQwJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=874039950762256723&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'UymvULQ2IQwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="https://scholarbank.nus.edu.sg/bitstream/handle/10635/15994/SHIRUI_PHDThesis_BayesianLearningofConceptOntologyforAutomaticImageAnnotation.pdf?sequence=1" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://scholarbank.nus.edu.sg/handle/10635/15994" class=yC15>Bayesian learning of concept ontology for automatic image annotation</a></h3><div class="gs_a">RUI SHI - 2007 - scholarbank.nus.edu.sg</div><div class="gs_rs">Automatic image annotation (AIA) has been a hot research topic in recent years since it can <br>be used to support concept-based image retrieval. In the field of AIA, characterizing image <br>concepts by mixture models is one of the most effective techniques. However, mixture <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:hevhtMNxNoEJ:scholar.google.com/&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9310754365002345349&amp;hl=en&amp;num=13&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'hevhtMNxNoEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
