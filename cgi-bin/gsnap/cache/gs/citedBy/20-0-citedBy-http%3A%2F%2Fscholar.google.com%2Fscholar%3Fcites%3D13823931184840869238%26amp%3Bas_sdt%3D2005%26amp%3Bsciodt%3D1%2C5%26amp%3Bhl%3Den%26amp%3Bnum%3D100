Total results = 20
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.5116&amp;rep=rep1&amp;type=pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291448" class=yC0>Semantic concept-based query expansion and re-ranking for multimedia retrieval</a></h3><div class="gs_a"><a href="/citations?user=Ade_7YoAAAAJ&amp;hl=en&amp;oi=sra">AP Natsev</a>, A Haubold, J TeÅ¡iÄ, <a href="/citations?user=u0xUDSoAAAAJ&amp;hl=en&amp;oi=sra">L Xie</a>&hellip; - Proceedings of the 15th  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract We study the problem of semantic concept-based query expansion and re-ranking <br>for multimedia retrieval. In particular, we explore the utility of a fixed lexicon of visual <br>semantic concepts for automatic multimedia retrieval and re-ranking purposes. In this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2405995674058464620&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 101</a> <a href="/scholar?q=related:bJ1ha1HQYyEJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2405995674058464620&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'bJ1ha1HQYyEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://jivp.eurasipjournals.com/content/pdf/1687-5281-2007-056928.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurasipjournals.com</span><span class="gs_ggsS">eurasipjournals.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://jivp.eurasipjournals.com/content/2007/1/056928/" class=yC2>Image and video indexing using networks of operators</a></h3><div class="gs_a">S Ayache, G QuÃ©not, J Gensel - EURASIP Journal on  &hellip;, 2007 - jivp.eurasipjournals.com</div><div class="gs_rs">Indexing image and video documents by concepts is a key issue for an efficient <br>management of multimedia repositories. It is necessary and also a very challenging problem <br>because, unlike in the case of the text media, there is no simple correspondence between <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14404638447372103528&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 17</a> <a href="/scholar?q=related:aG-MTVmO58cJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14404638447372103528&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'aG-MTVmO58cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://lms.comp.nus.edu.sg/papers/media/2007/acmmm07-huanboOFF.PDF" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291295" class=yC4>Segregated feedback with performance-based adaptive sampling for interactive news video retrieval</a></h3><div class="gs_a">HB Luan, SY Neo, HK Goh, YD Zhang, SX Lin&hellip; - Proceedings of the 15th &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Existing video research incorporates the use of relevance feedback based on user-<br>dependent interpretations to improve the retrieval results. In this paper, we segregate the <br>process of relevance feedback into 2 distinct facets:(a) recall-directed feedback; and (b) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18005967219541332703&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 16</a> <a href="/scholar?q=related:39IWVskL4vkJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18005967219541332703&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'39IWVskL4vkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/civr08-yantao.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1386378" class=yC6>Probabilistic optimized ranking for multimedia semantic concept detection via RVM</a></h3><div class="gs_a">YT Zheng, SY Neo, TS Chua, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - Proceedings of the 2008  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract We present a probabilistic ranking-driven classifier for the detection of video <br>semantic concept, such as airplane, building, etc. Most existing concept detection systems <br>utilize Support Vector Machines (SVM) to perform the detection and ranking of retrieved <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2912878446129882030&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 15</a> <a href="/scholar?q=related:rqN2f4OfbCgJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2912878446129882030&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'rqN2f4OfbCgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://db.aquaphoenix.com/publication/civr2008_ic.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aquaphoenix.com</span><span class="gs_ggsS">aquaphoenix.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1386408" class=yC8>Web-based information content and its application to concept-based video retrieval</a></h3><div class="gs_a">A Haubold, <a href="/citations?user=Ade_7YoAAAAJ&amp;hl=en&amp;oi=sra">A Natsev</a> - Proceedings of the 2008 international conference &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Semantic similarity between words or phrases is frequently used to find matching <br>correlations between search queries and documents when straightforward matching of <br>terms fails. This is particularly important for searching in visual databases, where pictures <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=342358306974839972&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 15</a> <a href="/scholar?q=related:pLQj-RBNwAQJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=342358306974839972&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'pLQj-RBNwAQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://lms.comp.nus.edu.sg/papers/media/2007/civr07-zhengYT.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1282341" class=yCA>The use of temporal, semantic and visual partitioning model for efficient near-duplicate keyframe detection in large scale news corpus</a></h3><div class="gs_a">YT Zheng, SY Neo, TS Chua, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - Proceedings of the 6th ACM  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Near-duplicate keyframes (NDKs) are important visual cues to link news stories <br>from different TV channel, time, language, etc. However, the quadratic complexity required <br>for NDK detection renders it intractable in large-scale news video corpus. To address this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4266303494236248305&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 12</a> <a href="/scholar?q=related:8Zj1lWz0NDsJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4266303494236248305&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'8Zj1lWz0NDsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://staff.science.uva.nl/~mdr/Publications/Files/mir2007.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/ft_gateway.cfm?id=1290109&amp;type=pdf" class=yCC>Exploiting redundancy in cross-channel video retrieval</a></h3><div class="gs_a">B Huurnink, <a href="/citations?user=AVDkgFIAAAAJ&amp;hl=en&amp;oi=sra">M de Rijke</a> - &hellip; : Proceedings of the international workshop on  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">ABSTRACT Video producers, in telling a news story, tend to repeat important visual and <br>speech material multiple times in adjacent shots, thus creating a certain level of redundancy. <br>We describe this phenomenon, and use it to develop a framework to incorporate <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=413798725791125024&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 9</a> <a href="/scholar?q=related:IA5Kq78bvgUJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=413798725791125024&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'IA5Kq78bvgUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://lms.comp.nus.edu.sg/papers/media/2008/civr08-luan.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1386352.1386411" class=yCE>Adaptive multiple feedback strategies for interactive video search</a></h3><div class="gs_a">H Luan, Y Zheng, SY Neo, Y Zhang, S Lin&hellip; - Proceedings of the 2008 &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose adaptive multiple feedback strategies for interactive video <br>retrieval. We first segregate interactive feedback into 3 distinct types (recall-driven relevance <br>feedback, precision-driven active learning and locality-driven relevance feedback) so that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12517552171279368847&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 8</a> <a href="/scholar?q=related:j8KGAmFHt60J:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12517552171279368847&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'j8KGAmFHt60J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284711" class=yC10>Interactive spatio-temporal visual map model for web video retrieval</a></h3><div class="gs_a">HB Luan, SX Lin, S Tang, SY Neo&hellip; - Multimedia and Expo,  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The massive amount of multimedia information especially video available on the <br>Web requires a more precise and interactive retrieval. Current operational video retrieval <br>systems do not make use of the implicit visual features but rely only on textual metadata <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9686095857886798670&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 5</a> <a href="/scholar?q=related:TqO5Fd3sa4YJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9686095857886798670&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'TqO5Fd3sa4YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://www.science.uva.nl/~mdr/Publications/Files/civr2007.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1282322" class=yC11>The value of stories for speech-based video search</a></h3><div class="gs_a">B Huurnink, <a href="/citations?user=AVDkgFIAAAAJ&amp;hl=en&amp;oi=sra">M de Rijke</a> - Proceedings of the 6th ACM international  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Anecdotal evidence suggests that story-level information is important for the speech <br>component of video retrieval. In this paper we perform a systematic examination of the <br>combination of shot-level and story-level speech, using a document expansion approach. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8781016341545898656&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 3</a> <a href="/scholar?q=related:oFK-vNZv3HkJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8781016341545898656&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'oFK-vNZv3HkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://lms.comp.nus.edu.sg/papers/media/2007/civr07-huanbo.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1282378" class=yC13>Active learning approach to interactive spatio-temporal news video retrieval</a></h3><div class="gs_a">HB Luan, SY Neo, TS Chua, YT Zheng, S Tang&hellip; - Proceedings of the 6th  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Interactive news video retrieval requires the effective communication between the <br>human searchers and the search engine to locate relevant video segments. We propose a <br>spatio-temporal visual map (STVM) retrieval [1] system coupled with active learning to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1190246465691386912&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 2</a> <a href="/scholar?q=related:IASzttqahBAJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1190246465691386912&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'IASzttqahBAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/VisionGo-%20Towards%20video%20retrieval%20with%20joint%20exploration%20of%20human%20and%20computer.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0020025511002672" class=yC15>VisionGo: towards video retrieval with joint exploration of human and computer</a></h3><div class="gs_a">H Luan, YT Zheng, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, TS Chua - Information Sciences, 2011 - Elsevier</div><div class="gs_rs">Abstract This paper introduces an effective interactive video retrieval system named <br>VisionGo. It jointly explores human and computer to accomplish video retrieval with high <br>effectiveness and efficiency. It assists the interactive video retrieval process in different <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15423134762805473061&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 2</a> <a href="/scholar?q=related:Ja__-1n7CdYJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15423134762805473061&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Ja__-1n7CdYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/95131x/200906/32774763.html" class=yC17>æ°é»è§é¢ä¸­åºäº âåºæ¯è¯æ±â çæäºååç¸ä¼¼åº¦åæ</a></h3><div class="gs_a">æåï¼ å´ç²è¾¾ï¼ æ¾çï¼ è°¢æ¯æ¹ - å½é²ç§æå¤§å­¦å­¦æ¥, 2009 - cqvip.com</div><div class="gs_rs">æ°é»è§é¢ä¸­æäºååçç¸ä¼¼åº¦è®¡ç®å¯¹äºè§é¢æµè§, æ£ç´¢åè·è¸ªæäºååç­åºç¨å·æç¹å«éè¦ç<br>æä¹. ç ç©¶æåºäºä¸ç§å©ç¨&quot; åºæ¯è¯æ±&quot; è®¡ç®æäºååç¸ä¼¼åº¦çæ¹æ³, å°åç¬çå³é®å¸§ä½ä¸ºä¸ä¸ª<br>å®æ´&quot; è¯æ±&quot;, å°æ¯ä¸ªæäºååçä½æ¯ä¸ç³»å&quot; åºæ¯è¯æ±&quot; æè¿°ç&quot; ææ¡£&quot;. å¨æ­¤åºç¡ä¸ç ç©¶äº&quot; åºæ¯<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12941633081064449317&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=20">Cited by 3</a> <a href="/scholar?q=related:JWVKMrjqmbMJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12941633081064449317&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'JWVKMrjqmbMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://mcg.ict.ac.cn/download/pdf/paper/2008/29.VisionGo%20A%20High-performance%20and%20Multifunctional.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mcg.ict.ac.cn/download/pdf/paper/2008/29.VisionGo%20A%20High-performance%20and%20Multifunctional.pdf" class=yC18>VisionGo: A High-performance and Multi-functional Interactive Video Retrieval System</a></h3><div class="gs_a">H Luan, S Lin, Y Zhang, SY Neo, TS Chua - mcg.ict.ac.cn</div><div class="gs_rs">Abstract One of the most critical tasks in multimedia retrieval is the interactive video search. <br>This paper proposes an effective interactive video retrieval system named VisionGo to <br>improve the overall search performance. VisionGo provides three key functions to assist <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'D5KOBLqaupIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md13', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md13" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:D5KOBLqaupIJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607645" class=yC1A>Personalized event-based news video retrieval with dynamic user-log</a></h3><div class="gs_a">M Li, Y Zheng, SY Neo, X Wang&hellip; - Multimedia and Expo,  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Personalization especially in the domain of information retrieval is essentially <br>important, as users might pose the same query even when they are searching for different <br>information. It is thus necessary to create a retrieval engine which takes into consideration <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:nYuFN91PGNIJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'nYuFN91PGNIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://scholarbank.nus.edu.sg/bitstream/handle/10635/15902/thesis.pdf?sequence=1" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu.sg/handle/10635/15902" class=yC1B>Multi-graph based active learning for interactive video retrieval</a></h3><div class="gs_a">Z XIAOMING - 2009 - scholarbank.nus.edu.sg</div><div class="gs_rs">Active learning and semi-supervised learning are important machine learning techniques <br>when labeled data is scarce or expensive to obtain. We employ a graph based semi-<br>supervised learning method where a node in the graph represents a video shot and they <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:QucDPVEHpPEJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17412050104818591554&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'QucDPVEHpPEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://downloads.hindawi.com/journals/ivp/2007/056928.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://downloads.hindawi.com/journals/ivp/2007/056928.pdf" class=yC1D>Image and Video Indexing Using Networks of Operators</a></h3><div class="gs_a">G QuÃ© - EURASIP Journal on Image and Video  &hellip;, 2007 - downloads.hindawi.com</div><div class="gs_rs">This article presents a framework for the design of concept detection systems for image and <br>video indexing. This framework integrates in a homogeneous way all the data and <br>processing types. The semantic gap is crossed in a number of steps, each producing a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:UR0LUxNr6H0J:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'UR0LUxNr6H0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md16', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md16" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:UR0LUxNr6H0J:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.4651&amp;rep=rep1&amp;type=pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.4651&amp;rep=rep1&amp;type=pdf" class=yC1F>Ontology-enriched Semantic Space for Video Retrieval</a></h3><div class="gs_a">X WEI, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW NGO</a> - Citeseer</div><div class="gs_rs">Abstract Multimedia-based ontology construction and reasoning have recently been <br>recognized as two important issues in video search, particularly for bridging semantic gap. <br>The lack of coincidence between low-level features and user expectation makes <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:-VFogG0FvD4J:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4520494093837029881&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'-VFogG0FvD4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md17', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md17" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:-VFogG0FvD4J:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212004341" class=yC21>Semantic concept detection for video based on extreme learning machine</a></h3><div class="gs_a">B Lu, G Wang, Y Yuan, D Han - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Abstract Semantic concept detection is an important step in concept-based semantic video <br>retrieval, which can be regarded as an intermediate descriptor to bridge the semantic gap. <br>Most existing concept detection methods utilize Support Vector Machines (SVM) as <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:C9e0xs6rTysJ:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'C9e0xs6rTysJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://mcg.ict.ac.cn/download/pdf/paper/2008/17.PERSONALIZED%20EVENT-BASED%20NEWS%20VIDEO%20RETRIEVAL%20WITH%20DYNAMIC%20USER-LOG.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mcg.ict.ac.cn/download/pdf/paper/2008/17.PERSONALIZED%20EVENT-BASED%20NEWS%20VIDEO%20RETRIEVAL%20WITH%20DYNAMIC%20USER-LOG.pdf" class=yC22>Ming Li1, 2, 3 Yantao Zheng1 Shi-Yong Neo1 Xiangdong Wang2 Sheng Tang2 Shou-Xun Lin2 School of Computing, NUS 2Key Laboratory of Intelligent  &hellip;</a></h3><div class="gs_a">X Wang, S Tang, SX Lin - mcg.ict.ac.cn</div><div class="gs_rs">ABSTRACT Personalization especially in the domain of information retrieval is essentially <br>important, as users might pose the same query even when they are searching for different <br>information. It is thus necessary to create a retrieval engine which takes into consideration <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'0rhu1D5UbJ8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md19', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md19" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:0rhu1D5UbJ8J:scholar.google.com/&amp;hl=en&amp;num=20&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
