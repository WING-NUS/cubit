Total results = 55
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www2009.eprints.org/37/1/p361.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eprints.org</span><span class="gs_ggsS">eprints.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1526758" class=yC0>Learning to tag</a></h3><div class="gs_a"><a href="/citations?user=-F1rk68AAAAJ&amp;hl=en&amp;oi=sra">L Wu</a>, <a href="/citations?user=cvgKxDQAAAAJ&amp;hl=en&amp;oi=sra">L Yang</a>, N Yu, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a> - &hellip;  of the 18th international conference on  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Social tagging provides valuable and crucial information for large-scale web image <br>retrieval. It is ontology-free and easy to obtain; however, irrelevant tags frequently appear, <br>and users typically will not tag all semantic objects in the image, which is also called <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15119094119347111873&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 91</a> <a href="/scholar?q=related:wcdhRf_P0dEJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15119094119347111873&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'wcdhRf_P0dEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www1bpt.bridgeport.edu/~jelee/courses/CS550_S10/paper/8.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bridgeport.edu</span><span class="gs_ggsS">bridgeport.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631285" class=yC2>Descriptive visual words and visual phrases for image applications</a></h3><div class="gs_a">S Zhang, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, <a href="/citations?user=7SgUlggAAAAJ&amp;hl=en&amp;oi=sra">G Hua</a>, Q Huang, <a href="/citations?user=2sQYtYwAAAAJ&amp;hl=en&amp;oi=sra">S Li</a> - Proceedings of the 17th ACM  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract The Bag-of-visual Words (BoW) image representation has been applied for various <br>problems in the fields of multimedia and computer vision. The basic idea is to represent <br>images as visual documents composed of repeatable and distinctive visual elements, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12935039777028912212&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 87</a> <a href="/scholar?q=related:VDRb-SR-grMJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12935039777028912212&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'VDRb-SR-grMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=yCNOErJ4ENwC&amp;oi=fnd&amp;pg=PA1&amp;ots=MaEbftYYqs&amp;sig=dmDCpP3g65FCcVRZa92s5dxu6T0" class=yC4>Information theory in computer vision and pattern recognition</a></h3><div class="gs_a"><a href="/citations?user=pAe4Pf8AAAAJ&amp;hl=en&amp;oi=sra">FE Ruiz</a>, PS PÃ©rez, <a href="/citations?user=iJDyvasAAAAJ&amp;hl=en&amp;oi=sra">BI Bonev</a> - 2009 - books.google.com</div><div class="gs_rs">Information theory has proved to be effective for solving many computer vision and pattern <br>recognition (CVPR) problems (such as image matching, clustering and segmentation, <br>saliency detection, feature selection, optimal classifier design and many others). <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17012810012402909561&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 29</a> <a href="/scholar?q=related:eQF8jJGkGewJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17012810012402909561&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'eQF8jJGkGewJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md2', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md2" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:eQF8jJGkGewJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=15797107253606673083&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://vipl.ict.ac.cn/sites/default/files/papers/files/2010_ACMMM_slzhang_Building%20Contextual%20Visual%20Vocabulary%20for%20Large-scale%20Image%20Applications.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874018" class=yC5>Building contextual visual vocabulary for large-scale image applications</a></h3><div class="gs_a">S Zhang, Q Huang, <a href="/citations?user=7SgUlggAAAAJ&amp;hl=en&amp;oi=sra">G Hua</a>, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, W Gao&hellip; - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Not withstanding its great success and wide adoption in Bag-of-visual Words <br>representation, visual vocabulary created from single image local features is often shown to <br>be ineffective largely due to three reasons. First, many detected local features are not <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2877257642299667411&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 28</a> <a href="/scholar?q=related:07OM0JQS7icJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2877257642299667411&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'07OM0JQS7icJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www.cs.dartmouth.edu/~lorenzo/Papers/tsf-cvpr09.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dartmouth.edu</span><span class="gs_ggsS">dartmouth.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5206582" class=yC7>Learning query-dependent prefilters for scalable image retrieval</a></h3><div class="gs_a">L Torresani, M Szummer&hellip; - Computer Vision and  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We describe an algorithm for similar-image search which is designed to be efficient <br>for extremely large collections of images. For each query, a small response set is selected <br>by a fast prefilter, after which a more accurate ranker may be applied to each image in the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8374680569069385112&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 16</a> <a href="/scholar?q=related:mDW_SpvXOHQJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8374680569069385112&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'mDW_SpvXOHQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5365277" class=yC9>Object categorization using hierarchical wavelet packet texture descriptors</a></h3><div class="gs_a">X Qian, G Liu, D Guo, Z Li, Z Wang&hellip; - Multimedia, 2009. ISM&#39; &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Object categorization plays an important role in computer vision, semantic based <br>image content understanding, and image retrieval. Wavelet packet transform provides a very <br>good observation for the images by sub-band filtering. Different objects have distinctive <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2668572807996731636&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 13</a> <a href="/scholar?q=related:9HDphdSsCCUJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2668572807996731636&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'9HDphdSsCCUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.lv-nus.org/papers/2010/mm10-learn-photograph.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lv-nus.org</span><span class="gs_ggsS">lv-nus.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1873992" class=yCA>Learning to photograph</a></h3><div class="gs_a">B Cheng, <a href="/citations?user=V9W87PYAAAAJ&amp;hl=en&amp;oi=sra">B Ni</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - Proceedings of the international  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose an intelligent photography system, which automatically <br>and professionally generates/recommends user-favorite photo (s) from a wide view or a <br>continuous view sequence. This task is quite challenging given that the evaluation of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2244103125228909971&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 12</a> <a href="/scholar?q=related:k_WcEuenJB8J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2244103125228909971&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'k_WcEuenJB8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://mrim.imag.fr/publications/2010/albatal/VisPhrase_CBMI10.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from imag.fr</span><span class="gs_ggsS">imag.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5529909" class=yCC>Visual Phrases for automatic images annotation</a></h3><div class="gs_a">R Albatal, P Mulhem&hellip; - Content-Based Multimedia &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Visual characteristics of objects of a class vary with the considered instance and the <br>shooting conditions. In this paper we proposed a visual characterization of object parts, <br>called&quot; Visual Phrase&quot;, robust to these variations. A Visual Phrase is a set of regions of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2817513654932788981&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 7</a> <a href="/scholar?q=related:9dYUyL3RGScJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2817513654932788981&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'9dYUyL3RGScJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://lms.comp.nus.edu.sg/papers/media/2009/visualcomputer09-yantao.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/5237302510781925.pdf" class=yCE>Toward a higher-level visual representation for object-based image retrieval</a></h3><div class="gs_a">YT Zheng, SY Neo, TS Chua, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - The Visual Computer, 2009 - Springer</div><div class="gs_rs">Abstract We propose a higher-level visual representation, visual synset, for object-based <br>image retrieval beyond visual appearances. The proposed visual representation improves <br>the traditional part-based bag-of-words image representation, in two aspects. First, the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8719956845336969929&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 7</a> <a href="/scholar?q=related:ycIL74yCA3kJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8719956845336969929&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'ycIL74yCA3kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://smartech.gatech.edu/bitstream/handle/1853/44645/iccv11_vs.pdf?sequence=1" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gatech.edu</span><span class="gs_ggsS">gatech.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6126295" class=yC10>Large-scale image annotation using visual synset</a></h3><div class="gs_a"><a href="/citations?user=h3XP9JkAAAAJ&amp;hl=en&amp;oi=sra">D Tsai</a>, Y Jing, <a href="/citations?user=AIOpZ50AAAAJ&amp;hl=en&amp;oi=sra">Y Liu</a>, <a href="/citations?user=6fPYTesAAAAJ&amp;hl=en&amp;oi=sra">HA Rowley</a>&hellip; - &hellip;  Vision (ICCV), 2011  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We address the problem of large-scale annotation of web images. Our approach is <br>based on the concept of visual synset, which is an organization of images which are visually-<br>similar and semantically-related. Each visual synset represents a single prototypical visual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2322476971650943494&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 7</a> <a href="/scholar?q=related:BopHLoAYOyAJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2322476971650943494&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'BopHLoAYOyAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC12>TRECVID 2010 Known-item Search by NUS</a></h3><div class="gs_a">XY Chen, J Yuan, L Nie, ZJ Zha, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - TRECVID  &hellip;, 2010 - www-nlpir.nist.gov</div><div class="gs_rs">Abstract. This paper describes our system for auto search and interactive search in the <br>known-item search (KIS) task in TRECVID 2010. KIS task aims to find an unique video <br>answer for each text query. The shift from traditional video search has prompted a series <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12664714192218118309&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'pVgCEXUawq8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://cse.iitkgp.ac.in/~pabitra/paper/wacv11.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iitkgp.ac.in</span><span class="gs_ggsS">iitkgp.ac.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5711495" class=yC14>Quadtree decomposition based extended vector space model for image retrieval</a></h3><div class="gs_a"><a href="/citations?user=Zdl00bEAAAAJ&amp;hl=en&amp;oi=sra">V Ramanathan</a>, S Mishra, P Mitra - Applications of Computer  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Bag of visual words approach for image retrieval does not exploit the spatial <br>distribution of visual words in an image. Previous attempts to incorporate the spatial <br>distribution include modification of visual vocabulary using visual phrases along with <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8674751885806935397&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:ZR0sHOHoYngJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8674751885806935397&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ZR0sHOHoYngJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://home.ustc.edu.cn/~junjcai/1876.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ustc.edu.cn</span><span class="gs_ggsS">ustc.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5583896" class=yC16>Evaluation of histogram based interest point detector in web image classification and search</a></h3><div class="gs_a"><a href="/citations?user=qioooCAAAAAJ&amp;hl=en&amp;oi=sra">J Cai</a>, ZJ Zha, Y Zhao, Z Wang - Multimedia and Expo (ICME),  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Local image feature has received increasing attention in various applications, such <br>as web image classification and search. The process of local feature extraction consists of <br>two main steps: interest point detection and local feature description. A wealth of interest <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=461953506403695871&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:_zzwO0QwaQYJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=461953506403695871&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'_zzwO0QwaQYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/GJX36T4585772716.pdf" class=yC18>Building descriptive and discriminative visual codebook for large-scale image applications</a></h3><div class="gs_a"><a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, S Zhang, W Zhou, <a href="/citations?user=lRSD7PQAAAAJ&amp;hl=en&amp;oi=sra">R Ji</a>, <a href="/citations?user=V9W87PYAAAAJ&amp;hl=en&amp;oi=sra">B Ni</a>, <a href="/citations?user=fOmROdkAAAAJ&amp;hl=en&amp;oi=sra">N Sebe</a> - Multimedia Tools and  &hellip;, 2011 - Springer</div><div class="gs_rs">Abstract Inspired by the success of textual words in large-scale textual information <br>processing, researchers are trying to extract visual words from images which function similar <br>as textual words. Visual words are commonly generated by clustering a large amount of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8228951882804184815&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 5</a> <a href="/scholar?q=related:70ZTeCMcM3IJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8228951882804184815&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'70ZTeCMcM3IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://www.jdl.ac.cn/doc/2010/ICASSP_Building%20Pair-Wise%20Visual%20Word%20Tree%20for%20Efficient%20Image%20Re-ranking.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jdl.ac.cn</span><span class="gs_ggsS">jdl.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5494964" class=yC19>Building pair-wise visual word tree for efficent image re-ranking</a></h3><div class="gs_a">S Zhang, Q Huang, Y Lu, G Wen&hellip; - Acoustics Speech and  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Bag-of-visual Words (BoW) image representation is getting popular in computer <br>vision and multimedia communities. However, experiments show that the traditional BoW <br>representation is not as effective as it is desired. One of the most important reasons for its <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8989609525745692132&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:5C2KEzyCwXwJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8989609525745692132&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'5C2KEzyCwXwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/N72T093111111053.pdf" class=yC1B>Scene categorization using boosted back-propagation neural networks</a></h3><div class="gs_a">X Qian, Z Yan, <a href="/citations?user=gmlQ0msAAAAJ&amp;hl=en&amp;oi=sra">K Hang</a>, G Liu, H Wang, <a href="/citations?user=8WbHl5cAAAAJ&amp;hl=en&amp;oi=sra">Z Wang</a>&hellip; - Advances in Multimedia  &hellip;, 2010 - Springer</div><div class="gs_rs">Scene categorization plays an important role in computer vision, image content <br>understanding, and image retrieval. In this paper, back-propagation neural network (BPN) is <br>served as the basic classifier for multi-class scene/image categorization. Four features, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7482177415042595424&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:YHoQercI1mcJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7482177415042595424&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'YHoQercI1mcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.nlpr.ia.ac.cn/2009papers/gjhy/gh7.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ia.ac.cn</span><span class="gs_ggsS">ia.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5413588" class=yC1C>Expanded bag of words representation for object classification</a></h3><div class="gs_a">T Liu, J Liu, Q Liu, H Lu - Image Processing (ICIP), 2009 16th  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Currently, the bag of visual words (BOW) representation has received wide <br>applications in object categorization. However, the BOW representation ignores the <br>dependency relationship among visual words, which could provide informative knowledge <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18046701450821652309&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:VU_An1rDcvoJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18046701450821652309&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'VU_An1rDcvoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://www.cs.stevens.edu/~ghua/publication/TIP2011.pdf" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from stevens.edu</span><span class="gs_ggsS">stevens.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5732695" class=yC1E>Generating descriptive visual words and visual phrases for large-scale image applications</a></h3><div class="gs_a">S Zhang, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, <a href="/citations?user=7SgUlggAAAAJ&amp;hl=en&amp;oi=sra">G Hua</a>, Q Huang&hellip; - Image Processing, IEEE  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Bag-of-visual Words (BoWs) representation has been applied for various problems <br>in the fields of multimedia and computer vision. The basic idea is to represent images as <br>visual documents composed of repeatable and distinctive visual elements, which are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8203781017236577761&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 6</a> <a href="/scholar?q=related:4Z16o16v2XEJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8203781017236577761&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'4Z16o16v2XEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://home.hiroshima-u.ac.jp/tkurita/papers/matsukawa-ACCV09-MP3-18-332.pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hiroshima-u.ac.jp</span><span class="gs_ggsS">hiroshima-u.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/5g3p497220h5wt30.pdf" class=yC20>Image classification using probability higher-order local auto-correlations</a></h3><div class="gs_a"><a href="/citations?user=l-N65msAAAAJ&amp;hl=en&amp;oi=sra">T Matsukawa</a>, <a href="/citations?user=v3_UV7AAAAAJ&amp;hl=en&amp;oi=sra">T Kurita</a> - Computer VisionâACCV 2009, 2010 - Springer</div><div class="gs_rs">Abstract. In this paper, we propose a novel method for generic object recognition by using <br>higher-order local auto-correlations on probability images. The proposed method is an <br>extension of bag-of-features approach to posterior probability images. Standard bag-of-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10411000857684881568&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 2</a> <a href="/scholar?q=related:oDCYxh5Oe5AJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10411000857684881568&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'oDCYxh5Oe5AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5474128" class=yC22>Object recognition via adaptive multi-level feature integration</a></h3><div class="gs_a">M Wang, Y Wu, G Li, <a href="/citations?user=QUrLihYAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a> - Web Conference (APWEB), 2010 &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Object category recognition is a challenging task due to the low level and non-<br>discrimination in visual representation. Most previous methods concentrate to find better <br>high level visual features. Recently, optimally integrating various features to solve the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13042184371758821094&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 2</a> <a href="/scholar?q=related:5u7V7ZUl_7QJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13042184371758821094&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'5u7V7ZUl_7QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/F0U534236WJ065V3.pdf" class=yC23>Boosted scene categorization approach by adjusting inner structures and outer weights of weak classifiers</a></h3><div class="gs_a">X Qian, Z Yan, K Hang - Advances in Multimedia Modeling, 2011 - Springer</div><div class="gs_rs">Scene categorization plays an important role in computer vision and image content <br>understanding. It is a multi-class pattern classification problem. Usually, multi-class pattern <br>classification can be completed by using several component classifiers. Each component <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16551691523398203728&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:UHn-875rs-UJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16551691523398203728&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'UHn-875rs-UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/y25l77042853q86u.pdf" class=yC24>Bag of spatio-temporal synonym sets for human action recognition</a></h3><div class="gs_a">L Pang, J Cao, J Guo, S Lin, Y Song - Advances in Multimedia Modeling, 2010 - Springer</div><div class="gs_rs">Abstract. Recently, bag of spatio-temporal local features based methods have received <br>significant attention in human action recognition. However, it remains a big challenge to <br>overcome intra-class variations in cases of viewpoint, geometric and illumination variance. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15157727839093141931&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 1</a> <a href="/scholar?q=related:qzEyGykRW9IJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15157727839093141931&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'qzEyGykRW9IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5655732" class=yC25>Extraction of image semantic features with spatial-range mean shift clustering algorithm</a></h3><div class="gs_a">M Wang, C Zhang, Y Song - Signal Processing (ICSP), 2010  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In recent years, the Bag-of-visual Words image representation has led to many <br>significant results in visual object recognition and categorization. However, experiments <br>show that the unsupervised clustering of primitive visual features tends to result in the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6480590771006432582&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 3</a> <a href="/scholar?q=related:RmWDePiu71kJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'RmWDePiu71kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/X728067217X2L533.pdf" class=yC26>Improving Image Classification Using Semantic Attributes</a></h3><div class="gs_a"><a href="/citations?user=wsTP0u0AAAAJ&amp;hl=en&amp;oi=sra">Y Su</a>, <a href="/citations?user=Gb5a92sAAAAJ&amp;hl=en&amp;oi=sra">F Jurie</a> - International journal of computer vision, 2012 - Springer</div><div class="gs_rs">Abstract The Bag-of-Words (BoW) modelâcommonly used for image classificationâhas two <br>strong limitations: on one hand, visual words are lacking of explicit meanings, on the other <br>hand, they are usually polysemous. This paper proposes to address these two limitations <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=172651704819726346&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 1</a> <a href="/scholar?q=related:Cjilj89hZQIJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=172651704819726346&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'Cjilj89hZQIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://www2.iiia.csic.es/~aramisa/pub/aribes11ccia.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from csic.es</span><span class="gs_ggsS">csic.es <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www2.iiia.csic.es/~aramisa/pub/aribes11ccia.pdf" class=yC27>Self-Supervised Clustering for Codebook Construction: An Application to Object Localization</a></h3><div class="gs_a">A RIBES, <a href="/citations?user=pkcZkXgAAAAJ&amp;hl=en&amp;oi=sra">SJIARR LOPEZ</a>, DE MANTARAS - iiia.csic.es</div><div class="gs_rs">Abstract. Approaches to object localization based on codebooks do not exploit the <br>dependencies between appearance and geometric information present in training data. This <br>work addresses the problem of computing a codebook tailored to the task of localization <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7KSBHbbaUnIJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8237887144127997164&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'7KSBHbbaUnIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md24', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md24" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:7KSBHbbaUnIJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://home.hiroshima-u.ac.jp/tkurita/papers/matsukawa-FCV2010-O7-4.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hiroshima-u.ac.jp</span><span class="gs_ggsS">hiroshima-u.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://home.hiroshima-u.ac.jp/tkurita/papers/matsukawa-FCV2010-O7-4.pdf" class=yC29>Combined Feature Extraction from Global/Local Statistics of Visual Words using Relevant Operations</a></h3><div class="gs_a"><a href="/citations?user=l-N65msAAAAJ&amp;hl=en&amp;oi=sra">T MATSUKAWA</a>, <a href="/citations?user=v3_UV7AAAAAJ&amp;hl=en&amp;oi=sra">T KURITA</a> - hiroshima-u.ac.jp</div><div class="gs_rs">Abstract This paper presents a combined feature extraction method to improve the <br>performance of bag-of-features image classification. The bag-of-features approach is the <br>most popular approach for generic object recognition and uses global statistics (histogram<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:zBbkH5vmovQJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'zBbkH5vmovQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md25', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md25" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:zBbkH5vmovQJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://ori-oai.u-bordeaux1.fr/pdf/2011/KARAMAN_SVEBOR_2011.pdf" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-bordeaux1.fr</span><span class="gs_ggsS">u-bordeaux1.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ori-oai.u-bordeaux1.fr/pdf/2011/KARAMAN_SVEBOR_2011.pdf" class=yC2B>L&#39;UNIVERSITÃ BORDEAUX I</a></h3><div class="gs_a">S KARAMAN - 2011 - ori-oai.u-bordeaux1.fr</div><div class="gs_rs">Summary The research of this PhD thesis is fulfilled in the context of wearable video <br>monitoring of patients with aged dementia. The idea is to provide a new tool to medical <br>practitioners for the early diagnosis of elderly dementia such as the Alzheimer disease. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:sGRKJFyngf0J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18267065577779127472&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'sGRKJFyngf0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:sGRKJFyngf0J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://www.hindawi.com/isrn/ai/2012/376804/" class=yC2E><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[HTML]</span><span class="gs_ct2">[HTML]</span></span> <a href="http://www.hindawi.com/isrn/ai/2012/376804/" class=yC2D>Bag-of-Words Representation in Image Annotation: A Review</a></h3><div class="gs_a">CF Tsai - ISRN Artificial Intelligence, 2012 - hindawi.com</div><div class="gs_rs">Content-based image retrieval (CBIR) systems require users to query images by their low-<br>level visual content; this not only makes it hard for users to formulate queries, but also can <br>lead to unsatisfied retrieval results. To this end, image annotation was proposed. The aim <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'uTXtKetKoCkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md27', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md27" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:uTXtKetKoCkJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212001518" class=yC2F>Exploring multi-modality structure for cross domain adaptation in video concept annotation</a></h3><div class="gs_a">S Xu, S Tang, Y Zhang, J Li, YT Zheng - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Domain adaptive video concept detection and annotation has recently received significant <br>attention, but in existing video adaptation processes, all the features are treated as one <br>modality, while multi-modalities, the unique and important property of video data, is <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:02zCLL48PksJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5421837788893048019&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'02zCLL48PksJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ZhaZJ_C02_Locally%20Regressive%20G-Optimal%20Design%20for%20Image%20Retrieval.pdf" class=yC31><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1992055" class=yC30>Locally regressive G-optimal design for image retrieval</a></h3><div class="gs_a">ZJ Zha, YT Zheng, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, F Chang&hellip; - Proceedings of the 1st  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Content Based Image Retrieval (CBIR) has attracted increasing attention from both <br>academia and industry. Relevance Feedback is one of the most effective techniques to <br>bridge the semantic gap in CBIR. One of the key research problems related to relevance <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MdyaLWAb31kJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6475924889077996593&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MdyaLWAb31kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://202.114.89.42/resource/pdf/5547.pdf" class=yC33><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 202.114.89.42</span><span class="gs_ggsS">202.114.89.42 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S104732031000091X" class=yC32>Boosting image object retrieval and indexing by automatically discovered pseudo-objects</a></h3><div class="gs_a">KT Chen, KH Lin, YH Kuo, YL Wu, <a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a> - Journal of Visual  &hellip;, 2010 - Elsevier</div><div class="gs_rs">State-of-the-art object retrieval systems are mostly based on the bag-of-visual-words <br>representation which encodes local appearance information of an image in a feature vector. <br>An image object search is performed by comparing query object&#39;s feature vector with <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14413714812316257247&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 1</a> <a href="/scholar?q=related:34vlMUHNB8gJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14413714812316257247&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'34vlMUHNB8gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://www.comp.nus.edu.sg/~harishk/homepage_material/research_description.pdf" class=yC35><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.comp.nus.edu.sg/~harishk/homepage_material/research_description.pdf" class=yC34>Brief summary of work done during PhD</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a> - comp.nus.edu.sg</div><div class="gs_rs">The focus of my PhD thesis has been to get a better understanding of visual perception and <br>attention as people interact with digital images and video. My first problem was on finding <br>how low level global and local information in images influence category discrimination <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:wlNrPDqYTssJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'wlNrPDqYTssJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md31', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md31" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:wlNrPDqYTssJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://www.willfulwreckords.com/GinsuScience/CVPR2012/data/papers/444_P3C-16.pdf" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from willfulwreckords.com</span><span class="gs_ggsS">willfulwreckords.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6248094" class=yC36>Omni-range spatial contexts for visual classification</a></h3><div class="gs_a"><a href="/citations?user=V9W87PYAAAAJ&amp;hl=en&amp;oi=sra">B Ni</a>, M Xu, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - Computer Vision and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Spatial contexts encode rich discriminative information for visual classification. <br>However, as object shapes and scales vary significantly among images, spatial contexts <br>with manually specified distance ranges are not guaranteed with optimality. In this work, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:QRQ1SN0w3g0J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=999289894280172609&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'QRQ1SN0w3g0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202012/pdfs/0000893.pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6288028" class=yC38>Discriminative bag-of-visual phrase learning for landmark recognition</a></h3><div class="gs_a"><a href="/citations?user=w3OoFL0AAAAJ&amp;hl=en&amp;oi=sra">T Chen</a>, <a href="/citations?user=nr86m98AAAAJ&amp;hl=en&amp;oi=sra">KH Yap</a>, D Zhang - Acoustics, Speech and Signal  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Bag-of-visual phrase (BoP) has been proposed and developed for landmark <br>recognition recently. However, existing BoP methods for landmark recognition have two <br>major shortcomings:(i) they try to construct a universal phrase vocabulary for all object <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:m1dkTG-QyUkJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5316939642784995227&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'m1dkTG-QyUkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212007618" class=yC3A>ISABoost: A weak classifier inner structure adjusting based adaboost algorithmâISABoost based application in scene categorization</a></h3><div class="gs_a">X Qian, Y Yan Tang, Z Yan, K Hang - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">AdaBoost algorithms fuse weak classifiers to be a strong classifier by adaptively determine <br>fusion weights of weak classifiers. In this paper, an enhanced AdaBoost algorithm by <br>adjusting inner structure of weak classifiers (ISABoost) is proposed. In the traditional <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'2AMoXayZVfIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://home.hiroshima-u.ac.jp/tkurita/spr/member/project/publications/VISAPP2010-paper(camera).pdf" class=yC3C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hiroshima-u.ac.jp</span><span class="gs_ggsS">hiroshima-u.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://home.hiroshima-u.ac.jp/tkurita/spr/member/project/publications/VISAPP2010-paper(camera).pdf" class=yC3B>SCENE CLASSIFICATION USING SPATIAL RELATIONSHIP BETWEEN LOCAL POSTERIOR PROBABILITIES</a></h3><div class="gs_a"><a href="/citations?user=l-N65msAAAAJ&amp;hl=en&amp;oi=sra">T Matsukawa</a>, <a href="/citations?user=v3_UV7AAAAAJ&amp;hl=en&amp;oi=sra">T Kurita</a> - hiroshima-u.ac.jp</div><div class="gs_rs">Abstract: This paper presents scene classification methods using spatial relationship <br>between local posterior probabilities of each category. Recently, the authors proposed the <br>probability higher-order local autocorrelations (PHLAC) feature. This method uses <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1qswbmQKs9UJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15398662977371548630&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'1qswbmQKs9UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md35', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md35" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:1qswbmQKs9UJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0957417411004295" class=yC3D>Visual content representation using semantically similar visual words</a></h3><div class="gs_a"><a href="/citations?user=DZ3WuzUAAAAJ&amp;hl=en&amp;oi=sra">K Kesorn</a>, S Chimlek, S Poslad&hellip; - Expert Systems with  &hellip;, 2011 - Elsevier</div><div class="gs_rs">Local feature analysis of visual content, namely using Scale Invariant Feature Transform <br>(SIFT) descriptors, have been deployed in the &#39;bag-of-visual words&#39; model (BVW) as an <br>effective method to represent visual content information and to enhance its classification <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:BiwwePoTHuMJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16365540062390135814&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'BiwwePoTHuMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://research.microsoft.com/pubs/156919/mm2011_contextual_synonym_dictionary.pdf" class=yC3F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from microsoft.com</span><span class="gs_ggsS">microsoft.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072364" class=yC3E>Contextual synonym dictionary for visual object retrieval</a></h3><div class="gs_a">W Tang, <a href="/citations?user=6WCyi64AAAAJ&amp;hl=en&amp;oi=sra">R Cai</a>, Z Li, <a href="/citations?user=fIlGZToAAAAJ&amp;hl=en&amp;oi=sra">L Zhang</a> - Proceedings of the 19th ACM international &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we study the problem of visual object retrieval by introducing a <br>dictionary of contextual synonyms to narrow down the semantic gap in visual word <br>quantization. The basic idea is to expand a visual word in the query image with its <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16539045110655731755&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 1</a> <a href="/scholar?q=related:K8jLiOZ9huUJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16539045110655731755&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'K8jLiOZ9huUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://www.lifl.fr/~urruty/publis/icme2011.pdf" class=yC41><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lifl.fr</span><span class="gs_ggsS">lifl.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6011867" class=yC40>A semantically significant visual representation for social image retrieval</a></h3><div class="gs_a">I El Sayad, J Martinet, T Urruty&hellip; - Multimedia and Expo &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Having effective methods to access the desired images is essential nowadays with <br>the availability of a huge amount of digital images. We propose a higher-level visual <br>representation that enhances the traditional part-based Bag of Visual Words (BOW) <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ExXMkHomkrsJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13515907739549439251&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ExXMkHomkrsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Self-Supervised Clustering for Codebook Construction: An Application to Object Localization</h3><div class="gs_a">C FernÃ¡ndez - &hellip;  of the 14th International Conference of  &hellip;, 2011 - IOS Press, Incorporated</div><div class="gs_fl"><a href="/scholar?q=related:XbbUl8ufc1oJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6517728782509717085&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'XbbUl8ufc1oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2324831" class=yC42>Visual pattern discovery for architecture image classification and product image search</a></h3><div class="gs_a"><a href="/citations?user=DcltNjQAAAAJ&amp;hl=en&amp;oi=sra">WT Chu</a>, MH Tsai - Proceedings of the 2nd ACM International  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Many objects have repetitive elements, and finding repetitive patterns facilitates <br>object recognition and numerous applications. We devise a representation to describe <br>configurations of repetitive elements. By modeling spatial configurations, visual patterns <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10142266565952180987&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 1</a> <a href="/scholar?q=related:-5LmqLSRwIwJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'-5LmqLSRwIwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB41" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW41"><a href="http://www.jdl.ac.cn/doc/2011/201163017584923775_modeling%20spatial%20and%20semantic%20cues%20for%20large-scale%20near-duplicated%20image%20retrieval.pdf" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jdl.ac.cn</span><span class="gs_ggsS">jdl.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S107731421000233X" class=yC43>Modeling spatial and semantic cues for large-scale near-duplicated image retrieval</a></h3><div class="gs_a">S Zhang, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, <a href="/citations?user=7SgUlggAAAAJ&amp;hl=en&amp;oi=sra">G Hua</a>, W Zhou, Q Huang, <a href="/citations?user=7sFMIKoAAAAJ&amp;hl=en&amp;oi=sra">H Li</a>&hellip; - Computer Vision and  &hellip;, 2011 - Elsevier</div><div class="gs_rs">Bag-of-visual Words (BoW) image representation has been illustrated as one of the most <br>promising solutions for large-scale near-duplicated image retrieval. However, the traditional <br>visual vocabulary is created in an unsupervised way by clustering a large number of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6354301785476967729&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=55">Cited by 1</a> <a href="/scholar?q=related:MS38rs4DL1gJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6354301785476967729&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'MS38rs4DL1gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0031320311003062" class=yC45>Image representation for generic object recognition using higher-order local autocorrelation features on posterior probability images</a></h3><div class="gs_a"><a href="/citations?user=l-N65msAAAAJ&amp;hl=en&amp;oi=sra">T Matsukawa</a>, <a href="/citations?user=v3_UV7AAAAAJ&amp;hl=en&amp;oi=sra">T Kurita</a> - Pattern Recognition, 2012 - Elsevier</div><div class="gs_rs">This paper presents a novel image representation method for generic object recognition by <br>using higher-order local autocorrelations on posterior probability images. The proposed <br>method is an extension of the bag-of-features approach to posterior probability images. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3eWgyFXwugMJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=268791379010840029&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'3eWgyFXwugMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6376678" class=yC46>Image classification by exploiting the spatial context information</a></h3><div class="gs_a">S Yan, D Li-Rong, Y Li - Audio, Language and Image  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Finding the effective image representation is an important problem for classification. <br>Previous approaches have demonstrated the utility of the bag-of-feature (BoF) models. <br>These methods are interesting due to the computational efficiency and conceptual <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'pvla0_rWS48J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6005980" class=yC47>Visual word pairs for similar image search</a></h3><div class="gs_a">Y Li, X Cao - Image and Graphics (ICIG), 2011 Sixth  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The state-of-the-art large scale image retrieval systems have mainly relied on two <br>seminal works: the SIFT descriptor and bag-of-features (BOF) model. However, with the <br>growth of image dataset, the discriminative power of SIFT descriptors was weakened <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Re2v4Q98U84J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14867363202532896069&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'Re2v4Q98U84J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5583852" class=yC48>Multiple instance learning using visual phrases for object classification</a></h3><div class="gs_a">Y Song, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, M Wang, H Liu&hellip; - Multimedia and Expo ( &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Recently, bag of words (BoW) model has led to many significant results in visual <br>object classification. However, due to the limited descriptive and discriminative ability of <br>visual words, the resulting performance of visual object classification is still incomparable <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gjvu4fcHKhIJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1308867402933353346&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'gjvu4fcHKhIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6323029" class=yC49>High-Order Local Spatial Context Modeling by Spatialized Random Forest</a></h3><div class="gs_a"><a href="/citations?user=V9W87PYAAAAJ&amp;hl=en&amp;oi=sra">B Ni</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, M Wang, <a href="/citations?user=JmcDlF8AAAAJ&amp;hl=en&amp;oi=sra">A Kassim</a>, Q Tian - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose a novel method for spatial context modeling towards <br>boosting the visual discriminating power. Particularly, we are interested in how to model <br>high-order local spatial contexts, instead of the intensively studied 2ndorder spatial <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'N1XT6lvB1G0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ri"><h3 class="gs_rt"><a href="http://comjnl.oxfordjournals.org/content/early/2012/07/29/comjnl.bxs101.short" class=yC4A>MC2: A Framework and Service for MPEG-7 Content-Modelling Communities</a></h3><div class="gs_a">H Agius, MC Angelides, DD Zad - The Computer Journal, 2012 - Br Computer Soc</div><div class="gs_rs">Abstract Harnessing the power of Web communities, the effort on creating metadata can be <br>greatly reduced. Collaborative communities can create, update and maintain content models <br>for multimedia resources more effectively than single users working alone. This paper <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:mt9O8IFHXlIJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'mt9O8IFHXlIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/bg43m7646164nw67.pdf" class=yC4B>HWVP: hierarchical wavelet packet descriptors and their applications in scene categorization and semantic concept retrieval</a></h3><div class="gs_a">X Qian, D Guo, X Hou, Z Li, H Wang, G Liu, Z Wang - Multimedia Tools and  &hellip; - Springer</div><div class="gs_rs">Abstract Wavelet packet transform is an effective texture analysis approach by sub-band <br>filtering. Different texture patterns have distinctive responses to the sub-bands of wavelet <br>packets. The responses are valuable for texture description. Utilizing all the responses of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:KaVV9ZiQS8oJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'KaVV9ZiQS8oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/92082x/201208/43267820.html" class=yC4C>é¢åå¾åè¯­ä¹åç±»çè§è§åè¯éæå­¦ä¹ æ¹æ³</a></h3><div class="gs_a">éèªï¼ éæ¢ç - çµå­æµéææ¯, 2012 - cqvip.com</div><div class="gs_rs">éå¯¹å¾åçç¹æ§, æåºäº1 ç§è§è§åè¯éæå­¦ä¹ æ¹æ³. è¯¥æ¹æ³å»ºç«å¨3 ç§åå§æ å°æ¹æ³çåºç¡ä¸, <br>å¹¶ååå©ç¨å¾åçç©, çº¹çç´æ¹å¾, å¾ååéå¶æè¿°å­ç­å¾åè§è§ç¹å¾æ¥åç±»å¾åè¯­ä¹. ç¸å¯¹äº3 <br>ç§åå§æ å°æ¹æ³, éç¨Boosting éæå­¦ä¹ æ¹æ³çæçè§è§åè¯éåå¨å¾åè¯­ä¹åç±»ä¸æ¯åç¬<b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'5AxuMFDFIekJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB50" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW50"><a href="http://asso-aria.org/coria/2010/3.pdf" class=yC4E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from asso-aria.org</span><span class="gs_ggsS">asso-aria.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://asso-aria.org/coria/2010/3.pdf" class=yC4D>Phrases Visuelles pour l&#39;annotation automatique d&#39;images</a></h3><div class="gs_a">R Albatal, P Mulhem, Y Chiaramella - asso-aria.org</div><div class="gs_rs">RÃSUMÃ. L&#39;annotation automatique d&#39;images photographiques est un problÃ¨me complexe. <br>En effet, les caractÃ©ristiques visuelles des objets d&#39;une classe varient selon l&#39;instance <br>considÃ©rÃ©e et les conditions de prise de vue. Nous proposons dans cet article une <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:W1StNQlsHR0J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2097951788235576411&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'W1StNQlsHR0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md50', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md50" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:W1StNQlsHR0J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://www.gissky.net/paper/UploadFiles_4495/201207/2012072420235029.pdf" class=yC50><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gissky.net</span><span class="gs_ggsS">gissky.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.gissky.net/paper/UploadFiles_4495/201207/2012072420235029.pdf" class=yC4F>ç®æ è¯å«ä¸­çç¨³å®å¾åç¹å¾ç»ååæ</a></h3><div class="gs_a">å§æ°¸åµï¼ å½­å¯æ° - ä¸­å½å¾è±¡å¾å½¢å­¦æ¥, 2012 - gissky.net</div><div class="gs_rs">æè¦: éå¯¹å¾åå±é¨ç¹å¾ç»åç¨³å®æ§å·®ååºååä¸è¶³çé®é¢, éè¿å¯¹ç±å¾ååå±é¨é»åç¹å¾ææ<br>å¾å°çé¢ç¹é¡¹éè¿è¡ç»è®¡å­¦è¿æ»¤, æ¨¡å¼åè§£, æ¨¡å¼æ»ç»åæ¨¡å¼ç»æé¡¹é´å ä½å³ç³»çå»ºæ¨¡, <br>æåºä¸¤ç§å·æè¾å¼ºè¡¨å¾åååºååçå¾åä¸­å±è¡¨ç¤ºæ¨¡å: ç±»é´å±ç¨ç¨³å®æ¨¡å¼(inter-class <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:mUdpckpgTlAJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5786668444081014681&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'mUdpckpgTlAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md51', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md51" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:mUdpckpgTlAJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB52" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW52"><a href="http://hal.archives-ouvertes.fr/docs/00/66/65/31/PDF/thesis.pdf" class=yC52><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/tel-00666531/" class=yC51>Une reprÃ©sentation visuelle avancÃ©e pour l&#39;apprentissage sÃ©mantique dans les bases d&#39;images</a></h3><div class="gs_a">I El Sayad - 2011 - hal.archives-ouvertes.fr</div><div class="gs_rs">RÃ©sumÃ© Avec l&#39;augmentation exponentielle de nombre d&#39;images disponibles sur Internet, le <br>besoin en outils efficaces d&#39;indexation et de recherche d&#39;images est devenu important. Dans <br>cette these, nous nous baserons sur le contenu visuel des images comme source <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:0iRtduNlN_sJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18102049254857843922&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'0iRtduNlN_sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://tel.archives-ouvertes.fr/docs/00/68/98/55/ANNEX/thesisSveborKaraman.pdf" class=yC54><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://tel.archives-ouvertes.fr/tel-00689855/" class=yC53>Indexation de la VidÃ©o PortÃ©e: Application Ã  l&#39;Ãtude ÃpidÃ©miologique des Maladies LiÃ©es Ã  l&#39;Ãge</a></h3><div class="gs_a"><a href="/citations?user=P4nYUqMAAAAJ&amp;hl=en&amp;oi=sra">S Karaman</a> - 2011 - tel.archives-ouvertes.fr</div><div class="gs_rs">Summary The research of this PhD thesis is fulfilled in the context of wearable video <br>monitoring of patients with aged dementia. The idea is to provide a new tool to medical <br>practitioners for the early diagnosis of elderly dementia such as the Alzheimer disease. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:uRkNpe1hbosJ:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10047075492014397881&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'uRkNpe1hbosJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB54" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW54"><a href="http://isc.cs.bit.edu.cn/faculties/liuxiabi/papers/Dissertation_Wangyanjie.pdf" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bit.edu.cn</span><span class="gs_ggsS">bit.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://isc.cs.bit.edu.cn/faculties/liuxiabi/papers/Dissertation_Wangyanjie.pdf" class=yC55>åºäºæ¾èå±é¨ç¹å¾çè§è§ç©ä½è¡¨ç¤ºæ¹æ³</a></h3><div class="gs_a">çå½¦æ° - 2010 - isc.cs.bit.edu.cn</div><div class="gs_rs">æè¦è§è§ç©ä½è¡¨ç¤º(visual object representation) æ¯èç³»åºå±å¾åä¿¡æ¯åé«å±è¯­ä¹æ¦å¿µççº½å¸¦, <br>å¨å¾åæç¥, åºæ¯çè§£ç­è®¡ç®æºè§è§ä»»å¡ä¸­èµ·çå³é®æ§çä½ç¨. åºäºå±é¨ç¹å¾çè§è§ç©ä½è¡¨ç¤º<br>å·æè¡¨ç¤ºè½åå¼º, å¯¹å¾åé®æ¡åèæ¯æ··æ·è¾ä¸ºé²æ£çç¹ç¹, è¿å¹´æ¥å¼èµ·äººä»¬çé«åº¦éè§. æ¬æä»<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:TFXs66wiY64J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12565925511381669196&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'TFXs66wiY64J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md54', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md54" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TFXs66wiY64J:scholar.google.com/&amp;hl=en&amp;num=55&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
