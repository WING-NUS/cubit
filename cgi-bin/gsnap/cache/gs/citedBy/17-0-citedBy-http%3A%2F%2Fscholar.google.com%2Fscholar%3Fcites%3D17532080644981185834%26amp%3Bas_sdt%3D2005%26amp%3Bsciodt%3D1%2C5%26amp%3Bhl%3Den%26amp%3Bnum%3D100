Total results = 17
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.lv-nus.org/papers/2011/cvpr11-260.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lv-nus.org</span><span class="gs_ggsS">lv-nus.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5995369" class=yC0>Accelerated low-rank visual recovery by random projection</a></h3><div class="gs_a"><a href="/citations?user=Fqqx4HsAAAAJ&amp;hl=en&amp;oi=sra">Y Mu</a>, J Dong, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">X Yuan</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a> - Computer Vision and Pattern  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Exact recovery from contaminated visual data plays an important role in various <br>tasks. By assuming the observed data matrix as the addition of a low-rank matrix and a <br>sparse matrix, theoretic guarantee exists under mild conditions for exact data recovery. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17841340091212304779&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 12</a> <a href="/scholar?q=related:ixGlFUksmfcJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17841340091212304779&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'ixGlFUksmfcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC2>TRECVID 2010 Known-item Search by NUS</a></h3><div class="gs_a">XY Chen, J Yuan, L Nie, ZJ Zha, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - TRECVID  &hellip;, 2010 - www-nlpir.nist.gov</div><div class="gs_rs">Abstract. This paper describes our system for auto search and interactive search in the <br>known-item search (KIS) task in TRECVID 2010. KIS task aims to find an unique video <br>answer for each text query. The shift from traditional video search has prompted a series <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12664714192218118309&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 3</a> <a href="/scholar?q=related:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'pVgCEXUawq8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/MM2011.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072334" class=yC4>Towards multi-semantic image annotation with graph regularized exclusive group lasso</a></h3><div class="gs_a">X Chen, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">X Yuan</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, J Tang, <a href="/citations?user=uOJH_AEAAAAJ&amp;hl=en&amp;oi=sra">Y Rui</a>&hellip; - Proceedings of the 19th  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract To bridge the semantic gap between low level feature and human perception, most <br>of the existing algorithms aim mainly at annotating images with concepts coming from only <br>one semantic space, eg cognitive or affective. The naive combination of the outputs from <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11933754482419175968&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 3</a> <a href="/scholar?q=related:IBJSWXk2naUJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11933754482419175968&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'IBJSWXk2naUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://ima.ac.uk/papers/Hao2012d.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ima.ac.uk</span><span class="gs_ggsS">ima.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ima.ac.uk/papers/Hao2012d.pdf" class=yC6>Fast Semantic Image Retrieval Based on Random Forest</a></h3><div class="gs_a">H Fu, <a href="/citations?user=pHkKtyMAAAAJ&amp;hl=en&amp;oi=sra">G Qiu</a> - ACM MM, 2012 - ima.ac.uk</div><div class="gs_rs">ABSTRACT This paper introduces random forest as a computational and data structure <br>paradigm for fusing low-level visual features and high-level semantic concepts for image <br>retrieval. We use visual features to split the tree nodes and use the image labels to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6324048158802542345&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 1</a> <a href="/scholar?q=related:CX_jT0uIw1cJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'CX_jT0uIw1cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:CX_jT0uIw1cJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6335467" class=yC8>Sparsity Induced Similarity Measure and Its Applications</a></h3><div class="gs_a">H Cheng, Z Liu, L Hou, J Yang - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The structures of feature vectors based semi-supervised/supervised learning has <br>gained considerable interests in the past several years thanks to its effectiveness for better <br>object modeling and classication. In many machine learning and computer vision tasks, a <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'uCpLO_te7KYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ICCV11.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6126323" class=yC9>Multi-label visual classification with label exclusive context</a></h3><div class="gs_a">X Chen, <a href="/citations?user=yzU6g24AAAAJ&amp;hl=en&amp;oi=sra">XT Yuan</a>, Q Chen, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - Computer Vision (ICCV &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We introduce in this paper a novel approach to multi-label image classification <br>which incorporates a new type of context-label exclusive context-with linear representation <br>and classification. Given a set of exclusive label groups that describe the negative <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2422096281759318535&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 2</a> <a href="/scholar?q=related:BwIPRrwDnSEJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2422096281759318535&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'BwIPRrwDnSEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231213000192" class=yCB>Jing Liu, Yifan Zhang, Zechao Li, Hanqing Lu</a></h3><div class="gs_a">J Liu, Y Zhang, Z Li, H Lu - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract With the permeation of Web 2.0, large-scale user contributed images with tags are <br>easily available on social websites. However, the noisy or incomplete correspondence <br>between images and tags prohibit us from precise image retrieval and effective <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'mhd7TfI-GV8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/P683840235086106.pdf" class=yCC>Improving image tags by exploiting web search results</a></h3><div class="gs_a">X Zhang, Z Li, W Chao - Multimedia Tools and Applications, 2011 - Springer</div><div class="gs_rs">Abstract Automatic image tagging automatically assigns image with semantic keywords <br>called tags, which significantly facilitates image search and organization. Most of present <br>image tagging approaches are constrained by the training model learned from the training <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:bJ_eVAvtPIwJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'bJ_eVAvtPIwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.ee.columbia.edu/ln/dvmm/publications/12/ECCV_HashOpt.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ee.columbia.edu/ln/dvmm/publications/12/ECCV_HashOpt.pdf" class=yCD>Accelerated Large Scale Optimization by Concomitant Hashing</a></h3><div class="gs_a"><a href="/citations?user=Fqqx4HsAAAAJ&amp;hl=en&amp;oi=sra">Y Mu</a>, J Wright, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - ee.columbia.edu</div><div class="gs_rs">Abstract. Traditional locality-sensitive hashing (LSH) techniques aim to tackle the curse of <br>explosive data scale by guaranteeing that similar samples are projected onto proximal hash <br>buckets. Despite the success of LSH on numerous vision tasks like image retrieval and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Nvf2ZovqVa8J:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Nvf2ZovqVa8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Nvf2ZovqVa8J:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://jcc2011.utalca.cl/actas/ET/et2011_submission_16.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utalca.cl</span><span class="gs_ggsS">utalca.cl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2071974" class=yCF>Automatic image tagging through information propagation in a query log based graph structure</a></h3><div class="gs_a">T Bracamonte, <a href="/citations?user=eZBFWGEAAAAJ&amp;hl=en&amp;oi=sra">B Poblete</a> - Proceedings of the 19th ACM international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Annotating or tagging multimedia objects is an important task for enhancing <br>multimedia information retrieval processes. In the context of the Web, automatic tagging <br>deals with many issues, such as loosely tagged images and huge collections of images <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Lwz-eGfpeF4J:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6807447467391323183&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'Lwz-eGfpeF4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/U2413311423U2722.pdf" class=yC11>Automatic tagging by exploring tag information capability and correlation</a></h3><div class="gs_a">X Zhang, Z Huang, <a href="/citations?user=krryaDkAAAAJ&amp;hl=en&amp;oi=sra">HT Shen</a>, <a href="/citations?user=PVv2xDYAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, Z Li - World Wide Web, 2012 - Springer</div><div class="gs_rs">Abstract Automatic tagging can automatically label images and videos with semantic tags to <br>significantly facilitate multimedia search and organization. However, most of existing tagging <br>algorithms often don&#39;t differentiate between tags used to describe visual content, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1763491691324278466&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 2</a> <a href="/scholar?q=related:woa8kVkueRgJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1763491691324278466&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'woa8kVkueRgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212006984" class=yC12>Nonlinear matrix factorization with unified embedding for social tag relevance learning</a></h3><div class="gs_a">Z Li, J Liu, H Lu - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Abstract With the proliferation of social images, social image tagging is an essential issue for <br>text-based social image retrieval. However, the original tags annotated by web users are <br>always noisy, irrelevant and incomplete to interpret the image visual contents. In this <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'dfqYv1IpuOYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/1188v3u67847u332.pdf" class=yC13>Refining Annotations by Spreading Activation Theory</a></h3><div class="gs_a">S Zhu, L Zou, Z Liang, B Wang - National Academy Science Letters, 2012 - Springer</div><div class="gs_rs">Abstract The overwhelming amounts of digital images on the Web and personal computers <br>have triggered the requirement of an effective tool to retrieve images of interest using <br>semantic concepts. Due to the semantic gap between low-level features of image content <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:LC2jz9_5jrEJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12794438331062758700&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'LC2jz9_5jrEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://ylu.cc/cikm2012.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ylu.cc</span><span class="gs_ggsS">ylu.cc <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2398532" class=yC14>Semantic context learning with large-scale weakly-labeled image set</a></h3><div class="gs_a">Y Lu, W Zhang, K Zhang, X Xue - Proceedings of the 21st ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract There are a large number of images available on the web; meanwhile, only a <br>subset of web images can be labeled by professionals because manual annotation is time-<br>consuming and labor-intensive. Although we can now use the collaborative image tagging <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'GkUs5GLH-eIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://cmsassets.comp.nus.edu.sg/~tancl/publications/c2012/ictai2012_annotation.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cmsassets.comp.nus.edu.sg/~tancl/publications/c2012/ictai2012_annotation.pdf" class=yC16>Automatic Image Annotation using Word Embedding Learning</a></h3><div class="gs_a">Q Chen, <a href="/citations?user=2vaXBtEAAAAJ&amp;hl=en&amp;oi=sra">A Yip</a>, <a href="/citations?user=nmFSOaEAAAAJ&amp;hl=en&amp;oi=sra">CL Tan</a> - cmsassets.comp.nus.edu.sg</div><div class="gs_rs">AbstractâAutomatically annotating words for images is a key to semantic-level image <br>retrieval. Recently, several embedding learning based methods achieve good performance <br>in this task which inspires this paper. Here we propose a novel word embedding model in <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'wKvz92MaN1YJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md14', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md14" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:wKvz92MaN1YJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICASSP%202012/pdfs/0001041.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6288064" class=yC18>Spreading activation theory based image annotation</a></h3><div class="gs_a">S Zhu, B Wang, Y Liu - Acoustics, Speech and Signal  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The overwhelming amounts of digital images on the Web and personal computers <br>have triggered the requirement of an effective tool to retrieve images of interest using <br>semantic concepts. Due to the semantic gap between low level content features and its <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:k4lk-2c7dWAJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6950526917701568915&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'k4lk-2c7dWAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6244550" class=yC1A>Labelling images with spreading activation theory</a></h3><div class="gs_a">Z Songhao, S Wei, L Zhiwei - Control and Decision Conference  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The overwhelming amounts of digital images on the Web and personal computers <br>have triggered the requirement of an effective tool to retrieve images of interest using <br>semantic concepts. Due to the semantic gap between low-level features of image content <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:IxK6vrO7ar8J:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'IxK6vrO7ar8J')" href="#" class="gs_nph">Cite</a></div></div></div>
