Total results = 12
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://strum.googlecode.com/svn-history/r41/trunk/Research/CAMIT.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from googlecode.com</span><span class="gs_ggsS">googlecode.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1290156" class=yC0>Effective use of multimedia for computer-assisted musical instrument tutoring</a></h3><div class="gs_a"><a href="/citations?user=xCnDkXAAAAAJ&amp;hl=en&amp;oi=sra">G Percival</a>, Y Wang, <a href="/citations?user=yPgxxpwAAAAJ&amp;hl=en&amp;oi=sra">G Tzanetakis</a> - Proceedings of the international  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract This paper presents a survey of recent work in computer-assisted musical <br>instrumental tutoring and outlines several questions to consider when developing future <br>projects. In particular, we suggest that the area ingreatest need of computer assistance is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=963958909237274801&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=12">Cited by 18</a> <a href="/scholar?q=related:sSgpS4WrYA0J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=963958909237274801&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">All 21 versions</a> <a onclick="return gs_ocit(event,'sSgpS4WrYA0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="https://www1.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2007_Educational_Violin_Transcription_by_Fusing_Multimedia_Streams.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1290154" class=yC2>Educational violin transcription by fusing multimedia streams</a></h3><div class="gs_a">Y Wang, B Zhang, O Schleusing - Proceedings of the international  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Computer-assisted violin tutoring requires accurate violin transcription. For pitched <br>non-percussive (PNP) sounds such as from the violin, note segmentation is a much more <br>difficult task than pitch detection. This issue is accentuated when the audio is recorded <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1988379198861831619&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=12">Cited by 11</a> <a href="/scholar?q=related:w23vslgkmBsJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1988379198861831619&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'w23vslgkmBsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.mistic.ece.uvic.ca/publications/2007_ismir_sitar.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uvic.ca</span><span class="gs_ggsS">uvic.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.mistic.ece.uvic.ca/publications/2007_ismir_sitar.pdf" class=yC4>Pedagogical transcription for multimodal sitar performance</a></h3><div class="gs_a">A Kapur, <a href="/citations?user=xCnDkXAAAAAJ&amp;hl=en&amp;oi=sra">G Percival</a>, <a href="/citations?user=Z-D1LQwAAAAJ&amp;hl=en&amp;oi=sra">M Lagrange</a>&hellip; - Proc. Int&#39;l Conf. Music  &hellip;, 2007 - mistic.ece.uvic.ca</div><div class="gs_rs">ABSTRACT Most automatic music transcription research is concerned with producing sheet <br>music from the audio signal alone. However, the audio data does not include certain <br>performance data which is vital for the preservation of instrument performance techniques <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2768606964165807622&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=12">Cited by 6</a> <a href="/scholar?q=related:Bo7GfV0RbCYJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2768606964165807622&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'Bo7GfV0RbCYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md2', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md2" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Bo7GfV0RbCYJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www.comp.nus.edu.sg/~wangye/papers/3.Applications_in_Edutainment_(e-Learning)/2008_Application_Specific_Music_Transcription_for_Tutoring.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4623948" class=yC6>Application-specific music transcription for tutoring</a></h3><div class="gs_a">Y Wang, B Zhang - Multimedia, IEEE, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic music transcription (AMT) refers to the ability of computers to write note <br>information, such as the pitch, onset time, duration, and source of each sound, after listening <br>to the music. Our application scenario is computer-assisted, musical-instrument tutoring, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=176626421973561276&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=12">Cited by 5</a> <a href="/scholar?q=related:vK-ug8uAcwIJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=176626421973561276&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">All 18 versions</a> <a onclick="return gs_ocit(event,'vK-ug8uAcwIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="https://dl.comp.nus.edu.sg/dspace/bitstream/1900.100/3056/1/TRA7-09.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://dl.comp.nus.edu.sg/dspace/handle/1900.100/3056" class=yC8>Automatic music transcription using audio-visual fusion for violin practice in home environment</a></h3><div class="gs_a">B Zhang, Y Wang - 2009 - dl.comp.nus.edu.sg</div><div class="gs_rs">Abstract: Violin practice in a home environment, where there is often no teacher available, <br>can benefit from automatic music transcription to provide feedback to the student. This paper <br>describes a high performance violin transcription system with three main contributions. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15951167519198165592&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=12">Cited by 2</a> <a href="/scholar?q=related:WDqOhljuXd0J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15951167519198165592&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'WDqOhljuXd0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://wwwiti.cs.uni-magdeburg.de/~stober/publ/mtap2012adaptiveMIR.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-magdeburg.de</span><span class="gs_ggsS">uni-magdeburg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/76x78554q0526132.pdf" class=yCA>Adaptive music retrievalâa state of the art</a></h3><div class="gs_a">S Stober, <a href="/citations?user=LuMoBX0AAAAJ&amp;hl=en&amp;oi=sra">A NÃ¼rnberger</a> - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract With the development of more and more sophisticated Music Information Retrieval <br>approaches, aspects of adaptivity are becoming an increasingly important research topic. <br>Even though, adaptive techniques have already found their way into Music Information <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MvLbFPg5nPEJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17409853997172126258&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MvLbFPg5nPEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://digitallibrary.usc.edu/assetserver/controller/item/etd-Chuan-20080724.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from usc.edu</span><span class="gs_ggsS">usc.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://digitallibrary.usc.edu/assetserver/controller/item/etd-Chuan-20080724.pdf" class=yCC>Hybrid methods for music analysis and synthesis: Audio key finding and automatic style-specific accompaniment</a></h3><div class="gs_a">CH Chuan - 2008 - digitallibrary.usc.edu</div><div class="gs_rs">The Mirror of Erised at Hogwards that Harry Potter discovers in his first year there shows not <br>your reflections, but your heart&#39;s desire. During the years of my doctoral study, I cannot <br>remember just how many times I felt that if I looked into the Mirror of Erised, I would simply <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:dkoAG5E2Ib8J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13772349132326849142&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'dkoAG5E2Ib8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md6', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md6" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:dkoAG5E2Ib8J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://ismir2012.ismir.net/event/papers/379-ismir-2012.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ismir.net</span><span class="gs_ggsS">ismir.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ismir2012.ismir.net/event/papers/379-ismir-2012.pdf" class=yCE>AUTOMATIC MUSIC TRANSCRIPTION: BREAKING THE GLASS CEILING</a></h3><div class="gs_a"><a href="/citations?user=Wg49oI4AAAAJ&amp;hl=en&amp;oi=sra">E Benetos</a>, <a href="/citations?user=9GvyqXEAAAAJ&amp;hl=en&amp;oi=sra">S Dixon</a>, D Giannoulis, H Kirchhoff&hellip; - 2012 - ismir2012.ismir.net</div><div class="gs_rs">ABSTRACT Automatic music transcription is considered by many to be the Holy Grail in the <br>field of music signal analysis. However, the performance of transcription systems is still <br>significantly below that of a human expert, and accuracies reported in recent years seem <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:XdGqHjeJcC0J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3274267798929068381&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'XdGqHjeJcC0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:XdGqHjeJcC0J:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://137.132.14.55/bitstream/handle/10635/20949/ZhangBJ.pdf?sequence=1" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.14.55</span><span class="gs_ggsS">137.132.14.55 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://137.132.14.55/handle/10635/20949" class=yC10>Adaptive multimodal fusion based similarity measures in music information retrieval</a></h3><div class="gs_a">Z BINGJUN - 2010 - 137.132.14.55</div><div class="gs_rs">In the field of music information retrieval (MIR), one fundamental research problem is the <br>measuring of the similarity between music documents. Based on a viable similarity measure, <br>MIR systems can be made more effective to help users retrieve relevant music information. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9FjpHoxTlRIJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1339068325491726580&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'9FjpHoxTlRIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://www.numediart.org/download/numediart_2012_s18_p3_report.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from numediart.org</span><span class="gs_ggsS">numediart.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.numediart.org/download/numediart_2012_s18_p3_report.pdf" class=yC12>REALTIME TABLATURE</a></h3><div class="gs_a">L ReboursiÃ¨re, <a href="/citations?user=gPwpmhQAAAAJ&amp;hl=en&amp;oi=sra">S Dupont</a> - numediart.org</div><div class="gs_rs">ABSTRACT In this project, we present a system which generates real-time and complete (ie <br>with all guitar playing techniques) guitar tablature. It uses guitar playing techniques <br>algorithms developed in previous Guitar As Controller numediart project and is <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'csITSlH5HCQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:csITSlH5HCQJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://up.stevetjoa.com/p25.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from stevetjoa.com</span><span class="gs_ggsS">stevetjoa.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2390855" class=yC14>Learning and extraction of violin instrumental controls from audio signal</a></h3><div class="gs_a">A Perez Carrillo, MM Wanderley - Proceedings of the second  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Acquisition of instrumental gestures in musical performances is an important task <br>used in different fields ranging from acoustics and sound synthesis to motor learning or <br>electroacoustic performances. The most common approach for acquiring gestures is by <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'_B5jhH9ug8gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://www.sfu.ca/~eigenfel/ICMC08_Multiagent_Multimodal.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sfu.ca</span><span class="gs_ggsS">sfu.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.sfu.ca/~eigenfel/ICMC08_Multiagent_Multimodal.pdf" class=yC16>MULTI-AGENT MULTIMODAL PERFORMANCE ANALYSIS</a></h3><div class="gs_a"><a href="/citations?user=BOxiIGYAAAAJ&amp;hl=en&amp;oi=sra">A Eigenfeldt</a>, A Kapur - 2008 - sfu.ca</div><div class="gs_rs">ABSTRACT This paper describes a custom built system which extracts high-level musical <br>information from real-time multimodal gesture data. Data is collected from sensors during <br>rehearsal using one program, GATHER, and, combined with audio analysis, is used to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:KsPMitJqg0EJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4720734286918763306&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'KsPMitJqg0EJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:KsPMitJqg0EJ:scholar.google.com/&amp;hl=en&amp;num=12&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
