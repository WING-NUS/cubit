Total results = 102
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://academiccommons.columbia.edu/download/fedora_content/download/ac:148561/CONTENT/EllisP07-coversongs.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4218379" class=yC0>Identifyingcover songs&#39; with chroma features and dynamic programming beat tracking</a></h3><div class="gs_a"><a href="/citations?user=1H4HuCkAAAAJ&amp;hl=en&amp;oi=sra">DPW Ellis</a>, GE Poliner - Acoustics, Speech and Signal  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Large music collections, ranging from thousands to millions of tracks, are unsuited <br>to manual searching, motivating the development of automatic search methods. When <br>different musicians perform the same underlying song or piece, these are known ascover&#39;<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17047730646036020079&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 184</a> <a href="/scholar?q=related:b3uEJ7O0lewJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17047730646036020079&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'b3uEJ7O0lewJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://mpac.ee.ntu.edu.tw/~yihsuan/pub/TASLP08.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432654" class=yC2>A regression approach to music emotion recognition</a></h3><div class="gs_a"><a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, YC Lin, YF Su&hellip; - Audio, Speech, and  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Content-based retrieval has emerged in the face of content explosion as a <br>promising approach to information access. In this paper, we focus on the challenging issue <br>of recognizing the emotion content of music signals, or music emotion recognition (MER). <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9038955809862194709&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 114</a> <a href="/scholar?q=related:FRZKK2vScH0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/16/31/RN222855522.html?source=googlescholar" class="gs_nph" class=yC4>BL Direct</a> <a href="/scholar?cluster=9038955809862194709&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'FRZKK2vScH0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.violconsort.com/mark/IEEEASLP.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from violconsort.com</span><span class="gs_ggsS">violconsort.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432648" class=yC5>Structural segmentation of musical audio by constrained clustering</a></h3><div class="gs_a"><a href="/citations?user=g_ESIWoAAAAJ&amp;hl=en&amp;oi=sra">M Levy</a>, M Sandler - Audio, Speech, and Language Processing, &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We describe a method of segmenting musical audio into structural sections based <br>on a hierarchical labeling of spectral features. Frames of audio are first labeled as belonging <br>to one of a number of discrete states using a hidden Markov model trained on the features. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15726336302078246268&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 66</a> <a href="/scholar?q=related:fD20h4srP9oJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/35/3A/RN222855410.html?source=googlescholar" class="gs_nph" class=yC7>BL Direct</a> <a href="/scholar?cluster=15726336302078246268&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'fD20h4srP9oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4459285" class=yC8>Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval</a></h3><div class="gs_a"><a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, YT Zhuang, F Wu&hellip; - &hellip; , IEEE Transactions on, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we consider the problem of multimedia document (MMD) semantics <br>understanding and content-based cross-media retrieval. An MMD is a set of media objects of <br>different modalities but carrying the same semantics and the content-based cross-media <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8802451763731361544&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 77</a> <a href="/scholar?q=related:CO-34T2XKHoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0B/2B/RN226783670.html?source=googlescholar" class="gs_nph" class=yC9>BL Direct</a> <a href="/scholar?cluster=8802451763731361544&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'CO-34T2XKHoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://koti.kapsi.fi/paulus/pubs/paulus_ismir10_star_presentation.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kapsi.fi</span><span class="gs_ggsS">kapsi.fi <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://koti.kapsi.fi/paulus/pubs/paulus_ismir10_star_presentation.pdf" class=yCA>State of the art report: Audio-based music structure analysis</a></h3><div class="gs_a">J Paulus, <a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a>, <a href="/citations?user=opK6nQ4AAAAJ&amp;hl=en&amp;oi=sra">A Klapuri</a> - &hellip;  of the 11th international society for music  &hellip;, 2010 - koti.kapsi.fi</div><div class="gs_rs">Page 1. Paulus, MÃ¼ller, Klapuri â Audio-Based Music Structure Analysis â ISMIR 2010 Jouni<br>Paulus* (Fraunhofer Institute for Integrated Circuits IIS, Germany) Meinard MÃ¼ller (Saarland<br>University and Max-Planck-Institut fÃ¼r Informatik, Germany) <b>...</b> </div><div class="gs_fl"><a href="/scholar?cites=4649864227187706501&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 60</a> <a href="/scholar?q=related:hcpR6-Cih0AJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4649864227187706501&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'hcpR6-Cih0AJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:hcpR6-Cih0AJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.4311&amp;rep=rep1&amp;type=pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1661200" class=yCC>Extraction of high-level musical structure from audio data and its application to thumbnail generation</a></h3><div class="gs_a"><a href="/citations?user=g_ESIWoAAAAJ&amp;hl=en&amp;oi=sra">M Levy</a>, M Sandier, <a href="/citations?user=2dTrwzAAAAAJ&amp;hl=en&amp;oi=sra">M Casey</a> - Acoustics, Speech and Signal  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A method for segmenting musical audio with a hierarchical timbre model is <br>introduced. New evidence is presented to show that music segmentation can be recast as <br>clustering of timbre features, and a new clustering algorithm is described. A prototype <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18111865083665858380&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 52</a> <a href="/scholar?q=related:TD-0M1VFWvsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18111865083665858380&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'TD-0M1VFWvsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://sites.google.com/site/feipingnie/acmmm09_ranking_LRGA.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631298" class=yCE>Ranking with local regression and global alignment for cross media retrieval</a></h3><div class="gs_a"><a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, D Xu, <a href="/citations?user=2oB4nAIAAAAJ&amp;hl=en&amp;oi=sra">F Nie</a>, <a href="/citations?user=CcbnBvgAAAAJ&amp;hl=en&amp;oi=sra">J Luo</a>, Y Zhuang - Proceedings of the 17th ACM  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Rich multimedia content including images, audio and text are frequently used to <br>describe the same semantics in E-Learning and Ebusiness web pages, instructive slides, <br>multimedia cyclopedias, and so on. In this paper, we present a framework for cross-media <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15912432037925230564&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 53</a> <a href="/scholar?q=related:5BcUfaFQ1NwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15912432037925230564&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'5BcUfaFQ1NwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.2405&amp;rep=rep1&amp;type=pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.2405&amp;rep=rep1&amp;type=pdf" class=yC10>Theory and evaluation of a Bayesian music structure extractor</a></h3><div class="gs_a"><a href="/citations?user=ZFJpmMYAAAAJ&amp;hl=en&amp;oi=sra">S Abdallah</a>, K Noland, M Sandler, <a href="/citations?user=2dTrwzAAAAAJ&amp;hl=en&amp;oi=sra">M Casey</a>&hellip; - Proc. of 6th International  &hellip;, 2005 - Citeseer</div><div class="gs_rs">ABSTRACT We introduce a new model for extracting classified structural segments, such as <br>intro, verse, chorus, break and so forth, from recorded music. Our approach is to classify <br>signal frames on the basis of their audio properties and then to agglomerate contiguous <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14633783171145416769&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 45</a> <a href="/scholar?q=related:QUA21EKkFcsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14633783171145416769&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'QUA21EKkFcsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:QUA21EKkFcsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432631" class=yC12>Mining semantic correlation of heterogeneous multimedia data for cross-media retrieval</a></h3><div class="gs_a">YT Zhuang, <a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, F Wu - Multimedia, IEEE Transactions on, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Although multimedia objects such as images, audios and texts are of different <br>modalities, there are a great amount of semantic correlations among them. In this paper, we <br>propose a method of transductive learning to mine the semantic correlations among <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11074042123883829462&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 41</a> <a href="/scholar?q=related:1tjOjaPmrpkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/4B/17/RN223257230.html?source=googlescholar" class="gs_nph" class=yC13>BL Direct</a> <a href="/scholar?cluster=11074042123883829462&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'1tjOjaPmrpkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://www.mpi-inf.mpg.de/departments/d4/teaching/ss2008/ir_mm/2008_Meinard_Mueller_Lecture_IR_MATLAB_exercise.pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://www.mpi-inf.mpg.de/departments/d4/teaching/ss2008/ir_mm/2008_Meinard_Mueller_Lecture_IR_MATLAB_exercise.pdf" class=yC14>Information retrieval for music and motion</a></h3><div class="gs_a"><a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a> - 2007 - mpi-inf.mpg.de</div><div class="gs_rs">Probieren Sie weiterhin die folgenden wichtigen, allgemeinen Matlab-Funktionen aus. ans <br>Most recent answer. cd Change current working directory. clear Clear variables and <br>functions from memory. demo Run demonstrations. dir List directory. doc Display HTML <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5688487003992563075&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 230</a> <a href="/scholar?q=related:gx1njsqQ8U4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5688487003992563075&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'gx1njsqQ8U4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:gx1njsqQ8U4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:gx1njsqQ8U4J:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=11947113820059534644&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://asp.eurasipjournals.com/content/pdf/1687-6180-2007-089686.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurasipjournals.com</span><span class="gs_ggsS">eurasipjournals.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Towards structural analysis of audio recordings in the presence of musical variations</h3><div class="gs_a"><a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a>, F Kurth - EURASIP Journal on Advances in Signal Processing, 2007</div><div class="gs_fl"><a href="/scholar?cites=13053987245830761969&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 39</a> <a href="/scholar?q=related:8e04sjwUKbUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13053987245830761969&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 18 versions</a> <a onclick="return gs_ocit(event,'8e04sjwUKbUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://www.ircam.fr/equipes/analyse-synthese/peeters/ARTICLES/Peeters_2007_ISMIR_Structure.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ircam.fr</span><span class="gs_ggsS">ircam.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ircam.fr/equipes/analyse-synthese/peeters/ARTICLES/Peeters_2007_ISMIR_Structure.pdf" class=yC17>Sequence representation of music structure using higher-order similarity matrix and maximum-likelihood approach</a></h3><div class="gs_a">G Peeters - Proc. ISMIR, Vienna, Austria, 2007 - ircam.fr</div><div class="gs_rs">ABSTRACT In this paper, we present a novel method for the automatic estimation of the <br>structure of music tracks using a sequence representation. A set of timbre-related (MFCC <br>and Spectral Contrast) and pitch-related (Pitch Class Profile) features are first extracted <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3339229770850492744&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 40</a> <a href="/scholar?q=related:SOHz2shTVy4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3339229770850492744&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'SOHz2shTVy4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:SOHz2shTVy4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://koti.kapsi.fi/paulus/pubs/amcmm718-paulus_copy.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kapsi.fi</span><span class="gs_ggsS">kapsi.fi <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1178733" class=yC19>Music structure analysis by finding repeated parts</a></h3><div class="gs_a">J Paulus, <a href="/citations?user=opK6nQ4AAAAJ&amp;hl=en&amp;oi=sra">A Klapuri</a> - Proceedings of the 1st ACM Workshop on Audio  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract The structure of a musical piece can be described with segments having a certain <br>time range and a label. Segments having the same label are considered as occurrences of a <br>certain structural part. Here, a system for finding structural descriptions is presented. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14322336839170754803&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 39</a> <a href="/scholar?q=related:84hD5Hgpw8YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14322336839170754803&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'84hD5Hgpw8YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://www.cmlab.csie.ntu.edu.tw/~known/known200611/slides/[200908]Music%20Information%20Retrieval%20Using%20Social%20Tags%20and%20Audio-slide.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4802376" class=yC1B>Music information retrieval using social tags and audio</a></h3><div class="gs_a"><a href="/citations?user=g_ESIWoAAAAJ&amp;hl=en&amp;oi=sra">M Levy</a>, M Sandler - Multimedia, IEEE Transactions on, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper we describe a novel approach to applying text-based information <br>retrieval techniques to music collections. We represent tracks with a joint vocabulary <br>consisting of both conventional words, drawn from social tags, and audio muswords, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15919025841187699965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 45</a> <a href="/scholar?q=related:_bxB8qi969wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15919025841187699965&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'_bxB8qi969wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://www.mirlab.org/conference_papers/International_Conference/ACM%202005/docs/mm735.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101309" class=yC1D>Automatic generation of personalized music sports video</a></h3><div class="gs_a">J Wang, C Xu, <a href="/citations?user=FJodrCcAAAAJ&amp;hl=en&amp;oi=sra">E Chng</a>, L Duan, K Wan&hellip; - Proceedings of the 13th  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose a novel automatic approach for personalized music sports <br>video generation. Two research challenges, semantic sports video content selection and <br>automatic video composition, are addressed. For the first challenge, we propose to use <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3258662872762225524&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 35</a> <a href="/scholar?q=related:dBO_HZ4YOS0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3258662872762225524&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'dBO_HZ4YOS0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www.comp.nus.edu.sg/~mohan/papers/music_struct_det.pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1580435" class=yC1F>Automatic structure detection for popular music</a></h3><div class="gs_a">NC Maddage - Multimedia, IEEE, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Our proposed approach detects music structures by looking at beat-space <br>segmentation, chords, singing-voice boundaries, and melody-and content-based similarity <br>regions. Experiments illustrate that the proposed approach is capable of extracting useful <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7556639789070752933&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 36</a> <a href="/scholar?q=related:pRwl89mT3mgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3D/1C/RN183745851.html?source=googlescholar" class="gs_nph" class=yC21>BL Direct</a> <a href="/scholar?cluster=7556639789070752933&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'pRwl89mT3mgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www1.i2r.a-star.edu.sg/~hli/papers/101109TASL2006876756.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from a-star.edu.sg</span><span class="gs_ggsS">a-star.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4067048" class=yC22>Exploring vibrato-motivated acoustic features for singer identification</a></h3><div class="gs_a">TL Nwe, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - Audio, Speech, and Language Processing, IEEE &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Vibrato is a slightly tremulous effect imparted to vocal or instrumental tone for added <br>warmth and expressiveness through slight variation in pitch. It corresponds to a periodic <br>fluctuation of the fundamental frequency. It is common for a singer to develop a vibrato <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11242109298055456239&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 32</a> <a href="/scholar?q=related:7yW25Nf-A5wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/03/0A/RN204606041.html?source=googlescholar" class="gs_nph" class=yC24>BL Direct</a> <a href="/scholar?cluster=11242109298055456239&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'7yW25Nf-A5wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5410051" class=yC25>Towards timbre-invariant audio features for harmony-based music</a></h3><div class="gs_a"><a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M Muller</a>, <a href="/citations?user=kkwnObwAAAAJ&amp;hl=en&amp;oi=sra">S Ewert</a> - Audio, Speech, and Language Processing,  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Chroma-based audio features are a well-established tool for analyzing and <br>comparing harmony-based Western music that is based on the equal-tempered scale. By <br>identifying spectral components that differ by a musical octave, chroma features possess a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=410436739843607812&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 34</a> <a href="/scholar?q=related:BM0lUQoqsgUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=410436739843607812&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'BM0lUQoqsgUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.8768&amp;rep=rep1&amp;type=pdf" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/QJK515429217U131.pdf" class=yC26>Using duration models to reduce fragmentation in audio segmentation</a></h3><div class="gs_a"><a href="/citations?user=ZFJpmMYAAAAJ&amp;hl=en&amp;oi=sra">S Abdallah</a>, M Sandler, C Rhodes, <a href="/citations?user=2dTrwzAAAAAJ&amp;hl=en&amp;oi=sra">M Casey</a> - Machine Learning, 2006 - Springer</div><div class="gs_rs">Abstract We investigate explicit segment duration models in addressing the problem of <br>fragmentation in musical audio segmentation. The resulting probabilistic models are <br>optimised using Markov Chain Monte Carlo methods; in particular, we introduce a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13832652747695128349&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 28</a> <a href="/scholar?q=related:Hf-yr2J0978J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/58/4E/RN198962994.html?source=googlescholar" class="gs_nph" class=yC28>BL Direct</a> <a href="/scholar?cluster=13832652747695128349&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'Hf-yr2J0978J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://www.ntu.edu.sg/home/dongxu/TPAMI-Retrieval.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5989829" class=yC29>A multimedia retrieval framework based on semi-supervised ranking and relevance feedback</a></h3><div class="gs_a"><a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, <a href="/citations?user=2oB4nAIAAAAJ&amp;hl=en&amp;oi=sra">F Nie</a>, D Xu, <a href="/citations?user=CcbnBvgAAAAJ&amp;hl=en&amp;oi=sra">J Luo</a>, Y Zhuang&hellip; - Pattern Analysis and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present a new framework for multimedia content analysis and retrieval which <br>consists of two independent algorithms. First, we propose a new semi-supervised algorithm <br>called ranking with Local Regression and Global Alignment (LRGA) to learn a robust <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5223703788302730835&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 34</a> <a href="/scholar?q=related:U4alSetSfkgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5223703788302730835&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'U4alSetSfkgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://www.jdl.ac.cn/doc/2008/affective%20mtv%20analysis%20based%20on%20arousal%20and%20valence%20features.pdf" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jdl.ac.cn</span><span class="gs_ggsS">jdl.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607698" class=yC2B>Affective MTV analysis based on arousal and valence features</a></h3><div class="gs_a">S Zhang, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, Q Huang&hellip; - Multimedia and Expo,  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Nowadays, MTV has become an important favorite pastime to modern people <br>because of its conciseness, convenience to play and the characteristic that can bring both <br>audio and visual experiences to audiences. In this paper, we propose an affective MTV <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11056272765712975143&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 26</a> <a href="/scholar?q=related:J8UldIDFb5kJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11056272765712975143&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'J8UldIDFb5kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4106817" class=yC2D>Learning semantic correlations for cross-media retrieval</a></h3><div class="gs_a">F Wu, H Zhang, Y Zhuang - Image Processing, 2006 IEEE  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper proposes a novel cross-media retrieval approach. First, an isomorphic <br>subspace is constructed based on canonical correlation analysis (CCA) to learn multi-modal <br>correlations of media objects; second, polar coordinates are used to judge the general <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9092601148577109251&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 26</a> <a href="/scholar?q=related:A1U_6ZFoL34J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9092601148577109251&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'A1U_6ZFoL34J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://acccn.net/cr569/Rstuff/keys/Discriminaton_and_Retrieval_of_Animal_Sounds.pdf" class=yC2F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from acccn.net</span><span class="gs_ggsS">acccn.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1651344" class=yC2E>Discrimination and retrieval of animal sounds</a></h3><div class="gs_a">D Mitrovic, M Zeppelzauer&hellip; - Multi-Media Modelling  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Until recently few research has been performed in the area of animal sound <br>retrieval. The authors identify state-of-the-art techniques in general purpose sound <br>recognition by a broad survey of literature. Based on the findings, this paper gives a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17827314553027088179&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 28</a> <a href="/scholar?q=related:M89UACJYZ_cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17827314553027088179&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'M89UACJYZ_cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://www.comp.nus.edu.sg/~mohan/papers/sigir2006.pdf" class=yC31><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1148170.1148185" class=yC30>Music structure based vector space retrieval</a></h3><div class="gs_a">NC Maddage, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, MS Kankanhalli - Proceedings of the 29th annual  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract This paper proposes a novel framework for music content indexing and retrieval. <br>The music structure information, ie, timing, harmony and music region content, is <br>represented by the layers of the music structure pyramid. We begin by extracting this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17630696023791774416&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 22</a> <a href="/scholar?q=related:0CoASZ_QrPQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17630696023791774416&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'0CoASZ_QrPQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1112857" class=yC32>Key, chord, and rhythm tracking of popular music recordings</a></h3><div class="gs_a">A Shenoy, Y Wang - Computer Music Journal, 2005 - dl.acm.org</div><div class="gs_rs">In this article, we propose a framework to analyze a musical audio signal (sampled from a <br>popular music CD) and determine its key, provide usable chord transcriptions, and obtain <br>the hierarchical rhythm structure representation comprising the quarternote, half-note, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16290336487138294495&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 22</a> <a href="/scholar?q=related:33p2o7nmEuIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/2E/0B/RN175256451.html?source=googlescholar" class="gs_nph" class=yC33>BL Direct</a> <a href="/scholar?cluster=16290336487138294495&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'33p2o7nmEuIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://lsas2008.dke-research.de/proceedings/lsas2008_p45-59_PeiszerLidyRauber.pdf" class=yC35><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dke-research.de</span><span class="gs_ggsS">dke-research.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://lsas2008.dke-research.de/proceedings/lsas2008_p45-59_PeiszerLidyRauber.pdf" class=yC34>Automatic audio segmentation: Segment boundary and structure detection in popular music</a></h3><div class="gs_a">E Peiszer, T Lidy, A Rauber - Proc. of LSAS, 2008 - lsas2008.dke-research.de</div><div class="gs_rs">Abstract. Automatic Audio Segmentation aims at extracting information on a song&#39;s structure, <br>ie, segment boundaries, musical form and semantic labels like verse, chorus, bridge etc. <br>This information can be used to create representative song excerpts or summaries, to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16490954063835602987&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 23</a> <a href="/scholar?q=related:K6A0Jlmj2-QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16490954063835602987&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'K6A0Jlmj2-QJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md25', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md25" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:K6A0Jlmj2-QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://staff.aist.go.jp/m.goto/PAPER/ISMIR2007turnbull.pdf" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://staff.aist.go.jp/m.goto/PAPER/ISMIR2007turnbull.pdf" class=yC36>A supervised approach for detecting boundaries in music using difference features and boosting</a></h3><div class="gs_a"><a href="/citations?user=CaEaSR8AAAAJ&amp;hl=en&amp;oi=sra">D Turnbull</a>, G Lanckriet, E Pampalk&hellip; - Proc. of 8th International  &hellip;, 2007 - staff.aist.go.jp</div><div class="gs_rs">ABSTRACT A musical boundary is a transition between two musical segments such as a <br>verse and a chorus. Our goal is to automatically detect musical boundaries using <br>temporallylocal audio features. We develop a set of difference features that indicate when <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16387809607861868001&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 20</a> <a href="/scholar?q=related:4RVXEwMybeMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16387809607861868001&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'4RVXEwMybeMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:4RVXEwMybeMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.4497&amp;rep=rep1&amp;type=pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4130378" class=yC38>Generation of personalized music sports video using multimodal cues</a></h3><div class="gs_a">J Wang, <a href="/citations?user=FJodrCcAAAAJ&amp;hl=en&amp;oi=sra">E Chng</a>, C Xu, H Lu&hellip; - &hellip; , IEEE Transactions on, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose a novel automatic approach for personalized music sports <br>video generation. Two research challenges are addressed, specifically the semantic sports <br>video content extraction and the automatic music video composition. For the first <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5301732336943148437&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 18</a> <a href="/scholar?q=related:la2yfXiJk0kJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5301732336943148437&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'la2yfXiJk0kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.1178&amp;rep=rep1&amp;type=pdf" class=yC3B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1076097" class=yC3A>Automatic music video summarization based on audio-visual-text analysis and alignment</a></h3><div class="gs_a">C Xu, X Shao, NC Maddage&hellip; - Proceedings of the 28th  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose a novel approach for automatic music video <br>summarization based on audio-visual-text analysis and alignment. The music video is <br>separated into the music and video tracks. For the music track, the chorus is detected <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4445671957548823743&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 17</a> <a href="/scholar?q=related:v_ScWRczsj0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4445671957548823743&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'v_ScWRczsj0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://academiccommons.columbia.edu/download/fedora_content/download/ac:148603/CONTENT/Ellis06-coversongs.pdf" class=yC3D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://academiccommons.columbia.edu/catalog/ac%3A148602" class=yC3C>Identifying&#39;cover songs&#39; with beat-synchronous chroma features</a></h3><div class="gs_a"><a href="/citations?user=1H4HuCkAAAAJ&amp;hl=en&amp;oi=sra">DPW Ellis</a> - MIREX 2006, 2006 - academiccommons.columbia.edu</div><div class="gs_rs">Abstract: Large music collections, ranging from thousands to millions of tracks, are unsuited <br>to manual searching, motivating the development of automatic search methods. When two <br>musical groups perform the same underlying song or piece, these are known as&#39; cover&#39;<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15408065272752485748&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 17</a> <a href="/scholar?q=related:dLVx6rpx1NUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15408065272752485748&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'dLVx6rpx1NUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.5668&amp;rep=rep1&amp;type=pdf" class=yC3F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.5668&amp;rep=rep1&amp;type=pdf" class=yC3E>SEMANTIC SEGMENTATION OF MUSIC AUDIO</a></h3><div class="gs_a">BS Ong, <a href="/citations?user=x4X0Ia8AAAAJ&amp;hl=en&amp;oi=sra">P Herrera</a> - Proceedings of the... International Computer Music  &hellip;, 2005 - Citeseer</div><div class="gs_rs">ABSTRACT This paper proposes a novel approach to detect structural changes in music <br>audio signals and provides a way to separate the different âsectionsâ of a piece, ie <br>âintroâ,âverseâ or âchorusâ. Herein, we divide the segment boundaries detection task into a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8565057776359964071&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 17</a> <a href="/scholar?q=related:p_2hAKsy3XYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8565057776359964071&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'p_2hAKsy3XYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md30', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md30" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:p_2hAKsy3XYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.205.8740&amp;rep=rep1&amp;type=pdf" class=yC41><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.205.8740&amp;rep=rep1&amp;type=pdf" class=yC40>Algorithms for determining and labelling approximate hierarchical self-similarity</a></h3><div class="gs_a">C Rhodes, <a href="/citations?user=2dTrwzAAAAAJ&amp;hl=en&amp;oi=sra">M Casey</a> - Proc. ISMIR, Vienna, Austria, 2007 - Citeseer</div><div class="gs_rs">ABSTRACT We describe an algorithm for finding approximate sequence similarity at all <br>scales of interest, being explicit about our modelling assumptions and the parameters of the <br>algorithm. We further present an algorithm for producing section labels based on the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13762110470085271981&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:reEdyY7W_L4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13762110470085271981&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'reEdyY7W_L4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md31', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md31" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:reEdyY7W_L4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284628" class=yC42>Automatic generation of music thumbnails</a></h3><div class="gs_a">T Zhang, R Samadani - Multimedia and Expo, 2007 IEEE  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic generation of music thumbnails based on content analysis is important <br>for efficient management and better consumption of digital music. In this paper, a unique <br>method for extracting thumbnails from a song based on structure analysis of the music <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15336833044439360712&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 13</a> <a href="/scholar?q=related:yORHWWVg19QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15336833044439360712&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'yORHWWVg19QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.8948&amp;rep=rep1&amp;type=pdf" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.8948&amp;rep=rep1&amp;type=pdf" class=yC43>Towards automatic music structural analysis: identifying characteristic within-song excerpts in popular music</a></h3><div class="gs_a">BS Ong - 2005 - Citeseer</div><div class="gs_rs">For real world popular music, it is common that listeners are apt at recognizing a piece of <br>music by just listening to certain excerpts of the music. For instance, when a prior heard <br>song is played halfway through on the radio, listeners are able to recognize the song <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11900861978047668262&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:JuB7YupaKKUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11900861978047668262&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'JuB7YupaKKUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md33', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md33" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:JuB7YupaKKUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://www.elec.qmul.ac.uk/people/markl/LevyNolandSandler07-icassp.pdf" class=yC46><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from qmul.ac.uk</span><span class="gs_ggsS">qmul.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4218380" class=yC45>A comparison of timbral and harmonic music segmentation algorithms</a></h3><div class="gs_a"><a href="/citations?user=g_ESIWoAAAAJ&amp;hl=en&amp;oi=sra">M Levy</a>, K Noland, M Sandler - Acoustics, Speech and Signal  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Four music segmentation algorithms are presented, one based on purely timbral <br>features, one on purely harmonic features, and two on different combinations of features. <br>They are compared against each other and against human annotations of two albums by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3624474000871434899&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 13</a> <a href="/scholar?q=related:k6qUh-a3TDIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3624474000871434899&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'k6qUh-a3TDIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://www.eurasip.org/Proceedings/Eusipco/Eusipco2006/papers/1568981704.pdf" class=yC48><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurasip.org</span><span class="gs_ggsS">eurasip.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.eurasip.org/Proceedings/Eusipco/Eusipco2006/papers/1568981704.pdf" class=yC47>New methods in structural segmentation of musical audio</a></h3><div class="gs_a"><a href="/citations?user=g_ESIWoAAAAJ&amp;hl=en&amp;oi=sra">M Levy</a>, M Sandler - Proceedings of the European Signal Processing  &hellip;, 2006 - eurasip.org</div><div class="gs_rs">ABSTRACT We describe a simple model of musical structure and two related methods of <br>extracting a high-level segmentation of a music track from the audio data, including a novel <br>use of hidden semi-Markov models. We introduce a semi-supervised segmentation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5176512794755681086&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:Pkut4vWq1kcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5176512794755681086&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Pkut4vWq1kcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md35', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md35" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Pkut4vWq1kcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf" class=yC4A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255" class=yC49>Singing voice detection for karaoke application</a></h3><div class="gs_a">A Shenoy, Y Wu, Y Wang - Visual  &hellip;, 2005 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract We present a framework to detect the regions of singing voice in musical audio <br>signals. This work is oriented towards the development of a robust transcriber of lyrics for <br>karaoke applications. The technique leverages on a combination of low-level audio <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 11</a> <a href="/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3A/63/RN177222664.html?source=googlescholar" class="gs_nph" class=yC4B>BL Direct</a> <a href="/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'WkuJAZ0HHakJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.8423&amp;rep=rep1&amp;type=pdf" class=yC4D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101293" class=yC4C>Multimodal content-based structure analysis of karaoke music</a></h3><div class="gs_a">Y Zhu, K Chen, Q Sun - Proceedings of the 13th annual ACM  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract This paper presents a novel approach for content-based analysis of karaoke music, <br>which utilizes multimodal contents including synchronized lyrics text from the video channel <br>and original singing audio as well as accompaniment audio in the two audio channels. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9085165552633813654&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:lppaUu_9FH4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9085165552633813654&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'lppaUu_9FH4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB38" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW38"><a href="http://publik.tuwien.ac.at/files/PubDat_186351.pdf" class=yC4F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tuwien.ac.at</span><span class="gs_ggsS">tuwien.ac.at <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0065245810780037" class=yC4E>Features for content-based audio retrieval</a></h3><div class="gs_a">D MitroviÄ, M Zeppelzauer, <a href="/citations?user=AvNBy7MAAAAJ&amp;hl=en&amp;oi=sra">C Breiteneder</a> - Advances in computers, 2010 - Elsevier</div><div class="gs_rs">Abstract Today, a large number of audio features exists in audio retrieval for different <br>purposes, such as automatic speech recognition, music information retrieval, audio <br>segmentation, and environmental sound retrieval. The goal of this chapter is to review <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10438406953689636585&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 14</a> <a href="/scholar?q=related:6V6jv9Gr3JAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10438406953689636585&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'6V6jv9Gr3JAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/F50L41LP153W4T34.pdf" class=yC50>Understanding multimedia document semantics for cross-media retrieval</a></h3><div class="gs_a">F Wu, Y Yang, Y Zhuang, Y Pan - Advances in Multimedia Information  &hellip;, 2005 - Springer</div><div class="gs_rs">Abstract. Multimedia Document (MMD) such as Web Page and Multimedia cyclopedias is <br>composed of media objects of different modalities, and its integrated semantics is always <br>expressed by the combination of all media objects in it. Since the contents in MMDs are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=274023652182972362&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 9</a> <a href="/scholar?q=related:ytdrUA-HzQMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0E/2A/RN178111319.html?source=googlescholar" class="gs_nph" class=yC51>BL Direct</a> <a href="/scholar?cluster=274023652182972362&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ytdrUA-HzQMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1198303" class=yC52>Content-adaptive digital music watermarking based on music structure analysis</a></h3><div class="gs_a">C Xu, NC Maddage, X Shao, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - ACM Transactions on Multimedia  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract A novel content-adaptive music watermarking technique is proposed in this article. <br>To optimally balance inaudibility and robustness when embedding and extracting <br>watermarks, the embedding scheme is highly related to the music structure and human <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10434868207967935251&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:ExvyTVkZ0JAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10434868207967935251&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ExvyTVkZ0JAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/54X3MW16X062VK68.pdf" class=yC53>A digital library framework for heterogeneous music collections: from document acquisition to cross-modal interaction</a></h3><div class="gs_a">D Damm, C Fremerey, V Thomas, M Clausen&hellip; - International Journal on  &hellip;, 2012 - Springer</div><div class="gs_rs">Abstract In this paper, we present a digital library system for managing heterogeneous music <br>collections. The heterogeneity refers to various document types and formats as well as to <br>different modalities, eg, CD-audio recordings, scanned sheet music, and lyrics. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12788631027349358712&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 10</a> <a href="/scholar?q=related:eIiyjClYerEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12788631027349358712&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'eIiyjClYerEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB42" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW42"><a href="http://jdl.ac.cn/doc/2010/1-Affective%20Visualization%20and%20Retrieval%20for%20Music%20Video.pdf" class=yC55><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jdl.ac.cn</span><span class="gs_ggsS">jdl.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5571814" class=yC54>Affective visualization and retrieval for music video</a></h3><div class="gs_a">S Zhang, Q Huang, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, W Gao&hellip; - &hellip; , IEEE Transactions on, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In modern times, music video (MV) has become an important favorite pastime to <br>people because of its conciseness, convenience, and the ability to bring both audio and <br>visual experiences to audiences. As the amount of MVs is explosively increasing, it has <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1798714933518924347&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 10</a> <a href="/scholar?q=related:O_qVuLNR9hgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1798714933518924347&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'O_qVuLNR9hgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB43" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW43"><a href="http://wwwiti.cs.uni-magdeburg.de/~stober/publ/cbmi2008.pdf" class=yC57><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-magdeburg.de</span><span class="gs_ggsS">uni-magdeburg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4564924" class=yC56>Enhancing chord classification through neighbourhood histograms</a></h3><div class="gs_a">J Reinhard, S Stober&hellip; - Content-Based Multimedia  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The chord progression of a song is an important high-level feature which enables <br>indexing as well as deeper analysis of musical recordings. Different approaches to chord <br>recognition have been suggested in the past. Though their performance increased, still <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10861208389787583936&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:wAXIQHzDupYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10861208389787583936&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'wAXIQHzDupYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="https://www.ims.tuwien.ac.at/media/documents/publications/Discrimination_and_Retrieval_of_Environmental_Sounds_Technical_Report.pdf" class=yC59><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tuwien.ac.at</span><span class="gs_ggsS">tuwien.ac.at <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://www.ims.tuwien.ac.at/media/documents/publications/Discrimination_and_Retrieval_of_Environmental_Sounds_Technical_Report.pdf" class=yC58>Discrimination and Retrieval of Environmental sounds</a></h3><div class="gs_a">D Mitrovic - 2005 - ims.tuwien.ac.at</div><div class="gs_rs">Abstract The human auditory sense may be regarded as the second most important sense <br>after the sense of sight. This valuation is reflected in the field of information retrieval where <br>until recently research concentrated on visual information retrieval. Even research in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2132536459987182221&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:jeIamJxKmB0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2132536459987182221&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'jeIamJxKmB0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md44', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md44" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jeIamJxKmB0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/K82440K44VGUJ186.pdf" class=yC5A>Manifold learning based cross-media retrieval: a solution to media object complementary nature</a></h3><div class="gs_a">Y Zhuang, <a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, F Wu, Y Pan - The Journal of VLSI Signal Processing, 2007 - Springer</div><div class="gs_rs">Abstract Media objects of different modalities always exist jointly and they are naturally <br>complementary of each other, either in the view of semantics or in the view of modality. In <br>this paper, we propose a manifold learning based cross-media retrieval approach that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3739602644366444475&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:u8eSssq85TMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/1D/2A/RN206292634.html?source=googlescholar" class="gs_nph" class=yC5B>BL Direct</a> <a href="/scholar?cluster=3739602644366444475&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'u8eSssq85TMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB46" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW46"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.8385&amp;rep=rep1&amp;type=pdf" class=yC5D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1415618" class=yC5C>Automatic music summarization based on music structure analysis</a></h3><div class="gs_a">X Shao, NC Maddage, C Xu&hellip; - Acoustics, Speech, and  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present a novel approach for music summarization based on <br>music structure analysis. From the audio signal, we first extract the note onset representing <br>the time tempo of the song and the music structure analysis can be performed based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18355835064326460938&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:CpbW1LwGvf4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18355835064326460938&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'CpbW1LwGvf4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB47" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW47"><a href="http://users.ece.cmu.edu/~hengtzec/papers/iscas09_musicstructure.pdf" class=yC5F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5118096" class=yC5E>Multimodal structure segmentation and analysis of music using audio and textual information</a></h3><div class="gs_a">HT Cheng, <a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, YC Lin&hellip; - Circuits and Systems,  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present a multimodal approach to structure segmentation of music <br>with applications to audio content analysis and music information retrieval. In particular, <br>since lyrics contain rich information about the semantic structure of a song, our approach <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=416598635599225015&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:t-DNeEAOyAUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=416598635599225015&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'t-DNeEAOyAUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://arrow.dit.ie/cgi/viewcontent.cgi?article=1071&amp;context=sciendoc" class=yC61><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dit.ie</span><span class="gs_ggsS">dit.ie <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://arrow.dit.ie/cgi/viewcontent.cgi?article=1071&amp;context=sciendoc" class=yC60>Machine annotation of traditional Irish dance music</a></h3><div class="gs_a"><a href="/citations?user=9z7Y7DsAAAAJ&amp;hl=en&amp;oi=sra">B Duggan</a> - Doctoral, 2009 - arrow.dit.ie</div><div class="gs_rs">Abstract The work presented in this thesis is validated in experiments using 130 realworld <br>field recordings of traditional music from sessions, classes, concerts and commercial <br>recordings. Test audio includes solo and ensemble playing on a variety of instruments <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13955237370635574821&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:JV79WHP2qsEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13955237370635574821&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'JV79WHP2qsEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md48', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md48" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:JV79WHP2qsEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=17991099106956573299&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://theses.eurasip.org/media/theses/documents/giannakopoulos-theodoros-study-and-application-of-acoustic-information-for-the-detection-of-harmful-content-and-fusion-with-visual-information.pdf" class=yC63><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurasip.org</span><span class="gs_ggsS">eurasip.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://theses.eurasip.org/media/theses/documents/giannakopoulos-theodoros-study-and-application-of-acoustic-information-for-the-detection-of-harmful-content-and-fusion-with-visual-information.pdf" class=yC62>Study and application of acoustic information for the detection of harmful content, and fusion with visual information</a></h3><div class="gs_a"><a href="/citations?user=BeIoqhwAAAAJ&amp;hl=en&amp;oi=sra">T Giannakopoulos</a> - Department of Informatics and  &hellip;, 2009 - theses.eurasip.org</div><div class="gs_rs">Abstract This thesis aims at investigating and developing techniques for content-based <br>segmentation and classification of multimedia files, based on audio information. Emphasis <br>has been given to analyzing the content of films based on audio information. In addition, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7997052863895675572&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:tKKFojU9-24J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7997052863895675572&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'tKKFojU9-24J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md49', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md49" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:tKKFojU9-24J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB50" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW50"><a href="http://ismir2009.ismir.net/proceedings/PS4-22.pdf" class=yC65><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ismir.net</span><span class="gs_ggsS">ismir.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ismir2009.ismir.net/proceedings/PS4-22.pdf" class=yC64>An integrated approach to music boundary detection</a></h3><div class="gs_a">MY Su, <a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, YC Lin&hellip; - Proceedings of the 10th  &hellip;, 2009 - ismir2009.ismir.net</div><div class="gs_rs">ABSTRACT Music boundary detection is a fundamental step of music analysis and <br>summarization. Existing works use either unsupervised or supervised methodologies to <br>detect boundary. In this paper, we propose an integrated approach that takes advantage <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15104889617882105971&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:c0hZDRRZn9EJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15104889617882105971&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'c0hZDRRZn9EJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md50', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md50" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:c0hZDRRZn9EJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://www.music.mcgill.ca/~jordan/documents/smith2010thesis.pdf" class=yC67><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mcgill.ca</span><span class="gs_ggsS">mcgill.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.music.mcgill.ca/~jordan/documents/smith2010thesis.pdf" class=yC66>A comparison and evaluation of approaches to the automatic formal analysis of musical audio</a></h3><div class="gs_a">JBL Smith - 2010 - music.mcgill.ca</div><div class="gs_rs">Abstract Analyzing the form or structure of pieces of music is a fundamental task for music <br>theorists. Several algorithms have been developed to automatically produce formal <br>analyses of music. However, comparing these algorithms to one another and judging their <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9705581456097759913&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:qVJTdekmsYYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9705581456097759913&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'qVJTdekmsYYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md51', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md51" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:qVJTdekmsYYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:qVJTdekmsYYJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=15378652721518508377&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB52" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW52"><a href="http://arxiv.org/pdf/1111.5297" class=yC69><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/X284267782408663.pdf" class=yC68>Reoccurring patterns in hierarchical protein materials and music: The power of analogies</a></h3><div class="gs_a"><a href="/citations?user=-38e7AwAAAAJ&amp;hl=en&amp;oi=sra">T Giesa</a>, DI Spivak, MJ Buehler - BioNanoScience, 2011 - Springer</div><div class="gs_rs">Abstract Complex hierarchical structures composed of simple nanoscale building blocks <br>form the basis of most biological materials. Here, we demonstrate how analogies between <br>seemingly different fields enable the understanding of general principles by which <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10827996810358639201&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:YUp91rrFRJYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10827996810358639201&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'YUp91rrFRJYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/Q8205NH13L1RH912.pdf" class=yC6A>Effectiveness of signal segmentation for music content representation</a></h3><div class="gs_a">N Maddage, M Kankanhalli, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - Advances in Multimedia Modeling, 2008 - Springer</div><div class="gs_rs">In this paper we compare the effectiveness of rhythm based signal segmentation technique <br>with the traditional fixed length segmentation for music contents representation. We consider <br>vocal regions, instrumental regions and chords which represent the harmony as different <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10581131093821192891&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:u3oNKau615IJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/59/2C/RN221721717.html?source=googlescholar" class="gs_nph" class=yC6B>BL Direct</a> <a href="/scholar?cluster=10581131093821192891&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'u3oNKau615IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB54" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW54"><a href="http://koasas.kaist.ac.kr/bitstream/10203/23072/1/31.pdf" class=yC6D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4064552" class=yC6C>A music summarization scheme using tempo tracking and two stage clustering</a></h3><div class="gs_a">S Kim, S Kim, S Kwon, H Kim - Multimedia Signal Processing,  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present effective methods for music summarization which <br>automatically extract a representative portion of the music by signal processing technology. <br>Our proposed method uses 2-dimensional similarity matrix, tempo tracking, and clustering <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3745868000617663939&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:w41RoBn_-zMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3745868000617663939&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'w41RoBn_-zMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB55" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW55"><a href="https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/534/HARTETowardsAutomatic2010.pdf?sequence=1" class=yC6F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from qmul.ac.uk</span><span class="gs_ggsS">qmul.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://qmro.qmul.ac.uk/xmlui/handle/123456789/534" class=yC6E>Towards automatic extraction of harmony information from music signals</a></h3><div class="gs_a"><a href="/citations?user=KUW54BsAAAAJ&amp;hl=en&amp;oi=sra">C Harte</a> - 2010 - qmro.qmul.ac.uk</div><div class="gs_rs">In this thesis we address the subject of automatic extraction of harmony information from <br>audio recordings. We focus on chord symbol recognition and methods for evaluating <br>algorithms designed to perform that task. We present a novel six-dimensional model for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16895747196309534705&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:8ffUuo7AeeoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16895747196309534705&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'8ffUuo7AeeoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB56" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW56"><a href="http://hal.inria.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf" class=yC71><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from inria.fr</span><span class="gs_ggsS">inria.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://hal.inria.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf" class=yC70>Joint estimation of musical content information from an audio signal</a></h3><div class="gs_a">H Papadopoulos - 2010 - hal.inria.fr</div><div class="gs_rs">RÃ©sumÃ© Dans cette these, nous nous intÃ©ressons au probleme de l&#39;extraction automatique <br>d&#39;informations de contenu d&#39;un signal audio de musique. La plupart des travaux existants <br>abordent ce probleme en considÃ©rant les attributs musicaux de maniere indÃ©pendante les <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5504565459161893914&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:Gsw65B4lZEwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Gsw65B4lZEwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md56', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md56" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Gsw65B4lZEwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:Gsw65B4lZEwJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=14696771562731948953&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB57" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW57"><a href="http://www-mmdb.iai.uni-bonn.de/lehre/materialMusikWS0607/06_MuKuCl_MIR.pdf" class=yC73><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-bonn.de</span><span class="gs_ggsS">uni-bonn.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-mmdb.iai.uni-bonn.de/lehre/materialMusikWS0607/06_MuKuCl_MIR.pdf" class=yC72>Aktuelle Aspekte des Music Information Retrieval</a></h3><div class="gs_a"><a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a>, F Kurth, M Clausen - Erscheint in:  &hellip;, 2006 - www-mmdb.iai.uni-bonn.de</div><div class="gs_rs">Music Information Retrieval (MIR) ist eine junge Disziplin, die es sich zur Aufgabe gemacht <br>hat, Methoden zu erforschen und Systeme zu entwickeln, die Benutzern groÃe, in digitaler <br>Form vorliegende Musikkollektionen in vielfÃ¤ltiger Weise zugÃ¤nglich machen. WÃ¤hrend <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15426674609879520140&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:jBNo29KOFtYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15426674609879520140&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'jBNo29KOFtYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md57', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md57" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jBNo29KOFtYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB58" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW58"><a href="http://www3.ntu.edu.sg/home/aseschng/SpeechTechWeb/members/Conformation_theses/Thesis_Wang_Jinjun_20071007.pdf" class=yC75><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www3.ntu.edu.sg/home/aseschng/SpeechTechWeb/members/Conformation_theses/Thesis_Wang_Jinjun_20071007.pdf" class=yC74>Content-Based Sports Video Analysis and Composition</a></h3><div class="gs_a">W Jinjun - 2006 - ntu.edu.sg</div><div class="gs_rs">Abstract This thesis proposes solutions for content-based sports video analysis, including <br>multimodal feature extraction, middle-level representation and semantic event detection. In <br>addition, solutions for sports video composition and personalization are also examined. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8897660065604983548&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:_FqQFrDWensJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8897660065604983548&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'_FqQFrDWensJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md58', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md58" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_FqQFrDWensJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2168752.2168754" class=yC76>Machine recognition of music emotion: A review</a></h3><div class="gs_a"><a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, HH Chen - ACM Transactions on Intelligent Systems and  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract The proliferation of MP3 players and the exploding amount of digital music content <br>call for novel ways of music organization and retrieval to meet the ever-increasing demand <br>for easy and effective information access. As almost every music piece is created to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16197274473790159708&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:XLvDK1FHyOAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16197274473790159708&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'XLvDK1FHyOAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPATAPP12440337&amp;id=dgLTAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC77>APPARATUS AND METHODS FOR MUSIC SIGNAL ANALYSIS</a></h3><div class="gs_a">NC Maddage, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - US Patent App. 12/440,337, 2007 - Google Patents</div><div class="gs_rs">An apparatus for modelling layers in a music signal comprises a rhythm modelling module <br>configured to model rhythm features of the music signal; a harmony modelling module <br>configured to model harmony features of the music signal; and a music region modelling <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4454238154428016447&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:P2esewCi0D0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4454238154428016447&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'P2esewCi0D0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/97390x/200909/31573345.html" class=yC78>åºäºç»¼åæ¨ççå¤åªä½è¯­ä¹ææåè·¨åªä½æ£ç´¢</a></h3><div class="gs_a">æ¨æï¼ é­åå¼ºï¼ åºè¶æºï¼ çæå - è®¡ç®æºè¾å©è®¾è®¡ä¸å¾å½¢å­¦å­¦æ¥, 2009 - cqvip.com</div><div class="gs_rs">ä¸ºäºæ´åç¡®å°è¿è¡è·¨åªä½æ£ç´¢, éè¦ææ, å­¦ä¹ ä¸åç±»åå¤åªä½å¯¹è±¡ä¹é´çè¯­ä¹å³è, <br>ä¸ºæ­¤æåºä¸ç§åºäºç»¼åæ¨çæ¨¡åçå¤åªä½è¯­ä¹ææåè·¨åªä½æ£ç´¢ææ¯. é¦åæ ¹æ®å¤åªä½å¯¹è±¡ç<br>åºå±ç¹å¾æé æ¨çæº, æ ¹æ®å¤åªä½å¯¹è±¡çå±çå³ç³»æé å½±åæºåºæ¥è¿è¡ç»¼åæ¨ç, å¹¶æé åº<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3923775936342789146&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:Ggzwz2wNdDYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3923775936342789146&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'Ggzwz2wNdDYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB62" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW62"><a href="http://www.ntu.edu.sg/home/aseschng/SpeechTechWeb/members/Conformation_theses/WangLei_ConfirmationReport_Final.pdf" class=yC7A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ntu.edu.sg/home/aseschng/SpeechTechWeb/members/Conformation_theses/WangLei_ConfirmationReport_Final.pdf" class=yC79>Unsupervised Techniques for Audio Content Analysis and Summarization</a></h3><div class="gs_a">W Lei - 2008 - ntu.edu.sg</div><div class="gs_rs">Abstract This thesis explores unsupervised algorithms for broadcast audio analysis and <br>summarization. For this work, the audio content analysis and summarization task will be to <br>extract semantic structures from audio databases and organize them into a hierarchical <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7070979980582480431&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:L2qEJNQqIWIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7070979980582480431&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'L2qEJNQqIWIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md62', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md62" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:L2qEJNQqIWIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB63" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW63"><a href="https://www.ims.tuwien.ac.at/media/documents/publications/Discriminaton_and_Retrieval_of_Animal_Sounds_Technical_Report.pdf" class=yC7C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tuwien.ac.at</span><span class="gs_ggsS">tuwien.ac.at <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://www.ims.tuwien.ac.at/media/documents/publications/Discriminaton_and_Retrieval_of_Animal_Sounds_Technical_Report.pdf" class=yC7B>Discrimination and retrieval of animal sounds</a></h3><div class="gs_a">M Zeppelzauer - 2005 - ims.tuwien.ac.at</div><div class="gs_rs">Abstract Hearing is the second most important human sense after vision. Research primarily <br>concentrated on visual information retrieval in the past. Only few research took place in the <br>area of audio information retrieval. The main focus was speech recognition for a long time. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1539793523510951143&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:5zxRLA1yXhUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1539793523510951143&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'5zxRLA1yXhUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md63', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md63" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:5zxRLA1yXhUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/q1t7q1552t2p3658.pdf" class=yC7D>Boosting cross-media retrieval by learning with positive and negative examples</a></h3><div class="gs_a">Y Zhuang, Y Yang - Advances in Multimedia Modeling, 2006 - Springer</div><div class="gs_rs">Content-based cross-media retrieval is a new category of retrieval methods by which the <br>modality of query examples and the returned results need not to be the same, for example, <br>users may query images by an example of audio and vice versa. Multimedia Document (<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11262276374973132216&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:uC2MZLGkS5wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0C/03/RN201122684.html?source=googlescholar" class="gs_nph" class=yC7E>BL Direct</a> <a href="/scholar?cluster=11262276374973132216&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'uC2MZLGkS5wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB65" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW65"><a href="http://www.mpi-inf.mpg.de/~mmueller/publications_Thesis/2011_StypRekowskyPhilipp_AdaptiveSegmentation_MasterThesis.pdf" class=yC80><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.mpi-inf.mpg.de/~mmueller/publications_Thesis/2011_StypRekowskyPhilipp_AdaptiveSegmentation_MasterThesis.pdf" class=yC7F>Towards time-adaptive feature design in music signal processing</a></h3><div class="gs_a">P von Styp-Rekowsky - 2011 - mpi-inf.mpg.de</div><div class="gs_rs">Abstract In music signal processing, it is often necessary to transform an audio signal into a <br>representation that closely correlates to a certain musical property while being invariant to <br>other musical aspects. The property that is captured by such a representation, which is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3623944267840942105&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:GfB4chzWSjIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'GfB4chzWSjIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md65', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md65" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:GfB4chzWSjIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7812241&amp;id=YWPXAAAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC81>Methods and systems for identifying similar songs</a></h3><div class="gs_a"><a href="/citations?user=1H4HuCkAAAAJ&amp;hl=en&amp;oi=sra">DPW Ellis</a> - US Patent 7,812,241, 2010 - Google Patents</div><div class="gs_rs">Methods and systems for identifying similar songs are provided. In accordance with some <br>embodiments, methods for identifying similar songs are provided, the methods comprising: <br>identifying beats in at least a portion of a song; generating beat-level descriptors of the at <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17597513555088718753&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:oTsvvVftNvQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17597513555088718753&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'oTsvvVftNvQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:333"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB67" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW67"><a href="http://micron.gelbukh.com/CV/Publications/2010/Music%20Composition%20Based%20on%20Linguistic%20Approach.pdf" class=yC83><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gelbukh.com</span><span class="gs_ggsS">gelbukh.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/H8260X035024724L.pdf" class=yC82>Music composition based on linguistic approach</a></h3><div class="gs_a">H GarcÃ­a Salas, <a href="/citations?user=g-dRTkQAAAAJ&amp;hl=en&amp;oi=sra">A Gelbukh</a>, H Calvo - Advances in Artificial Intelligence, 2010 - Springer</div><div class="gs_rs">Music is a form of expression. Since machines have limited capabilities in this sense, our <br>main goal is to model musical composition process, to allow machines to express <br>themselves musically. Our model is based on a linguistic approach. It describes music as <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17516112707140998422&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:FlXY6rO7FfMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17516112707140998422&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'FlXY6rO7FfMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:332"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/90012x/201002/33097227.html" class=yC84>åºäºé³è²åååå¸çé³ä¹ç»æåæ</a></h3><div class="gs_a">æç¸è²ï¼ ææï¼ åè¥ä¼¦ï¼ é¢æ°¸çº¢ - å£°å­¦å­¦æ¥, 2010 - cqvip.com</div><div class="gs_rs">é³ä¹çç»ææ¯é³ä¹ä½åè¡¨è¾¾ä½èææ³çä¸ç§éè¦å½¢å¼, ä¹æ¯å¬ä¼çè§£é³ä¹ä½ååæ¶µçææéå¾. <br>æ¬æç ç©¶äºåºäºé³ä¹ç¹å¾çé³è²ååå»ºæ¨¡æ¹æ³, ç ç©¶äºå¨Fisher ååä¸, æ ¹æ®å±é¨èå´é³è²åå<br>çåå¸, éç¨éçç£èç±»æ¹æ³åæé³ä¹çç»æ. å®éªç»æè¯æäºåºäºç¦»æ£ä½å¼¦åæ¢çé³è²ç¹å¾, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6889640143907825333&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:tablPzXrnF8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6889640143907825333&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'tablPzXrnF8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:331"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB69" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW69"><a href="http://wwwiti.cs.uni-magdeburg.de/~stober/publ/mtap2012adaptiveMIR.pdf" class=yC86><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-magdeburg.de</span><span class="gs_ggsS">uni-magdeburg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/76x78554q0526132.pdf" class=yC85>Adaptive music retrievalâa state of the art</a></h3><div class="gs_a">S Stober, <a href="/citations?user=LuMoBX0AAAAJ&amp;hl=en&amp;oi=sra">A NÃ¼rnberger</a> - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract With the development of more and more sophisticated Music Information Retrieval <br>approaches, aspects of adaptivity are becoming an increasingly important research topic. <br>Even though, adaptive techniques have already found their way into Music Information <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MvLbFPg5nPEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17409853997172126258&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MvLbFPg5nPEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:330"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB70" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW70"><a href="http://cdn.intechweb.org/pdfs/9261.pdf" class=yC88><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from intechweb.org</span><span class="gs_ggsS">intechweb.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cdn.intechweb.org/pdfs/9261.pdf" class=yC87>Music Structure Analysis Statistics for Popular Songs</a></h3><div class="gs_a">NC Maddage, L Haizhou, MS Kankanhalli - cdn.intechweb.org</div><div class="gs_rs">Abstract In this chapter, we have proposed a better procedure for manual annotation of <br>music information. The proposed annotation procedure involves carrying out listening tests <br>and then incorporating music knowledge to iteratively refine the detected music <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7LRoGihTg1UJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6161860146879837420&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'7LRoGihTg1UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md70', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md70" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:7LRoGihTg1UJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:329"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB71" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW71"><a href="http://www.paulmckevitt.com/pubs/smythdmrn05.pdf" class=yC8A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from paulmckevitt.com</span><span class="gs_ggsS">paulmckevitt.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.paulmckevitt.com/pubs/smythdmrn05.pdf" class=yC89>A Framework for the Automatic Discovery and Description of Musical Structure</a></h3><div class="gs_a">E Smyth, <a href="/citations?user=XuncvX4AAAAJ&amp;hl=en&amp;oi=sra">K Curran</a>, P McKevitt, T Lunney - paulmckevitt.com</div><div class="gs_rs">ABSTRACT Automated discovery of recurrent patterns in music plays an important role in <br>computational music analysis. Patterns emerge from repetitions within the music itself, from <br>which musical structure can be inferred. Many papers have been written in relation to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:G4GZIMWUW4cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9753552992377340187&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'G4GZIMWUW4cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md71', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md71" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:G4GZIMWUW4cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:328"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB72" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW72"><a href="http://www.cs.helsinki.fi/u/linden/teaching/lta07/essays/hanna_ngrams.pdf" class=yC8C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from helsinki.fi</span><span class="gs_ggsS">helsinki.fi <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cs.helsinki.fi/u/linden/teaching/lta07/essays/hanna_ngrams.pdf" class=yC8B>Using N-Grams in Music Information Retrieval</a></h3><div class="gs_a">H Sirola - 2007 - cs.helsinki.fi</div><div class="gs_rs">As the electronic storage space for different kinds of music collections has been increasing <br>dramatically over the last years, music collections have grown to quite large sizes. This calls <br>for efficient tools and methods of searching, categorizing and indexing the music. Music <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:AmxtLeUKpI8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10350409823030176770&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'AmxtLeUKpI8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md72', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md72" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:AmxtLeUKpI8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:327"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB73" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW73"><a href="http://cecs.uci.edu/~papers/icme06/pdfs/0001897.pdf" class=yC8E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uci.edu</span><span class="gs_ggsS">uci.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036995" class=yC8D>Fully and Semi-Automatic Music Sports Video Composition</a></h3><div class="gs_a">J Wang, <a href="/citations?user=FJodrCcAAAAJ&amp;hl=en&amp;oi=sra">E Chng</a>, C Xu - Multimedia and Expo, 2006 IEEE  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video composition is important for music video production. In this paper we propose <br>an automatic method to assist the music sports video composition operation. Our approach <br>is based on dynamic programming algorithm which finds a set of video shots that best <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9az_NWhpKksJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5416257398169906421&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'9az_NWhpKksJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:326"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB74" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW74"><a href="http://mpac.ee.ntu.edu.tw/~yihsuan/pub/ISMIR09_segmentation.pdf" class=yC90><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mpac.ee.ntu.edu.tw/~yihsuan/pub/ISMIR09_segmentation.pdf" class=yC8F>AN INTEGRATED APPROACH TO MUSIC BOUNDARY DETECTION</a></h3><div class="gs_a">S Min-Yian, <a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, YC Lin, H Chen - 2009 - mpac.ee.ntu.edu.tw</div><div class="gs_rs">ABSTRACT Music boundary detection is a fundamental step of music analysis and <br>summarization. Existing works either use unsupervised or supervised methods to detect <br>boundary. In this paper, we propose an integrated approach that takes advantage of both <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:R-mRR-dw3RgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'R-mRR-dw3RgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md74', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md74" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:R-mRR-dw3RgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:325"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/FT671731J0403662.pdf" class=yC91>A Survey of Music Structure Analysis Techniques for Music Applications</a></h3><div class="gs_a">N Maddage, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, M Kankanhalli - Recent Advances in Multimedia Signal  &hellip;, 2009 - Springer</div><div class="gs_rs">Music carries multilayer information which forms different structures. The information <br>embedded in the music can be categorized into time information, harmony/melody, music <br>regions, music similarities, song structures and music semantics. In this chapter, we first <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3vo8SvZRZuUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16529989600559299294&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'3vo8SvZRZuUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:324"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Music-Driven Dance Synthesis by Multimodal Dance Performance Analysis</h3><div class="gs_a">Y Demir - 2008 - KoÃ§ University</div><div class="gs_fl"><a href="/scholar?q=related:7dn-VyF6fckJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'7dn-VyF6fckJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:323"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/N2227J7487X82461.pdf" class=yC92>Universality-Diversity Paradigm: Music, Materiomics, and Category Theory</a></h3><div class="gs_a"><a href="/citations?user=2-nC-jUAAAAJ&amp;hl=en&amp;oi=sra">SW Cranford</a>, MJ Buehler - Biomateriomics, 2012 - Springer</div><div class="gs_rs">The transition from the material structure to function, or from nanoscale components to the <br>macroscale system, is a challenging proposition. Recognizing how Nature accomplishes <br>such a featâthrough universal structural elements, relatively weak building blocks, and <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'aFO-X4ET7SIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:322"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB78" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW78"><a href="http://nlp.cic.ipn.mx/Publications/2008/Musical%20composer%20based%20on%20detection%20of%20typical%20patterns.pdf" class=yC94><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ipn.mx</span><span class="gs_ggsS">ipn.mx <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://nlp.cic.ipn.mx/Publications/2008/Musical%20composer%20based%20on%20detection%20of%20typical%20patterns.pdf" class=yC93>Musical composer based on detection of typical patterns in a human composer&#39;s style</a></h3><div class="gs_a">HAG Salas, <a href="/citations?user=g-dRTkQAAAAJ&amp;hl=en&amp;oi=sra">A Gelbukh</a> - XXIV Simposio Internacional de  &hellip;, 2008 - nlp.cic.ipn.mx</div><div class="gs_rs">Abstract. We present an evolutionary automatic music composer that finds and emphasizes <br>typical patterns in the style of a human composer according to training. First, from a training <br>corpus of melodies the system learns a matrix of conditional probabilities of note <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:6yVVZI9ZWDAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3483632784169575915&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'6yVVZI9ZWDAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md78', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md78" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:6yVVZI9ZWDAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:321"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB79" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW79"><a href="http://eprints2008.lib.hokudai.ac.jp/dspace/bitstream/2115/39829/1/WA-L4-2.pdf" class=yC96><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hokudai.ac.jp</span><span class="gs_ggsS">hokudai.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://eprints2008.lib.hokudai.ac.jp/dspace/handle/2115/39829" class=yC95>Efficient Repeating Segments Discovery in Music using Adaptive Motif Generation Algorithm</a></h3><div class="gs_a">L Wang, <a href="/citations?user=FJodrCcAAAAJ&amp;hl=en&amp;oi=sra">ES Chng</a>, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - Proceedings: APSIPA ASC  &hellip;, 2009 - eprints2008.lib.hokudai.ac.jp</div><div class="gs_rs">This paper introduces an efficient unsupervised algorithm to discover motifs in multivariate <br>data sequence. Specifically, we apply our proposed work to detect repeating segments on <br>music feature vectors. The proposed algorithm, namely Adaptive Motif Generation, scans <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:0Wx1bcCwqYYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9703481212663065809&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'0Wx1bcCwqYYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:320"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB80" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW80"><a href="http://www.visualization.berkeley.edu/papers/underscore/underscore.pdf" class=yC98><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from berkeley.edu</span><span class="gs_ggsS">berkeley.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.visualization.berkeley.edu/papers/underscore/underscore.pdf" class=yC97>UnderScore: Musical Underlays for Audio Stories</a></h3><div class="gs_a"><a href="/citations?user=h75J8VMAAAAJ&amp;hl=en&amp;oi=sra">S Rubin</a>, <a href="/citations?user=d-m0t1sAAAAJ&amp;hl=en&amp;oi=sra">F Berthouzoz</a>, <a href="/citations?user=xjArBPQAAAAJ&amp;hl=en&amp;oi=sra">GJ Mysore</a>, W Li, <a href="/citations?user=YPzKczYAAAAJ&amp;hl=en&amp;oi=sra">M Agrawala</a> - 2012 - visualization.berkeley.edu</div><div class="gs_rs">ABSTRACT Audio producers often use musical underlays to emphasize key moments in <br>spoken content and give listeners time to reflect on what was said. Yet, creating such <br>underlays is timeconsuming as producers must carefully (1) mark an emphasis point in the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:6WPPbcMvor4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13736594329840870377&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'6WPPbcMvor4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md80', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md80" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:6WPPbcMvor4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:319"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5381211" class=yC99>A Review on Objective Music Structure Analysis</a></h3><div class="gs_a"><a href="/citations?user=huHRvLQAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, R Liu, M Li - &hellip;  and Multimedia Technology, 2009. ICIMT&#39;09.  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper summarizes the applications and the state of the art of objective music <br>structure analysis. Two principal types of methods, namelyÂ¿ stateÂ¿ andÂ¿ sequenceÂ¿ <br>approaches are reviewed after applications are presented. Two kinds of objective features<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:bdBdcQJ2GE4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5627377487263420525&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'bdBdcQJ2GE4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:318"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6022090" class=yC9A>Structure analysis of Chinese Peking Opera</a></h3><div class="gs_a">Z Zhang, X Wang - Natural Computation (ICNC), 2011 Seventh  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic processing of Chinese Peking Opera becomes more important than ever <br>before. This paper presents a multistage system for structure analysis of Chinese Peking <br>Opera. Taking advantage of opera theory, all procedures fits human&#39;s usual practice <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:V0i6d0CY88kJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14552142223618230359&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'V0i6d0CY88kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:317"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB83" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW83"><a href="http://www.oifii.org/ns-org/nsd/ar/cp/audio-segmentation_ep_2007-07-30/Ewald_Peiszer_Thesis_Automatic_Audio_Segmentation.pdf" class=yC9C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from oifii.org</span><span class="gs_ggsS">oifii.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.oifii.org/ns-org/nsd/ar/cp/audio-segmentation_ep_2007-07-30/Ewald_Peiszer_Thesis_Automatic_Audio_Segmentation.pdf" class=yC9B>Automatic Audio Segmentation: Segment Boundary and Structure Detection in Popular Music</a></h3><div class="gs_a">DIT Lidy, E Peiszer - oifii.org</div><div class="gs_rs">Abstract Automatic Audio Segmentation aims at extracting information on a song&#39;s structure, <br>ie, segment boundaries, musical form and semantic labels like verse, chorus, bridge etc. <br>This information can be used to create representative song excerpts or summaries, to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:K4NsfvNoAKwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12394021569530987307&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'K4NsfvNoAKwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md83', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md83" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:K4NsfvNoAKwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:316"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> VIBRATO-MOTIVATED ACOUSTIC FEATURES FOR SINGER IDENTIFICATION</h3><div class="gs_a"><a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">LI Haizhou</a>, TL NWE</div><div class="gs_fl"><a href="/scholar?q=related:ZYc_l3H1LTQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3759931132141864805&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ZYc_l3H1LTQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:315"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB85" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW85"><a href="http://www.audiolabs-erlangen.de/meinard/students/thesis/2012_EwertSebastian_SignalProcessingMethods_Phd-Thesis.pdf" class=yC9E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from audiolabs-erlangen.de</span><span class="gs_ggsS">audiolabs-erlangen.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.audiolabs-erlangen.de/meinard/students/thesis/2012_EwertSebastian_SignalProcessingMethods_Phd-Thesis.pdf" class=yC9D>Signal Processing Methods for Music Synchronization, Audio Matching, and Source Separation</a></h3><div class="gs_a"><a href="/citations?user=kkwnObwAAAAJ&amp;hl=en&amp;oi=sra">S Ewert</a> - 2012 - audiolabs-erlangen.de</div><div class="gs_rs">Abstract The field of music information retrieval (MIR) aims at developing techniques and <br>tools for organizing, understanding, and searching multimodal information in large music <br>collections in a robust, efficient and intelligent manner. In this context, this thesis presents <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'Li_TocU8mCEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md85', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md85" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Li_TocU8mCEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:314"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB86" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW86"><a href="http://www.iaeme.com/MasterAdmin/UploadFolder/POWER%20EFFICIENT%20DATA%20AGGREGATION%20BASED%20ON%20SWARM%20INTELLIGENCE%20AND%20GAME%20THEORETIC/POWER%20EFFICIENT%20DATA%20AGGREGATION%20BASED%20ON%20SWARM%20INTELLIGENCE%20AND%20GAME%20THEORETIC.pdf" class=yCA0><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iaeme.com</span><span class="gs_ggsS">iaeme.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.iaeme.com/MasterAdmin/UploadFolder/POWER%20EFFICIENT%20DATA%20AGGREGATION%20BASED%20ON%20SWARM%20INTELLIGENCE%20AND%20GAME%20THEORETIC/POWER%20EFFICIENT%20DATA%20AGGREGATION%20BASED%20ON%20SWARM%20INTELLIGENCE%20AND%20GAME%20THEORETIC.pdf" class=yC9F>INTERNATIONAL JOURNAL OF COMPUTER ENGINEERING &amp; TECHNOLOGY (IJCET)</a></h3><div class="gs_a">IOFBIT IN, DL CHAOSES - Journal Impact Factor, 2012 - iaeme.com</div><div class="gs_rs">ABSTRACT While performing data aggregation in wireless sensor networks (WSN), <br>significant energy is consumed not only during active communication but also during idle <br>state. For this, an accurate estimate of energy cost and an intelligent routing technique <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'HKJhrVGNuqMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md86', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md86" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:HKJhrVGNuqMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:313"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB87" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW87"><a href="http://www.ee.columbia.edu/~dpwe/proposals/NSF-music-2006-11.pdf" class=yCA2><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ee.columbia.edu/~dpwe/proposals/NSF-music-2006-11.pdf" class=yCA1>Data-Driven Music Audio Understanding</a></h3><div class="gs_a"><a href="/citations?user=1H4HuCkAAAAJ&amp;hl=en&amp;oi=sra">DPW Ellis</a>, DI Repetto - ee.columbia.edu</div><div class="gs_rs">Broader Impact: Because music is important in the lives of so many people, new <br>technologies to help give insight into the structure and &#39;function&#39;of music have very broad <br>potential impact. The project highlights the commercial possibilities of a system for music <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:kFQntionbJkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'kFQntionbJkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md87', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md87" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:kFQntionbJkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:312"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB88" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW88"><a href="https://repository.library.georgetown.edu/bitstream/handle/10822/552965/henryMichael.pdf?sequence=1" class=yCA4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from georgetown.edu</span><span class="gs_ggsS">georgetown.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://repository.library.georgetown.edu/handle/10822/552965" class=yCA3>Learning techniques for identifying vocal regions in music using the wavelet transformation version 1.0</a></h3><div class="gs_a">M Henry - 2012 - repository.library.georgetown.edu</div><div class="gs_rs">Abstract In this research I present a machine learning method for the automatic detection of <br>vocal regions in music. I employ the wavelet transformation to extract wavelet coefficients, <br>from which I build feature sets capable of constructing a model that can distinguish <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:PpPPvEB0KowJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10100012935726207806&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'PpPPvEB0KowJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md88', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md88" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:PpPPvEB0KowJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=12517843942992201336&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:311"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB89" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW89"><a href="http://nguyendangbinh.org/Proceedings/IPCV08/Papers/DMI5487.pdf" class=yCA6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nguyendangbinh.org</span><span class="gs_ggsS">nguyendangbinh.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://nguyendangbinh.org/Proceedings/IPCV08/Papers/DMI5487.pdf" class=yCA5>Music Visualization with DISC and Structural Feature Diagram</a></h3><div class="gs_a">YH Tseng, <a href="/citations?user=sDqkqmwAAAAJ&amp;hl=en&amp;oi=sra">JL Hsu</a>, YF Li - nguyendangbinh.org</div><div class="gs_rs">ABSTRACT Music form and associated structural features is one of the most representative <br>feature of music object, especially in classic music work. In this paper, we propose a novel <br>approach to visualize these important structures of music. We first investigate six kinds of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Cz9dz9F82PsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Cz9dz9F82PsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md89', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md89" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Cz9dz9F82PsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:310"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631272.1631364" class=yCA7>Automatic and instant ring tone generation based on music structure analysis</a></h3><div class="gs_a">T Zhang, CK Fong, L Xiao, J Zhou - Proceedings of the 17th ACM  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Real tones, which are often excerpts from pop songs, have become popular as ring <br>tones. This paper describes how a ring tone can be produced by analyzing the structure of <br>music and selecting the most appropriate portion of the music. With audio feature analysis <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:YQzy0g82Tz8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4561924389141089377&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'YQzy0g82Tz8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:309"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB91" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW91"><a href="http://turing.iimas.unam.mx/~caleb/pubre/UniversityofManchester.Rascon2009.pdf" class=yCA9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unam.mx</span><span class="gs_ggsS">unam.mx <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://turing.iimas.unam.mx/~caleb/pubre/UniversityofManchester.Rascon2009.pdf" class=yCA8>Spectral component analysis on distorted data</a></h3><div class="gs_a">CAR Estebane - 2009 - turing.iimas.unam.mx</div><div class="gs_rs">Spectral Component Analysis (SCA) is an important element of data analysis in various <br>fields throughout the scientific community. This is due in large because a sampled spectrum <br>provides a concise visualisation of the fundamental properties of a process or material. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MKXd7AW2BLwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13548153715602335024&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MKXd7AW2BLwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md91', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md91" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:MKXd7AW2BLwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:308"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB92" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW92"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.6223&amp;rep=rep1&amp;type=pdf" class=yCAB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.6223&amp;rep=rep1&amp;type=pdf" class=yCAA>Effectiveness of Signal Segmentation for Music Content Representation</a></h3><div class="gs_a">NCMMS Kankanhalli, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - Citeseer</div><div class="gs_rs">Abstract. In this paper we compare the effectiveness of rhythm based signal segmentation <br>technique with the traditional fixed length segmentation for music contents representation. <br>We consider vocal regions, instrumental regions and chords which represent the harmony <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:tis58lxAeiEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2412311318355323830&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'tis58lxAeiEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md92', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md92" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:tis58lxAeiEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:307"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2043615" class=yCAC>Beat space segmentation and octave scale cepstral feature for sung language recognition in pop music</a></h3><div class="gs_a">NC Maddage, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - ACM Transactions on Multimedia Computing,  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Sung language recognition relies on both effective feature extraction and acoustic <br>modeling. In this paper, we study rhythm based music segmentation with the frame size <br>being the duration of the smallest note in the music, as opposed to fixed length <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:PDNw-UBaogIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'PDNw-UBaogIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:306"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB94" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW94"><a href="http://laurentoudre.fr/publis/LO-PHDfr-10.pdf" class=yCAE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from laurentoudre.fr</span><span class="gs_ggsS">laurentoudre.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://laurentoudre.fr/publis/LO-PHDfr-10.pdf" class=yCAD>Laurent OUDRE</a></h3><div class="gs_a">DE Rapporteurs, S Marchand - laurentoudre.fr</div><div class="gs_rs">Ce premier chapitre vise Ã  introduire les principales notions musicales nÃ©cessaires Ã  la <br>bonne comprÃ©hension du prÃ©sent document, mais aussi Ã  prÃ©ciser le contexte et les <br>principales applications de ce travail de thÃ¨se.</div><div class="gs_fl"><a href="/scholar?q=related:PPdVPuMCoxEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1270862694875264828&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'PPdVPuMCoxEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md94', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md94" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:PPdVPuMCoxEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:305"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ci.nii.ac.jp/naid/110006222781/" class=yCAF>å±éã¨ä¸è²«æ§ã«çç®ãããã¬ã¤ãªã¹ãè¦ç´: éè¦åºéé¸æã«é¢ããå®é¨ã¨ã·ã¹ãã ä½æã¸åããæ¤è¨</a></h3><div class="gs_a">æ©æ¬æµ©å©ï¼ æ¾ç°æå²ï¼ å¹³ç°å­äº - æå ±å¦çå­¦ä¼ç ç©¶å ±å.[é³æ¥½æå ±ç§å­¦], 2007 - ci.nii.ac.jp</div><div class="gs_rs">æé² æ¬ç¨¿ã§ã¯, ä¸ãããããã¬ã¤ãªã¹ãã«å¯¾ãã¦ãã®é°å²æ° (é³æ¥½ããåããå°è±¡ãææ) <br>ãä¿æããè¦ç´ãçæããææ³ã«é¢ããå¿çå­¦çãªå®é¨ã¨ãã®çµæã«ã¤ãã¦è¿°ã¹ã. <br>å±éã¨ä¸è²«æ§ãé°å²æ°ã«å½±é¿ãä¸ããã¨ããä»®å®ã®ãã¨, ãããã«çç®ããè¦ç´ææ³ãææ¡ã, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:vQIrVbz9ZuMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16386063279558689469&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'vQIrVbz9ZuMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md95', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md95" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:vQIrVbz9ZuMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:304"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB96" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW96"><a href="http://tel.archives-ouvertes.fr/docs/00/54/28/40/PDF/These_fr_en.pdf" class=yCB1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://tel.archives-ouvertes.fr/pastel-00542840/" class=yCB0>Reconnaissance d&#39;accords Ã  partir de signaux audio par l&#39;utilisation de gabarits thÃ©oriques</a></h3><div class="gs_a"><a href="/citations?user=aSXlodEAAAAJ&amp;hl=en&amp;oi=sra">L Oudre</a> - 2010 - tel.archives-ouvertes.fr</div><div class="gs_rs">RÃ©sumÃ© Cette thÃ¨se s&#39; inscrit dans le cadre du traitement du signal musical, en se focalisant <br>plus particuliÃ¨rement sur la transcription automatique de signaux audio en accords. En effet, <br>depuis une dizaine d&#39;annÃ©es, de nombreux travaux visent Ã  reprÃ©senter les signaux <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:afhmW5HEmVcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6312292481319237737&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'afhmW5HEmVcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:303"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB97" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW97"><a href="https://jyx.jyu.fi/dspace/bitstream/handle/123456789/36531/URN:NBN:fi:jyu-2011080311207.pdf?sequence=1" class=yCB3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jyu.fi</span><span class="gs_ggsS">jyu.fi <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://jyx.jyu.fi/dspace/handle/123456789/36531" class=yCB2>Testing a spectral-based feature set for audio genre classification</a></h3><div class="gs_a">M Hartmann - 2011 - jyx.jyu.fi</div><div class="gs_rs">Automatic musical genre classification is an important information retrieval task since it can <br>be applied for practical purposes such as the organization of data collections in the digital <br>music industry. However, this task remains an open question because the current state of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5781060625412570662&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:Jr62agJ0OlAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Jr62agJ0OlAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:302"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB98" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW98"><a href="http://tel.archives-ouvertes.fr/docs/00/54/89/52/PDF/Papadopoulos_Thesis.pdf" class=yCB5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://tel.archives-ouvertes.fr/tel-00548952/" class=yCB4>Estimation conjointe d&#39;information de contenu musical d&#39;un signal audio</a></h3><div class="gs_a">H Papadopoulos - 2010 - tel.archives-ouvertes.fr</div><div class="gs_rs">RÃ©sumÃ© Dans cette these, nous nous intÃ©ressons au probleme de l&#39;extraction automatique <br>d&#39;informations de contenu d&#39;un signal audio de musique. La plupart des travaux existants <br>abordent ce probleme en considÃ©rant les attributs musicaux de maniere indÃ©pendante les <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:B-SObITZT2wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7804695842036573191&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'B-SObITZT2wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:301"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB99" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW99"><a href="https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/4e77efd5c6e2ceadc12567530068624d/d3d83c1d4ae52000c125753f005f5543/$FILE/2008_DammFrKuMuCl_SyncPlayer_LWA.pdf" class=yCB7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/4e77efd5c6e2ceadc12567530068624d/d3d83c1d4ae52000c125753f005f5543/$FILE/2008_DammFrKuMuCl_SyncPlayer_LWA.pdf" class=yCB6>SyncPlayer-Multimodale Wiedergabe, Navigation und Suche in heterogenen digitalen Musikkollektionen</a></h3><div class="gs_a">D Damm, C Fremerey, F Kurth&hellip; - 6.-8. October 2008,  &hellip;, 2008 - domino.mpi-inf.mpg.de</div><div class="gs_rs">Abstract Durch systematische Digitalisierung sind in den letzten Jahren umfangreiche <br>BestÃ¤nde von Musikdokumenten entstanden, die digitale Inhalte unterschiedlichster <br>AusprÃ¤gungen und Formate enthalten. Man denke hier beispielsweise an CD-Aufnahmen <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7061302106467012340&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:9A6ve9rI_mEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7061302106467012340&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'9A6ve9rI_mEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md99', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md99" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:9A6ve9rI_mEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
