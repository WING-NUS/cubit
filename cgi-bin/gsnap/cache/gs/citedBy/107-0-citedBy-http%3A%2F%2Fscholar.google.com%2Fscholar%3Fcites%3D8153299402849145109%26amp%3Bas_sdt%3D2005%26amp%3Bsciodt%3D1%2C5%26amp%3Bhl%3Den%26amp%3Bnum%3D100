Total results = 107
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.cse.iitk.ac.in/users/vision/dipen/references/duan-minXu-05_semantic-shot-classification-multi-sport.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iitk.ac.in</span><span class="gs_ggsS">iitk.ac.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1542084" class=yC0>A unified framework for semantic shot classification in sports video</a></h3><div class="gs_a">LY Duan, M Xu, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, CS Xu&hellip; - &hellip; , IEEE Transactions on, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The extensive amount of multimedia information available necessitates content-<br>based video indexing and retrieval methods. Since humans tend to use high-level semantic <br>concepts when querying and browsing multimedia databases, there is an increasing need <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2202004273452228387&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 148</a> <a href="/scholar?q=related:Ix8DUTgXjx4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/34/2D/RN182300820.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=2202004273452228387&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 21 versions</a> <a onclick="return gs_ocit(event,'Ix8DUTgXjx4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=957018" class=yC3>Trajectory-based ball detection and tracking with applications to semantic analysis of broadcast soccer video</a></h3><div class="gs_a">X Yu, C Xu, HW Leong, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, Q Tang&hellip; - Proceedings of the  &hellip;, 2003 - dl.acm.org</div><div class="gs_rs">Abstract This paper first presents an improved trajectory-based algorithm for automatically <br>detecting and tracking the ball in broadcast soccer video. Unlike the object-based <br>algorithms, our algorithm does not evaluate whether a sole object is a ball. Instead, it <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17188376816174998443&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 144</a> <a href="/scholar?q=related:q6OreaVhie4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17188376816174998443&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'q6OreaVhie4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm03-duanly.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=957013.957020" class=yC4>A mid-level representation framework for semantic sports video analysis</a></h3><div class="gs_a">LY Duan, M Xu, TS Chua, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, CS Xu - Proceedings of the eleventh  &hellip;, 2003 - dl.acm.org</div><div class="gs_rs">Abstract Sports video has been widely studied due to its tremendous commercial potentials. <br>Despite encouraging results from various specific sports games, it is almost impossible to <br>extend a system for a new sports game because they usually employ different sets of low-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5909517765826077244&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 137</a> <a href="/scholar?q=related:PGLsGRnTAlIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5909517765826077244&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'PGLsGRnTAlIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320307000247" class=yC6>Video summarisation: A conceptual framework and survey of the state of the art</a></h3><div class="gs_a">AG Money, H Agius - Journal of Visual Communication and Image  &hellip;, 2008 - Elsevier</div><div class="gs_rs">Video summaries provide condensed and succinct representations of the content of a video <br>stream through a combination of still images, video segments, graphical representations and <br>textual descriptors. This paper presents a conceptual framework for video summarisation <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7380321859487654168&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 129</a> <a href="/scholar?q=related:GOHP6qArbGYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7380321859487654168&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'GOHP6qArbGYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1180699" class=yC7>Live sports event detection based on broadcast video and web-casting text</a></h3><div class="gs_a">C Xu, J Wang, K Wan, Y Li, L Duan - Proceedings of the 14th annual  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract Event detection is essential for sports video summarization, indexing and retrieval <br>and extensive research efforts have been devoted to this area. However, the previous <br>approaches are heavily relying on video content itself and require the whole video content <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10303468019780515411&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 85</a> <a href="/scholar?q=related:U3ad6ZJF_Y4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10303468019780515411&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'U3ad6ZJF_Y4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://avss2012.org/2008papers/gjkw/gk8.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from avss2012.org</span><span class="gs_ggsS">avss2012.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4469885" class=yC8>A novel framework for semantic annotation and personalized retrieval of sports video</a></h3><div class="gs_a">C Xu, J Wang, H Lu, Y Zhang - Multimedia, IEEE Transactions  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Sports video annotation is important for sports video semantic analysis such as <br>event detection and personalization. In this paper, we propose a novel approach for sports <br>video semantic annotation and personalized retrieval. Different from the state of the art <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9150671654910331118&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 70</a> <a href="/scholar?q=related:7ogqR2O3_X4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/1A/16/RN226783660.html?source=googlescholar" class="gs_nph" class=yCA>BL Direct</a> <a href="/scholar?cluster=9150671654910331118&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'7ogqR2O3_X4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://hcsi.cs.tsinghua.edu.cn/Paper/paper06/200622IEEE_Trans.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tsinghua.edu.cn</span><span class="gs_ggsS">tsinghua.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1621215" class=yCB>A flexible framework for key audio effects detection and auditory context inference</a></h3><div class="gs_a"><a href="/citations?user=6WCyi64AAAAJ&amp;hl=en&amp;oi=sra">R Cai</a>, L Lu, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a>, HJ Zhang&hellip; - Audio, Speech, and  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Key audio effects are those special effects that play critical roles in human&#39;s <br>perception of an auditory context in audiovisual materials. Based on key audio effects, high-<br>level semantic inference can be carried out to facilitate various content-based analysis <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7288366306301078591&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 63</a> <a href="/scholar?q=related:P2wASIp6JWUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7288366306301078591&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 23 versions</a> <a onclick="return gs_ocit(event,'P2wASIp6JWUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.cse.iitk.ac.in/users/vision/dipen/references/26.Automatic%20replay%20generation%20for%20soccer%20video%20broadcasting.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iitk.ac.in</span><span class="gs_ggsS">iitk.ac.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1027535" class=yCD>Automatic replay generation for soccer video broadcasting</a></h3><div class="gs_a">J Wang, C Xu, <a href="/citations?user=FJodrCcAAAAJ&amp;hl=en&amp;oi=sra">E Chng</a>, K Wah, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - Proceedings of the 12th annual  &hellip;, 2004 - dl.acm.org</div><div class="gs_rs">Abstract While most current approaches for sports video analysis are based on broadcast <br>video, in this paper, we present a novel approach for highlight detection and automatic <br>replay generation for soccer videos taken by the main camera. This research is important <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14298998970423604281&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 52</a> <a href="/scholar?q=related:OXTG480_cMYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14298998970423604281&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'OXTG480_cMYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://nlpr-web.ia.ac.cn/2008papers/gjkw/gk9.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ia.ac.cn</span><span class="gs_ggsS">ia.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4668533" class=yCF>Using webcast text for semantic event detection in broadcast sports video</a></h3><div class="gs_a">C Xu, YF Zhang, G Zhu, <a href="/citations?user=uOJH_AEAAAAJ&amp;hl=en&amp;oi=sra">Y Rui</a>, H Lu&hellip; - &hellip; , IEEE Transactions on, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Sports video semantic event detection is essential for sports video summarization <br>and retrieval. Extensive research efforts have been devoted to this area in recent years. <br>However, the existing sports video event detection approaches heavily rely on either <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10746102338593290273&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 44</a> <a href="/scholar?q=related:IfjYQyTTIZUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10746102338593290273&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'IfjYQyTTIZUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://www.cecs.uci.edu/~papers/icme05/defevent/papers/cr1328.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uci.edu</span><span class="gs_ggsS">uci.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1521476" class=yC11>Current and emerging topics in sports video processing</a></h3><div class="gs_a">X Yu, D Farin - Multimedia and Expo, 2005. ICME 2005. IEEE  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Sports video processing is an interesting topic for research, since the clearly <br>defined game rules in sports provide the rich domain knowledge for analysis. Moreover, it is <br>interesting because many specialized applications for sports video processing are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6796096769867793978&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 33</a> <a href="/scholar?q=related:OkYmQwGWUF4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6796096769867793978&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'OkYmQwGWUF4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.comp.nus.edu.sg/~mohan/papers/pcm03-basket.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1292722" class=yC13>Event detection in basketball video using multiple modalities</a></h3><div class="gs_a">M Xu, LY Duan, C Xu, M Kankanhalli&hellip; - &hellip;  and Signal Processing, &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Semantic sports video analysis has attracted more and more attention recently. In <br>this paper, we present a basketball event detection method by using multiple modalities. <br>Instead of using low-level features, the proposed method is built upon visual and auditory <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2364027804403038056&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 31</a> <a href="/scholar?q=related:aPeWUMO2ziAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2364027804403038056&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'aPeWUMO2ziAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1640552" class=yC15>Semantic event detection using conditional random fields</a></h3><div class="gs_a">T Wang, <a href="/citations?user=n44GlFcAAAAJ&amp;hl=en&amp;oi=sra">J Li</a>, Q Diao, W Hu, <a href="/citations?user=KCurmvwAAAAJ&amp;hl=en&amp;oi=sra">Y Zhang</a>&hellip; - Computer Vision and  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Semantic event detection is an active research field of video mining in recent years. <br>One of the challenging problems is how to effectively model temporal and multi-modality <br>characteristics of video. In this paper, we employ Conditional Random Fields (CRFs) to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10982423581136482300&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 30</a> <a href="/scholar?q=related:_G-DRg9oaZgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10982423581136482300&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'_G-DRg9oaZgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://asp.eurasipjournals.com/content/pdf/1687-6180-2006-032135.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurasipjournals.com</span><span class="gs_ggsS">eurasipjournals.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Multimodal semantic analysis and annotation for basketball video</h3><div class="gs_a">S Liu, M Xu, H Yi, <a href="/citations?user=Eeolw80AAAAJ&amp;hl=en&amp;oi=sra">LT Chia</a>&hellip; - &hellip;  on Advances in  &hellip;, 2006 - Hindawi Publishing Corporation</div><div class="gs_fl"><a href="/scholar?cites=5960866707969222526&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 30</a> <a href="/scholar?q=related:fif1WbBAuVIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/51/56/RN191189151.html?source=googlescholar" class="gs_nph" class=yC17>BL Direct</a> <a href="/scholar?cluster=5960866707969222526&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'fif1WbBAuVIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://cemnet.ntu.edu.sg/home/asltchia/publication/AudioAnalysisUnderstanding/Conference/HMM-Based%20Audio%20Keyword%20Generation.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/ppknqwau8qx26b4q.pdf" class=yC18>HMM-based audio keyword generation</a></h3><div class="gs_a">M Xu, LY Duan, <a href="/citations?user=B3_kPMcAAAAJ&amp;hl=en&amp;oi=sra">J Cai</a>, <a href="/citations?user=Eeolw80AAAAJ&amp;hl=en&amp;oi=sra">LT Chia</a>, C Xu, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - Advances in Multimedia  &hellip;, 2005 - Springer</div><div class="gs_rs">With the exponential growth in the production creation of multimedia data, there is an <br>increasing need for video semantic analysis. Audio, as a significant part of video, provides <br>important cues to human perception when humans are browsing and understanding video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3229366618422834530&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 30</a> <a href="/scholar?q=related:YvWYXNQD0SwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3229366618422834530&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'YvWYXNQD0SwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://research.microsoft.com/pubs/69263/unsupervisedaudiodiscovery_mm05.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from microsoft.com</span><span class="gs_ggsS">microsoft.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101292" class=yC1A>Unsupervised content discovery in composite audio</a></h3><div class="gs_a"><a href="/citations?user=6WCyi64AAAAJ&amp;hl=en&amp;oi=sra">R Cai</a>, L Lu, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - Proceedings of the 13th annual ACM  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract Automatically extracting semantic content from audio streams can be helpful in <br>many multimedia applications. Motivated by the known limitations of traditional supervised <br>approaches to content extraction, which are hard to generalize and require suitable <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11944170264021832642&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 27</a> <a href="/scholar?q=related:whPOe5I3wqUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11944170264021832642&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'whPOe5I3wqUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459457" class=yC1C>Hierarchical movie affective content analysis based on arousal and valence features</a></h3><div class="gs_a">M Xu, JS Jin, S Luo, L Duan - Proceedings of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Emotional factors directly reflect audiences&#39; attention, evaluation and memory. <br>Affective contents analysis not only create an index for users to access their interested movie <br>segments, but also provide feasible entry for video highlights. Most of the work focus on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2349814809648348901&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 30</a> <a href="/scholar?q=related:5b4KmR44nCAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2349814809648348901&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'5b4KmR44nCAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1352012.1352015" class=yC1D>Audio keywords generation for sports video analysis</a></h3><div class="gs_a">M Xu, C Xu, L Duan, JS Jin, S Luo - ACM Transactions on Multimedia  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Sports video has attracted a global viewership. Research effort in this area has <br>been focused on semantic event detection in sports video to facilitate accessing and <br>browsing. Most of the event detection methods in sports video are based on visual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13796356612686272358&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 26</a> <a href="/scholar?q=related:ZtfGNT6Bdr8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13796356612686272358&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ZtfGNT6Bdr8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1077314208000416" class=yC1E>Automatic camera calibration of broadcast tennis video with applications to 3D virtual content insertion and ball detection and tracking</a></h3><div class="gs_a">X Yu, <a href="/citations?user=rW2bJ-UAAAAJ&amp;hl=en&amp;oi=sra">N Jiang</a>, LF Cheong, HW Leong, X Yan - Computer Vision and Image  &hellip;, 2009 - Elsevier</div><div class="gs_rs">This paper presents an original algorithm to automatically acquire accurate camera <br>calibration from broadcast tennis video (BTV) as well as demonstrates two of its many <br>applications. Accurate camera calibration from BTV is challenging because the frame-data <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=740681615647150494&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 21</a> <a href="/scholar?q=related:npFPUAFuRwoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=740681615647150494&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'npFPUAFuRwoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ri"><h3 class="gs_rt"><a href="http://inderscience.metapress.com/index/0ne7rhwhpq8w907b.pdf" class=yC1F>A multimodal data mining framework for soccer goal detection based on decision tree logic</a></h3><div class="gs_a">SC Chen, ML Shyu, C Zhang, M Chen - International Journal of  &hellip;, 2006 - Inderscience</div><div class="gs_rs">In this paper, we propose a new multimedia data mining framework for the extraction of <br>soccer goal events in soccer videos by utilising both multimodal analysis and decision tree <br>logic. The extracted events can be used to index the soccer videos. We first adopt an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4453059263829818329&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 20</a> <a href="/scholar?q=related:2btWoM5xzD0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/45/58/RN202942476.html?source=googlescholar" class="gs_nph" class=yC20>BL Direct</a> <a href="/scholar?cluster=4453059263829818329&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'2btWoM5xzD0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://multimedialab.elis.ugent.be/users/chpoppe/Masterproef/sportsAnalysis/tennis/TrajectoryBased%20ball%20detection%20and%20tracking%20algorithm%20in%20broadcast%20tennis%20video.pdf" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ugent.be</span><span class="gs_ggsS">ugent.be <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1419482" class=yC21>A trajectory-based ball detection and tracking algorithm in broadcast tennis video</a></h3><div class="gs_a">X Yu, CH Sim, JR Wang&hellip; - Image Processing, 2004.  &hellip;, 2004 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Ball locations over frames facilitate tennis video analysis to a great extent. But so far <br>no algorithm is able to obtain satisfactory result in locating the ball in broadcast tennis video <br>(BTV). This paper presents a trajectory-based algorithm to detect and track the ball in BTV. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7797774624784910597&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 20</a> <a href="/scholar?q=related:BQnc9LRCN2wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7797774624784910597&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'BQnc9LRCN2wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="https://merl.com/reports/docs/TR2004-046.pdf" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from merl.com</span><span class="gs_ggsS">merl.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1026738" class=yC23>A time series clustering based framework for multimedia mining and summarization using audio features</a></h3><div class="gs_a">R Radhakrishnan, A Divakaran, Z Xiong - Proceedings of the 6th ACM  &hellip;, 2004 - dl.acm.org</div><div class="gs_rs">Abstract Past work on multimedia analysis has shown the utility of detecting specific <br>temporal patterns for different content genres. In this paper, we propose a unified, content-<br>adaptive, unsupervised mining framework to bring out such temporal patterns from <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12634291326245809648&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 19</a> <a href="/scholar?q=related:8C2umQUFVq8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12634291326245809648&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'8C2umQUFVq8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://www.mirlab.org/conference_papers/International_Conference/ICICS_PCM%202003/PDFFILES/PAPERS/2B12P0896.PDF" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1292595" class=yC25>Generation of sports highlights using a combination of supervised &amp; unsupervised learning in audio domain</a></h3><div class="gs_a">R Radhakrishan, Z Xiong, A Divakaran&hellip; - &hellip; Processing, 2003 and &hellip;, 2003 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In our past work we have used supervised audio classification to develop a <br>common audio-based platform for highlight extraction that works across three different <br>sports. We then use a heuristic to post-process the classification results to identify <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6138607473141221342&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 18</a> <a href="/scholar?q=related:3gckHPm2MFUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6138607473141221342&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'3gckHPm2MFUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://www.cse.iitk.ac.in/users/vision/dipen/references/28.SOCCER%20HIGHLIGHT%20DETECTION%20USING%20TWO-DEPENDENCE%20BAYESIAN%20NETWORK.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iitk.ac.in</span><span class="gs_ggsS">iitk.ac.in <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036927" class=yC27>Soccer highlight detection using two-dependence bayesian network</a></h3><div class="gs_a"><a href="/citations?user=n44GlFcAAAAJ&amp;hl=en&amp;oi=sra">J Li</a>, T Wang, W Hu, M Sun&hellip; - Multimedia and Expo,  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Soccer highlight detection is an active research topic in recent years. One of the <br>difficult problems is how to effectively fuse multi-modality cues, ie audio, visual and textual <br>information, to improve the detection performance. This paper proposes a novel two-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2992065312363558894&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 18</a> <a href="/scholar?q=related:7rcyiIzzhSkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2992065312363558894&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'7rcyiIzzhSkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://202.114.89.42/resource/pdf/2383.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 202.114.89.42</span><span class="gs_ggsS">202.114.89.42 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1415593" class=yC29>Towards a unified framework for content-based audio analysis</a></h3><div class="gs_a">L Lu, <a href="/citations?user=6WCyi64AAAAJ&amp;hl=en&amp;oi=sra">R Cai</a>, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - Acoustics, Speech, and Signal  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Audio content analysis is helpful in many multimedia applications. We present a <br>unified framework for content analysis of composite audio. The framework is designed to <br>extract relevant information from different available audio modalities and to discover high-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=330294372363773023&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:X6QNUPtwlQQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=330294372363773023&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'X6QNUPtwlQQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4959998" class=yC2B>Non-speech audio event detection</a></h3><div class="gs_a"><a href="/citations?user=0s2lJxQAAAAJ&amp;hl=en&amp;oi=sra">J Portelo</a>, M Bugalho, <a href="/citations?user=Auzu_DcAAAAJ&amp;hl=en&amp;oi=sra">I Trancoso</a>, J Neto&hellip; - &hellip; , Speech and Signal  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Audio event detection is one of the tasks of the European project VIDIVIDEO. This <br>paper focuses on the detection of non-speech events, and as such only searches for events <br>in audio segments that have been previously classified as non-speech. Preliminary <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9109808449915291031&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:l4UN9YSKbH4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9109808449915291031&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'l4UN9YSKbH4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1661201" class=yC2C>Audio elements based auditory scene segmentation</a></h3><div class="gs_a">L Lu, <a href="/citations?user=6WCyi64AAAAJ&amp;hl=en&amp;oi=sra">R Cai</a>, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - Acoustics, Speech and Signal  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Auditory scene segmentation is an important step in the process of high-level <br>semantic inference from audio data streams, and in particular, a prerequisite for auditory <br>scene categorization. In this paper, we analyze the limits of previous works on auditory <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9142897028656497185&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:IYILC2gY4n4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9142897028656497185&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'IYILC2gY4n4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://asp.eurasipjournals.com/content/pdf/1687-6180-2006-089013.pdf" class=yC2E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from eurasipjournals.com</span><span class="gs_ggsS">eurasipjournals.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1288263.1288434" class=yC2D>A content-adaptive analysis and representation framework for audio event discovery from unscripted multimedia</a></h3><div class="gs_a">R Radhakrishnan, A Divakaran, Z Xiong&hellip; - EURASIP Journal on  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract We propose a content-adaptive analysis and representation framework to discover <br>events using audio features from&quot; unscripted&quot; multimedia such as sports and surveillance for <br>summarization. The proposed analysis framework performs an inlier/outlier-based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12993683011992890020&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 13</a> <a href="/scholar?q=related:pCrdHdvVUrQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/00/20/RN191189186.html?source=googlescholar" class="gs_nph" class=yC2F>BL Direct</a> <a href="/scholar?cluster=12993683011992890020&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'pCrdHdvVUrQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4407815" class=yC30>Audio keywords discovery for text-like audio content analysis and retrieval</a></h3><div class="gs_a">L Lu, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - Multimedia, IEEE Transactions on, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Inspired by classical text document analysis employing the concept of (key) words, <br>this paper presents an unsupervised approach to discover (key) audio elements in general <br>audio documents. The (key) audio elements can be considered the equivalents of the text <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18222210759325332439&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 16</a> <a href="/scholar?q=related:1zPj4yNM4vwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/33/03/RN222407227.html?source=googlescholar" class="gs_nph" class=yC31>BL Direct</a> <a href="/scholar?cluster=18222210759325332439&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'1zPj4yNM4vwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://www.cs.fiu.edu/~chens/PDF/ISM05_NN.pdf" class=yC33><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fiu.edu</span><span class="gs_ggsS">fiu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1565809" class=yC32>Neural network based framework for goal event detection in soccer videos</a></h3><div class="gs_a">K Wickramaratna, M Chen, SC Chen&hellip; - Multimedia, Seventh  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, a neural network based framework for semantic event detection in <br>soccer videos is proposed. The framework provides a robust solution for soccer goal event <br>detection by combining the strength of multimodal analysis and the ability of neural <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3508363797172945971&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 13</a> <a href="/scholar?q=related:M4RVEUk2sDAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3508363797172945971&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'M4RVEUk2sDAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://www.inesc-id.pt/pt/indicadores/Ficheiros/5611.pdf" class=yC35><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from inesc-id.pt</span><span class="gs_ggsS">inesc-id.pt <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.inesc-id.pt/pt/indicadores/Ficheiros/5611.pdf" class=yC34>Detecting audio events for semantic video search</a></h3><div class="gs_a">M Bugalho, <a href="/citations?user=0s2lJxQAAAAJ&amp;hl=en&amp;oi=sra">J Portelo</a>, <a href="/citations?user=Auzu_DcAAAAJ&amp;hl=en&amp;oi=sra">I Trancoso</a>, <a href="/citations?user=5FvYex4AAAAJ&amp;hl=en&amp;oi=sra">T Pellegrini</a>&hellip; - InterSpeech, Brighton,  &hellip;, 2009 - inesc-id.pt</div><div class="gs_rs">Abstract This paper describes our work on audio event detection, one of our tasks in the <br>European project VIDIVIDEO. Preliminary experiments with a small corpus of sound effects <br>have shown the potential of this type of corpus for training purposes. This paper describes <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4689871117309017731&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:g36R5e7EFUEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4689871117309017731&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'g36R5e7EFUEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://hcsi.cs.tsinghua.edu.cn/Paper/paper05/200503.pdf" class=yC37><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tsinghua.edu.cn</span><span class="gs_ggsS">tsinghua.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1415594" class=yC36>Unsupervised auditory scene categorization via key audio effects and information-theoretic co-clustering</a></h3><div class="gs_a"><a href="/citations?user=6WCyi64AAAAJ&amp;hl=en&amp;oi=sra">R Cai</a>, L Lu, LH Cai - Acoustics, Speech, and Signal Processing &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic categorization of auditory scenes is very useful in various content-based <br>multimedia applications, such as video indexing and context-aware computing. An <br>unsupervised approach is proposed to group auditory scenes with similar semantics. In <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5865560553758302438&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 10</a> <a href="/scholar?q=related:5qBOHz-oZlEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5865560553758302438&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'5qBOHz-oZlEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://users.cs.fiu.edu/~chens/PDF/ISM06_Event.pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fiu.edu</span><span class="gs_ggsS">fiu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4061168" class=yC38>Exciting event detection using multi-level multimodal descriptors and data classification</a></h3><div class="gs_a">SC Chen, M Chen, C Zhang&hellip; - Multimedia, 2006. ISM&#39;06. &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Event detection is of great importance in high-level semantic indexing and selective <br>browsing of video clips. However, the use of low-level visual-audio feature descriptors alone <br>generally fails to yield satisfactory results in event identification due to the semantic gap <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17451694078689921009&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 12</a> <a href="/scholar?q=related:8ccZMU3fMPIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17451694078689921009&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'8ccZMU3fMPIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB32" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW32"><a href="http://nlpr-web.ia.ac.cn/2010papers/gjhy/gh105.pdf" class=yC3B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ia.ac.cn</span><span class="gs_ggsS">ia.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816057" class=yC3A>Coherent bag-of audio words model for efficient large-scale video copy detection</a></h3><div class="gs_a">Y Liu, <a href="/citations?user=-ECnpPEAAAAJ&amp;hl=en&amp;oi=sra">WL Zhao</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, CS Xu, HQ Lu - Proceedings of the ACM  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Current content-based video copy detection approaches mostly concentrate on the <br>visual cues and neglect the audio information. In this paper, we attempt to tackle the video <br>copy detection task resorting to audio information, which is equivalently important as well <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16606734763629511558&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 9</a> <a href="/scholar?q=related:hoNEAUj5duYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16606734763629511558&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'hoNEAUj5duYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.2667&amp;rep=rep1&amp;type=pdf" class=yC3C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Towards optimal audio&#39;keywords&#39; detection for audio content analysis and discovery</h3><div class="gs_a">L Lu, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - &hellip;  Multimedia Conference: Proceedings of the 14 th  &hellip;, 2006</div><div class="gs_fl"><a href="/scholar?cites=15137003144334387633&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:sakFFilwEdIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15137003144334387633&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'sakFFilwEdIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://sites.google.com/site/zhiwulu/acm-mm_lu09.pdf" class=yC3E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631428" class=yC3D>Semantic concept annotation based on audio PLSA model</a></h3><div class="gs_a">Y Peng, <a href="/citations?user=OUXS8doAAAAJ&amp;hl=en&amp;oi=sra">Z Lu</a>, J Xiao - Proceedings of the 17th ACM international  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract This paper proposes a new approach and algorithm for the semantic concept <br>annotation based on audio PLSA (probabilistic latent semantic analysis) model. The novelty <br>of our approach includes two sides: Audio vocabulary construction, and audio PLSA <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6992644325388896115&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:c8d0H_fcCmEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6992644325388896115&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'c8d0H_fcCmEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.3915&amp;rep=rep1&amp;type=pdf" class=yC40><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1421383" class=yC3F>Event detection based on non-broadcast sports video</a></h3><div class="gs_a">J Wang, C Xu, <a href="/citations?user=FJodrCcAAAAJ&amp;hl=en&amp;oi=sra">E Chng</a>, X Yu&hellip; - Image Processing, 2004.  &hellip;, 2004 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Most recent research work on sports video analysis focuses on broadcast video. <br>The broadcast video is a post-produced video and has much additional editing information <br>inserted. In this paper, we propose a novel event detection framework in sports game only <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3247290486813910157&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 8</a> <a href="/scholar?q=related:jWQMLX6xEC0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3247290486813910157&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'jWQMLX6xEC0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="http://pdf.aminer.org/000/503/142/audio_keyword_generation_for_sports_video_analysis.pdf" class=yC42><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aminer.org</span><span class="gs_ggsS">aminer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://pdf.aminer.org/000/503/142/audio_keyword_generation_for_sports_video_analysis.pdf" class=yC41>Audio keyword generation for sports video analysis</a></h3><div class="gs_a">M Xu, LY Duan, <a href="/citations?user=Eeolw80AAAAJ&amp;hl=en&amp;oi=sra">LT Chia</a>, C Xu - &hellip; : Proceedings of the 12 th annual  &hellip;, 2004 - pdf.aminer.org</div><div class="gs_rs">ABSTRACT Semantic sports video analysis has attracted many research interests and audio <br>cues have been shown to play an important role in semantics inference. To facilitate event <br>detection using audio information, we have introduced the concept of audio keyword (eg <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12267847641930873782&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:tjvnt24mQKoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12267847641930873782&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'tjvnt24mQKoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md36', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md36" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:tjvnt24mQKoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4021541" class=yC43>Audio content-based highlight detection using adaptive Hidden Markov Model</a></h3><div class="gs_a">B Zhang, W Dou, <a href="/citations?user=VOPW5YYAAAAJ&amp;hl=en&amp;oi=sra">L Chen</a> - Intelligent Systems Design and  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A hidden Markov model (HMM) is employed in this paper to model the structure, <br>and thus generate highlights, of table tennis games. Unlike existing approaches, we use <br>only one HMM for the table tennis games, which combines domain knowledge during <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7532545240378579948&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:7BtTq_z5iGgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7532545240378579948&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'7BtTq_z5iGgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4406118" class=yC44>Event detection in sports video based on multiple feature fusion</a></h3><div class="gs_a">L Hua-Yong, H Tingting, Z Hui - Fuzzy Systems and Knowledge &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Semantic sports video analysis has attracted more and more attention recently. In <br>this paper, we present a football event detection method by using multiple feature extraction <br>and fusion. Instead of using low-level features, the proposed method is built upon visual, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=723783562720012206&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:rmcxxlBlCwoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=723783562720012206&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'rmcxxlBlCwoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB39" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW39"><a href="http://www.ing.unibs.it/~cost292/pubs/neurel06/1_4_Neurel2006.pdf" class=yC46><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unibs.it</span><span class="gs_ggsS">unibs.it <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4147154" class=yC45>GA-based feature extraction for clapping sound detection</a></h3><div class="gs_a">J Olajec, R Jarina, M Kuba - Neural Network Applications in  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatically extracting semantic content from audio streams can be helpful in <br>many multimedia applications. In this paper, we introduce a framework for automatic feature <br>subspace selection from a common feature vector. The selected features build a new <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5190220862050456902&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:RtVWBGBeB0gJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5190220862050456902&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'RtVWBGBeB0gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB40" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW40"><a href="http://www.inesc-id.pt/pt/indicadores/Ficheiros/5004.pdf" class=yC48><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from inesc-id.pt</span><span class="gs_ggsS">inesc-id.pt <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.inesc-id.pt/pt/indicadores/Ficheiros/5004.pdf" class=yC47>Training audio events detectors with a sound effects corpus</a></h3><div class="gs_a"><a href="/citations?user=Auzu_DcAAAAJ&amp;hl=en&amp;oi=sra">I Trancoso</a>, <a href="/citations?user=0s2lJxQAAAAJ&amp;hl=en&amp;oi=sra">J Portlo</a>, M Bugalho, J Neto, <a href="/citations?user=ocgbpfgAAAAJ&amp;hl=en&amp;oi=sra">A Serralheiro</a>&hellip; - City, 2008 - inesc-id.pt</div><div class="gs_rs">Abstract This paper describes the work done in the framework of the VIDIVIDEO European <br>project in terms of audio event detection. Our first experiments concerned the detection of <br>nonvoice sounds, such as birds, machines, traffic, water and steps. Given the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=476515713165448065&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:gVMeNYTsnAYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=476515713165448065&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'gVMeNYTsnAYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md40', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md40" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:gVMeNYTsnAYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB41" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW41"><a href="http://www.sunshinedot.com/paper/DataMining_CLAUSCCA.pdf" class=yC4A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sunshinedot.com</span><span class="gs_ggsS">sunshinedot.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4523953" class=yC49>Co-clustering for auditory scene categorization</a></h3><div class="gs_a"><a href="/citations?user=6WCyi64AAAAJ&amp;hl=en&amp;oi=sra">R Cai</a>, L Lu, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - Multimedia, IEEE Transactions on, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Auditory scenes are temporal audio segments with coherent semantic content. <br>Automatically classifying and grouping auditory scenes with similar semantics into <br>categories is beneficial for many multimedia applications, such as semantic event <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17516806558155057057&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 7</a> <a href="/scholar?q=related:oacQssEyGPMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/2F/33/RN230927903.html?source=googlescholar" class="gs_nph" class=yC4B>BL Direct</a> <a href="/scholar?cluster=17516806558155057057&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'oacQssEyGPMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB42" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW42"><a href="http://hal.archives-ouvertes.fr/docs/00/53/29/44/PDF/these_Ewa_Kijak.pdf" class=yC4D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/tel-00532944/" class=yC4C>Structuration multimodale des vidos de sport par modles stochastiques</a></h3><div class="gs_a">E Kijak - 2003 - hal.archives-ouvertes.fr</div><div class="gs_rs">Ce chapitre propose un panorama des systmes spcifiques ddis plus particulirement <br>aux vnements sportifs. Aprs une description gnrale du contexte de l&#39;analyse des <br>vidos de sports, nous dcrirons quelles sont les informations a priori utilises par les <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10919920476736050839&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 10</a> <a href="/scholar?q=related:l9ZdP9FZi5cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10919920476736050839&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'l9ZdP9FZi5cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md42', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md42" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:l9ZdP9FZi5cJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=1547231355426720420&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1509216" class=yC4E>Personalized video adaptation based on video content analysis</a></h3><div class="gs_a">M Xu, JS Jin, S Luo - Proceedings of the 9th International Workshop on  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Personalized video adaptation is expected to satisfy individual users&#39; needs on <br>video content. Multimedia data mining plays a significant role of video annotation to meet <br>users&#39; preference on video content. In this paper, a comprehensive solution for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16436973552850877739&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:K70qCFzcG-QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16436973552850877739&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'K70qCFzcG-QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://ceur-ws.org/Vol-441/p04.pdf" class=yC50><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ceur-ws.org</span><span class="gs_ggsS">ceur-ws.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ceur-ws.org/Vol-441/p04.pdf" class=yC4F>A novel tool for quick video summarization using keyframe extraction techniques</a></h3><div class="gs_a"><a href="/citations?user=ucLHfl4AAAAJ&amp;hl=en&amp;oi=sra">M Lux</a>, <a href="/citations?user=GOIf9NIAAAAJ&amp;hl=en&amp;oi=sra">K Schffmann</a>, <a href="/citations?user=ZgWULzYAAAAJ&amp;hl=en&amp;oi=sra">O Marques</a>&hellip; - Proceedings of the 9th  &hellip;, 2009 - ceur-ws.org</div><div class="gs_rs">Abstract: The increasing availability of short, unstructured video clips on the Web has <br>generated an unprecedented need to organize, index, annotate and retrieve video contents <br>to make them useful to potential viewers. This paper presents a novel, simple, and easy-to<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5380210477144143690&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:SvcCY-xYqkoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'SvcCY-xYqkoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md44', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md44" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:SvcCY-xYqkoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB45" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW45"><a href="http://cecs.uci.edu/~papers/icme05/defevent/papers/cr1399.pdf" class=yC52><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uci.edu</span><span class="gs_ggsS">uci.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1521506" class=yC51>A mid-level visual concept generation framework for sports analysis</a></h3><div class="gs_a">X Tong, L Duan, H Lu, C Xu, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>&hellip; - Multimedia and Expo,  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The development of mid-level concepts helps to bridge the gap between low-level <br>feature and high-level semantics in video analysis. Most existing work combines the <br>customized mid-level concepts and statistical models to detect particular events. Based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10821976227201183062&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:Vm0MlgpiL5YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10821976227201183062&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'Vm0MlgpiL5YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00167ED1V01Y200812ICR002" class=yC53>Automated metadata in multimedia information systems: Creation, refinement, use in surrogates, and evaluation</a></h3><div class="gs_a">MG Christel - &hellip; Lectures on Information Concepts, Retrieval, and &hellip;, 2009 - morganclaypool.com</div><div class="gs_rs">Abstract Improvements in network bandwidth along with dramatic drops in digital storage <br>and processing costs have resulted in the explosive growth of multimedia (combinations of <br>text, image, audio, and video) resources on the Internet and in digital repositories. A suite <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=207521543820141715&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:k-Sjnr5D4QIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=207521543820141715&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'k-Sjnr5D4QIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md46', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md46" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:k-Sjnr5D4QIJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=6795887439985526845&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB47" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW47"><a href="http://tech.ebu.ch/webdav/site/tech/shared/events/aiempro10/presentations/aeimpro10_poppe.pdf" class=yC55><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ebu.ch</span><span class="gs_ggsS">ebu.ch <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1877865" class=yC54>Generic architecture for event detection in broadcast sports video</a></h3><div class="gs_a">C Poppe, S De Bruyne, R Van de Walle - Proceedings of the 3rd  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract An increasing amount of digital sports content is generated and made available <br>through broadcast and Internet. To deliver meaningful access for an end-user, <br>summarizations or highlights of the content are necessary. Hence, the automatic <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11917207589898966207&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:v1SEtiltYqUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11917207589898966207&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'v1SEtiltYqUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://www.cecs.uci.edu/~papers/icme06/pdfs/0000021.pdf" class=yC57><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uci.edu</span><span class="gs_ggsS">uci.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036526" class=yC56>On parallelization of a video mining system</a></h3><div class="gs_a">W Li, <a href="/citations?user=CIFkSBUAAAAJ&amp;hl=en&amp;oi=sra">E Li</a>, N Di, C Dulong, T Wang&hellip; - Multimedia and Expo,  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract As digital video data becomes more pervasive, mining information from multimedia <br>data becomes increasingly important. Although researches in multimedia mining area have <br>shown great potential in daily life, the huge computational requirement prohibits its wide <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6809858114202656263&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 4</a> <a href="/scholar?q=related:B17c6995gV4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6809858114202656263&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'B17c6995gV4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://vidosearch.com/publications/summarization_arthroscopic_videos.pdf" class=yC59><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from vidosearch.com</span><span class="gs_ggsS">vidosearch.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/u037362581245316.pdf" class=yC58>A novel tool for summarization of arthroscopic videos</a></h3><div class="gs_a"><a href="/citations?user=ucLHfl4AAAAJ&amp;hl=en&amp;oi=sra">M Lux</a>, <a href="/citations?user=ZgWULzYAAAAJ&amp;hl=en&amp;oi=sra">O Marques</a>, <a href="/citations?user=GOIf9NIAAAAJ&amp;hl=en&amp;oi=sra">K Schffmann</a>&hellip; - Multimedia Tools and  &hellip;, 2010 - Springer</div><div class="gs_rs">Abstract Arthroscopic surgery is a minimally invasive procedure that uses a small camera to <br>generate video streams, which are recorded and subsequently archived. In this paper we <br>present a video summarization tool and demonstrate how it can be successfully used in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13786597655895317496&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:-JvfBIbVU78J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13786597655895317496&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'-JvfBIbVU78J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4665148" class=yC5A>Highlight detection in soccer video using web-casting text</a></h3><div class="gs_a">XF Ding, YJ Miao, F Bu, LF Sun&hellip; - &hellip; Signal Processing, 2008 &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Highlight detection is a challenge task in soccer video analysis. Using Web-casting <br>text as external knowledge is proved to be a short cut to achieve both efficiency and <br>effectiveness. Based on the previous framework using Web-casting text, we have <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=77297618300392894&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:vlXHOMadEgEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'vlXHOMadEgEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4741053" class=yC5B>Automatic overlaid text detection, extraction and recognition for high level event/concept identification in soccer videos</a></h3><div class="gs_a"><a href="/citations?user=vwlkEDEAAAAJ&amp;hl=en&amp;oi=sra">AA Halin</a>, <a href="/citations?user=3O9nAIoAAAAJ&amp;hl=en&amp;oi=sra">M Rajeswari</a>&hellip; - Computer and Electrical  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Overlaid text appears frequently in broadcast sports video. They provide <br>supplementary information regarding the happenings of a particular game. Examples <br>include important events of interest such as bookings and substitutions in a soccer match. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10077667440745390040&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 3</a> <a href="/scholar?q=related:2B8mwiQR24sJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10077667440745390040&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'2B8mwiQR24sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4556928" class=yC5C>Development of a reference platform for generic audio classification</a></h3><div class="gs_a">R Jarina, M Paralic, M Kuba, J Olajec&hellip; - Image Analysis for  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Detection of key sounds, such as applause, laugh, music, environmental noise, etc., <br>is one of the challenges in intelligent management of multimedia information and content <br>understanding. In this paper, we report progress in development of a reference content-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17731083834854945410&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:gjIz8812EfYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17731083834854945410&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'gjIz8812EfYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://cjc.ict.ac.cn/quanwenjiansuo/2008-7/txf.zip" class=yC5E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cjc.ict.ac.cn/quanwenjiansuo/2008-7/txf.zip" class=yC5D></a></h3><div class="gs_a">   - , 2008 - cjc.ict.ac.cn</div><div class="gs_rs"><br>. , , , <br>, . ; <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15070747761267103782&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 6</a> <a href="/scholar?q=related:JtjWmT0NJtEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15070747761267103782&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'JtjWmT0NJtEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md53', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md53" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:JtjWmT0NJtEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB54" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW54"><a href="http://www3.ntu.edu.sg/home/aseschng/SpeechTechWeb/members/Conformation_theses/Thesis_Wang_Jinjun_20071007.pdf" class=yC60><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www3.ntu.edu.sg/home/aseschng/SpeechTechWeb/members/Conformation_theses/Thesis_Wang_Jinjun_20071007.pdf" class=yC5F>Content-Based Sports Video Analysis and Composition</a></h3><div class="gs_a">W Jinjun - 2006 - ntu.edu.sg</div><div class="gs_rs">Abstract This thesis proposes solutions for content-based sports video analysis, including <br>multimodal feature extraction, middle-level representation and semantic event detection. In <br>addition, solutions for sports video composition and personalization are also examined. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8897660065604983548&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:_FqQFrDWensJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8897660065604983548&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'_FqQFrDWensJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md54', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md54" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_FqQFrDWensJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB55" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW55"><a href="http://www.acoustics.hut.fi/~cerkut/pubs/preprint/2007/Olajec07_Finsig.pdf" class=yC62><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hut.fi</span><span class="gs_ggsS">hut.fi <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.acoustics.hut.fi/~cerkut/pubs/preprint/2007/Olajec07_Finsig.pdf" class=yC61>GA-based feature selection for synchronous and asynchronous applause detection</a></h3><div class="gs_a">J Olajec, <a href="/citations?user=NPYFs5EAAAAJ&amp;hl=en&amp;oi=sra">C Erkut</a>, R Jarina - Proc. Finnish Signal Processing  &hellip;, 2007 - acoustics.hut.fi</div><div class="gs_rs">ABSTRACT Automatically extracting semantic content from audio streams can be helpful in <br>many multimedia applications. For instance, asynchronous applause can mark the breaks <br>between the songs in a live performance, whereas synchronous applause can indicate <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8824446760351652368&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:EFobGpO7dnoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8824446760351652368&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'EFobGpO7dnoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md55', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md55" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:EFobGpO7dnoJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB56" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW56"><a href="http://202.114.89.42/resource/pdf/1462.pdf" class=yC64><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 202.114.89.42</span><span class="gs_ggsS">202.114.89.42 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4517544" class=yC63>Unsupervised anchor space generation for similarity measurement of general audio</a></h3><div class="gs_a">L Lu, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - &hellip;  and Signal Processing, 2008. ICASSP 2008.  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Reliably measuring similarity between audio clips is critical to many applications. <br>As opposed to the conventional way of measuring audio similarity using low-level features <br>directly, in this paper we consider the similarity computation using an anchor space. Each <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14308812151829306088&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:6BqUO9cck8YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14308812151829306088&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'6BqUO9cck8YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB57" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW57"><a href="http://tur-www1.massey.ac.nz/~dpparson/002.pdf" class=yC66><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from massey.ac.nz</span><span class="gs_ggsS">massey.ac.nz <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://tur-www1.massey.ac.nz/~dpparson/002.pdf" class=yC65>A feature based music content recognition method using simplified MFCC</a></h3><div class="gs_a">BK Sung, MB Chung, IJ Ko - Int J Princ Appl Inf Sci Technol, 2008 - tur-www1.massey.ac.nz</div><div class="gs_rs">Abstract. Since large amounts of digital music began to be distributed through the internet, <br>demand for music related content (lyrics, musician profiles and movie clips) has been <br>increasing. Recently, music related content has been offered through online services that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12815034913775116165&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:hV_La1sm2LEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12815034913775116165&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'hV_La1sm2LEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md57', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md57" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:hV_La1sm2LEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/8112667038622P50.pdf" class=yC67>Ball tracking and 3D trajectory approximation with applications to tactics analysis from single-camera volleyball sequences</a></h3><div class="gs_a">HT Chen, WJ Tsai, SY Lee, JY Yu - Multimedia Tools and Applications, 2012 - Springer</div><div class="gs_rs">Abstract Providing computer-assisted tactics analysis in sports is a growing trend. This paper <br>presents an automatic system for ball tracking and 3D trajectory approximation from single-<br>camera volleyball sequences as well as demonstrates several applications to tactics <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6082805743451807188&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:1J0meJh3alQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6082805743451807188&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'1J0meJh3alQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB59" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW59"><a href="http://multimedialab.elis.ugent.be/users/chpoppe/Masterproef/sportsAnalysis/tennis/Trajectory-based%20ball%20detection%20and%20tracking%20with%20aid%20of%20homography%20in%20broadcast%20tennis%20video.pdf" class=yC69><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ugent.be</span><span class="gs_ggsS">ugent.be <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://multimedialab.elis.ugent.be/users/chpoppe/Masterproef/sportsAnalysis/tennis/Trajectory-based%20ball%20detection%20and%20tracking%20with%20aid%20of%20homography%20in%20broadcast%20tennis%20video.pdf" class=yC68>Trajectory-based ball detection and tracking with aid of homography in broadcast tennis video</a></h3><div class="gs_a">X Yu, <a href="/citations?user=rW2bJ-UAAAAJ&amp;hl=en&amp;oi=sra">N Jiang</a>, EL Ang - Society of Photo-Optical  &hellip;, 2007 - multimedialab.elis.ugent.be</div><div class="gs_rs">ABSTRACT Ball-detection-and-tracking in broadcast tennis video (BTV) is a crucial but <br>challenging task in tennis video semantics analysis. Informally, the challenges are due to <br>camera motion and the other causes such as the presence of many balllike objects and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12079508828141392662&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:FhOQekcJo6cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/05/14/RN210982436.html?source=googlescholar" class="gs_nph" class=yC6A>BL Direct</a> <a href="/scholar?cluster=12079508828141392662&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'FhOQekcJo6cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md59', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md59" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:FhOQekcJo6cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5158956" class=yC6B>Integrating multiple feature fusion for semantic event detection in soccer video</a></h3><div class="gs_a">L Hua-Yong, H Tingting - Artificial Intelligence, 2009. JCAI&#39;09.  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Semantic soccer video analysis has attracted more and more attention recently. In <br>this paper, we present a football event detection method by using multiple feature extraction <br>and fusion. Instead of using low-level features, the proposed method is built upon visual, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16068016035572085878&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:dvSNzXMP_d4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16068016035572085878&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'dvSNzXMP_d4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB61" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW61"><a href="http://nlpr-web.ia.ac.cn/2007papers/gjhy/gh34.pdf" class=yC6D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ia.ac.cn</span><span class="gs_ggsS">ia.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://nlpr-web.ia.ac.cn/2007papers/gjhy/gh34.pdf" class=yC6C>Multimodal based highlight detection in broadcast soccer video</a></h3><div class="gs_a"><a href="/citations?user=uiA3yegAAAAJ&amp;hl=en&amp;oi=sra">Y Zhang</a>, Q Liu, <a href="/citations?user=o8PT69EAAAAJ&amp;hl=en&amp;oi=sra">J Cheng</a>, H Lu - Proceedings of Asia-Pacific  &hellip;, 2007 - nlpr-web.ia.ac.cn</div><div class="gs_rs">Abstract: In this paper, we propose an effective fusion scheme of audio and visual modals for <br>highlight detection in broadcast soccer videos. The Adaboost learning is adopted to learn <br>some discriminating audio features for some special audio classification. The logo-based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4456942366098984224&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:IA2403c92j0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4456942366098984224&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'IA2403c92j0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md61', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md61" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:IA2403c92j0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5190238" class=yC6E>Semantic event mining in soccer video based on multiple feature fusion</a></h3><div class="gs_a">L Hua-Yong, H Tingting - Information Technology and  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we present a football event detection method by using multiple feature <br>extraction and fusion. Instead of using low-level features, the proposed method is built upon <br>visual, auditory features, text and audio keywords. Promising event detection results have <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8496287107863351191&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:lw_t_Rvg6HUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8496287107863351191&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'lw_t_Rvg6HUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1734653" class=yC6F>Affective content analysis by mid-level representation in multiple modalities</a></h3><div class="gs_a">M Xu, S Luo, JS Jin, M Park - &hellip;  of the First International Conference on  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Movie affective content detection attracts ever-increasing research efforts. However, <br>the affective content analysis is still a challenging task due to the gap between low-level <br>perceptual features and high-level human perception of the media. Moreover, clues from <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5576848986335334335&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:v8-DDJvyZE0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'v8-DDJvyZE0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB64" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW64"><a href="http://wiki.cs.columbia.edu/download/attachments/1977/Analysis%20of%20interactive%20and%20asynchronous%20multimedia%20communication-11-08-2007.pdf" class=yC71><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://wiki.cs.columbia.edu/download/attachments/1977/Analysis%20of%20interactive%20and%20asynchronous%20multimedia%20communication-11-08-2007.pdf" class=yC70>Analysis of interactive and asynchronous multimedia communications to detect social and culturally-dependent communication patterns</a></h3><div class="gs_a">P Thermos, H Schulzrinne - Computer, 2007 - wiki.cs.columbia.edu</div><div class="gs_rs">Abstract Asynchronous and interactive multimedia communication methods are researched <br>in order to understand their effectiveness in bilateral and multilateral communications and <br>improve performance. By analyzing cultural dependencies and social interactions in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8519532601221783942&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:hiEvLMN1O3YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8519532601221783942&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'hiEvLMN1O3YJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md64', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md64" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:hiEvLMN1O3YJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB65" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW65"><a href="http://rvc.eng.miami.edu/Paper/2005/ISM05-unit.pdf" class=yC73><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from miami.edu</span><span class="gs_ggsS">miami.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1565849" class=yC72>Unit detection from American football TV broadcast using multimodal content analysis</a></h3><div class="gs_a">G Ravitz, ML Shyu - Multimedia, Seventh IEEE International  &hellip;, 2005 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, a multimodal unit detection framework to detect and extract units, a <br>novel concept towards event detection and extraction in sports TV broadcasts, is proposed. <br>The proposed unit is defined to be a segment of a sports TV broadcast that describes a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2901674704850629841&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:0bRbNMXRRCgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2901674704850629841&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'0bRbNMXRRCgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4287657" class=yC74>Automatic Audio Classification and Speaker Identification for Video Content Analysis</a></h3><div class="gs_a">SC Liu, J Bi, ZQ Jia, R Chen, <a href="/citations?user=l91It28AAAAJ&amp;hl=en&amp;oi=sra">J Chen</a>&hellip; - &hellip; , 2007. SNPD 2007.  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Recently, more literatures proposed to apply audio content analysis techniques in <br>content-based video parsing. This paper presents our works on audio classification and <br>speaker identification techniques for video content analysis. Firstly, soundtrack extracted <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1664590331137775349&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:9VYEOBbQGRcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1664590331137775349&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'9VYEOBbQGRcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:333"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4627013" class=yC75>Overlaid Text Recognition for Matching Soccer-Concept Keywords</a></h3><div class="gs_a"><a href="/citations?user=vwlkEDEAAAAJ&amp;hl=en&amp;oi=sra">AA Halin</a>, <a href="/citations?user=3O9nAIoAAAAJ&amp;hl=en&amp;oi=sra">M Rajeswari</a>&hellip; - &hellip;  Graphics, Imaging and  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Overlaid-text appears frequently in broadcast sports video. They provide a plethora <br>of information regarding the goings-on of a particular game. Examples include important <br>events and video segments of interest such as bookings and half-time analysis, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2499805219977972465&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:8Up0YJoXsSIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2499805219977972465&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'8Up0YJoXsSIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:332"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB68" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW68"><a href="http://downloads.hindawi.com/journals/ijdmb/2010/836357.pdf" class=yC77><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.hindawi.com/journals/ijdmb/2010/836357/abs/" class=yC76>Personalized sports video customization using content and context analysis</a></h3><div class="gs_a">C Liang, C Xu, H Lu - International Journal of Digital Multimedia  &hellip;, 2010 - hindawi.com</div><div class="gs_rs">We present an integrated framework on personalized sports video customization, which <br>addresses three research issues: semantic video annotation, personalized video retrieval <br>and summarization, and system adaptation. Sports video annotation serves as the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9839189406217011968&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:AANae6LSi4gJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9839189406217011968&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'AANae6LSi4gJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md68', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md68" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:AANae6LSi4gJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:331"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB69" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW69"><a href="http://webpages.iust.ac.ir/mahfathy/Papers/A%20Low%20Cost%20Algorithm%20for%20Expected%20Goal%20Events%20Detection%20in%20Broadcast%20Soccer%20Video%20.pdf" class=yC79><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iust.ac.ir</span><span class="gs_ggsS">iust.ac.ir <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://webpages.iust.ac.ir/mahfathy/Papers/A%20Low%20Cost%20Algorithm%20for%20Expected%20Goal%20Events%20Detection%20in%20Broadcast%20Soccer%20Video%20.pdf" class=yC78>A Low Cost Algorithm for Expected Goal Events Detection in Broadcast Soccer Video</a></h3><div class="gs_a">M Zameni, M Fathy, A Sadri - International Journal of Digital  &hellip;, 2010 - webpages.iust.ac.ir</div><div class="gs_rs">Abstract Recently, mining information in sports video data, especially soccer video, has <br>become an active research topic. In this paper, a new algorithm for detection of expected <br>goal events in soccer video is proposed. The proposed algorithm is composed of two main <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8600701763861400613&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:JcwIjq_UW3cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8600701763861400613&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'JcwIjq_UW3cJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md69', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md69" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:JcwIjq_UW3cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:330"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB70" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW70"><a href="http://www.researchgate.net/publication/215617307_DATA_MEDIA_AND_IMAGE_MINING/file/bc6522a3b4e90e4f6909c0e0e5e15849.doc" class=yC7B><span class="gs_ggsL"><span class=gs_ctg2>[DOC]</span> from researchgate.net</span><span class="gs_ggsS">researchgate.net <span class=gs_ctg2>[DOC]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[DOC]</span><span class="gs_ct2">[DOC]</span></span> <a href="http://www.researchgate.net/publication/215617307_DATA_MEDIA_AND_IMAGE_MINING/file/bc6522a3b4e90e4f6909c0e0e5e15849.doc" class=yC7A>Data, Media and Image Mining</a></h3><div class="gs_a">R Balu, T Devi - researchgate.net</div><div class="gs_rs">Abstract: For many years, statistics have been used to analyze data in an effort to find <br>correlations, patterns, and dependencies. However, with advancement in technology, more <br>and more data are available, which greatly exceed the human capacity to manually <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15164114088162327514&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:2ntplmvBcdIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'2ntplmvBcdIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md70', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md70" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:2ntplmvBcdIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:329"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/dq69t3n6r451g783.pdf" class=yC7C>Affective content detection by using timing features and fuzzy clustering</a></h3><div class="gs_a">M Xu, S Luo, J Jin - Advances in Multimedia Information Processing-PCM  &hellip;, 2008 - Springer</div><div class="gs_rs">Emotional factors directly reflect audiences&#39; attention, evaluation and memory. Movie <br>affective content detection attracts more and more research efforts. Most of the existing work <br>focus on developing efficient affective features or implementing feasible pattern <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12025810266822473759&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:H_gy4rhC5KYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12025810266822473759&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'H_gy4rhC5KYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:328"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/v0h8512h76h60431.pdf" class=yC7D>A flexible framework for audio semantic content detection</a></h3><div class="gs_a">Q Li, H Ma, K Zheng - &hellip;  in Multimedia Information Processing-PCM 2008, 2008 - Springer</div><div class="gs_rs">Audio semantic analysis is a crucial issue in multimedia applications. In this paper, a <br>hierarchical framework is proposed for high-level semantic content detection for a <br>continuous audio stream. In the proposed framework, basic audio events are modeled <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7046766839516494531&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:w_ph7Boly2EJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7046766839516494531&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'w_ph7Boly2EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:327"><div class="gs_ri"><h3 class="gs_rt"><a href="http://en.scientificcommons.org/29241650" class=yC7E>Two level discriminative training for audio event recognition in sport broadcasts</a></h3><div class="gs_rs"><a href="https://support.google.com/websearch/bin/answer.py?answer=45449&hl=en">This site may harm your computer.</a></div><div class="gs_a">K Biatov - &hellip;  of the 12th International Conference on Speech &hellip;, 2007 - en.scientificcommons.org</div><div class="gs_rs">Abstract In this paper, two level discriminative learning for audio events recognition in sport <br>broadcasts archive is described. The audio events recognition is based on the idea that <br>audio events are composed of basic units. Basic units are some elementary events. Audio <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17521492329975122904&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:2Avm9nDYKPMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17521492329975122904&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'2Avm9nDYKPMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md73', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md73" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:2Avm9nDYKPMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:326"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4494225" class=yC7F>Feature Based Same Audio Perception method for Filtering of Illegal Music Contents</a></h3><div class="gs_a">B Sung, M Jung, J Ham, J Kim&hellip; - &hellip; , 2008. ICACT 2008.  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Recently, sharing and distribution of illegal music contents is increasing. For <br>protecting that, Filtering have been using. Retrieval word interception of text searching and <br>DRM are main technology in these days. Retrieval word interception of text searching <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1319678652120951539&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:8-pV_b1wUBIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1319678652120951539&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'8-pV_b1wUBIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:325"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1937768" class=yC80>Adaptive local hyperplanes for MTV affective analysis</a></h3><div class="gs_a">M Xu, L Chen, X He, C Xu, JS Jin - Proceedings of the Second  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Affective analysis attracts increasing attention in multimedia domain since affective <br>factors directly reflect audiences&#39; attention, evaluation and memory. Existing study focuses <br>on mapping low-level affective features to high-level emotions by applying machine <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:6e1xKHzeS_wJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'6e1xKHzeS_wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:324"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/g445282622103777.pdf" class=yC81>Sports Video Analysis: From Semantics to Tactics</a></h3><div class="gs_a">G Zhu, C Xu, Q Huang - Multimedia Content Analysis, 2009 - Springer</div><div class="gs_rs">Sports content is expected to be a key driver for compelling new infotainment applications <br>and services because of its mass appeal and inherent structures that are amenable for <br>automatic processing. Due to its wide viewership and tremendous commercial value, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:DY3_P2qwst0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15975024798623304973&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'DY3_P2qwst0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:323"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB77" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW77"><a href="http://www.fsktm.upm.edu.my/~alfian/IAJIT-2b-published.pdf" class=yC83><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from upm.edu.my</span><span class="gs_ggsS">upm.edu.my <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.fsktm.upm.edu.my/~alfian/IAJIT-2b-published.pdf" class=yC82>Soccer Event Detection via Collaborative Multimodal Feature Analysis and Candidate Ranking</a></h3><div class="gs_a"><a href="/citations?user=vwlkEDEAAAAJ&amp;hl=en&amp;oi=sra">AA Halin</a>, <a href="/citations?user=3O9nAIoAAAAJ&amp;hl=en&amp;oi=sra">M Rajeswari</a>, M Abbasnejad - fsktm.upm.edu.my</div><div class="gs_rs">Abstract: This paper presents a framework for soccer event detection through collaborative <br>analysis of the textual, visual and aural modalities. The basic notion is to decompose a <br>match video into smaller segments until ultimately the desired eventful segment is <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7sh2Tg4lmSYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2781294988254169326&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'7sh2Tg4lmSYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md77', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md77" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:7sh2Tg4lmSYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:322"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB78" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW78"><a href="http://users.cis.fiu.edu/~mchen005/PDF/MDMBookChapter12.pdf" class=yC85><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fiu.edu</span><span class="gs_ggsS">fiu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/q31301510k17413k.pdf" class=yC84>Video Event Mining via Multimodal Content Analysis and Classification</a></h3><div class="gs_a">M Chen, SC Chen, ML Shyu, C Zhang - Multimedia Data Mining and  &hellip;, 2007 - Springer</div><div class="gs_rs">As digital video data become more and more pervasive, the issue of mining information from <br>video data becomes increasingly important. In this chapter, we present an effective <br>multimedia data mining framework for event mining with its application in the automatic <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1SL8Zhv6W_UJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17679999757771875029&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'1SL8Zhv6W_UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:321"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2362476" class=yC86>The video summary GWAP: summarization of videos based on a social game</a></h3><div class="gs_a">A Mller, <a href="/citations?user=ucLHfl4AAAAJ&amp;hl=en&amp;oi=sra">M Lux</a>, <a href="/citations?user=-LFUanwAAAAJ&amp;hl=en&amp;oi=sra">L Bszrmenyi</a> - &hellip;  of the 12th International Conference on &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Video summarization is a classical problem in multimedia. How can we reduce the <br>size of a video stream to a minium necessary to allow the user for deciding upon relevance <br>of the video. In this paper we present an approach to video summarization that <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:6tlMaS3Y0fkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'6tlMaS3Y0fkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:320"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB80" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW80"><a href="http://ceur-ws.org/Vol-902/paper_3.pdf" class=yC88><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ceur-ws.org</span><span class="gs_ggsS">ceur-ws.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ceur-ws.org/Vol-902/paper_3.pdf" class=yC87>Automatic Extraction of Soccer Game Events from Twitter</a></h3><div class="gs_a">G van Oorschot, <a href="/citations?user=QeZOR3AAAAAJ&amp;hl=en&amp;oi=sra">M van Erp</a>, C Dijkshoorn - &hellip; , and Exploitation of Events in the  &hellip; - ceur-ws.org</div><div class="gs_rs">Abstract. Sports events data is often compiled manually by companies who rarely make it <br>available for free to third parties. However, social media provide us with large amounts of <br>data that discuss these very same matches for free. In this study, we investigate to what <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:TNT8a2IyFukJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16795667258624758860&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'TNT8a2IyFukJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md80', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md80" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TNT8a2IyFukJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:319"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/f12t92v268738117.pdf" class=yC89>A method of generating table of contents for educational videos</a></h3><div class="gs_a">GG Lee, EJ Kim, J Kang, JG Kim, WY Kim - Advances in Multimedia  &hellip;, 2005 - Springer</div><div class="gs_rs">Abstract. The increasing amount of multimedia data enforces the development of automatic <br>video analysis techniques. In this paper, a method of ToC generation is proposed for <br>educational video contents. The proposed method consists of two parts: scene <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9e3EdvmOW4cJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/1A/56/RN178109636.html?source=googlescholar" class="gs_nph" class=yC8A>BL Direct</a> <a href="/scholar?cluster=9753546620091559413&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'9e3EdvmOW4cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:318"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB82" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW82"><a href="http://www.ee.columbia.edu/ln/dvmm/publications/13/IJMIR-EventRecognitionSurvey-v10.pdf" class=yC8C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/n4262424231v1533.pdf" class=yC8B>High-level event recognition in unconstrained videos</a></h3><div class="gs_a"><a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a>, <a href="/citations?user=WR_ah0wAAAAJ&amp;hl=en&amp;oi=sra">S Bhattacharya</a>, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a>&hellip; - International Journal of  &hellip;, 2012 - Springer</div><div class="gs_rs">Abstract The goal of high-level event recognition is to automatically detect complex high-<br>level events in a given video sequence. This is a difficult task especially when videos are <br>captured under unconstrained conditions by non-professionals. Such videos depicting <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'yxFXd_-Ig5MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:317"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB83" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW83"><a href="http://137.132.14.55/bitstream/handle/10635/14402/thesis.pdf?sequence=1" class=yC8E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.14.55</span><span class="gs_ggsS">137.132.14.55 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://137.132.14.55/handle/10635/14402" class=yC8D>Event detection in soccer video based on audio/visual keywords</a></h3><div class="gs_a">K YULIN - 2004 - 137.132.14.55</div><div class="gs_rs">In this thesis, we propose a multi-modal two-level event detection framework and <br>demonstrate it on soccer videos. We use a mid-level representation called Audio and Visual <br>Keyword (AVK) that can be learned and detected in video segments. AVKs are intended to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:8u8rejdXeoUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9618095849987633138&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'8u8rejdXeoUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:316"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT7962330&amp;id=_QfnAQAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC8F>Apparatus and method for automatic dissection of segmented audio signals</a></h3><div class="gs_a">S Goronzy, T Kemp, R Kompe, YH Lam&hellip; - US Patent  &hellip;, 2011 - Google Patents</div><div class="gs_rs">An apparatus for automatic dissection of segmented audio signals, wherein at least one <br>information signal for identifying programs included in said audio signals and for identifying <br>contents included in said programs. Content detection device detects programs and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:xAOofoVNmfwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18201664604745499588&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'xAOofoVNmfwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:315"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB85" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW85"><a href="http://www.waset.ac.nz/journals/waset/v26/v26-122.pdf" class=yC91><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from waset.ac.nz</span><span class="gs_ggsS">waset.ac.nz <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.waset.ac.nz/journals/waset/v26/v26-122.pdf" class=yC90>Effective Digital Music Retrieval System through Content-based Features</a></h3><div class="gs_a">B Sung, K Koo, J Kim, MB Jung, J Kwon, I Ko - waset.ac.nz</div><div class="gs_rs">AbstractIn this paper, we propose effective system for digital music retrieval. We divided <br>proposed system into Client and Server. Client part consists of pre-processing and Content-<br>based feature extraction stages. In pre-processing stage, we minimized Time code Gap <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:qQo4ijocdLIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12858933873802218153&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'qQo4ijocdLIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md85', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md85" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:qQo4ijocdLIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:314"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6155936" class=yC92>An event sequence based method for audio scene analysis</a></h3><div class="gs_a">Q Li, B Tian, M Zhang - &hellip;  Technology (IC-BNMT), 2011 4th IEEE  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Audio semantic analysis is an important issue for multimedia applications. In this <br>paper, we propose a neural network based approach to analyze the high-level semantic <br>content of audio event sequences for the action movies. According to the time interval <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:PTgCjqPmUR0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'PTgCjqPmUR0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:313"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/j1381666lx336577.pdf" class=yC93>A Neural Network Based Framework for Audio Scene Analysis in Audio Sensor Networks</a></h3><div class="gs_a">Q Li, H Ma, D Zhao - Advances in Multimedia Information Processing-PCM &hellip;, 2009 - Springer</div><div class="gs_rs">Abstract. In recent years, the audio sensor networks have been paid much attention. One of <br>the most important applications of audio sensor networks is audio scene analysis. In this <br>paper, we present a neural network based framework for analyzing the audio scene in the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:EYHgkfXocbUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13074487334645301521&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'EYHgkfXocbUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:312"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/71711R2722003173.pdf" class=yC94>Event detection models using 2d-BN and CRFs</a></h3><div class="gs_a">T Wang, J Li, W Hu, X Tong, <a href="/citations?user=KCurmvwAAAAJ&amp;hl=en&amp;oi=sra">Y Zhang</a>&hellip; - Advances in Multimedia  &hellip;, 2006 - Springer</div><div class="gs_rs">In this paper, we propose two novel semantic event detection models, ie, Two-dependence <br>Bayesian Network (2d-BN) and Conditional Random Fields (CRFs). 2d-BN is a simplified <br>Bayesian Network classifier which can characterize the feature relationships well and be <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:IMJnlffZ90gJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5D/50/RN201122604.html?source=googlescholar" class="gs_nph" class=yC95>BL Direct</a> <a href="/scholar?cluster=5257910747365098016&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'IMJnlffZ90gJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:311"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.worldscientific.com/doi/abs/10.1142/S021821301100005X" class=yC96>A PRACTICAL SYSTEM FOR ACOUSTIC SURVEILLANCE OF HAZARDOUS SITUATIONS</a></h3><div class="gs_a"><a href="/citations?user=Ktqy0r0AAAAJ&amp;hl=en&amp;oi=sra">S NTALAMPIRAS</a>, <a href="/citations?user=gWZ4dTUAAAAJ&amp;hl=en&amp;oi=sra">I POTAMITIS</a>&hellip; - International Journal on  &hellip;, 2011 - World Scientific</div><div class="gs_rs">The present study describes a practical methodology for automatic space monitoring based <br>solely on the perceived acoustic information. Our approach is based on a two stage <br>recognition schema that detects and classifies sound events related to hazardous <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:54QoqXEb0nsJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8922223986757633255&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'54QoqXEb0nsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:310"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/H5335754771H1436.pdf" class=yC97>Audio Content Discovery: An Unsupervised Approach</a></h3><div class="gs_a">L Lu, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - Multimedia Content Analysis, 2009 - Springer</div><div class="gs_rs">Automatically extracting semantic content from audio streams can be helpful in many <br>multimedia applications. Motivated by the known limitations of traditional supervised <br>approaches to content extraction, which are hard to generalize and require suitable <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:fjSmvxotxeMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16412574009929380990&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'fjSmvxotxeMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:309"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB91" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW91"><a href="http://epubs.surrey.ac.uk/7743/2/LongtonJackson_ICME08_pre.pdf" class=yC99><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from surrey.ac.uk</span><span class="gs_ggsS">surrey.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4607722" class=yC98>Parallel model combination and word recognition in soccer audio</a></h3><div class="gs_a">JH Longton, PJB Jackson - Multimedia and Expo, 2008 IEEE  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Audio from broadcast soccer can be used for identifying highlights from the game. <br>Audio cues derived from these sources provide valuable information about game events, as <br>can the detection of key words used by the commentators. In this paper we interpret the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:rYzlxHl-_hEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1296612804188802221&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'rYzlxHl-_hEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:308"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB92" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW92"><a href="http://www.papamarkos.gr/uploaded-files/69300216.pdf" class=yC9B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from papamarkos.gr</span><span class="gs_ggsS">papamarkos.gr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/D7766L7617169818.pdf" class=yC9A>Video summarization using a self-growing and self-organized neural gas network</a></h3><div class="gs_a"><a href="/citations?user=-_JAhdQAAAAJ&amp;hl=en&amp;oi=sra">D Papadopoulos</a>, <a href="/citations?user=n3DR7UEAAAAJ&amp;hl=en&amp;oi=sra">S Chatzichristofis</a>&hellip; - Computer Vision/ &hellip;, 2011 - Springer</div><div class="gs_rs">In this paper, a novel method to generate video summaries is proposed, which is allocated <br>mainly for being applied to on-line videos. The novelty of this approach lies in the fact that <br>the video summarization problem is considered as a single query image retrieval problem. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gz100tu95oEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9360377627362475395&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'gz100tu95oEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:307"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB93" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW93"><a href="http://repository.tudelft.nl/assets/uuid:ad50215b-e446-4ead-9a02-25f6f9bf7da7/thesis.pdf" class=yC9D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tudelft.nl</span><span class="gs_ggsS">tudelft.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://repository.tudelft.nl/assets/uuid:ad50215b-e446-4ead-9a02-25f6f9bf7da7/thesis.pdf" class=yC9C>Content Discovery from Composite Audio</a></h3><div class="gs_a">LU Lie - 2009 - repository.tudelft.nl</div><div class="gs_rs">In the age of information explosion, the amount of published information and available data <br>is rapidly increasing. As the amount of available data grows, the problem of managing the <br>information contained in this data becomes more and more difficult. Searchto quickly find <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gwIv1FFUntwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15897236445093364355&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'gwIv1FFUntwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md93', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md93" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:gwIv1FFUntwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:306"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/B353106477JR1402.pdf" class=yC9E>A three-level framework for affective content analysis and its case studies</a></h3><div class="gs_a">M Xu, J Wang, X He, JS Jin, S Luo, H Lu - Multimedia Tools and  &hellip;, 2012 - Springer</div><div class="gs_rs">Abstract Emotional factors directly reflect audiences&#39; attention, evaluation and memory. <br>Recently, video affective content analysis attracts more and more research efforts. Most of <br>the existing methods map low-level affective features directly to emotions by applying <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1572084190074695607&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:t-9o2j0q0RUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'t-9o2j0q0RUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:305"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB95" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW95"><a href="https://scholarbank.nus.edu.sg/bitstream/handle/10635/14160/Thesis_Semantic%20Soccer%20Video%20Analysis.pdf?sequence=1" class=yCA0><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://scholarbank.nus.edu.sg/handle/10635/14160" class=yC9F>Semantic soccer video analysis</a></h3><div class="gs_a">SUN HAIPING - 2004 - scholarbank.nus.edu.sg</div><div class="gs_rs">This thesis describes a study on a novel intermediate representation to bridge the gap <br>between low-level features and semantic meanings in soccer video analysis. Based on our <br>study, we conclude that shot is not suitable as a mid-level representation for soccer video <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:so5C26Y8NRYJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1600251929940889266&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'so5C26Y8NRYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:304"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0167739X12001689" class=yCA1>Fusing audio vocabulary with visual features for pornographic video detection</a></h3><div class="gs_a">Y Liu, Y Yang, H Xie, S Tang - Future Generation Computer Systems, 2012 - Elsevier</div><div class="gs_rs">Abstract Pornographic video detection based on multimodal fusion is an effective approach <br>for filtering pornography. However, existing methods lack accurate representation of audio <br>semantics and pay little attention to the characteristics of pornographic audios. In this <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:xqVO6GFWLr4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'xqVO6GFWLr4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:303"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4895314" class=yCA2>Text-like segmentation of general audio for content-based retrieval</a></h3><div class="gs_a">L Lu, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - Multimedia, IEEE Transactions on, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic detection of (semantically) meaningful audio segments, or audio scenes, <br>is an important step in high-level semantic inference from general audio signals, and can <br>benefit various content-based applications involving both audio and multimodal (<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7841593336952784468&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 5</a> <a href="/scholar?q=related:VAYk6pfv0mwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7841593336952784468&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'VAYk6pfv0mwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:302"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/6T0L1548233P4514.pdf" class=yCA3>Structuring sport video through audio event classification</a></h3><div class="gs_a">K Lin, M Pwint - Advances in Multimedia Information Processing-PCM  &hellip;, 2010 - Springer</div><div class="gs_rs">Automatic audio information retrieval is facing great challenge due to the advances of <br>information technology, more and more digital audio, images and video are being captured, <br>produced and stored. To develop an automatic audio signal classification for a large <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gQtG7Y8gJCAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2316011910908480385&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'gQtG7Y8gJCAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:301"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB99" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW99"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/14771/PhD_Thesis_YuXinguo.pdf?sequence=1" class=yCA5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/14771" class=yCA4>An effective trajectory-based algorithm for ball detection and tracking with application to the analysis of broadcast sports video</a></h3><div class="gs_a">YU XINGUO - 2005 - scholarbank.nus.sg</div><div class="gs_rs">This thesis is on sports video analysis and enhancement. It addresses three closely-related <br>problems. It first addresses the ball detection and tracking problem in broadcast sports <br>video. It proposes an effective trajectory-based algorithm for locating the ball in a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:K6ID80CJaqgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12135663057951236651&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'K6ID80CJaqgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
