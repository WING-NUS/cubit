Total results = 61
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="ftp://ftp.cse.ohio-state.edu/pub/tech-report/2005/TR61.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ohio-state.edu</span><span class="gs_ggsS">ohio-state.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4156205" class=yC0>Separation of singing voice from music accompaniment for monaural recordings</a></h3><div class="gs_a"><a href="/citations?user=OCQSEIsAAAAJ&amp;hl=en&amp;oi=sra">Y Li</a>, <a href="/citations?user=yO59sggAAAAJ&amp;hl=en&amp;oi=sra">DL Wang</a> - Audio, Speech, and Language Processing,  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Separating singing voice from music accompaniment is very useful in many <br>applications, such as lyrics recognition and alignment, singer identification, and music <br>information retrieval. Although speech separation has been extensively studied for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16877915460734066236&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 74</a> <a href="/scholar?q=related:PJ4gQrBmOuoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/03/1B/RN211148698.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=16877915460734066236&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'PJ4gQrBmOuoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://staff.aist.go.jp/h.fujihara/pdf/ism2006_fujihara.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4061176" class=yC3>Automatic synchronization between lyrics and music CD recordings based on Viterbi alignment of segregated vocal signals</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a>, J Ogata, K Komatani&hellip; - &hellip; , 2006. ISM&#39;06.  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes a system that can automatically synchronize between <br>polyphonic musical audio signals and corresponding lyrics. Although there were methods <br>that can synchronize between monophonic speech signals and corresponding text <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13888616118180065235&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 50</a> <a href="/scholar?q=related:0y_jQcZGvsAJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13888616118180065235&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'0y_jQcZGvsAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.119.3200&amp;rep=rep1&amp;type=pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4432643" class=yC5>LyricAlly: Automatic synchronization of textual lyrics to acoustic music signals</a></h3><div class="gs_a"><a href="/citations?user=aNVcd3EAAAAJ&amp;hl=en&amp;oi=sra">MY Kan</a>, Y Wang, D Iskandar, TL New&hellip; - Audio, Speech, and  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We present LyricAlly, a prototype that automatically aligns acoustic musical signals <br>with their corresponding textual lyrics, in a manner similar to manually-aligned karaoke. We <br>tackle this problem based on a multimodal approach, using an appropriate pairing of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6994750038097962320&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 27</a> <a href="/scholar?q=related:UKV6kRlYEmEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/47/0A/RN222855432.html?source=googlescholar" class="gs_nph" class=yC7>BL Direct</a> <a href="/scholar?cluster=6994750038097962320&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'UKV6kRlYEmEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://ismir2007.ismir.net/proceedings/ismir2007_p369_gruhne.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ismir.net</span><span class="gs_ggsS">ismir.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ismir2007.ismir.net/proceedings/ismir2007_p369_gruhne.pdf" class=yC8>Phoneme recognition in popular music</a></h3><div class="gs_a">M Gruhne, K Schmidt, C Dittmar - Proc. ISMIR 2007, 2007 - ismir2007.ismir.net</div><div class="gs_rs">ABSTRACT Automatic lyrics synchronization for karaoke applications is a major challenge in <br>the field of music information retrieval. An important pre-requisite in order to precisely <br>synchronize the music and corresponding text is the detection of single phonemes in the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14292217003741744257&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 22</a> <a href="/scholar?q=related:gQzTOKQnWMYJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14292217003741744257&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'gQzTOKQnWMYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:gQzTOKQnWMYJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1112857" class=yCA>Key, chord, and rhythm tracking of popular music recordings</a></h3><div class="gs_a">A Shenoy, Y Wang - Computer Music Journal, 2005 - dl.acm.org</div><div class="gs_rs">In this article, we propose a framework to analyze a musical audio signal (sampled from a <br>popular music CD) and determine its key, provide usable chord transcriptions, and obtain <br>the hierarchical rhythm structure representation comprising the quarternote, half-note, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16290336487138294495&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 22</a> <a href="/scholar?q=related:33p2o7nmEuIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/2E/0B/RN175256451.html?source=googlescholar" class="gs_nph" class=yCB>BL Direct</a> <a href="/scholar?cluster=16290336487138294495&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'33p2o7nmEuIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2006_Syllabic_Level_Automatic_Synchronization_of_Music_Signals_and_Text_Lyrics.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1180777" class=yCC>Syllabic level automatic synchronization of music signals and text lyrics</a></h3><div class="gs_a">D Iskandar, Y Wang, <a href="/citations?user=aNVcd3EAAAAJ&amp;hl=en&amp;oi=sra">MY Kan</a>, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - Proceedings of the 14th annual ACM &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract We present a framework to synchronize pop music to corresponding text lyric. We <br>refine line level alignment achievable by existing work to syllabic level by using a dynamic <br>programming process. Our main contribution is using music knowledge to constrain the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16827243944926346092&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 21</a> <a href="/scholar?q=related:bHstnjZhhukJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16827243944926346092&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 23 versions</a> <a onclick="return gs_ocit(event,'bHstnjZhhukJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="https://mpi-inf.mpg.de/~mmueller/publications/2007_MuellerKuDaFrCl_LyricsBasedAudioRetrieval_ECDL.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/0PK181480J557877.pdf" class=yCE>Lyrics-based audio retrieval and multimodal navigation in music collections</a></h3><div class="gs_a"><a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a>, F Kurth, D Damm, C Fremerey&hellip; - Research and Advanced  &hellip;, 2007 - Springer</div><div class="gs_rs">Modern digital music libraries contain textual, visual, and audio data describing music on <br>various semantic levels. Exploiting the availability of different semantically interrelated <br>representations for a piece of music, this paper presents a query-by-lyrics retrieval system <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3621452896424959536&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 23</a> <a href="/scholar?q=related:MB441jj8QTIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5B/29/RN216317166.html?source=googlescholar" class="gs_nph" class=yC10>BL Direct</a> <a href="/scholar?cluster=3621452896424959536&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'MB441jj8QTIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://staff.aist.go.jp/h.fujihara/pdf/icassp2008_fujihara.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4517548" class=yC11>Three techniques for improving automatic synchronization between music and lyrics: Fricative detection, filler model, and novel feature vectors for vocal activity  &hellip;</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Acoustics, Speech and Signal Processing, &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Three techniques are described that improve a previously developed system for <br>automatically synchronizing lyrics with musical audio signals. Although this system achieves <br>state-of-the-art accuracy by extracting vocal vowels from polyphonic sound mixtures and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6469992732547923333&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 19</a> <a href="/scholar?q=related:hb3gWhwIylkJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6469992732547923333&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'hb3gWhwIylkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.1526&amp;rep=rep1&amp;type=pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/j1h044n4v1767603.pdf" class=yC13>Automatic lyrics alignment for Cantonese popular music</a></h3><div class="gs_a">CH Wong, WM Szeto, KH Wong - Multimedia Systems, 2007 - Springer</div><div class="gs_rs">Abstract From lyrics-display on electronic music players and Karaoke videos to surtitles for <br>live Chinese opera performance, one feature is common to all these everyday functionalities <br>temporal: synchronization of the written text and its corresponding musical phrase. Our <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=782808755015182321&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 16</a> <a href="/scholar?q=related:8ee-SWoY3QoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5E/5A/RN202853430.html?source=googlescholar" class="gs_nph" class=yC15>BL Direct</a> <a href="/scholar?cluster=782808755015182321&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'8ee-SWoY3QoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md8', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md8" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:8ee-SWoY3QoJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=18158445423803726775&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://www.comp.nus.edu/~kanmy/dossier/papers/jcdl2007.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1255192" class=yC16>SlideSeer: A digital library of aligned document and presentation pairs</a></h3><div class="gs_a"><a href="/citations?user=aNVcd3EAAAAJ&amp;hl=en&amp;oi=sra">MY Kan</a> - Proceedings of the 7th ACM/IEEE-CS joint conference  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Research findings are often transmitted both as written documents and narrated <br>slide presentations. As these two forms of media contain both unique and replicated <br>information, it is useful to combine and align these two views to create a single <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1297095921926915075&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 14</a> <a href="/scholar?q=related:A8QPYt41ABIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1297095921926915075&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'A8QPYt41ABIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1255085" class=yC18>A voice-to-MIDI system for singing melodies with lyrics</a></h3><div class="gs_a">N Itou, K Nishimoto - Proceedings of the international conference on  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose a robust Voice-to-MIDI (V to M) system with which a user <br>can input MIDI sequence data by naturally singing melodies with lyrics. A Voice-to-MIDI <br>system translates singing voices into digital musical data, ie, MIDI sequence data. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8739183131844270419&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 16</a> <a href="/scholar?q=related:U9mpmMHQR3kJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8739183131844270419&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'U9mpmMHQR3kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="https://mpi-sb.mpg.de/~mmueller/publications/2005_KurthMuDaFrRiCl_SyncPlayer_ISMIR.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://mpi-sb.mpg.de/~mmueller/publications/2005_KurthMuDaFrRiCl_SyncPlayer_ISMIR.pdf" class=yC19>Syncplayer-an advanced system for multimodal music access</a></h3><div class="gs_a">F Kurth, <a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a>, D Damm, C Fremerey&hellip; - ISMIR, London,  &hellip;, 2005 - mpi-sb.mpg.de</div><div class="gs_rs">ABSTRACT In this paper, we present the SyncPlayer system for multimodal presentation of <br>high quality audio and associated music-related data. Using the SyncPlayer client interface, <br>a user may play back an audio recording that is locally available on his computer. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16457381887275047757&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 13</a> <a href="/scholar?q=related:TRv6q6FdZOQJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16457381887275047757&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'TRv6q6FdZOQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:TRv6q6FdZOQJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://ismir2008.ismir.net/papers/ISMIR2008_126.pdf.." class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ismir.net</span><span class="gs_ggsS">ismir.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA395&amp;ots=oDOSrGfCd1&amp;sig=kPsNwxZsTSsxgP0kEeqzbIr60so" class=yC1B>Segmentation-based lyrics-audio alignment using dynamic programming</a></h3><div class="gs_a">K Lee, M Cremer - &hellip;  of the 9th International Conference on Music  &hellip;, 2008 - books.google.com</div><div class="gs_rs">ABSTRACT In this paper, we present a system for automatic alignment of textual lyrics with <br>musical audio. Given an input audio signal, structural segmentation is ï¬rst performed and <br>similar segments are assigned a label by computing the distance between the segment <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13686225526189618520&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 14</a> <a href="/scholar?q=related:WHVqdJo9770J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13686225526189618520&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'WHVqdJo9770J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="https://www1.comp.nus.edu.sg/~wangye/papers/1.Audio_and_Music_Analysis_and_Retrieval/2005_Singing_Voice_Detection_for_Karaoke_Application.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=876255" class=yC1D>Singing voice detection for karaoke application</a></h3><div class="gs_a">A Shenoy, Y Wu, Y Wang - Visual  &hellip;, 2005 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract We present a framework to detect the regions of singing voice in musical audio <br>signals. This work is oriented towards the development of a robust transcriber of lyrics for <br>karaoke applications. The technique leverages on a combination of low-level audio <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12185904537651465050&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 11</a> <a href="/scholar?q=related:WkuJAZ0HHakJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3A/63/RN177222664.html?source=googlescholar" class="gs_nph" class=yC1F>BL Direct</a> <a href="/scholar?cluster=12185904537651465050&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'WkuJAZ0HHakJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.8423&amp;rep=rep1&amp;type=pdf" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1101293" class=yC20>Multimodal content-based structure analysis of karaoke music</a></h3><div class="gs_a">Y Zhu, K Chen, Q Sun - Proceedings of the 13th annual ACM  &hellip;, 2005 - dl.acm.org</div><div class="gs_rs">Abstract This paper presents a novel approach for content-based analysis of karaoke music, <br>which utilizes multimodal contents including synchronized lyrics text from the video channel <br>and original singing audio as well as accompaniment audio in the two audio channels. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9085165552633813654&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 12</a> <a href="/scholar?q=related:lppaUu_9FH4J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9085165552633813654&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'lppaUu_9FH4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www.mirlab.org/conference_papers/International_Conference/ISMIR%202008/papers/ISMIR2008_111.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mirlab.org</span><span class="gs_ggsS">mirlab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=OHp3sRnZD-oC&amp;oi=fnd&amp;pg=PA281&amp;ots=oDOSrGfCd1&amp;sig=IaRH4uny3di1f5dRCw4wP1Ual-k" class=yC22>Hyperlinking lyrics: a method for creating hyperlinks between phrases in song lyrics</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a>, J Ogata - Proceedings of the International  &hellip;, 2008 - books.google.com</div><div class="gs_rs">ABSTRACT We describe a novel method for creating a hyperlink from a phrase in the lyrics <br>of a song to the same phrase in the lyrics of another song. This method can be applied to <br>various applications, such as song clustering based on the meaning of the lyrics and a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=200319467387757747&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 11</a> <a href="/scholar?q=related:syQ-h36txwIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=200319467387757747&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'syQ-h36txwIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.212.6683&amp;rep=rep1&amp;type=pdf" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.212.6683&amp;rep=rep1&amp;type=pdf" class=yC24>Automatic alignment of music audio and lyrics</a></h3><div class="gs_a">A Mesaros, <a href="/citations?user=hRjwVoMAAAAJ&amp;hl=en&amp;oi=sra">T Virtanen</a> - Proceedings of the 11th Int. Conference on Digital  &hellip;, 2008 - Citeseer</div><div class="gs_rs">ABSTRACT This paper proposes an algorithm for aligning singing in polyphonic music <br>audio with textual lyrics. As preprocessing, the system uses a voice separation algorithm <br>based on melody transcription and sinusoidal modeling. The alignment is based on a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6177492370271242508&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 10</a> <a href="/scholar?q=related:DAm0vJTculUJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6177492370271242508&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'DAm0vJTculUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md16', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md16" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:DAm0vJTculUJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/54X3MW16X062VK68.pdf" class=yC26>A digital library framework for heterogeneous music collections: from document acquisition to cross-modal interaction</a></h3><div class="gs_a">D Damm, C Fremerey, V Thomas, M Clausen&hellip; - International Journal on  &hellip;, 2012 - Springer</div><div class="gs_rs">Abstract In this paper, we present a digital library system for managing heterogeneous music <br>collections. The heterogeneity refers to various document types and formats as well as to <br>different modalities, eg, CD-audio recordings, scanned sheet music, and lyrics. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12788631027349358712&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 10</a> <a href="/scholar?q=related:eIiyjClYerEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12788631027349358712&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'eIiyjClYerEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/fff4f54111fe8e56c12567530068624e/91006d6f98299d09c125767b002b9920/$FILE/2009_EwertMuellerDannenberg_ReliableAlignments_AMR.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/018G51633707W350.pdf" class=yC27>Towards reliable partial music alignments using multiple synchronization strategies</a></h3><div class="gs_a"><a href="/citations?user=kkwnObwAAAAJ&amp;hl=en&amp;oi=sra">S Ewert</a>, <a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a>, R Dannenberg - &hellip;  Media and Adapting to the User, 2011 - Springer</div><div class="gs_rs">The general goal of music synchronization is to align multiple information sources related to <br>a given piece of music. This becomes a hard problem when the various representations to <br>be aligned reveal significant differences not only in tempo, instrumentation, or dynamics <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10378892347966087867&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 9</a> <a href="/scholar?q=related:u-KJypk7CZAJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10378892347966087867&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'u-KJypk7CZAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://staff.aist.go.jp/m.goto/PAPER/IEEEJSTSP201110fujihara.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5876296" class=yC29>Lyricsynchronizer: Automatic synchronization system between musical audio signals and lyrics</a></h3><div class="gs_a">H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a>, J Ogata&hellip; - Selected Topics in Signal &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes a system that can automatically synchronize polyphonic <br>musical audio signals with their corresponding lyrics. Although methods for synchronizing <br>monophonic speech signals and corresponding text transcriptions by using Viterbi <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2316631857609331925&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 7</a> <a href="/scholar?q=related:1VQohGZUJiAJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2316631857609331925&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'1VQohGZUJiAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.2653&amp;rep=rep1&amp;type=pdf" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.2653&amp;rep=rep1&amp;type=pdf" class=yC2B>Relationships between lyrics and melody in popular music</a></h3><div class="gs_a"><a href="/citations?user=Q37xBqwAAAAJ&amp;hl=en&amp;oi=sra">E Nichols</a>, <a href="/citations?user=j03zZgcAAAAJ&amp;hl=en&amp;oi=sra">D Morris</a>, S Basu, C Raphael - Proceedings of the 10&#39;th  &hellip;, 2009 - Citeseer</div><div class="gs_rs">ABSTRACT Composers of popular music weave lyrics, melody, and instrumentation <br>together to create a consistent and compelling emotional scene. The relationships among <br>these elements are critical to musical communication, and understanding the statistics <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15915262251703357463&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 5</a> <a href="/scholar?q=related:F1D2-7Fe3twJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15915262251703357463&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 22 versions</a> <a onclick="return gs_ocit(event,'F1D2-7Fe3twJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md20', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md20" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:F1D2-7Fe3twJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/Q8205NH13L1RH912.pdf" class=yC2D>Effectiveness of signal segmentation for music content representation</a></h3><div class="gs_a">N Maddage, M Kankanhalli, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - Advances in Multimedia Modeling, 2008 - Springer</div><div class="gs_rs">In this paper we compare the effectiveness of rhythm based signal segmentation technique <br>with the traditional fixed length segmentation for music contents representation. We consider <br>vocal regions, instrumental regions and chords which represent the harmony as different <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10581131093821192891&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 3</a> <a href="/scholar?q=related:u3oNKau615IJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/59/2C/RN221721717.html?source=googlescholar" class="gs_nph" class=yC2E>BL Direct</a> <a href="/scholar?cluster=10581131093821192891&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'u3oNKau615IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://www.matthiasmauch.net/_pdf/mauch_laa_2010.pdf" class=yC30><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from matthiasmauch.net</span><span class="gs_ggsS">matthiasmauch.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.matthiasmauch.net/_pdf/mauch_laa_2010.pdf" class=yC2F>Lyrics-to-audio alignment and phrase-level segmentation using incomplete internet-style chord annotations</a></h3><div class="gs_a"><a href="/citations?user=_gfIN8AAAAAJ&amp;hl=en&amp;oi=sra">M Mauch</a>, H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - &hellip;  of the 7th Sound and Music  &hellip;, 2010 - matthiasmauch.net</div><div class="gs_rs">ABSTRACT We propose two novel lyrics-to-audio alignment methods which make use of <br>additional chord information. In the first method we extend an existing hidden Markov model <br>(HMM) for lyrics alignment [1] by adding a chord model based on the chroma features <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11978336140451915224&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 3</a> <a href="/scholar?q=related:2CXce0GZO6YJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11978336140451915224&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'2CXce0GZO6YJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md22', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md22" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:2CXce0GZO6YJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/b9ef55817af82095c125675300686247/2c78ab21924a46ddc125753f0065baa3/$FILE/2008_EwertMueller_RefinementStrategies_CMMR.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/d014803786723k32.pdf" class=yC31>Refinement strategies for music synchronization</a></h3><div class="gs_a"><a href="/citations?user=kkwnObwAAAAJ&amp;hl=en&amp;oi=sra">S Ewert</a>, <a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼ller</a> - Computer Music Modeling and Retrieval. Genesis of  &hellip;, 2009 - Springer</div><div class="gs_rs">For a single musical work, there often exists a large number of relevant digital documents <br>including various audio recordings, MIDI files, or digitized sheet music. The general goal of <br>music synchronization is to automatically align the multiple information sources related to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14411184700656666008&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 4</a> <a href="/scholar?q=related:mEUvqyHQ_scJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14411184700656666008&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'mEUvqyHQ_scJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://144.206.159.178/FT/CONF/16408942/16408945.pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 144.206.159.178</span><span class="gs_ggsS">144.206.159.178 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/data/Conferences/SPIEP/17645/682004_1.pdf" class=yC33>Enriching text with images and colored light</a></h3><div class="gs_a">D Sekulovski, G Geleijnse&hellip; - &hellip;  of the is&amp;t/spie  &hellip;, 2008 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">ABSTRACT We present an unsupervised method to enrich textual applications with relevant <br>images and colors. The images are collected by querying large image repositories and <br>subsequently the colors are computed using image processing. A prototype system based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7761141948238539376&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 4</a> <a href="/scholar?q=related:cCokTHsdtWsJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7761141948238539376&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'cCokTHsdtWsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/93832a04987390a3c12567530068622d/b991405b573fcbb5c125775400566182/$FILE/2010_MuellerClausenKonzEwertFremerey_MusicSynchronization_ISR.pdf" class=yC36><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.ingentaconnect.com/content/maney/isr/2010/00000035/00000002/art00004" class=yC35>A multimodal way of experiencing and exploring music</a></h3><div class="gs_a"><a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M MÃ¼LLER</a>, V KOnZ, M CLAusEn&hellip; - Interdisciplinary  &hellip;, 2010 - ingentaconnect.com</div><div class="gs_rs">Abstract: Significant digitization efforts have resulted in large multimodal music collections, <br>which comprise music-related documents of various types and formats including text, <br>symbolic data, audio, image, and video. The challenge is to organize, understand, and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7498888887562735115&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 3</a> <a href="/scholar?q=related:Cw5NYbZnEWgJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7498888887562735115&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'Cw5NYbZnEWgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/611548724U535241.pdf" class=yC37>Online music search by tapping</a></h3><div class="gs_a">G Peters, D Cukierman, C Anthony&hellip; - Ambient Intelligence in  &hellip;, 2006 - Springer</div><div class="gs_rs">Query by Tapping is an emerging paradigm for content-based music retrieval, which we <br>have explored through our web-based music search system. Based on the results obtained <br>from our system we argue that searching for music by tapping the rhythm of a song&#39;s <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9197516584673736101&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 2</a> <a href="/scholar?q=related:pfU9bZokpH8J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/45/23/RN194755205.html?source=googlescholar" class="gs_nph" class=yC38>BL Direct</a> <a href="/scholar?cluster=9197516584673736101&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'pfU9bZokpH8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://staff.aist.go.jp/m.goto/PAPER/IEEETASLP201201mauch.pdf" class=yC3A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5876304" class=yC39>Integrating Additional Chord Information into HMM-Based Lyrics-to-Audio Alignment</a></h3><div class="gs_a"><a href="/citations?user=_gfIN8AAAAAJ&amp;hl=en&amp;oi=sra">M Mauch</a>, H Fujihara, <a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a> - Audio, Speech, and Language  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Aligning lyrics to audio has a wide range of applications such as the automatic <br>generation of karaoke scores, song-browsing by lyrics, and the generation of audio <br>thumbnails. Existing methods are restricted to using only lyrics and match them to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3269594826642557923&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 3</a> <a href="/scholar?q=related:4wt8-SvvXy0J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3269594826642557923&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'4wt8-SvvXy0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8005666&amp;id=RDDsAQAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC3B>Automatic system for temporal alignment of music audio signal with lyrics</a></h3><div class="gs_a"><a href="/citations?user=4JJCMq8AAAAJ&amp;hl=en&amp;oi=sra">M Goto</a>, H Fujihara, <a href="/citations?user=8cX3pewAAAAJ&amp;hl=en&amp;oi=sra">H Okuno</a> - US Patent 8,005,666, 2011 - Google Patents</div><div class="gs_rs">An automatic system for temporal alignment between a music audio signal and lyrics is <br>provided. The automatic system can prevent accuracy for temporal alignment from being <br>lowered due to the influence of non-vocal sections. Alignment means of the system is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16439048177327345927&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 1</a> <a href="/scholar?q=related:BxEhLzg7I-QJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16439048177327345927&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'BxEhLzg7I-QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://www.dse.nl/~gijsg/AmbiSys-GeleijnseEtAl.pdf" class=yC3D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dse.nl</span><span class="gs_ggsS">dse.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1363164" class=yC3C>Enriching music with synchronized lyrics, images and colored lights</a></h3><div class="gs_a">G Geleijnse, D Sekulovski, <a href="/citations?user=OweuZLoAAAAJ&amp;hl=en&amp;oi=sra">J Korst</a>, S Pauws&hellip; - Proceedings of the 1st  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract We present a method to synchronize popular music with its lyrics at the stanza level. <br>First we apply an algorithm to segment audio content into harmonically similar and/or <br>contrasting progressions, ie the stanzas. We map the stanzas found to a sequence of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1472177553128121368&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 1</a> <a href="/scholar?q=related:GGSBRK85bhQJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1472177553128121368&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'GGSBRK85bhQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB30" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW30"><a href="http://marcs.uws.edu.au/links/ICoMusic/ArchiveCD/Full_Paper_PDF/Gruhne_Schmidt_Dittmar.pdf" class=yC3F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uws.edu.au</span><span class="gs_ggsS">uws.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://marcs.uws.edu.au/links/ICoMusic/ArchiveCD/Full_Paper_PDF/Gruhne_Schmidt_Dittmar.pdf" class=yC3E>Detecting Phonemes within the Singing of Polyphonic Music</a></h3><div class="gs_a">M Gruhne, K Schmidt, C Dittmar - Proceedings of ICoMCS  &hellip;, 2007 - marcs.uws.edu.au</div><div class="gs_rs">ABSTRACT Automated detection of phonemes in polyphonic music is an important <br>prerequisite for synchronizing music and corresponding lyrics. This paper describes a novel <br>approach of automatic phoneme estimation within digitized music pieces. Since there are <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14938857226867589862&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 1</a> <a href="/scholar?q=related:5kK4lX97Uc8J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'5kK4lX97Uc8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md30', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md30" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:5kK4lX97Uc8J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://www.cse.bgu.ac.il/hadar/Pdf/27%20-%20A%20musical%20source%20separation.pdf" class=yC41><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bgu.ac.il</span><span class="gs_ggsS">bgu.ac.il <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cse.bgu.ac.il/hadar/Pdf/27%20-%20A%20musical%20source%20separation.pdf" class=yC40>A musical source separation system with lyrics alignment.</a></h3><div class="gs_a">O Hadar, D Bykhovsky, G Goldwasser&hellip; - WSEAS Transactions on  &hellip;, 2006 - cse.bgu.ac.il</div><div class="gs_rs">Abstract: This paper examines the use of the generalized likelihood ratio test (GLRT) for the <br>purposes of audio extraction and manipulation. The GLRT, which was designed originally <br>for distinguishing between harmonic and non-harmonic audio frames, is extended for two <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9927149255201753183&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 1</a> <a href="/scholar?q=related:X4iVvKZRxIkJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9927149255201753183&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'X4iVvKZRxIkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md31', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md31" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:X4iVvKZRxIkJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1823753" class=yC42>Word level automatic alignment of music and lyrics using vocal synthesis</a></h3><div class="gs_a">NC Maddage, <a href="/citations?user=jnU62sUAAAAJ&amp;hl=en&amp;oi=sra">KC Sim</a>, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a> - ACM Transactions on Multimedia  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract We propose a signal-based approach instead of the commonly used model-based <br>approach, to automatically align vocal music with text lyrics at the word level. In this <br>approach, we use a text-to-speech system to synthesize the singing voice according to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16739146216492474140&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 1</a> <a href="/scholar?q=related:HJ9bzctkTegJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16739146216492474140&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'HJ9bzctkTegJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.1122&amp;rep=rep1&amp;type=pdf" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/pn3430677t4406l4.pdf" class=yC43>A cross-modal approach for karaoke artifacts correction</a></h3><div class="gs_a">WQ Yan, MS Kankanhalli - Multimedia Tools and Applications, 2008 - Springer</div><div class="gs_rs">Abstract Karaoke singing is a popular form of entertainment in several parts of the world. <br>Since this genre of performance attracts amateurs, the singing often has artifacts related to <br>scale, tempo, and synchrony. We have developed an approach to correct these artifacts <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12356163693489506090&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 1</a> <a href="/scholar?q=related:KkcF2WnpeasJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/19/4E/RN232981854.html?source=googlescholar" class="gs_nph" class=yC45>BL Direct</a> <a href="/scholar?cluster=12356163693489506090&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'KkcF2WnpeasJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://www-mmdb.iai.uni-bonn.de/download/Diplomarbeiten/Diplomarbeit_Damm.pdf" class=yC47><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-bonn.de</span><span class="gs_ggsS">uni-bonn.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-mmdb.iai.uni-bonn.de/download/Diplomarbeiten/Diplomarbeit_Damm.pdf" class=yC46>Textbasierte Musiksuche im Rahmen des SyncPlayer-Frameworks</a></h3><div class="gs_a">D Damm - 2007 - www-mmdb.iai.uni-bonn.de</div><div class="gs_rs">Wer kennt nicht die Situation: Man hÃ¶rt ein MusikstÃ¼ck, das einem gefÃ¤llt, weiÃ aber nicht, <br>wie es heiÃt oder von wem es ist. Man kann sich nur an die Melodie und einige einschlÃ¤gige <br>Textphrasen des Gesangs erinnern. Zwar existieren mit der zunehmenden Verbreitung <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18318982605424066714&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 1</a> <a href="/scholar?q=related:moDcGp8ZOv4J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'moDcGp8ZOv4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md34', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md34" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:moDcGp8ZOv4J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://staff.aist.go.jp/m.goto/PAPER/SIGMUS200608fujihara.pdf" class=yC49><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aist.go.jp</span><span class="gs_ggsS">aist.go.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://staff.aist.go.jp/m.goto/PAPER/SIGMUS200608fujihara.pdf" class=yC48>é³æ¥½é³é¿ä¿¡å·ã¨æ­è©ã®æéçå¯¾å¿ä»ãææ³: æ­å£°ã®åé¢ã¨æ¯é³ã® Viterbi ã¢ã©ã¤ã³ã¡ã³ã</a></h3><div class="gs_a">è¤åå¼å°ï¼ å¾è¤çå­ï¼ ç·æ¹æ·³ï¼ é§è°·åç¯&hellip; - æå ±å¦çå­¦ä¼ç ç©¶å ±å. &hellip;, 2006 - staff.aist.go.jp</div><div class="gs_rs">æ¬ç¨¿ã§ã¯, ä¼´å¥é³ãå«ãé³æ¥½é³é¿ä¿¡å·ã¨å¯¾å¿ããæ­è©ã®æéçãªå¯¾å¿ä»ãææ³ã«ã¤ãã¦è¿°ã¹ã. <br>ã¯ãªã¼ã³ãªé³å£°ä¿¡å·ã¨ãã®çºè©±åå®¹ã®æéçå¯¾å¿ä»ããæ¨å®ããã Viterbi ã¢ã©ã¤ã³ã¡ã³ãææ³ã¯<br>ããã¾ã§ãå­å¨ããã, æ­å£°ã¨åæã«æ¼å¥ãããä¼´å¥é³ã®æªå½±é¿ã§å¸è²© CD ä¸­ã®æ­å£°ã«ã¯é©ç¨<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11217294609239502640&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 1</a> <a href="/scholar?q=related:MO-f7APWq5sJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11217294609239502640&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MO-f7APWq5sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf" class=yC4B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-bonn.de</span><span class="gs_ggsS">uni-bonn.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-kd.iai.uni-bonn.de/pubattachments/336/da.pdf" class=yC4A>Melody Separation from Polyphonic Audio Recordings</a></h3><div class="gs_a">S Vembu - 2005 - www-kd.iai.uni-bonn.de</div><div class="gs_rs">Abstract In the field of music information retrieval, query-by-humming is an interesting <br>application area in which humming sequences or melodies are matched with a database of <br>songs to come up with a list of indexed songs that are similar in melodic content to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16884184643397726041&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Wa_TF3qsUOoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md36', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md36" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Wa_TF3qsUOoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB37" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW37"><a href="http://cdn.intechweb.org/pdfs/9261.pdf" class=yC4D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from intechweb.org</span><span class="gs_ggsS">intechweb.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cdn.intechweb.org/pdfs/9261.pdf" class=yC4C>Music Structure Analysis Statistics for Popular Songs</a></h3><div class="gs_a">NC Maddage, L Haizhou, MS Kankanhalli - cdn.intechweb.org</div><div class="gs_rs">Abstract In this chapter, we have proposed a better procedure for manual annotation of <br>music information. The proposed annotation procedure involves carrying out listening tests <br>and then incorporating music knowledge to iteratively refine the detected music <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7LRoGihTg1UJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6161860146879837420&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'7LRoGihTg1UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md37', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md37" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:7LRoGihTg1UJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Phonetic Segmentation of Singing Voice Using MIDI and Parallel Speech</h3><div class="gs_a">M Dong, P Chan, L Cen, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, J Teo, PJ Kua - Eleventh Annual Conference of the  &hellip;, 2010</div><div class="gs_fl"><a href="/scholar?q=related:JTF8G2KpMasJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12335827093177512229&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'JTF8G2KpMasJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/lh7413485m406rv4.pdf" class=yC4E>Cross-Modal Approach for Karaoke Artifacts Correction</a></h3><div class="gs_a">WQ Yan, MS Kankanhalli - &hellip;  of Multimedia for Digital Entertainment and  &hellip;, 2009 - Springer</div><div class="gs_rs">In this chapter, we combine adaptive sampling in conjunction with video analogies (VA) to <br>correct the audio stream in the karaoke environment k= k (t): k (t)=(U (t), K (t)), t Ã (ts, te) Îº=\ <br>left {Îº (t):\ kappa (t)=\ left (U (t),\ K (t)\ right),\ t â\ left (t _ s,{t _ e\ right)\ right\} where ts and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:oYqzO5N08cYJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14335367264607636129&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'oYqzO5N08cYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB40" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW40"><a href="http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf" class=yC50><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf" class=yC4F>A repetition-based framework for lyric alignment in popular songs</a></h3><div class="gs_a">LM Thang, KANM Yen - National University of Singapore  &hellip;, 2007 - w.comp.nus.edu</div><div class="gs_rs">ABSTRACT We examine the problem of automatically aligning acoustic musical audio and <br>textual lyric in popular songs. Existing works have tackled the problem using <br>computationally-expensive audio processing techniques, resulting in solutions unsuitable <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17581284984694044119&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 27 versions</a> <a onclick="return gs_ocit(event,'1_WYP4tF_fMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md40', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md40" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/FT671731J0403662.pdf" class=yC51>A Survey of Music Structure Analysis Techniques for Music Applications</a></h3><div class="gs_a">N Maddage, <a href="/citations?user=z8_x7C8AAAAJ&amp;hl=en&amp;oi=sra">H Li</a>, M Kankanhalli - Recent Advances in Multimedia Signal  &hellip;, 2009 - Springer</div><div class="gs_rs">Music carries multilayer information which forms different structures. The information <br>embedded in the music can be categorized into time information, harmony/melody, music <br>regions, music similarities, song structures and music semantics. In this chapter, we first <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3vo8SvZRZuUJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16529989600559299294&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'3vo8SvZRZuUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> An Advanced System for Content-based Access to Music Information</h3><div class="gs_a">F Kurth, <a href="/citations?user=uggxDWIAAAAJ&amp;hl=en&amp;oi=sra">M Muller</a></div><div class="gs_fl"><a href="/scholar?q=related:vwUgTwhhUfgJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'vwUgTwhhUfgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Monaural musical sound separation</h3><div class="gs_a"><a href="/citations?user=OCQSEIsAAAAJ&amp;hl=en&amp;oi=sra">Y Li</a> - 2008 - The Ohio State University</div><div class="gs_fl"><a href="/scholar?q=related:jpa0VTJdi4EJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9334657123423131278&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'jpa0VTJdi4EJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md43', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md43" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:jpa0VTJdi4EJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=14713022477718088856&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=1144&amp;context=etd_projects" class=yC53><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sjsu.edu</span><span class="gs_ggsS">sjsu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarworks.sjsu.edu/etd_projects/145/" class=yC52>Common Emotion Modeling in Distinct Medium Analysis and Matching</a></h3><div class="gs_a">A Cho - 2009 - scholarworks.sjsu.edu</div><div class="gs_rs">Abstract With the ever growing amount of digital information and multimedia on the World <br>Wide Web and the current trend towards personalizing technology, users find themselves <br>wanting a more intuitive way of finding related information, and not just any information but <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:WBKxWryTez8J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4574412283709559384&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'WBKxWryTez8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/x31g53215j053046.pdf" class=yC54>Instant customized summaries streaming: a service for immediate awareness of new video content</a></h3><div class="gs_a">Ã GarcÃ­a-MartÃ­n, J Molina, <a href="/citations?user=amTTgLQAAAAJ&amp;hl=en&amp;oi=sra">F LÃ³pez</a>, V ValdÃ©s&hellip; - &hellip;  Media and Adapting to  &hellip;, 2011 - Springer</div><div class="gs_rs">This paper presents the Instant Customized Summaries Streaming service, a multimedia <br>service able to deliver customized video summaries with a minimum delay after the original <br>content has been uploaded to a video repository. Uploaded videos start being analyzed, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:OauSajPSutsJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15833198558247562041&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'OauSajPSutsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Singing Voice Detection in Western Popular Music</h3><div class="gs_a">W Yuansheng</div><div class="gs_fl"><a href="/scholar?q=related:dx-cdoqIEO4J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'dx-cdoqIEO4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB47" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW47"><a href="http://www.audiolabs-erlangen.de/meinard/students/thesis/2012_EwertSebastian_SignalProcessingMethods_Phd-Thesis.pdf" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from audiolabs-erlangen.de</span><span class="gs_ggsS">audiolabs-erlangen.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.audiolabs-erlangen.de/meinard/students/thesis/2012_EwertSebastian_SignalProcessingMethods_Phd-Thesis.pdf" class=yC55>Signal Processing Methods for Music Synchronization, Audio Matching, and Source Separation</a></h3><div class="gs_a"><a href="/citations?user=kkwnObwAAAAJ&amp;hl=en&amp;oi=sra">S Ewert</a> - 2012 - audiolabs-erlangen.de</div><div class="gs_rs">Abstract The field of music information retrieval (MIR) aims at developing techniques and <br>tools for organizing, understanding, and searching multimodal information in large music <br>collections in a robust, efficient and intelligent manner. In this context, this thesis presents <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'Li_TocU8mCEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md47', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md47" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Li_TocU8mCEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/730_Paper.pdf" class=yC58><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from lrec-conf.org</span><span class="gs_ggsS">lrec-conf.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/730_Paper.pdf" class=yC57>A Parallel Corpus of Music and Lyrics Annotated with Emotions</a></h3><div class="gs_a"><a href="/citations?user=VTbb1LEAAAAJ&amp;hl=en&amp;oi=sra">C Strapparava</a>, <a href="/citations?user=UetM7FgAAAAJ&amp;hl=en&amp;oi=sra">R Mihalcea</a>, A Battocchi - lrec-conf.org</div><div class="gs_rs">Abstract In this paper, we introduce a novel parallel corpus of music and lyrics, annotated <br>with emotions at line level. We first describe the corpus, consisting of 100 popular songs, <br>each of them including a music component, provided in the MIDI format, as well as a lyrics <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:t6syon1vUekJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'t6syon1vUekJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md48', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md48" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:t6syon1vUekJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.5609&amp;rep=rep1&amp;type=pdf" class=yC5A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.5609&amp;rep=rep1&amp;type=pdf" class=yC59>Efficient Lyrics Retrieval and Alignment</a></h3><div class="gs_a">JKG Geleijnse - Citeseer</div><div class="gs_rs">Abstract We present an algorithm to efficiently retrieve from the Web multiple versions of the <br>lyrics of a given song. First, multiple web pages are collected that potentially contain the <br>lyrics of the given song, by querying Google with the song title and artist name. Next, from <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:bj2vrbZp62UJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7344079850676632942&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'bj2vrbZp62UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md49', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md49" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:bj2vrbZp62UJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB50" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW50"><a href="http://ismir2007.ismir.net/posters/ISMIR2007_p369_gruhne_poster.pdf" class=yC5C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ismir.net</span><span class="gs_ggsS">ismir.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ismir2007.ismir.net/posters/ISMIR2007_p369_gruhne_poster.pdf" class=yC5B>Phoneme Detection in Popular Music</a></h3><div class="gs_a">M Gruhne, K Schmidt, C Dittmar - ismir2007.ismir.net</div><div class="gs_rs">Phoneme detection in polyphonic music is an important prerequisite for lyrics <br>synchronization for karaoke applications or browsing in music catalogues. Phoneme <br>detection might be furthermore used for the singing recognition in the polyphonic music.</div><div class="gs_fl"><a href="/scholar?q=related:c-q8reHPoYIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'c-q8reHPoYIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md50', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md50" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:c-q8reHPoYIJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://140.92.88.20/ir/bitstream/105076416/2285/2/IA97FA3132B0232B02.pdf" class=yC5E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 140.92.88.20</span><span class="gs_ggsS">140.92.88.20 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://140.92.88.20/ir/handle/105076416/2285" class=yC5D>æ­è²åï§ªä»¥åæ°£é³éå»º</a></h3><div class="gs_a">è¨±èå - 2008 - 140.92.88.20</div><div class="gs_rs">Separating singing voice from music accompaniment is an appealing but challenging <br>problem, especially in the monaural case. One existing approach is based on computational <br>audio scene analysis which uses pitch as the cue to resynthesize the singing voice. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:K-OcsGb44J8J:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11520480966747153195&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'K-OcsGb44J8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB52" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW52"><a href="http://www.pharos-audiovisual-search.eu/res/files/publications/papers/Phoneme%20Detection.pdf" class=yC60><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from pharos-audiovisual-search.eu</span><span class="gs_ggsS">pharos-audiovisual-search.eu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.pharos-audiovisual-search.eu/res/files/publications/papers/Phoneme%20Detection.pdf" class=yC5F>Phoneme Detection for Lyrics Synchronization</a></h3><div class="gs_a">M Gruhne, C Dittmar - 2008 - pharos-audiovisual-search.eu</div><div class="gs_rs">Automated detection of phonemes in polyphonic music is an important prerequisite for <br>synchronizing music and corresponding lyrics. This paper describes a novel approach of <br>automatic phoneme estimation within digitized music pieces. Since there are already a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:dyvltK_NKdUJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15360034158661675895&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'dyvltK_NKdUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md52', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md52" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:dyvltK_NKdUJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/13382/thesis_21_amendment_2.pdf?sequence=1" class=yC62><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/13382" class=yC61>REFINING MUSIC SIGNAL TO LYRIC TEXT SYNCHRONIZATION FROM LINE-LEVEL TO SYLLABLE-LEVEL BY CONSTRAINING DYNAMIC TIME WARPING  &hellip;</a></h3><div class="gs_a">D ISKANDAR - 2007 - scholarbank.nus.sg</div><div class="gs_rs">The problem we consider in this thesis is synchronization between lyric text and the <br>corresponding singing voice recording. We limit the singing to the pop genre to make the <br>problem manageable. The recordings we consider in this problem are those we can <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:q87ftenzmuwJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17049207524468706987&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'q87ftenzmuwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB54" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW54"><a href="http://acl.eldoc.ub.rug.nl/mirror/D/D12/D12-1054.pdf" class=yC64><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from rug.nl</span><span class="gs_ggsS">rug.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://acl.eldoc.ub.rug.nl/mirror/D/D12/D12-1054.pdf" class=yC63>Lyrics, Music, and Emotions</a></h3><div class="gs_a"><a href="/citations?user=UetM7FgAAAAJ&amp;hl=en&amp;oi=sra">R Mihalcea</a>, <a href="/citations?user=VTbb1LEAAAAJ&amp;hl=en&amp;oi=sra">C Strapparava</a> - acl.eldoc.ub.rug.nl</div><div class="gs_rs">Abstract In this paper, we explore the classification of emotions in songs, using the music <br>and the lyrics representation of the songs. We introduce a novel corpus of music and lyrics, <br>consisting of 100 songs annotated for emotions. We show that textual and musical <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:umhiuyjuoLEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12799491999696840890&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'umhiuyjuoLEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md54', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md54" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:umhiuyjuoLEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB55" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW55"><a href="http://www.cse.ohio-state.edu/~dwang/papers/HWJH.taslp12.pdf" class=yC66><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ohio-state.edu</span><span class="gs_ggsS">ohio-state.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6121941" class=yC65>A Tandem Algorithm for Singing Pitch Extraction and Voice Separation From Music Accompaniment</a></h3><div class="gs_a">CL Hsu, <a href="/citations?user=yO59sggAAAAJ&amp;hl=en&amp;oi=sra">DL Wang</a>, <a href="/citations?user=xPAxmk0AAAAJ&amp;hl=en&amp;oi=sra">JSR Jang</a>&hellip; - Audio, Speech, and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Singing pitch estimation and singing voice separation are challenging due to the <br>presence of music accompaniments that are often nonstationary and harmonic. Inspired by <br>computational auditory scene analysis (CASA), this paper investigates a tandem algorithm <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=346665256327419678&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 3</a> <a href="/scholar?q=related:HpddwzaazwQJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=346665256327419678&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'HpddwzaazwQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB56" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW56"><a href="http://computationalcreativity.net/iccc2012/wp-content/uploads/2012/05/087-Monteith.pdf" class=yC68><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from computationalcreativity.net</span><span class="gs_ggsS">computationalcreativity.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://computationalcreativity.net/iccc2012/wp-content/uploads/2012/05/087-Monteith.pdf" class=yC67>Automatic Generation of Melodic Accompaniments for Lyrics</a></h3><div class="gs_a">K Monteith, T Martinez&hellip; - International  &hellip;, 2012 - computationalcreativity.net</div><div class="gs_rs">Abstract Music and speech are two realms predominately species-specific to humans, and <br>many human creative endeavors involve these two modalities. The pairing of music and <br>spoken text can heighten the emotional and cognitive impact of both-the complete song <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:KN5B1YxAHFYJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'KN5B1YxAHFYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md56', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md56" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:KN5B1YxAHFYJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB57" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW57"><a href="http://ismir2012.ismir.net/event/papers/361-ismir-2012.pdf" class=yC6A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ismir.net</span><span class="gs_ggsS">ismir.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://ismir2012.ismir.net/event/papers/361-ismir-2012.pdf" class=yC69>RANKING LYRICS FOR ONLINE SEARCH</a></h3><div class="gs_a">R Macrae, <a href="/citations?user=9GvyqXEAAAAJ&amp;hl=en&amp;oi=sra">S Dixon</a> - ismir2012.ismir.net</div><div class="gs_rs">ABSTRACT When someone wishes to find the lyrics for a song they typically go online and <br>use a search engine. There are a large number of lyrics available on the internet as the effort <br>required to transcribe and post lyrics is minimal. These lyrics are promptly returned to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:882m1oY-pkoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'882m1oY-pkoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md57', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md57" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:882m1oY-pkoJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB58" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW58"><a href="https://ohm.fing.edu.uy/publicaciones/2008/SPD08/SPD08.pdf" class=yC6C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fing.edu.uy</span><span class="gs_ggsS">fing.edu.uy <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://ohm.fing.edu.uy/publicaciones/2008/SPD08/SPD08.pdf" class=yC6B>SeparaciÃ³n de Voz Cantada (Singing Voice Separation)</a></h3><div class="gs_a">A Samas, A Palermo, A Decarlini, M Rocamora&hellip; - ohm.fing.edu.uy</div><div class="gs_rs">El problema principal que abordÃ³ este proyecto de fin de carrera es la extracciÃ³n de la voz <br>cantada en una grabaciÃ³n musical. El objetivo es construir un sistema que reciba como <br>entrada un archivo de mÃºsica, y que devuelva como salida la voz cantada. Con el fin de <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:dDo3aajjYasJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12349401965685848692&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'dDo3aajjYasJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md58', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md58" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:dDo3aajjYasJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB59" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW59"><a href="https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/4e77efd5c6e2ceadc12567530068624d/d3d83c1d4ae52000c125753f005f5543/$FILE/2008_DammFrKuMuCl_SyncPlayer_LWA.pdf" class=yC6E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mpg.de</span><span class="gs_ggsS">mpg.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/4e77efd5c6e2ceadc12567530068624d/d3d83c1d4ae52000c125753f005f5543/$FILE/2008_DammFrKuMuCl_SyncPlayer_LWA.pdf" class=yC6D>SyncPlayer-Multimodale Wiedergabe, Navigation und Suche in heterogenen digitalen Musikkollektionen</a></h3><div class="gs_a">D Damm, C Fremerey, F Kurth&hellip; - 6.-8. October 2008,  &hellip;, 2008 - domino.mpi-inf.mpg.de</div><div class="gs_rs">Abstract Durch systematische Digitalisierung sind in den letzten Jahren umfangreiche <br>BestÃ¤nde von Musikdokumenten entstanden, die digitale Inhalte unterschiedlichster <br>AusprÃ¤gungen und Formate enthalten. Man denke hier beispielsweise an CD-Aufnahmen <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7061302106467012340&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=61">Cited by 1</a> <a href="/scholar?q=related:9A6ve9rI_mEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7061302106467012340&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'9A6ve9rI_mEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md59', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md59" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:9A6ve9rI_mEJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB60" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW60"><a href="http://aless.mine.nu/paper_proyecto.pdf" class=yC70><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mine.nu</span><span class="gs_ggsS">mine.nu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://aless.mine.nu/paper_proyecto.pdf" class=yC6F>Separacion de Voz Cantada (Singing Voice Separation)</a></h3><div class="gs_a">A Decarlini, A Palermo, A Samas - aless.mine.nu</div><div class="gs_rs">Abstract El problema principal que abordÃ³ este proyecto de fin de carrera es la extracciÃ³n <br>de la voz cantada en una grabaciÃ³n musical. El objetivo es construir un sistema que reciba <br>como entrada un archivo de mÃºsica, y que devuelva como salida la voz cantada. Con el <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:o4RX933RTWkJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7587951286139978915&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'o4RX933RTWkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md60', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md60" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:o4RX933RTWkJ:scholar.google.com/&amp;hl=en&amp;num=61&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
