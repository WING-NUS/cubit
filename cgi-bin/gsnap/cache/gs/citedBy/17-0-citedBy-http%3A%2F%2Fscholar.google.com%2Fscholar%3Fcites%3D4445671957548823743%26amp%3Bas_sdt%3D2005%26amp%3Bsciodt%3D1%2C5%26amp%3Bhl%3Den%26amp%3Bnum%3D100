Total results = 17
<div class="gs_r" style="z-index:400"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4459285" class=yC0>Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval</a></h3><div class="gs_a"><a href="/citations?user=RMSuNFwAAAAJ&amp;hl=en&amp;oi=sra">Y Yang</a>, YT Zhuang, F Wu&hellip; - &hellip; , IEEE Transactions on, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we consider the problem of multimedia document (MMD) semantics <br>understanding and content-based cross-media retrieval. An MMD is a set of media objects of <br>different modalities but carrying the same semantics and the content-based cross-media <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8802451763731361544&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 77</a> <a href="/scholar?q=related:CO-34T2XKHoJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0B/2B/RN226783670.html?source=googlescholar" class="gs_nph" class=yC1>BL Direct</a> <a href="/scholar?cluster=8802451763731361544&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'CO-34T2XKHoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.cis.strath.ac.uk/cis/research/publications/papers/strath_cis_publication_2230.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from strath.ac.uk</span><span class="gs_ggsS">strath.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/aris.2008.1440420109/full" class=yC2>Interactive information retrieval</a></h3><div class="gs_a">I Ruthven - Annual review of information science and  &hellip;, 2009 - Wiley Online Library</div><div class="gs_rs">Information retrieval is a fundamental component of human information behavior. The ability <br>to extract useful information from large electronic resources not only is one of the main <br>activities of individuals online but is an essential skill for most professional groups and a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1037840860338712824&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 51</a> <a href="/scholar?q=related:-PDfhcMmZw4J:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/40/1E/RN220687877.html?source=googlescholar" class="gs_nph" class=yC4>BL Direct</a> <a href="/scholar?cluster=1037840860338712824&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'-PDfhcMmZw4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:-PDfhcMmZw4J:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=8162642096806047487&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1386375" class=yC5>Clip based video summarization and ranking</a></h3><div class="gs_a">Y Gao, QH Dai - Proceedings of the 2008 international conference on  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we present a new algorithm for video clip summarization and ranking, <br>which is mainly based on a clip based video similarity measure and the affinity propagation <br>clustering (AP) algorithm. We propose a proportional max-weighted bipartite matching <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8036333654932049436&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 15</a> <a href="/scholar?q=related:HM69fuHKhm8J:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8036333654932049436&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'HM69fuHKhm8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5571819" class=yC6>Affective audio-visual words and latent topic driving model for realizing movie affective scene classification</a></h3><div class="gs_a">G Irie, T Satou, A Kojima, T Yamasaki&hellip; - &hellip; , IEEE Transactions on, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents a novel method for movie affective scene classification that <br>outputs the emotion (in the form of labels) that the scene is likely to arouse in viewers. Since <br>the affective preferences of users play an important role in movie selection, affective scene <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8143955890443589313&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 9</a> <a href="/scholar?q=related:wRZX3LskBXEJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8143955890443589313&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'wRZX3LskBXEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.262&amp;rep=rep1&amp;type=pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0306457307001665" class=yC7>Contextual factors affecting the utility of surrogates within exploratory search</a></h3><div class="gs_a">I Ruthven, M Baillie, <a href="/citations?user=8V5fIbkAAAAJ&amp;hl=en&amp;oi=sra">L Azzopardi</a>, R Bierig&hellip; - Information Processing  &hellip;, 2008 - Elsevier</div><div class="gs_rs">In this paper we investigate how information surrogates might be useful in exploratory <br>search and what information it is useful for a surrogate to contain. By comparing <br>assessments based on artificially created information surrogates, we investigate the effect <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3659697033000831728&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 7</a> <a href="/scholar?q=related:8MqQxQ_byTIJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3659697033000831728&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'8MqQxQ_byTIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5529899" class=yC9>Multi-video summarization based on AV-MMR</a></h3><div class="gs_a">Y Li, B Merialdo - Content-Based Multimedia Indexing (CBMI),  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper presents an algorithm for video summarization, Audio Video Maximal <br>Marginal Relevance (AV-MMR), exploiting both audio and video information. It is an <br>extension of the Video Maximal Marginal Relevance (Video-MMR) algorithm which was <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10708551992134566433&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 7</a> <a href="/scholar?q=related:IYL77UxrnJQJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'IYL77UxrnJQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://cs.nju.edu.cn/ywguo/webs/paperdownload/Multi-view_Video_Summarization.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nju.edu.cn</span><span class="gs_ggsS">nju.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5482155" class=yCA>Multi-view video summarization</a></h3><div class="gs_a">Y Fu, Y Guo, Y Zhu, F Liu, C Song&hellip; - &hellip; , IEEE Transactions on, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Previous video summarization studies focused on monocular videos, and the <br>results would not be good if they were applied to multi-view videos directly, due to problems <br>such as the redundancy in multiple views. In this paper, we present a method for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15891594998246259562&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 11</a> <a href="/scholar?q=related:arvXIHRJitwJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15891594998246259562&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'arvXIHRJitwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0957417410006603" class=yCC>An integrated music video browsing system for personalized television</a></h3><div class="gs_a">HG Kim, JY Kim, JG Baek - Expert systems with Applications, 2011 - Elsevier</div><div class="gs_rs">In this paper, we propose an integrated music video browsing system for personalized <br>digital television. The system has the functions of automatic music emotion classification, <br>automatic theme-based music classification, salient region detection, and shot <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16545480876754435380&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 4</a> <a href="/scholar?q=related:NH33GDJbneUJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16545480876754435380&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'NH33GDJbneUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/61J7427324K92QW5.pdf" class=yCD>Video summarization based on balanced AV-MMR</a></h3><div class="gs_a">Y Li, B Merialdo - Advances in Multimedia Modeling, 2012 - Springer</div><div class="gs_rs">Among the techniques of video processing, video summarization is a promising approach to <br>process the multimedia content. In this paper we present a novel summarization algorithm, <br>Balanced Audio Video Maximal Marginal Relevance (Balanced AV-MMR or BAV-MMR), <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=442820216374335574&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 2</a> <a href="/scholar?q=related:VphaAKQ2JQYJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=442820216374335574&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'VphaAKQ2JQYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5698304" class=yCE>ViSum: Video Summarization Using Dyanamic Threshold</a></h3><div class="gs_a">SL Varma, SN Talbar - Emerging Trends in Engineering and  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Summarized shots can be helpful in video analysis, indexing, browsing, and <br>retrieval. Summarizing long duration shots of video sequences is an open research issues. <br>The ViSum system basically deals with identification and extraction of key frames. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17599638311581072285&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 1</a> <a href="/scholar?q=related:nTslKcx5PvQJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17599638311581072285&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'nTslKcx5PvQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://drops.dagstuhl.de/opus/volltexte/2012/3465/pdf/4.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dagstuhl.de</span><span class="gs_ggsS">dagstuhl.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://drops.dagstuhl.de/opus/volltexte/2012/3465/" class=yCF>Fusion of Multimodal Information in Music Content Analysis</a></h3><div class="gs_a">S Essid, G Richard - 2012 - drops.dagstuhl.de</div><div class="gs_rs">Abstract Music is often processed through its acoustic realization. This is restrictive in the <br>sense that music is clearly a highly multimodal concept where various types of <br>heterogeneous information can be associated to a given piece of music (a musical score, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8166200321751406748&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=17">Cited by 1</a> <a href="/scholar?q=related:nFisHu0rVHEJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8166200321751406748&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'nFisHu0rVHEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://w.comp.nus.edu/undergraduates/undergraduates/urop_papers/LuongMinhThang_U056889M.pdf" class=yC11>A repetition-based framework for lyric alignment in popular songs</a></h3><div class="gs_a">LM Thang, KANM Yen - National University of Singapore  &hellip;, 2007 - w.comp.nus.edu</div><div class="gs_rs">ABSTRACT We examine the problem of automatically aligning acoustic musical audio and <br>textual lyric in popular songs. Existing works have tackled the problem using <br>computationally-expensive audio processing techniques, resulting in solutions unsuitable <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17581284984694044119&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 27 versions</a> <a onclick="return gs_ocit(event,'1_WYP4tF_fMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:1_WYP4tF_fMJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://hive.asu.edu/sigmod12/images/originalphotos/groupphotos/1454/1955/acmmm2011-poster.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from asu.edu</span><span class="gs_ggsS">asu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072068" class=yC13>Static and dynamic video summaries</a></h3><div class="gs_a">Y Li, B Merialdo, <a href="/citations?user=0fmu-VsAAAAJ&amp;hl=en&amp;oi=sra">M Rouvier</a>, G Linares - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Currently there are a lot of algorithms for video summarization; however most of <br>them only represent visual information. In this paper, we propose two approaches for the <br>construction of the summary using both video and text. One approach focuses on static <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:zwAbRlcg92cJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7491492064364527823&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'zwAbRlcg92cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Features for Content Analysis</h3><div class="gs_a"><a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, F Wang - Springer</div><div class="gs_fl"><a href="/scholar?q=related:E07DYJW0q8UJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'E07DYJW0q8UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.google.com/patents?hl=en&amp;lr=&amp;vid=USPAT8013229&amp;id=c0jtAQAAEBAJ&amp;oi=fnd&amp;printsec=abstract" class=yC15>Automatic creation of thumbnails for music videos</a></h3><div class="gs_a">C Xu - US Patent 8,013,229, 2011 - Google Patents</div><div class="gs_rs">There is provided a method for automatically creating a music video thumbnail (50) from a <br>music video signal (12). The music video signal is separated into a music signal (16) and a <br>video signal (18). The music signal is analysed by detecting similarity regions and the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:joE1fTcSfmEJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7025072498277712270&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'joE1fTcSfmEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.106.1363&amp;rep=rep1&amp;type=pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.worldscientific.com/doi/abs/10.1142/S021946780800312X" class=yC16>A User Interface Design for Acquiring Statistics from Video</a></h3><div class="gs_a">MK Leung, FU CHI-WING - International Journal of Image and  &hellip;, 2008 - World Scientific</div><div class="gs_rs">This paper introduces a graphical user interface approach to facilitate an efficient and timely <br>generation of statistic data from input videos. By means of a carefully-designed graphical <br>user interface, users can interactively add in various kinds of markers, known as the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:cT5etVCOFCoJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3032204926418566769&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'cT5etVCOFCoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=qD-SbLK71TcC&amp;oi=fnd&amp;pg=PA19&amp;ots=J9L714jCVD&amp;sig=v9WNRZ3rHyGN65Fq_XOCFHB1Yso" class=yC18>El Resumen Automatico Y La Evaluacion De Traducciones En El Contexto De La Traduccion Especializada</a></h3><div class="gs_a">MCT BÃ¡ez - 2010 - books.google.com</div><div class="gs_rs">Las pÃ¡ginas que se abren a continuaciÃ³n son el resultado de varios aÃ±os de investigaciÃ³n <br>en el Ã¡mbito del Procesamiento del Lenguaje Natural aplicado a la enseÃ±anza y la prÃ¡ctica <br>de la traducciÃ³n profesional. La autora, MarÃ­a Cristina Toledo BÃ¡ez, doctora (con menciÃ³n <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:F4uoFZKyk2EJ:scholar.google.com/&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'F4uoFZKyk2EJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md16', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md16" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:F4uoFZKyk2EJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=17&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=13570848835308492833&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
