Total results = 67
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://brutal.googlecode.com/svn/trunk/Adding%20Semantics%20to%20Detectors%20for%20Video%20Retrieval.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from googlecode.com</span><span class="gs_ggsS">googlecode.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4276717" class=yC0>Adding semantics to detectors for video retrieval</a></h3><div class="gs_a"><a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, B Huurnink, <a href="/citations?user=T6b4scsAAAAJ&amp;hl=en&amp;oi=sra">L Hollink</a>&hellip; - Multimedia, IEEE  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose an automatic video retrieval method based on high-level <br>concept detectors. Research in video analysis has reached the point where over 100 <br>concept detectors can be learned in a generic fashion, albeit with mixed performance. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7315134674580442679&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 126</a> <a href="/scholar?q=related:N46CsjqUhGUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/39/51/RN213262190.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=7315134674580442679&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 26 versions</a> <a onclick="return gs_ocit(event,'N46CsjqUhGUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.5031&amp;rep=rep1&amp;type=pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1576260" class=yC3>Concept-based video retrieval</a></h3><div class="gs_a"><a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Foundations and Trends in Information  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we review 300 references on video retrieval, indicating when text-only <br>solutions are unsatisfactory and showing the promising alternatives which are in majority <br>concept-based. Therefore, central to our discussion is the notion of a semantic concept: an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1240556430566916602&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 140</a> <a href="/scholar?q=related:-oHEN4BXNxEJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1240556430566916602&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'-oHEN4BXNxEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:-oHEN4BXNxEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=868717410844952179&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.5116&amp;rep=rep1&amp;type=pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291448" class=yC5>Semantic concept-based query expansion and re-ranking for multimedia retrieval</a></h3><div class="gs_a"><a href="/citations?user=Ade_7YoAAAAJ&amp;hl=en&amp;oi=sra">AP Natsev</a>, A Haubold, J TeÅ¡iÄ, <a href="/citations?user=u0xUDSoAAAAJ&amp;hl=en&amp;oi=sra">L Xie</a>&hellip; - Proceedings of the 15th  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract We study the problem of semantic concept-based query expansion and re-ranking <br>for multimedia retrieval. In particular, we explore the utility of a fixed lexicon of visual <br>semantic concepts for automatic multimedia retrieval and re-ranking purposes. In this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2405995674058464620&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 101</a> <a href="/scholar?q=related:bJ1ha1HQYyEJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2405995674058464620&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'bJ1ha1HQYyEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4276711" class=yC7>Can high-level concepts fill the semantic gap in video retrieval? A case study with broadcast news</a></h3><div class="gs_a"><a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">A Hauptmann</a>, <a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=PlBXfHEAAAAJ&amp;hl=en&amp;oi=sra">WH Lin</a>&hellip; - &hellip;  IEEE Transactions on, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A number of researchers have been building high-level semantic concept detectors <br>such as outdoors, face, building, to help with semantic video retrieval. Our goal is to examine <br>how many concepts would be needed, and how they should be selected and used. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17182694289177070523&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 96</a> <a href="/scholar?q=related:u0uIDmsxde4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/09/4B/RN213262178.html?source=googlescholar" class="gs_nph" class=yC8>BL Direct</a> <a href="/scholar?cluster=17182694289177070523&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'u0uIDmsxde4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://166.111.138.19/paper/xirongli_civr2007_conceptsubspace.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 166.111.138.19</span><span class="gs_ggsS">166.111.138.19 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://166.111.138.19/paper/xirongli_civr2007_conceptsubspace.pdf" class=yC9>Video search in concept subspace: a text-like paradigm</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, D Wang, J Li, B Zhang - &hellip;  Video Retrieval: Proceedings of the 6 &hellip;, 2007 - 166.111.138.19</div><div class="gs_rs">ABSTRACT Though both quantity and quality of semantic concept detection in video are <br>continuously improving, it still remains unclear how to exploit these detected concepts as <br>semantic indices in video search, given a specific query. In this paper, we tackle this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=321217045020478241&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 45</a> <a href="/scholar?q=related:IQs_WDMxdQQJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=321217045020478241&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'IQs_WDMxdQQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:IQs_WDMxdQQJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.yugangjiang.info/publication/mm09_sct.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from yugangjiang.info</span><span class="gs_ggsS">yugangjiang.info <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631272.1631296" class=yCB>Semantic context transfer across heterogeneous sources for domain adaptive video search</a></h3><div class="gs_a"><a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=OMVTRscAAAAJ&amp;hl=en&amp;oi=sra">SF Chang</a> - Proceedings of the 17th ACM  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Automatic video search based on semantic concept detectors has recently received <br>significant attention. Since the number of available detectors is much smaller than the size of <br>human vocabulary, one major challenge is to select appropriate detectors to response <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17552397467153358488&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 38</a> <a href="/scholar?q=related:mF4k8n-klvMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17552397467153358488&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'mF4k8n-klvMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.cs.cityu.edu.hk/~xiaoyong/papers/mm07.pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.cs.cityu.edu.hk/~xiaoyong/papers/mm07.pdf" class=yCD>Ontology-enriched semantic space for video search</a></h3><div class="gs_a">XY Wei, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - &hellip;  Multimedia Conference: Proceedings of the 15 th &hellip;, 2007 - cs.cityu.edu.hk</div><div class="gs_rs">ABSTRACT Multimedia-based ontology construction and reasoning have recently been <br>recognized as two important issues in video search, particularly for bridging semantic gap. <br>The lack of coincidence between low-level features and user expectation makes concept-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7389686429032648877&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 37</a> <a href="/scholar?q=related:rRAErKdwjWYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7389686429032648877&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'rRAErKdwjWYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md6', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md6" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:rRAErKdwjWYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv8.papers/mcg-ict-cas.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv8.papers/mcg-ict-cas.pdf" class=yCF>TRECVID 2008 high-level feature extraction by MCG-ICT-CAS</a></h3><div class="gs_a">S Tang, JT Li, M Li, C Xie, YZ Liu, K Tao&hellip; - Proc. TRECVID  &hellip;, 2008 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT For TRECVID 2008 concept detection task, we principally focus on:(1) Early <br>fusion of texture, edge and color features TECM, abbreviation of the combined TF* IDF <br>weights based on SIFT features, Edge Histogram, and Color Moments.(2) To improve the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9754870363319714301&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 34</a> <a href="/scholar?q=related:_UGoc-lCYIcJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9754870363319714301&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'_UGoc-lCYIcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_UGoc-lCYIcJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/r742245481q23631.pdf" class=yC11>A review of text and image retrieval approaches for broadcast news video</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">AG Hauptmann</a> - Information Retrieval, 2007 - Springer</div><div class="gs_rs">Abstract The effectiveness of a video retrieval system largely depends on the choice of <br>underlying text and image retrieval components. The unique properties of video collections <br>(eg, multiple sources, noisy features and temporal relations) suggest we examine the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9278500574809399146&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 35</a> <a href="/scholar?q=related:ap_F-Rzbw4AJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0D/2C/RN216038550.html?source=googlescholar" class="gs_nph" class=yC12>BL Direct</a> <a href="/scholar?cluster=9278500574809399146&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'ap_F-Rzbw4AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://www.lti.cs.cmu.edu/research/thesis/2006/rong_yan.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cmu.edu</span><span class="gs_ggsS">cmu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.lti.cs.cmu.edu/research/thesis/2006/rong_yan.pdf" class=yC13>Probabilistic models for combining diverse knowledge sources in multimedia retrieval</a></h3><div class="gs_a"><a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a> - 2006 - lti.cs.cmu.edu</div><div class="gs_rs">Abstract In recent years, the multimedia retrieval community is gradually shifting its <br>emphasis from analyzing one media source at a time to exploring the opportunities of <br>combining diverse knowledge sources from correlated media types and context. In order <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1282854589144669096&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 32</a> <a href="/scholar?q=related:qJtytHOdzREJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1282854589144669096&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 17 versions</a> <a onclick="return gs_ocit(event,'qJtytHOdzREJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:qJtytHOdzREJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:qJtytHOdzREJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=9607173453927529710&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="https://www.ee.columbia.edu/~xlx/research/papers/pieee-events.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4468738" class=yC15>Event mining in multimedia streams</a></h3><div class="gs_a"><a href="/citations?user=u0xUDSoAAAAJ&amp;hl=en&amp;oi=sra">L Xie</a>, <a href="/citations?user=Z962IGQAAAAJ&amp;hl=en&amp;oi=sra">H Sundaram</a>, <a href="/citations?user=8rykXfcAAAAJ&amp;hl=en&amp;oi=sra">M Campbell</a> - Proceedings of the IEEE, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Events are real-world occurrences that unfold over space and time. Event mining <br>from multimedia streams improves the access and reuse of large media collections, and it <br>has been an active area of research with notable progress. This paper contains a survey <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17740260980691592677&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 31</a> <a href="/scholar?q=related:5WXYuF4RMvYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/55/30/RN226708441.html?source=googlescholar" class="gs_nph" class=yC17>BL Direct</a> <a href="/scholar?cluster=17740260980691592677&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'5WXYuF4RMvYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://www.cs.cityu.edu.hk/~xiaoyong/papers/itm08.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4657461" class=yC18>Selection of concept detectors for video search by ontology-enriched semantic spaces</a></h3><div class="gs_a">XY Wei, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a> - Multimedia, IEEE Transactions on, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes the construction and utilization of two novel semantic spaces, <br>namely ontology-enriched semantic space (OSS) and ontology-enriched orthogonal <br>semantic space (OS 2), to facilitate the selection of concept detectors for video search. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9722615393071150163&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 22</a> <a href="/scholar?q=related:UzTA9y-r7YYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9722615393071150163&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'UzTA9y-r7YYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://vireo.cs.cityu.edu.hk/papers/mm08_xywei.pdf" class=yC1B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459371" class=yC1A>Fusing semantics, observability, reliability and diversity of concept detectors for video search</a></h3><div class="gs_a">XY Wei, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Proceedings of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Effective utilization of semantic concept detectors for large-scale video search has <br>recently become a topic of intensive studies. One of main challenges is the selection and <br>fusion of appropriate detectors, which considers not only semantics but also the reliability <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4188565563715984114&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 18</a> <a href="/scholar?q=related:8pq5LzDGIDoJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4188565563715984114&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'8pq5LzDGIDoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.113.1181&amp;rep=rep1&amp;type=pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291293" class=yC1C>The importance of query-concept-mapping for automatic video retrieval</a></h3><div class="gs_a">D Wang, <a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, J Li, B Zhang - &hellip;  of the 15th international conference on  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract A new video retrieval paradigm of query-by-concept emerges recently. However, it <br>remains unclear how to exploit the detected concepts in retrieval given a multimedia query. <br>In this paper, we point out that it is important to map the query to a few relevant concepts <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12654125591700011954&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 18</a> <a href="/scholar?q=related:snt1ZC58nK8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12654125591700011954&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'snt1ZC58nK8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm07-neosyOFF.PDF" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291278" class=yC1E>The use of topic evolution to help users browse and find answers in news video corpus</a></h3><div class="gs_a">SY Neo, Y Ran, HK Goh, Y Zheng, TS Chua&hellip; - Proceedings of the 15th  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Earlier research in news video has been focusing mainly on improving retrieval <br>accuracies given the limited amount of extractable video semantics. In this paper, we <br>propose an enhancement to news video searching by leveraging extractable video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2755699895959884965&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 18</a> <a href="/scholar?q=related:pRRFfnQ2PiYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2755699895959884965&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'pRRFfnQ2PiYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1077314208001343" class=yC20>Video retrieval based on object discovery</a></h3><div class="gs_a"><a href="/citations?user=QRMqM20AAAAJ&amp;hl=en&amp;oi=sra">D Liu</a>, T Chen - Computer vision and image understanding, 2009 - Elsevier</div><div class="gs_rs">State-of-the-art video retrieval methods use global image statistics to provide low level <br>descriptors or use object recognizers to provide high level features. Using global image <br>statistics can be hindered by lack of explicitly characterizing the object of interest hence <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17318943307900952131&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 18</a> <a href="/scholar?q=related:Q56QyC0_WfAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17318943307900952131&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Q56QyC0_WfAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.dcs.bbk.ac.uk/~sjmaybank/survey%20video%20indexing.pdf" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from bbk.ac.uk</span><span class="gs_ggsS">bbk.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5729374" class=yC21>A survey on visual content-based video indexing and retrieval</a></h3><div class="gs_a">W Hu, N Xie, L Li, X Zeng&hellip; - Systems, Man, and  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video indexing and retrieval have a wide spectrum of promising applications, <br>motivating the interest of researchers worldwide. This paper offers a tutorial and an overview <br>of the landscape of general strategies in visual content-based video indexing and retrieval<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5775956488998965554&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 24</a> <a href="/scholar?q=related:MlHlDNNRKFAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5775956488998965554&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'MlHlDNNRKFAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="http://lms.comp.nus.edu.sg/papers/media/2007/acmmm07-huanboOFF.PDF" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1291295" class=yC23>Segregated feedback with performance-based adaptive sampling for interactive news video retrieval</a></h3><div class="gs_a">HB Luan, SY Neo, HK Goh, YD Zhang, SX Lin&hellip; - Proceedings of the 15th &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Existing video research incorporates the use of relevance feedback based on user-<br>dependent interpretations to improve the retrieval results. In this paper, we segregate the <br>process of relevance feedback into 2 distinct facets:(a) recall-directed feedback; and (b) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18005967219541332703&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 16</a> <a href="/scholar?q=related:39IWVskL4vkJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18005967219541332703&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'39IWVskL4vkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://db.aquaphoenix.com/publication/civr2008_ic.pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aquaphoenix.com</span><span class="gs_ggsS">aquaphoenix.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1386408" class=yC25>Web-based information content and its application to concept-based video retrieval</a></h3><div class="gs_a">A Haubold, <a href="/citations?user=Ade_7YoAAAAJ&amp;hl=en&amp;oi=sra">A Natsev</a> - Proceedings of the 2008 international conference &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Semantic similarity between words or phrases is frequently used to find matching <br>correlations between search queries and documents when straightforward matching of <br>terms fails. This is particularly important for searching in visual databases, where pictures <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=342358306974839972&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 15</a> <a href="/scholar?q=related:pLQj-RBNwAQJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=342358306974839972&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'pLQj-RBNwAQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://ikt420sentimentanalysis.googlecode.com/svn/wiki/PDF/white_papers/Utilizing%20Semantic%20Word%20Similarity%20Measures%20for%20Video%20Retrieval.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from googlecode.com</span><span class="gs_ggsS">googlecode.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4587822" class=yC27>Utilizing semantic word similarity measures for video retrieval</a></h3><div class="gs_a"><a href="/citations?user=0ncQNL8AAAAJ&amp;hl=en&amp;oi=sra">Y Aytar</a>, <a href="/citations?user=p8gsO3gAAAAJ&amp;hl=en&amp;oi=sra">M Shah</a>, <a href="/citations?user=CcbnBvgAAAAJ&amp;hl=en&amp;oi=sra">J Luo</a> - Computer Vision and Pattern  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This is a high level computer vision paper, which employs concepts from Natural <br>Language Understanding in solving the video retrieval problem. Our main contribution is the <br>utilization of the semantic word similarity measures (Lin and PMI-IR similarities) for video <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=774385432920040583&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 17</a> <a href="/scholar?q=related:h6RvyHIrvwoJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=774385432920040583&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 22 versions</a> <a onclick="return gs_ocit(event,'h6RvyHIrvwoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://lms.comp.nus.edu.sg/papers/media/2007/civr07-zhengYT.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1282341" class=yC29>The use of temporal, semantic and visual partitioning model for efficient near-duplicate keyframe detection in large scale news corpus</a></h3><div class="gs_a">YT Zheng, SY Neo, TS Chua, <a href="/citations?user=HJt0niEAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a> - Proceedings of the 6th ACM  &hellip;, 2007 - dl.acm.org</div><div class="gs_rs">Abstract Near-duplicate keyframes (NDKs) are important visual cues to link news stories <br>from different TV channel, time, language, etc. However, the quadratic complexity required <br>for NDK detection renders it intractable in large-scale news video corpus. To address this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4266303494236248305&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 12</a> <a href="/scholar?q=related:8Zj1lWz0NDsJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4266303494236248305&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'8Zj1lWz0NDsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://www.ee.columbia.edu/~yjiang/publication/tcsvt_cdvs.pdf" class=yC2C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from columbia.edu</span><span class="gs_ggsS">columbia.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5686924" class=yC2B>Concept-driven multi-modality fusion for video search</a></h3><div class="gs_a">XY Wei, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Circuits and Systems for Video  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract As it is true for human perception that we gather information from different sources <br>in natural and multi-modality forms, learning from multi-modalities has become an effective <br>scheme for various information retrieval problems. In this paper, we propose a novel multi-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9058963900236874973&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 14</a> <a href="/scholar?q=related:3cS0uqvnt30J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9058963900236874973&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'3cS0uqvnt30J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://press.liacs.nl/students.mir/Assessing%20Concept%20Selection%20for%20Video%20Retrieval.pdf" class=yC2E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from liacs.nl</span><span class="gs_ggsS">liacs.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1460170" class=yC2D>Assessing concept selection for video retrieval</a></h3><div class="gs_a">B Huurnink, <a href="/citations?user=bHsjbLwAAAAJ&amp;hl=en&amp;oi=sra">K Hofmann</a>, <a href="/citations?user=AVDkgFIAAAAJ&amp;hl=en&amp;oi=sra">M de Rijke</a> - Proceedings of the 1st ACM  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract We explore the use of benchmarks to address the problem of assessing concept <br>selection in video retrieval systems. Two benchmarks are presented, one created by human <br>association of queries to concepts, the other generated from an extensively tagged <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6199615850227908893&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 10</a> <a href="/scholar?q=related:HUG30MR1CVYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6199615850227908893&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'HUG30MR1CVYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://www.science.uva.nl/research/publications/2010/deRooijITM2010/deRooij_RotorBrowser2010.pdf" class=yC30><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5340554" class=yC2F>Browsing video along multiple threads</a></h3><div class="gs_a"><a href="/citations?user=znjsUsYAAAAJ&amp;hl=en&amp;oi=sra">O De Rooij</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Multimedia, IEEE Transactions on, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper describes a novel method for browsing a large video collection. It links <br>various forms of related video fragments together as threads. These threads are based on <br>query results, the timeline as well as visual and semantic similarity. We design two <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3141577060277481148&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 11</a> <a href="/scholar?q=related:vO5aCLEfmSsJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3141577060277481148&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'vO5aCLEfmSsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/acmmm09-chua.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631069" class=yC31>From text question-answering to multimedia QA on web-scale media resources</a></h3><div class="gs_a">TS Chua, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, G Li, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a> - Proceedings of the First ACM workshop &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract With the proliferation of text and multimedia information, users are now able to find <br>answers to almost any questions on the Web. Meanwhile, they are also bewildered by the <br>huge amount of information routinely presented to them. Question-answering (QA) is a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18090986418949307243&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 10</a> <a href="/scholar?q=related:a7vjQUwYEPsJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18090986418949307243&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'a7vjQUwYEPsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.175.8643&amp;rep=rep1&amp;type=pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816045" class=yC33>Today&#39;s and tomorrow&#39;s retrieval practice in the audiovisual archive</a></h3><div class="gs_a">B Huurnink, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=AVDkgFIAAAAJ&amp;hl=en&amp;oi=sra">M de Rijke</a>&hellip; - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Content-based video retrieval is maturing to the point where it can be used in real-<br>world retrieval practices. One such practice is the audiovisual archive, whose users <br>increasingly require fine-grained access to broadcast television content. We investigate to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4783114741417717331&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 9</a> <a href="/scholar?q=related:Uy4j6oMJYUIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4783114741417717331&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'Uy4j6oMJYUIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://eprints.eemcs.utwente.nl/9253/01/phdmihajlovic.pdf" class=yC36><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utwente.nl</span><span class="gs_ggsS">utwente.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://eprints.eemcs.utwente.nl/9253/01/phdmihajlovic.pdf" class=yC35>Score Region Algebra: A flexible framework for structured information retrieval</a></h3><div class="gs_a">V Mihajlovic - 2006 - eprints.eemcs.utwente.nl</div><div class="gs_rs">In the origin of every intelligent life form on earth there is curiosity. For humans curiosity is a <br>desire to discover something new, to gather more data, more information, more knowledge. <br>This desire can be clearly seen when looking at the youngest in our population. Small <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9805224275084208481&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 8</a> <a href="/scholar?q=related:YWluI4cnE4gJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9805224275084208481&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'YWluI4cnE4gJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:YWluI4cnE4gJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:YWluI4cnE4gJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=2744093314723215873&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB27" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW27"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv7.papers/ict_nus.pdf" class=yC38><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv7.papers/ict_nus.pdf" class=yC37>Trecvid 2007 search tasks by nus-ict</a></h3><div class="gs_a">TS Chua, SY Neo, YT Zheng, HK Goh&hellip; - TRECVID  &hellip;, 2007 - www-nlpir.nist.gov</div><div class="gs_rs">ABSTRACT This paper describes the details of our systems for our automated and <br>interactive search in TRECVID 2007. The shift from news video to documentary video this <br>year has prompted a series of changes in processing techniques from that developed over <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=173529439328118389&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 6</a> <a href="/scholar?q=related:daY5DRuAaAIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=173529439328118389&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'daY5DRuAaAIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md27', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md27" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:daY5DRuAaAIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://mpac.ee.ntu.edu.tw/~yihsuan/pub/TCSVT09.pdf" class=yC3A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5159452" class=yC39>Online reranking via ordinal informative concepts for context fusion in concept detection and video search</a></h3><div class="gs_a"><a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, <a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a>, HH Chen - Circuits and Systems for Video  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract To exploit the co-occurrence patterns of semantic concepts while keeping the <br>simplicity of context fusion, a novel reranking approach is proposed in this paper. The <br>approach, called ordinal reranking, adjusts the ranking of an initial search (or detection) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11286279959416668923&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 7</a> <a href="/scholar?q=related:-wKkaNProJwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11286279959416668923&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'-wKkaNProJwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/civr10-yuanjin.pdf" class=yC3C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1816041.1816053" class=yC3B>Utilizing related samples to learn complex queries in interactive concept-based video search</a></h3><div class="gs_a">J Yuan, ZJ Zha, Z Zhao, <a href="/citations?user=QUrLihYAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a>, TS Chua - Proceedings of the ACM  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract One of the main challenges in interactive concept-based video search is the <br>insufficient relevant sample problem, especially for queries with complex semantics. To <br>address this problem, in this paper, we propose to utilize&quot; related samples&quot; to learn the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9335504255283517338&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 7</a> <a href="/scholar?q=related:mldSl6hfjoEJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9335504255283517338&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'mldSl6hfjoEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6022804" class=yC3D>Utilizing related samples to enhance interactive concept-based video search</a></h3><div class="gs_a">J Yuan, ZJ Zha, YT Zheng, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>&hellip; - Multimedia, IEEE  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract One of the main challenges in interactive concept-based video search is the <br>problem of insufficient relevant samples, especially for queries with complex semantics. In <br>this paper,ârelated samplesâ are exploited to enhance interactive video search. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16391758592167997752&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 5</a> <a href="/scholar?q=related:OHEhopc5e-MJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16391758592167997752&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'OHEhopc5e-MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC3F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC3E>TRECVID 2010 Known-item Search by NUS</a></h3><div class="gs_a">XY Chen, J Yuan, L Nie, ZJ Zha, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - TRECVID  &hellip;, 2010 - www-nlpir.nist.gov</div><div class="gs_rs">Abstract. This paper describes our system for auto search and interactive search in the <br>known-item search (KIS) task in TRECVID 2010. KIS task aims to find an unique video <br>answer for each text query. The shift from traditional video search has prompted a series <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12664714192218118309&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'pVgCEXUawq8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md31', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md31" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/qx4j3147784r1516.pdf" class=yC40>Exploration and Management of Web Based Multimedia Information Resources</a></h3><div class="gs_a">U Rashid, MA Bhatti - Innovations and Advanced Techniques in Systems,  &hellip;, 2008 - Springer</div><div class="gs_rs">WWW is a huge multimedia information resource. It is composed of diverse heterogeneous <br>unorganized information resources. Information resources include multimedia data from <br>different diversities. Information exploration services are required for searching and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15150394109930907940&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:JCmaOSwDQdIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'JCmaOSwDQdIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB33" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW33"><a href="http://www.cs.clemson.edu/~jzwang/1201863/mm2011/p453-yuan.pdf" class=yC42><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072357" class=yC41>Learning concept bundles for video search with complex queries</a></h3><div class="gs_a">J Yuan, ZJ Zha, YT Zheng, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=QUrLihYAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a>&hellip; - Proceedings of the 19th  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Classifiers for primitive visual concepts like&quot; car&quot;,&quot; sky&quot; have been well developed <br>and widely used to support video search on simple queries. However, it is usually ineffective <br>for complex queries like&quot; one or more people at a table or desk with a computer visible&quot;, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8459346073814893797&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 4</a> <a href="/scholar?q=related:5QTVQW-iZXUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8459346073814893797&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'5QTVQW-iZXUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1386364" class=yC43>Learning structured concept-segments for interactive video retrieval</a></h3><div class="gs_a"><a href="/citations?user=5vj1VV8AAAAJ&amp;hl=en&amp;oi=sra">Z Wang</a>, D Wang, J Li, B Zhang - &hellip; of the 2008 international conference on &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Now with a large lexicon of over 300 semantic concepts available for indexing <br>purpose, video retrieval can be made easier by leveraging on the available semantic <br>indices. However, any successful concept-based video retrieval approach must take the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13513896392690841913&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:OTnFYSsBi7sJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'OTnFYSsBi7sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/vt737521865008m3.pdf" class=yC44>Filling the semantic gap in video retrieval: An exploration</a></h3><div class="gs_a"><a href="/citations?user=Py54GcEAAAAJ&amp;hl=en&amp;oi=sra">A Hauptmann</a>, <a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a>, <a href="/citations?user=PlBXfHEAAAAJ&amp;hl=en&amp;oi=sra">WH Lin</a>, M Christel&hellip; - Semantic multimedia and &hellip;, 2008 - Springer</div><div class="gs_rs">Digital images and motion video have proliferated in the past few years, ranging from ever-<br>growing personal photo and video collections to professional news and documentary <br>archives. In searching through these archives, digital imagery indexing based on low-level <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7245539427021096793&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:WQeuabhTjWQJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7245539427021096793&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'WQeuabhTjWQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4284772" class=yC45>News video retrieval using implicit event semantics</a></h3><div class="gs_a">SY Neo, Y Zheng, HK Goh, TS Chua&hellip; - Multimedia and Expo,  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Current state-of-the-art news video retrieval systems mainly focus on automated <br>speech recognition (ASR) text to perform retrieval. This paradigm greatly affects retrieval <br>performance as ASR text alone is not sufficient to provide an accurate representation of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12014342483156170817&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 4</a> <a href="/scholar?q=related:QXhobtWEu6YJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'QXhobtWEu6YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5680970" class=yC46>Optimizing visual search reranking via pairwise learning</a></h3><div class="gs_a">Y Liu, <a href="/citations?user=7Yq4wf4AAAAJ&amp;hl=en&amp;oi=sra">T Mei</a> - Multimedia, IEEE Transactions on, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Visual search reranking is defined as reordering visual documents (images or video <br>clips) based on the initial search results or some auxiliary knowledge to improve the search <br>precision. Conventional approaches to visual search reranking empirically take the â<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8958252504035822793&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:yQxPhzAbUnwJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8958252504035822793&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'yQxPhzAbUnwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/K55481H9446731Q7.pdf" class=yC47>Unified multimodal search framework for multimedia information retrieval</a></h3><div class="gs_a">U Rashid, IA Niaz, MA Bhatti - Advanced Techniques in Computing  &hellip;, 2010 - Springer</div><div class="gs_rs">There is a trend towards construction of multimedia digital information resources which may <br>hold diverse data types in the form of image, graphics, audio, video, and text based retrieval <br>artifacts or objects. WWW is a huge multimedia information resource. Existing search <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1037896888578917647&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:D8U6nbhZZw4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1037896888578917647&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'D8U6nbhZZw4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5381708" class=yC48>Dms-1 driven data model to enable a semantic middleware for multimedia information retrieval in a broadcaster</a></h3><div class="gs_a">G Marcos, K Alonso, <a href="/citations?user=TihmWmAAAAAJ&amp;hl=en&amp;oi=sra">IG Olaizola</a>&hellip; - &hellip; , 2009. SMAP&#39;09. 4th &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This article presents the motivation and the implementation of a semantic model <br>developed to support diverse semantic services in a multimedia asset management system <br>in a broadcaster. The model is mainly driven by DMS-1 (descriptive metadata scheme) <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=573997171069683876&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 4</a> <a href="/scholar?q=related:pAiwy2I_9wcJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=573997171069683876&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'pAiwy2I_9wcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB40" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW40"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.4267&amp;rep=rep1&amp;type=pdf" class=yC4A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/L430L012V407J8GX.pdf" class=yC49>Extracting semantics from multimedia content: challenges and solutions</a></h3><div class="gs_a"><a href="/citations?user=u0xUDSoAAAAJ&amp;hl=en&amp;oi=sra">L Xie</a>, <a href="/citations?user=NIIQFrEAAAAJ&amp;hl=en&amp;oi=sra">R Yan</a> - Multimedia Content Analysis, 2009 - Springer</div><div class="gs_rs">Multimedia content accounts for over 60% of traffic in the current Internet [74]. With many <br>users willing to spend their leisure time watching videos on YouTube or browsing photos <br>through Flickr, sifting through large multimedia collections for useful information, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9102153946033487200&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:YP0-5MlYUX4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9102153946033487200&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'YP0-5MlYUX4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/67069X2587LN07V3.pdf" class=yC4B>Semantic middleware to enhance multimedia retrieval in a broadcaster</a></h3><div class="gs_a">G Marcos, P KrÃ¤mer, A Illarramendi, <a href="/citations?user=TihmWmAAAAAJ&amp;hl=en&amp;oi=sra">I GarcÃ­a</a>&hellip; - Semantic Multimedia, 2008 - Springer</div><div class="gs_rs">The digitalization of video and recent progress in semantic multimedia indexing and retrieval <br>transform the workflow and tools involved in the information retrieval process of the <br>broadcasters. To this end, we present in this paper a theoretical framework which <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9178853511484711313&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:kaFkFaPWYX8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9178853511484711313&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'kaFkFaPWYX8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/L317L761T8271316.pdf" class=yC4C>A middleware to enhance current multimedia retrieval systems with content-based functionalities</a></h3><div class="gs_a">G Marcos, A Illarramendi, <a href="/citations?user=TihmWmAAAAAJ&amp;hl=en&amp;oi=sra">IG Olaizola</a>, J FlÃ³rez - Multimedia systems, 2011 - Springer</div><div class="gs_rs">Abstract Nowadays the retrieval of multimedia assets is mainly performed by text-based <br>retrieval systems with powerful and stable indexing mechanisms. Migration from those <br>systems to content-aware multimedia retrieval systems is a common aim for companies <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3692154176140466245&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:RdjTTKkqPTMJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3692154176140466245&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'RdjTTKkqPTMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB43" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW43"><a href="http://www.cmlab.csie.ntu.edu.tw/~frankwbd/papers/ctsp3757-wu.pdf" class=yC4E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459451" class=yC4D>Keyword-based concept search on consumer photos by web-based kernel function</a></h3><div class="gs_a">PT Wu, <a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, KT Chen, <a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a>, TH Li&hellip; - Proceedings of the 16th  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In light of the strong demands for semantic search over large-scale consumer <br>photos, which generally lack reliable user-provided annotations, we investigate the <br>feasibility and challenges entailed by the new paradigm, concept search-retrieving visual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13695379660375336139&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 2</a> <a href="/scholar?q=related:yyTIaD3DD74J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13695379660375336139&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'yyTIaD3DD74J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://www.cs.cityu.edu.hk/~xiaoyong/papers/cal.mm11.pdf" class=yC50><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072356" class=yC4F>Coached active learning for interactive video search</a></h3><div class="gs_a">XY Wei, ZQ Yang - Proceedings of the 19th ACM international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Active learning with uncertainty sampling has been popularly employed in <br>implementing interactive video search, due to its promise to reduce labeling efforts. <br>However, since the ultimate goal of interactive search is to find as many relevant shots as <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6399000406857213651&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:0-ZNePfQzVgJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6399000406857213651&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'0-ZNePfQzVgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB45" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW45"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/VisionGo-%20Towards%20video%20retrieval%20with%20joint%20exploration%20of%20human%20and%20computer.pdf" class=yC52><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0020025511002672" class=yC51>VisionGo: towards video retrieval with joint exploration of human and computer</a></h3><div class="gs_a">H Luan, YT Zheng, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, TS Chua - Information Sciences, 2011 - Elsevier</div><div class="gs_rs">Abstract This paper introduces an effective interactive video retrieval system named <br>VisionGo. It jointly explores human and computer to accomplish video retrieval with high <br>effectiveness and efficiency. It assists the interactive video retrieval process in different <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15423134762805473061&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 2</a> <a href="/scholar?q=related:Ja__-1n7CdYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15423134762805473061&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Ja__-1n7CdYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB46" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW46"><a href="http://staff.science.uva.nl/~xirong/pub/MappingQuery2SemanticConcepts.pdf" class=yC54><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4338364" class=yC53>Mapping query to semantic concepts: Leveraging semantic indices for automatic and interactive video retrieval</a></h3><div class="gs_a">D Wang, Z Wang, <a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, X Liu, J Li&hellip; - &hellip;  Computing, 2007. ICSC  &hellip;, 2007 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Quite recently, a few hundreds of semantic concepts are detected automatically with <br>varied performance and subsequently, a new video retrieval paradigm of query-by-concept <br>emerges. In this paper, we consider the problem of exploiting the potential of learned <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13310677909175952519&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:h2QSEAkHubgJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13310677909175952519&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'h2QSEAkHubgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB47" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW47"><a href="http://www.scholarpedia.org/article/Multimedia_Question_Answering" class=yC56><span class="gs_ggsL"><span class=gs_ctg2>[HTML]</span> from scholarpedia.org</span><span class="gs_ggsS">scholarpedia.org <span class=gs_ctg2>[HTML]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[HTML]</span><span class="gs_ct2">[HTML]</span></span> <a href="http://www.scholarpedia.org/article/Multimedia_Question_Answering" class=yC55>Multimedia question answering</a></h3><div class="gs_a">C Tat-Seng, <a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, J Tang - Scholarpedia, 2010 - scholarpedia.org</div><div class="gs_rs">With the proliferation of text and multimedia information, users are now able to find answers <br>to almost any questions on the Web. Meanwhile, they are also bewildered by the huge <br>amount of information routinely presented to them. Question-answering (QA) is a natural <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13904536500342593696&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:oJhfFUfW9sAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'oJhfFUfW9sAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md47', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md47" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:oJhfFUfW9sAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/u4103h782u560305.pdf" class=yC57>Graph-based multi-space semantic correlation propagation for video retrieval</a></h3><div class="gs_a">B Feng, J Cao, X Bao, L Bao, Y Zhang, S Lin&hellip; - The Visual Computer, 2011 - Springer</div><div class="gs_rs">Abstract By introducing the concept detection results to the retrieval process, concept-based <br>video retrieval (CBVR) has been successfully used for semantic content-based video <br>retrieval application. However, how to select and fuse the appropriate concepts for a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4196193837562780924&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 3</a> <a href="/scholar?q=related:_NycGBDgOzoJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4196193837562780924&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'_NycGBDgOzoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB49" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW49"><a href="http://libque.cityu.edu.hk/bitstream/2031/5752/1/abstract.html" class=yC59><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://libque.cityu.edu.hk/handle/2031/5752" class=yC58>Large scale semantic concept detection, fusion, and selection for domain adaptive video search</a></h3><div class="gs_a"><a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">Y Jiang</a> - 2009 - libque.cityu.edu.hk</div><div class="gs_rs">ï»¿ This thesis investigates the problem of video search based on semantic concepts. We <br>present approaches to handle three correlated issues that are critical to this problem:(1) how <br>to construct an eÂ® ective feature representation for semantic concept detection,(2) how to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=983719958947520362&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:al-HNxfgpg0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=983719958947520362&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'al-HNxfgpg0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md49', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md49" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:al-HNxfgpg0J:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=3706111295001372009&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/J4L3860L82K58081.pdf" class=yC5A>Knowledge-Based Concept Score Fusion for Multimedia Retrieval</a></h3><div class="gs_a">M Falelakis, L Karydas, A Delopoulos - Active Media Technology, 2009 - Springer</div><div class="gs_rs">Abstract. Automated detection of semantic concepts in multimedia documents has been <br>attracting intensive research efforts over the last years. These efforts can be generally <br>classified in two categories of methodologies: the ones that attempt to solve the problem <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14970484488893499876&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:5An-W1PYwc8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14970484488893499876&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'5An-W1PYwc8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/y3267476x3739220.pdf" class=yC5B>CLOVIS: towards precision-oriented text-based video retrieval through the unification of automatically-extracted concepts and relations of the visual and audio/speech  &hellip;</a></h3><div class="gs_a">M Belkhatir - Journal of Intelligent Information Systems, 2010 - Springer</div><div class="gs_rs">Abstract Traditional multimedia (video) retrieval systems use the keyword-based approach <br>in order to make the search process fast although this approach has several shortcomings <br>and limitations related to the way the user is able to formulate her/his information need. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3821138729793874310&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=67">Cited by 1</a> <a href="/scholar?q=related:hunZ8m1pBzUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3821138729793874310&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'hunZ8m1pBzUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB52" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW52"><a href="ftp://ftp.cg.cs.uni-bonn.de/pub/outgoing/IGD-Best-Paper-Award/2009/Articles/SR2008_g03a04.pdf" class=yC5D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-bonn.de</span><span class="gs_ggsS">uni-bonn.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="ftp://ftp.cg.cs.uni-bonn.de/pub/outgoing/IGD-Best-Paper-Award/2009/Articles/SR2008_g03a04.pdf" class=yC5C>Semantic Middleware to Enhance Multimedia Retrieval in a Broadcaster</a></h3><div class="gs_a">D Duke - cg.cs.uni-bonn.de</div><div class="gs_rs">Abstract: The digitalization of video and recent progress in semantic multimedia indexing <br>and retrieval transform the workflow and tools involved in the information retrieval process of <br>the broadcasters. To this end, we present in this paper a theoretical framework which <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:KC0wAWZ0Ma0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12479884023875775784&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'KC0wAWZ0Ma0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://mcg.ict.ac.cn/download/pdf/paper/2008/29.VisionGo%20A%20High-performance%20and%20Multifunctional.pdf" class=yC5F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ict.ac.cn</span><span class="gs_ggsS">ict.ac.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mcg.ict.ac.cn/download/pdf/paper/2008/29.VisionGo%20A%20High-performance%20and%20Multifunctional.pdf" class=yC5E>VisionGo: A High-performance and Multi-functional Interactive Video Retrieval System</a></h3><div class="gs_a">H Luan, S Lin, Y Zhang, SY Neo, TS Chua - mcg.ict.ac.cn</div><div class="gs_rs">Abstract One of the most critical tasks in multimedia retrieval is the interactive video search. <br>This paper proposes an effective interactive video retrieval system named VisionGo to <br>improve the overall search performance. VisionGo provides three key functions to assist <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'D5KOBLqaupIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md53', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md53" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:D5KOBLqaupIJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6398229" class=yC60>Video retrieval using singilar value decomposition and latent semantic indexing</a></h3><div class="gs_a">KS Thakare, AM Rajurkar, <a href="/citations?user=-GyZqa4AAAAJ&amp;hl=en&amp;oi=sra">R Manthalkar</a>&hellip; - Communication,  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Similarity matching algorithm plays an important role in Video retrieval system. Most <br>of the video retrieval systems are designed using traditional similarity matching algorithms <br>that are based on distance measures. As the Accuracy of retrieval system depends on the <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'Crz2Ha93vbcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB55" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW55"><a href="http://vision.eecs.ucf.edu/papers/theses/aytar_thesis.pdf" class=yC62><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ucf.edu</span><span class="gs_ggsS">ucf.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://vision.eecs.ucf.edu/papers/theses/aytar_thesis.pdf" class=yC61>Semantic Video Retrieval Using High Level Context</a></h3><div class="gs_a"><a href="/citations?user=0ncQNL8AAAAJ&amp;hl=en&amp;oi=sra">Y Aytar</a> - 2008 - vision.eecs.ucf.edu</div><div class="gs_rs">ABSTRACT Video retrievalâsearching and retrieving videos relevant to a user defined queryâ<br>is one of the most popular topics in both real life applications and multimedia research. This <br>thesis employs concepts from Natural Language Understanding in solving the video <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gp10J_5kIzAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3468727180751838594&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'gp10J_5kIzAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md55', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md55" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:gp10J_5kIzAJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a>  <a href="/scholar?q=info:gp10J_5kIzAJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=1924184434357700966&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6324438" class=yC63>Coaching the Exploration and Exploitation in Active Learning for Interactive Video Retrieval</a></h3><div class="gs_a">X Wei, Z Yang - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Conventional active learning approaches for interactive video/image retrieval <br>usually assume the query distribution is unknown, because it is difficult to estimate with only <br>a limited number of labeled instances available. It is thus easy to put the system in a <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'fTjIOPdSDJ0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB57" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW57"><a href="http://mpac.ee.ntu.edu.tw/~yihsuan/pub/TCSVT08_2_5.pdf" class=yC65><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mpac.ee.ntu.edu.tw/~yihsuan/pub/TCSVT08_2_5.pdf" class=yC64>Online Reranking via Ordinal Informative Concepts for Context Fusion in Video Detection and Search</a></h3><div class="gs_a"><a href="/citations?user=OL-XGxcAAAAJ&amp;hl=en&amp;oi=sra">YH Yang</a>, <a href="/citations?user=NOvDH3QAAAAJ&amp;hl=en&amp;oi=sra">WH Hsu</a>, HH Chen - mpac.ee.ntu.edu.tw</div><div class="gs_rs">AbstractâTo exploit co-occurrence patterns among semantic concepts while keeping the <br>simplicity of context fusion, a novel reranking methods is proposed for video detection and <br>search. The approach, ordinal reranking, reranks an initial search (or detection) list by <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:KK2KVJhqq6oJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'KK2KVJhqq6oJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md57', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md57" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:KK2KVJhqq6oJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Tat-Seng CHUA et al.(2009), Scholarpedia, 5 (5): 9546. doi: 10.4249/scholarpedia. 9546 revision# 91536 [link to/cite this article]</h3><div class="gs_a"><a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a></div><div class="gs_fl"><a href="/scholar?q=related:1IxXY7WSOosJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'1IxXY7WSOosJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6173547" class=yC66>Known-item Search (KIS) in video: Survey, experience and trend</a></h3><div class="gs_a">L Chaisorn, YT Zheng, K Sim - &hellip; , Communications and Signal  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper provides a survey on the notable performers submitted to TRECVid <br>2010, under Known-item Search (KIS) task. It also gives an insight as well as the lessons <br>learnt discovered by the top ranked system. Most systems used multi-modal features <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:hM6ibDPRxxcJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'hM6ibDPRxxcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5466951" class=yC67>Developing context model supporting spatial relations for semantic video retrieval</a></h3><div class="gs_a">S Memar, M Ektefa, LS Affendey - Information Retrieval &amp;  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Video retrieval is one of the most famous issues in multimedia research. Users <br>express their needs in terms of queries and expect to retrieve most relevant answers. This <br>task is becoming harder due to large amount of video archives including broadcast news, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:fEuoc-96Hl8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'fEuoc-96Hl8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB61" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW61"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.4651&amp;rep=rep1&amp;type=pdf" class=yC69><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.4651&amp;rep=rep1&amp;type=pdf" class=yC68>Ontology-enriched Semantic Space for Video Retrieval</a></h3><div class="gs_a">X WEI, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW NGO</a> - Citeseer</div><div class="gs_rs">Abstract Multimedia-based ontology construction and reasoning have recently been <br>recognized as two important issues in video search, particularly for bridging semantic gap. <br>The lack of coincidence between low-level features and user expectation makes <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:-VFogG0FvD4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4520494093837029881&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'-VFogG0FvD4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md61', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md61" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:-VFogG0FvD4J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB62" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW62"><a href="http://staff.science.uva.nl/~xirong/pub/JVCIR09.pdf" class=yC6B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S104732030800120X" class=yC6A>Query representation by structured concept threads with application to interactive video retrieval</a></h3><div class="gs_a">D Wang, <a href="/citations?user=5vj1VV8AAAAJ&amp;hl=en&amp;oi=sra">Z Wang</a>, J Li, <a href="/citations?user=28IxoocAAAAJ&amp;hl=en&amp;oi=sra">B Zhang</a>, <a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a> - Journal of Visual Communication and &hellip;, 2009 - Elsevier</div><div class="gs_rs">In this paper, we provide a new formulation for video queries as structured combination of <br>concept threads, contributing to the general query-by-concept paradigm. Occupying a low-<br>dimensional region in the concept space, concept thread defines a ranked list of video <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:0yqPjnLI-iYJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2808777711962499795&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'0yqPjnLI-iYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/D2MN0RP5V6G39042.pdf" class=yC6C>A lexicon-guided LSI method for semantic news video retrieval</a></h3><div class="gs_a">J Cao, S Tang, J Li, Y Zhang, X Pan - Advances in Multimedia Information  &hellip;, 2007 - Springer</div><div class="gs_rs">Many researchers try to utilize the semantic information extracted from visual feature to <br>directly realize the semantic video retrieval or to supplement the automated speech <br>recognition (ASR) text retrieval. But bridging the gap between the low-level visual feature <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:pmRAwF-_UA8J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3F/57/RN221517554.html?source=googlescholar" class="gs_nph" class=yC6D>BL Direct</a> <a href="/scholar?cluster=1103592326674015398&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'pmRAwF-_UA8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB64" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW64"><a href="http://www.ehu.es/argitalpenak/images/stories/tesis/Ciencia_y_Tecnologia/A%20semantic%20middleware%20to%20enhance%20current%20multimedia%20retrieval%20systems%20with%20content-based%20functionalities.pdf" class=yC6F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ehu.es</span><span class="gs_ggsS">ehu.es <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ehu.es/argitalpenak/images/stories/tesis/Ciencia_y_Tecnologia/A%20semantic%20middleware%20to%20enhance%20current%20multimedia%20retrieval%20systems%20with%20content-based%20functionalities.pdf" class=yC6E>A Semantic Middleware to enhance current Multimedia Retrieval Systems with Content-based functionalities</a></h3><div class="gs_a">GM Ortego - 2011 - ehu.es</div><div class="gs_rs">Summary This work reviews the information retrieval theory and focuses on the revolution <br>experimented in that field promoted by the digitalization and the widespread use of the <br>multimedia information. After analyzing the trends and promising results in the main <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'Ogao67Xjx-0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md64', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md64" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Ogao67Xjx-0J:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:335"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB65" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW65"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/16051/thesis-sub.pdf?sequence=1" class=yC71><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/16051" class=yC70>combining multimodal external resources for event-based news video retrieval and question answering</a></h3><div class="gs_a">NEOSHI YONG - 2008 - scholarbank.nus.edu</div><div class="gs_rs">The ever-increasing amount of multimedia data available online creates an urgent need on <br>how to index these information and support effective retrieval by users. In recent years, we <br>observe the gradual shift from performing retrieval solely based on analyzing one media <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:h5EV9TPOgHUJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8466994022007017863&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'h5EV9TPOgHUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:334"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB66" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW66"><a href="http://staff.science.uva.nl/~cgmsnoek/pub/huurnink-archive-tmm.pdf" class=yC73><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6179334" class=yC72>Content-Based Analysis Improves Audiovisual Archive Retrieval</a></h3><div class="gs_a">B Huurnink, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=AVDkgFIAAAAJ&amp;hl=en&amp;oi=sra">M de Rijke</a>&hellip; - Multimedia, IEEE  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Content-based video retrieval is maturing to the point where it can be used in real-<br>world retrieval practices. One such practice is the audiovisual archive, whose users <br>increasingly require fine-grained access to broadcast television content. In this paper, we <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:SIIv_L3fq-YJ:scholar.google.com/&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16621624856813732424&amp;hl=en&amp;num=67&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'SIIv_L3fq-YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
