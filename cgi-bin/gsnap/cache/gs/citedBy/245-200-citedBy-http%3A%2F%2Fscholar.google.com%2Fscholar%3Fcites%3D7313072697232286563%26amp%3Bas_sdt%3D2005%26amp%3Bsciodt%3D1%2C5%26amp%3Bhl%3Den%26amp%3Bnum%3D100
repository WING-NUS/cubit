<div class="gs_r" style="z-index:400"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.ncbi.nlm.nih.gov/pubmed/22392706" class=yC0>Large-margin Predictive Latent Subspace Learning for Multi-view Data Analysis.</a></h3><div class="gs_a"><a href="/citations?user=cSxeVz0AAAAJ&amp;hl=en&amp;oi=sra">N Chen</a>, <a href="/citations?user=axsP38wAAAAJ&amp;hl=en&amp;oi=sra">J Zhu</a>, F Sun, EP Xing - IEEE transactions on pattern  &hellip;, 2012 - ncbi.nlm.nih.gov</div><div class="gs_rs">Learning from multi-view data is important in many applications such as image classification, <br>retrieval and annotation. Standard predictive methods, such as support vector machines that <br>are built with all the variables available without taking into consideration the presence of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16836103174697201945&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:GTGnyaLapekJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16836103174697201945&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'GTGnyaLapekJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0167865512000347" class=yC1>Image classification by multimodal subspace learning</a></h3><div class="gs_a">J Yu, F Lin, HS Seah, C Li, Z Lin - Pattern Recognition Letters, 2012 - Elsevier</div><div class="gs_rs">In recent years we witnessed a surge of interest in subspace learning for image <br>classification. However, the previous methods lack of high accuracy since they do not <br>consider multiple features of the images. For instance, we can represent a color image by <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MfHRyT_KKOcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16656785597147509041&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'MfHRyT_KKOcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/ft_gateway.cfm?id=2398545&amp;ftid=1313392&amp;dwn=1" class=yC2>Mining Noisy Tagging from Multi-label Space</a></h3><div class="gs_a">Z Qi, M Yang, ZM Zhang, Z Zhang - 2012 - dl.acm.org</div><div class="gs_rs">ABSTRACT In this paper we study the problem of mining noisy tagging. Most of the existing <br>discriminative classification methods to this problem only consider one tag at a time as the <br>classification target, and completely ignore the rest of the given tags at the same time. In <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'9yyCylZMQz0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0165168412002447" class=yC3>An improved method of locality sensitive hashing for indexing large-scale and high-dimensional features</a></h3><div class="gs_a">X Gu, L Zhang, Y Zhang, D Zhang, J Li - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract In recent years, Locality Sensitive Hashing (LSH) has been popularly used as an <br>effective and efficient index structure of multimedia signals. LSH is originally proposed for <br>resolving the high-dimensional approximate similarity search problem. Until now, many <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:QQqMyu7L2tkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'QQqMyu7L2tkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB204" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW204"><a href="http://www.ami-lab.org/uploads/Publications/Journal/WP4/28_Weakly-Supervised%20Graph%20Propagation%20Towards%20Collective%20Image%20Parsing.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ami-lab.org</span><span class="gs_ggsS">ami-lab.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6069864" class=yC4>Weakly Supervised Graph Propagation Towards Collective Image Parsing</a></h3><div class="gs_a">S Liu, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, <a href="/citations?user=9sCGe-gAAAAJ&amp;hl=en&amp;oi=sra">T Zhang</a>, C Xu, J Liu&hellip; - &hellip; , IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this work, we propose a weakly supervised graph propagation method to <br>automatically assign the annotated labels at image level to those contextually derived <br>semantic regions. The graph is constructed with the over-segmented patches of the image <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:cQDRytL_94kJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9941695983254569073&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'cQDRytL_94kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396346" class=yC6>Towards relevance and saliency ranking of image tags</a></h3><div class="gs_a">S Feng, C Lang, B Li - Proceedings of the 20th ACM international  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Social image tag ranking has emerged as an important research topic recently due <br>to its potential application on web image search. This paper presents an adaptive all-season <br>tag ranking algorithm which can handle the images with and without distinct object (s) <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'kikuzFQC9noJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB206" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW206"><a href="http://users.dsic.upv.es/grupos/nle/ceri/papers/ceri2012_villegas.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from upv.es</span><span class="gs_ggsS">upv.es <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://users.dsic.upv.es/grupos/nle/ceri/papers/ceri2012_villegas.pdf" class=yC7>Image-Text Dataset Generation for Image Annotation and Retrievalâ</a></h3><div class="gs_a"><a href="/citations?user=18e7rpsAAAAJ&amp;hl=en&amp;oi=sra">M Villegas</a>, <a href="/citations?user=I815O2UAAAAJ&amp;hl=en&amp;oi=sra">R Paredes</a> - Prometeo, 2009 - users.dsic.upv.es</div><div class="gs_rs">Abstract. This paper presents a new dataset of images gathered from the Web with <br>corresponding text obtained from the webpages near where the images appeared. Already <br>extracted features are provided to ease the dataset usage for other researchers. An initial <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2891833030373263547&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 2</a> <a href="/scholar?q=related:u8gjzdHaISgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2891833030373263547&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'u8gjzdHaISgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md206', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md206" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:u8gjzdHaISgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/T645L212605444J0.pdf" class=yC9>Quantifying Visual-Representativeness of Social Image Tags Using Image Tag Clarity</a></h3><div class="gs_a">A Sun, <a href="/citations?user=o0F3sqEAAAAJ&amp;hl=en&amp;oi=sra">SS Bhowmick</a> - Social Media Modeling and Computing, 2011 - Springer</div><div class="gs_rs">Tags associated with images in various social media sharing web sites are valuable <br>information source for superior image retrieval experiences. Due to the nature of tagging, <br>many tags associated with images are not visually descriptive. In this chapter, we propose <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:vYqFzYBN-qAJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11599669005845367485&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'vYqFzYBN-qAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231213000180" class=yCA>Han Wang, Wei Liang, Xinxiao Wu, Peng Teng</a></h3><div class="gs_a">H Wang, W Liang, <a href="/citations?user=iZHC_EQAAAAJ&amp;hl=en&amp;oi=sra">X Wu</a>, P Teng - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract In this paper, we propose a novel method for scene image retrieval in which the <br>semantic meaning of an image and a new low-level feature are combined. The fluid nature <br>of scene images makes learning semantics essential in our retrieval task. Compared to a <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'xnuZzvZ4j0MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320312000387" class=yCB>Image annotation by semi-supervised cross-domain learning with group sparsity</a></h3><div class="gs_a">Y Yuan, F Wu, <a href="/citations?user=VUN-9cQAAAAJ&amp;hl=en&amp;oi=sra">J Shao</a>, Y Zhuang - Journal of Visual Communication and  &hellip;, 2012 - Elsevier</div><div class="gs_rs">Abstract With the explosive growth of multimedia data in the web, multi-label image <br>annotation has been attracted more and more attention. Although the amount of available <br>data is large and growing, the number of labeled data is quite small. This paper proposes <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:99PNz5Umyn0J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'99PNz5Umyn0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB210" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW210"><a href="http://dare.uva.nl/document/355661" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dare.uva.nl/en/record/410580" class=yCC>Content-based visual search learned from social media</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a> - 2012 - dare.uva.nl</div><div class="gs_rs">Abstract In een wereld waarin de hoeveelheid digitale afbeeldingen alsmaar groeit is het <br>belangrijk te kunnen zoeken op basis van beeldinhoud. Xirong Li liet zich inspireren door <br>sociale media en onderzocht de waarde van beelden met social tags voor visueel zoeken. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:xTQX0HIOAtIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15132673584198530245&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'xTQX0HIOAtIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6274248" class=yCE>Nonnegative Matrix Factorization for Multimodality Data from Multi-source Domain</a></h3><div class="gs_a">X Tan, S Ma, <a href="/citations?user=VUN-9cQAAAAJ&amp;hl=en&amp;oi=sra">J Shao</a>, Y Zhuang&hellip; - &hellip;  Information Hiding and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract With the growing popularity of social tagging, more and more images are annotated <br>by users on web sites (eg, Flickr, Blogspace and Youtube). Since the tags annotated by <br>users are often noisy, ambiguous, and subjective, it is beneficial to fully utilize the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:XoE90OdGoeMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16402469279305138526&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'XoE90OdGoeMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB212" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW212"><a href="http://hal.archives-ouvertes.fr/docs/00/74/43/20/PDF/HDR_HuetB_03102012.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://hal.archives-ouvertes.fr/docs/00/74/43/20/PDF/HDR_HuetB_03102012.pdf" class=yCF>Ãtude de Contenus MultimÃ©dia: Apporter du Contexte au Contenu</a></h3><div class="gs_a">H Benoit - 2012 - hal.archives-ouvertes.fr</div><div class="gs_rs">AbstractâIn this paper, we propose a framework to extend semantic labeling of images to <br>video shot sequences and achieve efficient and semantic-aware spatiotemporal video <br>segmentation. This task faces two major challenges, namely the temporal variations within <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'IfYv0WzGylkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md212', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md212" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:IfYv0WzGylkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB213" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW213"><a href="http://c2inet.sce.ntu.edu.sg/ivor/publication/BAMIL.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6247949" class=yC11>Batch mode Adaptive Multiple Instance Learning for computer vision tasks</a></h3><div class="gs_a"><a href="/citations?user=yjG4Eg4AAAAJ&amp;hl=en&amp;oi=sra">W Li</a>, <a href="/citations?user=inRIcS0AAAAJ&amp;hl=en&amp;oi=sra">L Duan</a>, <a href="/citations?user=rJMOlVsAAAAJ&amp;hl=en&amp;oi=sra">IW Tsang</a>, D Xu - Computer Vision and Pattern  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Multiple Instance Learning (MIL) has been widely exploited in many computer <br>vision tasks, such as image retrieval, object tracking and so on. To handle ambiguity of <br>instance labels in positive bags, the training process of traditional MIL methods is usually <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=423684804886504463&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:D6T90xU74QUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=423684804886504463&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'D6T90xU74QUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB214" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW214"><a href="http://run.unl.pt/bitstream/10362/5963/1/Silva_2011.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unl.pt</span><span class="gs_ggsS">unl.pt <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://run.unl.pt/handle/10362/5963" class=yC13>Automated image tagging through tag propagation</a></h3><div class="gs_a">MM Silva - 2011 - run.unl.pt</div><div class="gs_rs">Today, more and more data is becoming available on the Web. In particular, we have <br>recently witnessed an exponential increase of multimedia content within various content <br>sharing websites. While this content is widely available, great challenges have arisen to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:G3mJ1Woz5mkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7630843152567990555&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'G3mJ1Woz5mkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB215" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW215"><a href="http://mklab.iti.gr/files/Nikolopoulos_SignalProcessing_2012_authors-accepted-manuscript.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from iti.gr</span><span class="gs_ggsS">iti.gr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S016516841200271X" class=yC15>High order pLSA for indexing tagged images</a></h3><div class="gs_a">S Nikolopoulos, <a href="/citations?user=QKOH5iYAAAAJ&amp;hl=en&amp;oi=sra">S Zafeiriou</a>, <a href="/citations?user=OBYLxRkAAAAJ&amp;hl=en&amp;oi=sra">I Patras</a>, <a href="/citations?user=Nr7smP8AAAAJ&amp;hl=en&amp;oi=sra">I Kompatsiaris</a> - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract This work presents a method for the efficient indexing of tagged images. Tagged <br>images are a common resource of social networks and occupy a large portion of the social <br>media stream. Their basic characteristic is the co-existence of two heterogeneous <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7EMH2tvAltIJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15174528044959745004&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'7EMH2tvAltIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/085q9684774100x5.pdf" class=yC17>Collaborative event annotation in tagged photo collections</a></h3><div class="gs_a">C Zigkolis, <a href="/citations?user=GuhyORoAAAAJ&amp;hl=en&amp;oi=sra">S Papadopoulos</a>, G Filippou&hellip; - Multimedia Tools and  &hellip;, 2012 - Springer</div><div class="gs_rs">Abstract Events constitute a significant means of multimedia content organization and <br>sharing. Despite the recent interest in detecting events and annotating media content in an <br>event-centric way, there is currently insufficient support for managing events in large-scale <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Mbc12qccT3kJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Mbc12qccT3kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/C83807257U850771.pdf" class=yC18>High dimensionality reduction using CUR matrix decomposition and auto-encoder for web image classification</a></h3><div class="gs_a"><a href="/citations?user=yQks8w4AAAAJ&amp;hl=en&amp;oi=sra">Y Liu</a>, <a href="/citations?user=VUN-9cQAAAAJ&amp;hl=en&amp;oi=sra">J Shao</a> - Advances in Multimedia Information Processing-PCM  &hellip;, 2011 - Springer</div><div class="gs_rs">Reducing the dimensionality of image with high-dimensional feature plays a significant role <br>in image retrieval and classification. Recently, two methods have been proposed to improve <br>the efficiency and accuracy of dimensionality reduction, one uses CUR matrix <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1UXr4KtWB9IJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15134160369014752725&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'1UXr4KtWB9IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB218" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW218"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/Image%20label%20completion%20by%20pursuing%20contextual%20decomposability.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2169001" class=yC19>Image label completion by pursuing contextual decomposability</a></h3><div class="gs_a">X Liu, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, TS Chua, <a href="/citations?user=o02W0aEAAAAJ&amp;hl=en&amp;oi=sra">H Jin</a> - ACM Transactions on Multimedia  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract This article investigates how to automatically complete the missing labels for the <br>partially annotated images, without image segmentation. The label completion procedure is <br>formulated as a nonnegative data factorization problem, to decompose the global image <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:OEHA4hYPTGMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7155110498952823096&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'OEHA4hYPTGMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Word2Image: A System for Visual Interpretation of Concepts</h3><div class="gs_a">H Li, J Tang, G Li, TS Chua</div><div class="gs_fl"><a href="/scholar?q=related:46Y940t-rnMJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'46Y940t-rnMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/V2522J1U50407572.pdf" class=yC1B>Re-ranking by multi-modal relevance feedback for content-based social image retrieval</a></h3><div class="gs_a">J Li, Q Ma, Y Asano, M Yoshikawa - Web Technologies and Applications, 2012 - Springer</div><div class="gs_rs">With the recent rapid growth of social image hosting websites, it is becoming increasingly <br>easy to construct a large database of tagged images. In this paper, we investigate whether <br>and how social tags can be used for improving content-based image search results, which <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:I0Fq44WAURcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1680265448520302883&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'I0Fq44WAURcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB221" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW221"><a href="http://ylu.cc/cikm2012.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ylu.cc</span><span class="gs_ggsS">ylu.cc <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2398532" class=yC1C>Semantic context learning with large-scale weakly-labeled image set</a></h3><div class="gs_a">Y Lu, W Zhang, K Zhang, X Xue - Proceedings of the 21st ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract There are a large number of images available on the web; meanwhile, only a <br>subset of web images can be labeled by professionals because manual annotation is time-<br>consuming and labor-intensive. Although we can now use the collaborative image tagging <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'GkUs5GLH-eIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB222" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW222"><a href="http://cs229.stanford.edu/proj2010/Calo-MultimodalDeepLearning.pdf" class=yC1F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from stanford.edu</span><span class="gs_ggsS">stanford.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://cs229.stanford.edu/proj2010/Calo-MultimodalDeepLearning.pdf" class=yC1E>Unsupervised Learning of Multimodal Features: Images and Text</a></h3><div class="gs_a">MC Caligaris - 2010 - cs229.stanford.edu</div><div class="gs_rs">Multimodal learning involves relating information from disparate sources. For example, <br>Wikipedia contains text, audio and images; YouTube contains audio, video and text; and <br>Flickr contains images and text. Our goal is to find meaningful representations of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:2b925BRvzKkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12235276423181811673&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'2b925BRvzKkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md222', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md222" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:2b925BRvzKkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB223" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW223"><a href="http://arxiv.org/pdf/1210.4855" class=yC21><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://arxiv.org/abs/1210.4855" class=yC20>A Slice Sampler for Restricted Hierarchical Beta Process with Applications to Shared Subspace Learning</a></h3><div class="gs_a"><a href="/citations?user=bXeL2t8AAAAJ&amp;hl=en&amp;oi=sra">SK Gupta</a>, <a href="/citations?user=OtA9SwIAAAAJ&amp;hl=en&amp;oi=sra">DQ Phung</a>, <a href="/citations?user=AEkRUQcAAAAJ&amp;hl=en&amp;oi=sra">S Venkatesh</a> - arXiv preprint arXiv:1210.4855, 2012 - arxiv.org</div><div class="gs_rs">Abstract: Hierarchical beta process has found interesting applications in recent years. In this <br>paper we present a modified hierarchical beta process prior with applications to hierarchical <br>modeling of multiple data sources. The novel use of the prior over a hierarchical factor <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'qP9l5_JTos0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB224" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW224"><a href="http://hub.hku.hk/bitstream/10722/130770/1/FullText.pdf?accept=1" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hku.hk</span><span class="gs_ggsS">hku.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5497758" class=yC22>A survey of application of Web 2.0 in Chinese provincial and municipal libraries</a></h3><div class="gs_a">L Si, Y Tan, X Huang, W Xing - Future Computer and  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This paper selects all the provincial libraries and the libraries of their capital cities <br>for a total of 50 to investigate the application of Web2. 0 technologies. The investigation <br>includes RSS, Blog, Wiki, IM, Tag, Ajax, Toolbar and SNS and so on, and the paper <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:peXI56GQaxEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1255255946205652389&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'peXI56GQaxEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md224', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md224" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:peXI56GQaxEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=10620909287064965900&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2348283.2348473" class=yC24>Exploring tag relevance for image tag re-ranking</a></h3><div class="gs_a">J Xiao, W Zhou, Q Tian - Proceedings of the 35th international ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we propose to explore the relevance between tags for image tag re-<br>ranking. The key component is to define a global tag-tag similarity matrix, which is achieved <br>by analysis in both semantic and visual aspects. The text semantic relevance is explored <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2432906584015611835&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:u6f156VrwyEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'u6f156VrwyEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB226" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW226"><a href="http://www.willfulwreckords.com/GinsuScience/CVPR2012/data/papers/142_P1C-34.pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from willfulwreckords.com</span><span class="gs_ggsS">willfulwreckords.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0031320312003275" class=yC25>Image collection summarization via dictionary learning for sparse representation</a></h3><div class="gs_a">C Yang, <a href="/citations?user=d3h-zScAAAAJ&amp;hl=en&amp;oi=sra">J Shen</a>, J Peng, J Fan - Pattern Recognition, 2012 - Elsevier</div><div class="gs_rs">Abstract In this paper, a novel approach is developed to achieve automatic image collection <br>summarization. The effectiveness of the summary is reflected by its ability to reconstruct the <br>original set or each individual image in the set. We have leveraged the dictionary learning <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:F25n6dDUhQ8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1118534076205592087&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'F25n6dDUhQ8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB227" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW227"><a href="http://ylu.cc/mm11.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ylu.cc</span><span class="gs_ggsS">ylu.cc <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2071970" class=yC27>Automatic image annotation with weakly labeled dataset</a></h3><div class="gs_a">W Zhang, Y Lu, X Xue, J Fan - Proceedings of the 19th ACM international &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract It is very attractive to exploit weakly-labeled image dataset for multi-label annotation <br>applications. In our paper the meaning of the terminology weakly labeled is threefold: i) only <br>a small subset of the available images are labeled; ii) even for the labeled image, the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4370472051892388543&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:v4LR6SwJpzwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4370472051892388543&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'v4LR6SwJpzwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2379792" class=yC29>Label-to-region with continuity-biased bi-layer sparsity priors</a></h3><div class="gs_a">X Liu, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, B Cheng, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, TS Chua&hellip; - ACM Transactions on  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In this work, we investigate how to reassign the fully annotated labels at image level <br>to those contextually derived semantic regions, namely Label-to-Region (L2R), in a <br>collective manner. Given a set of input images with label annotations, the basic idea of our <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'ZIYQ6xvd2r4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0957417412004824" class=yC2A>Towards a universal detector by mining concepts with small semantic gaps</a></h3><div class="gs_a">C Lang, <a href="/citations?user=Q8iay0gAAAAJ&amp;hl=en&amp;oi=sra">J Feng</a>, Y Zheng - Expert Systems with Applications, 2012 - Elsevier</div><div class="gs_rs">Can we have a universal detector that could visually recognize unseen objects with no <br>training exemplars available? Such a detector is so desirable, as there are hundreds of <br>thousands of object concepts in human vocabulary but few labeled image examples <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:cWPc7fxEgDwJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4359560292407731057&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'cWPc7fxEgDwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6378284" class=yC2B>Transfer clustering via constraints generated from topics</a></h3><div class="gs_a">L Yu, Y Dang, G Yang - Systems, Man, and Cybernetics (SMC),  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Clustering technique is widely used in data mining like gene-microarray analysis <br>and natural language processing. When there are sufficient data samples and good <br>representations, traditional clustering algorithms such as K-means can work well. But <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'sJrq7yoGrC8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB231" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW231"><a href="http://home.postech.ac.kr/~kshkawa/publications/eccv2012.pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from postech.ac.kr</span><span class="gs_ggsS">postech.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://home.postech.ac.kr/~kshkawa/publications/eccv2012.pdf" class=yC2C>Sequential Spectral Learning to Hash with Multiple Representations</a></h3><div class="gs_a">S Kim, Y Kang, S Choi - 2012 - postech.ac.kr</div><div class="gs_rs">Abstract. Learning to hash involves learning hash functions from a set of images for <br>embedding high-dimensional visual descriptors into a similarity-preserving low-dimensional <br>Hamming space. Most of existing methods resort to a single representation of images, that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17825690930564531079&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:h4Mz8HSTYfcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'h4Mz8HSTYfcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md231', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md231" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:h4Mz8HSTYfcJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB232" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW232"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/31626/ZhangDX.pdf?sequence=1" class=yC2F><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/31626" class=yC2E>Efficient location-based spatial keyword query processing</a></h3><div class="gs_a">Z DONGXIANG - 2011 - scholarbank.nus.edu</div><div class="gs_rs">The emergence of Web $2.0 $ applications, including social networking sites, wikipedia and <br>multimedia sharing sites, has changed the way of how information is generated and shared. <br>Among these applications, map mashup is a popular and convenient means for data <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:2jHk8pyZ734J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9146698267581755866&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'2jHk8pyZ734J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212001671" class=yC30>Nearest-neighbor method using multiple neighborhood similarities for social media data mining</a></h3><div class="gs_a">S Wang, Q Huang, <a href="/citations?user=4Rvn-ykAAAAJ&amp;hl=en&amp;oi=sra">S Jiang</a>, <a href="/citations?user=61b6eYkAAAAJ&amp;hl=en&amp;oi=sra">Q Tian</a>, L Qin - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Currently, Nearest-Neighbor approaches (NN) have been applied to large scale real world <br>image data mining. However, the following three disadvantages prevent them from wider <br>application compared to other machine learning methods:(i) the performance is inferior on <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:X7E087Cr2VUJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6186164339652997471&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'X7E087Cr2VUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB234" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW234"><a href="http://www.sintelnet.eu/wiki/reports/sintelnet-report.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from sintelnet.eu</span><span class="gs_ggsS">sintelnet.eu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.sintelnet.eu/wiki/reports/sintelnet-report.pdf" class=yC31>SINTELNET STSM Report Collective Decision Making for Shared Online Cultural Experiences</a></h3><div class="gs_a"><a href="/citations?user=zZPB9jUAAAAJ&amp;hl=en&amp;oi=sra">R Confalonieri</a> - 2012 - sintelnet.eu</div><div class="gs_rs">Visiting a museum with friends can be considered one of the preferred social experiences of <br>people visiting a city. In such a social experience, users can share opinions with other users <br>and together decide which cultural artefacts to see. Unfortunately, museums have been <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lHqK8wvWTpgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'lHqK8wvWTpgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md234', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md234" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:lHqK8wvWTpgJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB235" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW235"><a href="http://www.ntu.edu.sg/home/asahtan/Papers/2012/PF-ART%20IJCNN%202012.pdf" class=yC34><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.sg</span><span class="gs_ggsS">ntu.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6252397" class=yC33>Semi-supervised hierarchical clustering for personalized web image organization</a></h3><div class="gs_a">L Meng, <a href="/citations?user=G0hdDqYAAAAJ&amp;hl=en&amp;oi=sra">AH Tan</a> - &hellip; , The 2012 International Joint Conference on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Existing efforts on web image organization usually transform the task into <br>surrounding text clustering. However, Current text clustering algorithms do not address the <br>problem of insufficient statistical information for image representation and noisy tags <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:2-R99AYYTsQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14145269897768068315&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'2-R99AYYTsQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB236" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW236"><a href="http://siam.omnibooksonline.com/2012datamining/data/papers/025.pdf" class=yC36><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from omnibooksonline.com</span><span class="gs_ggsS">omnibooksonline.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://siam.omnibooksonline.com/2012datamining/data/papers/025.pdf" class=yC35>A Bayesian Nonparametric Joint Factor Model for Learning Shared and Individual Subspaces from Multiple Data Sources</a></h3><div class="gs_a"><a href="/citations?user=bXeL2t8AAAAJ&amp;hl=en&amp;oi=sra">SK Gupta</a>, <a href="/citations?user=OtA9SwIAAAAJ&amp;hl=en&amp;oi=sra">D Phung</a>, <a href="/citations?user=AEkRUQcAAAAJ&amp;hl=en&amp;oi=sra">S Venkatesh</a> - siam.omnibooksonline.com</div><div class="gs_rs">Abstract Joint analysis of multiple data sources is becoming increasingly popular in transfer <br>learning, multi-task learning and cross-domain data mining. One promising approach to <br>model the data jointly is through learning the shared and individual factor subspaces. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1062990438375069630&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=100">Cited by 1</a> <a href="/scholar?q=related:vpf79iuAwA4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'vpf79iuAwA4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md236', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md236" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:vpf79iuAwA4J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB237" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW237"><a href="http://arxiv.org/pdf/1207.3809" class=yC38><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://arxiv.org/abs/1207.3809" class=yC37>Image labeling on a network: Using social-network metadata for image classification</a></h3><div class="gs_a"><a href="/citations?user=icbo4M0AAAAJ&amp;hl=en&amp;oi=sra">J McAuley</a>, <a href="/citations?user=Q_kKkIUAAAAJ&amp;hl=en&amp;oi=sra">J Leskovec</a> - arXiv preprint arXiv:1207.3809, 2012 - arxiv.org</div><div class="gs_rs">Abstract: Large-scale image retrieval benchmarks invariably consist of images from the Web. <br>Many of these benchmarks are derived from online photo sharing networks, like Flickr, which <br>in addition to hosting images also provide a highly interactive social community. Such <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lHbc-VGTpH8J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9197638319291004564&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'lHbc-VGTpH8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB238" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW238"><a href="http://books.nips.cc/papers/files/nips25/NIPS2012_0669.pdf" class=yC3A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nips.cc</span><span class="gs_ggsS">nips.cc <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://books.nips.cc/papers/files/nips25/NIPS2012_0669.pdf" class=yC39>Co-Regularized Hashing for Multimodal Data</a></h3><div class="gs_a"><a href="/citations?user=j88o8jQAAAAJ&amp;hl=en&amp;oi=sra">Y Zhen</a>, <a href="/citations?user=nEsOOx8AAAAJ&amp;hl=en&amp;oi=sra">DY Yeung</a> - &hellip;  in Neural Information Processing Systems 25, 2012 - books.nips.cc</div><div class="gs_rs">Abstract Hashing-based methods provide a very promising approach to large-scale <br>similarity search. To obtain compact hash codes, a recent trend seeks to learn the hash <br>functions from data automatically. In this paper, we study hash function learning in the <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'ySZQ-uHQe-QJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md238', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md238" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:ySZQ-uHQe-QJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/91657a/201204/43021747.html" class=yC3B>åºäºè§è§æ¾èæ§è¿é»æç¥¨çæ ç­¾æåºæ¹æ³</a></h3><div class="gs_a">ææ¼åï¼ èµµæ¥éï¼ åéè¾ï¼ ä¾¯è¿å¤ - åäº¬çå·¥å¤§å­¦å­¦æ¥: èªç¶ç§å­¦ç, 2012 - cqvip.com</div><div class="gs_rs">éå¯¹ç¤¾äº¤å¾åçæ ç­¾æ åºæ§é®é¢, æåºäºä¸ä¸ªæ°çæ ç­¾æåºæ¹æ³. æ ¹æ®æ ç­¾ä¸å¾ååå®¹çç¸å³æ§, <br>å°ç¤¾äº¤å¾åçæ ç­¾æ¬¡åºè¿è¡éæ. åºäºå¾åæ¾èæ§åºåçè§è§åå®¹, ç¨k è¿é»ç®æ³æ¾å°ç»å®å¾å<br>çè§è§è¿é», ç¨è¿é»å¾åçæ ç­¾åè¡¨å¯¹ç»å®å¾åçæ ç­¾è¿è¡æç¥¨å­¦ä¹ æ¯ä¸ªæ ç­¾çç¸å³æ§, æ<b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'92RnBxTqy2wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB240" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW240"><a href="http://www.ci.i.u-tokyo.ac.jp/~nakayama/pdf/nakayama_PhDjp_1.0.pdf" class=yC3D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ci.i.u-tokyo.ac.jp/~nakayama/pdf/nakayama_PhDjp_1.0.pdf" class=yC3C>Linear Distance Metric Learning for Large-scale Generic Image Recognition</a></h3><div class="gs_a">ä¸­å±±è±æ¨¹ - ci.iu-tokyo.ac.jp</div><div class="gs_rs">Abstract å¶ç´ã®ãªãå®ä¸çã®ç»åãè¨ç®æ©ã«èªè­ãã, è¨èªã«ããè¨è¿°ãããæè¡ãä¸è¬ç»åèªè­ <br>(generic image recognition) ã¨å¼ã¶. ä¸è¬ç»åèªè­ã¯æ±ãç»åãèªè­å¯¾è±¡ãå¤ç¨®å¤æ§ã§ãããã, <br>æ¥µãã¦é£ããã¿ã¹ã¯ã§ããã¨èªç¥ããã¦ãã. æ±ç¨æ§ã®é«ãä¸è¬ç»åèªè­ãå®ç¾ããããã«ã¯, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:X-23D6_1adEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15089862158713351519&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'X-23D6_1adEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md240', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md240" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:X-23D6_1adEJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/97390x/201012/36056173.html" class=yC3E>å±é¨æ ·æ¡åµå¥çæ­£äº¤åçç£å­ç©ºé´å­¦ä¹ ç®æ³</a></h3><div class="gs_a">æ±ç§ï¼ éµå¥ï¼ é­åå¼º - è®¡ç®æºè¾å©è®¾è®¡ä¸å¾å½¢å­¦å­¦æ¥, 2010 - cqvip.com</div><div class="gs_rs">ä¸ºäºæ´å åç¡®å°å¯¹å¾åè¿è¡èç±»ä¸åç±», æåºä¸ç§åºäºå±é¨æ ·æ¡åµå¥çæ­£äº¤åçç£å­ç©ºé´å­¦ä¹ <br>ç®æ³. éè¿å­¦ä¹ ä¸ä¸ªæ­£äº¤æå½±ç©éµ, ä½¿å¾è®­ç»æ ·æ¬ä¸­çæ æ³¨æ°æ®ç»è¿æå½±ç©éµéç»´åç±»é´ç¦»æ£åº¦<br>å°½éå¤§, ç±»åç¦»æ£åº¦å°½éå°; éç¨å±é¨æ ·æ¡åå½å°å±é¨ä½ç»´åµå¥åæ æ å°æå¨å±ä½ç»´åµå¥åæ , <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:PeapdV9I0CkJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3012987725544023613&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'PeapdV9I0CkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB242" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW242"><a href="http://hal.archives-ouvertes.fr/docs/00/66/65/31/PDF/thesis.pdf" class=yC40><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/tel-00666531/" class=yC3F>Une reprÃ©sentation visuelle avancÃ©e pour l&#39;apprentissage sÃ©mantique dans les bases d&#39;images</a></h3><div class="gs_a">I El Sayad - 2011 - hal.archives-ouvertes.fr</div><div class="gs_rs">RÃ©sumÃ© Avec l&#39;augmentation exponentielle de nombre d&#39;images disponibles sur Internet, le <br>besoin en outils efficaces d&#39;indexation et de recherche d&#39;images est devenu important. Dans <br>cette these, nous nous baserons sur le contenu visuel des images comme source <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:0iRtduNlN_sJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18102049254857843922&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'0iRtduNlN_sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB243" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW243"><a href="http://www.jos.org.cn/ch/reader/create_pdf.aspx?file_no=4154" class=yC42><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from jos.org.cn</span><span class="gs_ggsS">jos.org.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.jos.org.cn/ch/reader/create_pdf.aspx?file_no=4154" class=yC41>ç»åç»ç¨çæåºåå¤æ ¸å­¦ä¹ çå¾åæ æ³¨</a></h3><div class="gs_a">è¢è¹ï¼ éµå¥ï¼ å´é£ï¼ åºè¶æº - jos.org.cn</div><div class="gs_rs">æè¦: å¾åä¸­å­å¨ççº¹ç, é¢è²åå½¢ç¶ç­å¼æè§è§ç¹å¾, å¨è¡¨ç¤ºç¹å®é«å±è¯­ä¹æ¶æèµ·ä½ç¨çéè¦<br>ç¨åº¦ä¸å. ä¸ºäºå¨å¾åæ æ³¨è¿ç¨ä¸­æ´å ææå°å©ç¨è¿äºå¼æç¹å¾, æåºäºä¸ç§åºäºç»ç¨ç(group <br>sparsity) çå¤æ ¸å­¦ä¹ æ¹æ³(multiple kernel learning with group sparsity, ç®ç§°MKLGS), ä¸ºä¸å<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:GgRH2H52a20J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7884525859808674842&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'GgRH2H52a20J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md243', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md243" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:GgRH2H52a20J:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.dbpia.co.kr/view/ar_view.asp?arid=1808369" class=yC43>Visually Weighted Neighbor Voting ì ì´ì©í ì´ë¯¸ì§ íê·¸ ì ì  ê¸°ì </a></h3><div class="gs_a">ë¸ì©ë§ - 2011 ëë íêµ­ë°©ì¡ê³µíí íê³ íì ëí, 16~ 17 ìª½ (ì´ &hellip;, 2011 - dbpia.co.kr</div><div class="gs_rs">&amp;nbsp; &amp;nbsp; ì¨ë¼ì¸ì íµí ì´ë¯¸ì§ ê³µì ë ì¬ì©ìë¤ì´ íë°íê² ì´ì©íê³  ìë ë¶ì¼ ì¤ <br>íëì´ë¤. ì¬ì©ìì íë°í ì°¸ì¬ë¡ ê±°ëí´ì§ ì´ë¯¸ì§ ë°ì´í° ë² ì´ì¤ ë´ìì í¨ì¨ì ì¼ë¡ ì´ë¯¸ì§ <br>ê²ìì ìííê¸° ìí´ìë ì´ë¯¸ì§ë¥¼ ì ííê¸° íííê³  ìë íê·¸ì ì¡´ì¬ê° ë§¤ì° ì¤ìíë¤. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:OqzT8_6YCEQJ:scholar.google.com/&amp;hl=en&amp;num=100&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'OqzT8_6YCEQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
