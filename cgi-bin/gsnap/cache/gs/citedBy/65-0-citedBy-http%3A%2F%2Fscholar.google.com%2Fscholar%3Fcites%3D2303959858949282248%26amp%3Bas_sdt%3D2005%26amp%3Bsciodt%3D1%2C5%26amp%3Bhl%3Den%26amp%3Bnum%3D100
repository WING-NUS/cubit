Total results = 65
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://posgrado.escom.ipn.mx/biblioteca/Enabling%20context-aware%20multimedia%20annotation.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ipn.mx</span><span class="gs_ggsS">ipn.mx <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/KN92617280296473.pdf" class=yC0>Enabling context-aware multimedia annotation by a novel generic semantic problem-solving platform</a></h3><div class="gs_a"><a href="/citations?user=EczUsIYAAAAJ&amp;hl=en&amp;oi=sra">R Verborgh</a>, <a href="/citations?user=5Hf43N0AAAAJ&amp;hl=en&amp;oi=sra">D Van Deursen</a>, E Mannens&hellip; - Multimedia Tools and  &hellip;, 2012 - Springer</div><div class="gs_rs">Abstract Automatic generation of metadata, facilitating the retrieval of multimedia items, <br>potentially saves large amounts of manual work. However, the high specialization degree of <br>feature extraction algorithms makes them unaware of the context they operate in, which <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13521293858532082291&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 12</a> <a href="/scholar?q=related:c8J5HSBJpbsJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13521293858532082291&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'c8J5HSBJpbsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.aviarampatzis.com/publications/ECIR11_2stage.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aviarampatzis.com</span><span class="gs_ggsS">aviarampatzis.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0306457312000489" class=yC2>Dynamic two-stage image retrieval from large multimedia databases</a></h3><div class="gs_a"><a href="/citations?user=D31o7oUAAAAJ&amp;hl=en&amp;oi=sra">A Arampatzis</a>, <a href="/citations?user=GDAL3kUAAAAJ&amp;hl=en&amp;oi=sra">K Zagoris</a>, <a href="/citations?user=n3DR7UEAAAAJ&amp;hl=en&amp;oi=sra">SA Chatzichristofis</a> - Information Processing &amp;  &hellip;, 2012 - Elsevier</div><div class="gs_rs">Abstract Content-based image retrieval (CBIR) with global features is notoriously noisy, <br>especially for image queries with low percentages of relevant images in a collection. <br>Moreover, CBIR typically ranks the whole collection, which is inefficient for large <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=662655960359230944&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 8</a> <a href="/scholar?q=related:4N2BxRU6MgkJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=662655960359230944&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'4N2BxRU6MgkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0950705111000566" class=yC4>Exploiting information extraction techniques for automatic semantic video indexing with an application to Turkish news videos</a></h3><div class="gs_a">D KÃ¼Ã§Ã¼k - Knowledge-Based Systems, 2011 - Elsevier</div><div class="gs_rs">Abstract This paper targets at the problem of automatic semantic indexing of news videos by <br>presenting a video annotation and retrieval system which is able to perform automatic <br>semantic annotation of news video archives and provide access to the archives via these <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4895903816447006865&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 5</a> <a href="/scholar?q=related:kZjN2ZS-8UMJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4895903816447006865&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'kZjN2ZS-8UMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874062" class=yC5>Portfolio theory of multimedia fusion</a></h3><div class="gs_a">X Wang, M Kankanhalli - Proceedings of the international conference on &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract The number of multimedia applications has been increasing over the past two <br>decades. Multimedia information fusion has therefore attracted significant attention with <br>many techniques having been proposed. However, the uncertainty and correlation among <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5054339921941813865&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 5</a> <a href="/scholar?q=related:adpqRWGfJEYJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5054339921941813865&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'adpqRWGfJEYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www.informed.unal.edu.co/jccaicedo/papers/neurocomp2012.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unal.edu.co</span><span class="gs_ggsS">unal.edu.co <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231211004048" class=yC6>Multimodal representation, indexing, automated annotation and retrieval of image collections via non-negative matrix factorization</a></h3><div class="gs_a"><a href="/citations?user=U50zLvkAAAAJ&amp;hl=en&amp;oi=sra">JC Caicedo</a>, J BenAbdallah, <a href="/citations?user=IUB__IwAAAAJ&amp;hl=en&amp;oi=sra">FA GonzÃ¡lez</a>, <a href="/citations?user=SGscZDgAAAAJ&amp;hl=en&amp;oi=sra">O Nasraoui</a> - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Massive image collections are increasingly available on the Web. These collections often <br>incorporate complementary non-visual data such as text descriptions, comments, user <br>ratings and tags. These additional data modalities may provide a semantic complement to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10635947461954377015&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 5</a> <a href="/scholar?q=related:NxWCb915mpMJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10635947461954377015&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'NxWCb915mpMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://ivylab.kaist.ac.kr/htm/publication/1.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5523909" class=yC8>Automatic face annotation in personal photo collections using context-based unsupervised clustering and face information fusion</a></h3><div class="gs_a">JY Choi, <a href="/citations?user=VpjWb7wAAAAJ&amp;hl=en&amp;oi=sra">W De Neve</a>, YM Ro&hellip; - Circuits and Systems  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, a novel face annotation framework is proposed that systematically <br>leverages context information such as situation awareness information with current face <br>recognition (FR) solutions. In particular, unsupervised situation and subject clustering <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14547637655562451106&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 4</a> <a href="/scholar?q=related:onjl_F6X48kJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14547637655562451106&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'onjl_F6X48kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/D041T268487GX850.pdf" class=yCA>Capturing the functionality of Web services with functional descriptions</a></h3><div class="gs_a"><a href="/citations?user=EczUsIYAAAAJ&amp;hl=en&amp;oi=sra">R Verborgh</a>, <a href="/citations?user=K3TsGbgAAAAJ&amp;hl=en&amp;oi=sra">T Steiner</a>, <a href="/citations?user=5Hf43N0AAAAJ&amp;hl=en&amp;oi=sra">D Van Deursen</a>&hellip; - Multimedia Tools and  &hellip;, 2012 - Springer</div><div class="gs_rs">Abstract Many have left their footprints on the field of semantic RESTful Web service <br>description. Albeit some of the propositions are even W3C Recommendations, none of the <br>proposed standards could gain significant adoption with Web service providers. Some <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10056398366222127623&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 4</a> <a href="/scholar?q=related:BxJ9cgiBj4sJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'BxJ9cgiBj4sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="https://biblio.ugent.be/publication/2003291/file/2003308.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ugent.be</span><span class="gs_ggsS">ugent.be <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://biblio.ugent.be/publication/2003291/file/2003308.pdf" class=yCB>Description and interaction of RESTful services for automatic discovery and execution</a></h3><div class="gs_a"><a href="/citations?user=EczUsIYAAAAJ&amp;hl=en&amp;oi=sra">R Verborgh</a>, <a href="/citations?user=K3TsGbgAAAAJ&amp;hl=en&amp;oi=sra">T Steiner</a>, <a href="/citations?user=5Hf43N0AAAAJ&amp;hl=en&amp;oi=sra">D Van Deursen</a>, J De Roo&hellip; - 2011 - biblio.ugent.be</div><div class="gs_rs">Abstract Many have left their footprints on the field of semantic RESTful Web service <br>description. Albeit some of the propositions are even W3C Recommendations, none of the <br>proposed standards could gain significant adoption with Web service providers. Some <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4244872591651239799&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 3</a> <a href="/scholar?q=related:d_cpviHR6DoJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'d_cpviHR6DoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:d_cpviHR6DoJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.5646&amp;rep=rep1&amp;type=pdf" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072330" class=yCD>Personalizing automated image annotation using cross-entropy</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, E Gavves, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>&hellip; - Proceedings of the 19th &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Annotating the increasing amounts of user-contributed images in a personalized <br>manner is in great demand. However, this demand is largely ignored by the mainstream of <br>automated image annotation research. In this paper we aim for personalizing automated <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15388403704339045842&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 3</a> <a href="/scholar?q=related:0h1vSaOXjtUJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15388403704339045842&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'0h1vSaOXjtUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://doc.utwente.nl/78025/1/thesis_E_vd_Broek.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utwente.nl</span><span class="gs_ggsS">utwente.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[BOOK]</span><span class="gs_ct2">[B]</span></span> <a href="http://doc.utwente.nl/78025/" class=yCF>Affective Signal Processing (ASP): Unraveling the mystery of emotions</a></h3><div class="gs_a"><a href="/citations?user=QCLRwy4AAAAJ&amp;hl=en&amp;oi=sra">EL Broek</a> - 2011 - doc.utwente.nl</div><div class="gs_rs">Slowly computers are being dressed and becoming huggable and tangible. They are being <br>personalized and are expected to understand more of their users&#39; feelings, emotions, and <br>moods: This we refer to as affective computing. The work and experiences from 50+ <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11391136544110654253&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 3</a> <a href="/scholar?q=related:LbOvg1VyFZ4J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11391136544110654253&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'LbOvg1VyFZ4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:LbOvg1VyFZ4J:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=16294528366297068662&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.willfulwreckords.com/GinsuScience/CVPR2012/data/papers/268_P2C-06.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from willfulwreckords.com</span><span class="gs_ggsS">willfulwreckords.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6247918" class=yC11>Multi-view latent variable discriminative models for action recognition</a></h3><div class="gs_a"><a href="/citations?user=dNHNpxoAAAAJ&amp;hl=en&amp;oi=sra">Y Song</a>, <a href="/citations?user=APgaFK0AAAAJ&amp;hl=en&amp;oi=sra">L Morency</a>, R Davis - Computer Vision and Pattern  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Many human action recognition tasks involve data that can be factorized into <br>multiple views such as body postures and hand shapes. These views often interact with <br>each other over time, providing important cues to understanding the action. We present <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1564290268764331832&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a href="/scholar?q=related:OF9MI7Z5tRUJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1564290268764331832&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'OF9MI7Z5tRUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0262885612001825" class=yC13>Fusion of Facial expressions and EEG for implicit affective tagging</a></h3><div class="gs_a">S Koelstra, <a href="/citations?user=OBYLxRkAAAAJ&amp;hl=en&amp;oi=sra">I Patras</a> - Image and Vision Computing, 2012 - Elsevier</div><div class="gs_rs">Abstract The explosion of user-generated, untagged multimedia data in recent years, <br>generates a strong need for efficient search and retrieval of this data. The predominant <br>method for content-based tagging is through slow, labour-intensive manual annotation. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9361641921240343587&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a onclick="return gs_ocit(event,'I6TvLbo764EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.185.3416&amp;rep=rep1&amp;type=pdf" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.185.3416&amp;rep=rep1&amp;type=pdf" class=yC14>IRIT@ TRECVid 2010: Hidden Markov Models for Context-aware Late Fusion of Multiple Audio Classifiers</a></h3><div class="gs_a"><a href="/citations?user=obEwWjZOI0cC&amp;hl=en&amp;oi=sra">H Bredin</a>, L Koenig, J Farinas - TREC Video Retrieval Evaluation  &hellip;, 2010 - Citeseer</div><div class="gs_rs">HervÃ© Bredin*, Lionel Koenigâ  &amp; JÃ©rÃ´me Farinasâ  * LIMSI-CNRS, BP 133, F-91403 Orsay <br>Cedex, France â  University of Toulouse, IRIT, 118 Route de Narbonne, F-31062 Toulouse, France  <br><b> ...</b> Abstract This notebook paper describes the four runs submitted by IRIT at TRECVid <b> ...</b> </div><div class="gs_fl"><a href="/scholar?cites=14487894073338849164&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a href="/scholar?q=related:jGfZSOZWD8kJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14487894073338849164&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'jGfZSOZWD8kJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md12', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md12" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jGfZSOZWD8kJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://kuscholarworks.ku.edu/dspace/bitstream/1808/10207/1/Quanz_ku_0099D_12347_DATA_1.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ku.edu</span><span class="gs_ggsS">ku.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://kuscholarworks.ku.edu/dspace/handle/1808/10207" class=yC16>Learning with Low-Quality Data: Multi-View Semi-Supervised Learning with Missing Views</a></h3><div class="gs_a">B Quanz - 2012 - kuscholarworks.ku.edu</div><div class="gs_rs">Abstract: The focus of this thesis is on learning approaches for what we call``low-quality <br>data&#39;&#39;and in particular data in which only small amounts of labeled target data is available. <br>The first part provides background discussion on low-quality data issues, followed by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16227181453464924552&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a onclick="return gs_ocit(event,'iHGVf46HMuEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6327978" class=yC18>Automatic Audio-Visual Fusion for Aggression Detection Using Meta-information</a></h3><div class="gs_a"><a href="/citations?user=gLYiYy4AAAAJ&amp;hl=en&amp;oi=sra">I Lefter</a>, <a href="/citations?user=zN6afwwAAAAJ&amp;hl=en&amp;oi=sra">GJ Burghouts</a>&hellip; - Advanced Video and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We propose a new method for audio-visual sensor fusion and apply it to automatic <br>aggression detection. While a variety of definitions of aggression exist, in this paper we see <br>it as any kind of behavior that has a disturbing effect on others. We have collected multi-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11707965815073984807&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a onclick="return gs_ocit(event,'J9HmmeAMe6IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www.ra.ethz.ch/CDstore/www2011/companion/p427.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ethz.ch</span><span class="gs_ggsS">ethz.ch <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1963355" class=yC19>Application of semantic web technologies for multimedia interpretation</a></h3><div class="gs_a"><a href="/citations?user=EczUsIYAAAAJ&amp;hl=en&amp;oi=sra">R Verborgh</a>, R Van de Walle - Proceedings of the 20th international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Despite numerous outstanding results, highly complex and specialized multimedia <br>algorithms have not been able to fulfill the promise of fully automated multimedia <br>interpretation. An essential problem is that they are insufficiently aware of the context they <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3463443934950338856&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a href="/scholar?q=related:KDEyouifEDAJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3463443934950338856&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'KDEyouifEDAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6290490" class=yC1B>Learning the fusion of audio and video aggression assessment by meta-information from human annotations</a></h3><div class="gs_a"><a href="/citations?user=gLYiYy4AAAAJ&amp;hl=en&amp;oi=sra">I Lefter</a>, <a href="/citations?user=zN6afwwAAAAJ&amp;hl=en&amp;oi=sra">GJ Burghouts</a>&hellip; - &hellip;  Fusion (FUSION), 2012  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The focus of this paper is finding a method to predict aggression using a multimodal <br>system, given multiple unimodal features. The mechanism underlying multimodal sensor <br>fusion is complex and not completely clear. We try to understand the process of fusion and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3833135276521496295&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 3</a> <a href="/scholar?q=related:5x4VpTkIMjUJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'5x4VpTkIMjUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6349792" class=yC1C>On comparing hard and soft fusion of dependent detectors</a></h3><div class="gs_a">A Soriano, L Vergara, G Safont&hellip; - Machine Learning for  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A detection problem, where we have a set of two types of different measurements or <br>modalities of one event, is considered. The optimal fusion rule to combine both modalities in <br>one detector needs the knowledge of the joint statistics of modalities. In many cases we do <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13158527931520282940&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a onclick="return gs_ocit(event,'PHFS1W97nLYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://clef2011.org/resources/proceedings/Granados_Clef2011.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clef2011.org</span><span class="gs_ggsS">clef2011.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://clef2011.org/resources/proceedings/Granados_Clef2011.pdf" class=yC1D>Multimodal information approaches for the Wikipedia collection at Image-CLEF 2011</a></h3><div class="gs_a"><a href="/citations?user=YIReZq0AAAAJ&amp;hl=en&amp;oi=sra">R Granados</a>, J Benavent, X Benavent, E de Ves&hellip; - Petras et al.[10] - clef2011.org</div><div class="gs_rs">Abstract. The main goal of this paper it is to present our experiments in ImageCLEF 2011 <br>Campaign (Wikipedia retrieval task). This edition we focused on applying different strategies <br>of merging multimodal information, textual and visual, following both early and late fusion <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17321663195040994613&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a href="/scholar?q=related:NVVg2-boYvAJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'NVVg2-boYvAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md18', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md18" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:NVVg2-boYvAJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB19" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW19"><a href="http://streammining.ijs.si/first/papers/grcar_lavrac_heterogeneous_infonets.pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ijs.si</span><span class="gs_ggsS">ijs.si <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/E8J6H72148H45188.pdf" class=yC1F>A methodology for mining document-enriched heterogeneous information networks</a></h3><div class="gs_a">M GrÄar, N LavraÄ - Discovery Science, 2011 - Springer</div><div class="gs_rs">Abstract. The paper presents a new methodology for mining heterogeneous information <br>networks, motivated by the fact that, in many real-life scenarios, documents are available in <br>heterogeneous information networks, such as interlinked multimedia objects containing <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5848620987311104839&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 2</a> <a href="/scholar?q=related:R6OS9sx5KlEJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5848620987311104839&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'R6OS9sx5KlEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://surface.syr.edu/cgi/viewcontent.cgi?article=1314&amp;context=eecs_etd" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from syr.edu</span><span class="gs_ggsS">syr.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://surface.syr.edu/eecs_etd/310/" class=yC21>Decision-Making with Heterogeneous Sensors-A Copula Based Approach</a></h3><div class="gs_a">SG Iyengar - 2011 - surface.syr.edu</div><div class="gs_rs">Abstract Statistical decision making has wide ranging applications, from communications <br>and signal processing to econometrics and finance. In contrast to the classical one source-<br>one receiver paradigm, several applications have been identified in the recent past that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14798559996368647608&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a href="/scholar?q=related:uPXbGukLX80J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14798559996368647608&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'uPXbGukLX80J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://staff.science.uva.nl/~xirong/pub/ICMR2012-geo-context.pdf" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2324801" class=yC23>Fusing concept detection and geo context for visual search</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>&hellip; - Proceedings of the 2nd  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Given the proliferation of geo-tagged images, the question of how to exploit geo <br>tags and the underlying geo context for visual search is emerging. Based on the observation <br>that the importance of geo context varies over concepts, we propose a concept-based <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MmFIG_rJF_kJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17949036915945136434&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'MmFIG_rJF_kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0167865513000068" class=yC25>I. Lefter, LJM Rothkrantz, GJ Burghouts</a></h3><div class="gs_a"><a href="/citations?user=gLYiYy4AAAAJ&amp;hl=en&amp;oi=sra">I Lefter</a>, <a href="/citations?user=YPXhqgUAAAAJ&amp;hl=en&amp;oi=sra">LJM Rothkrantz</a>, GJ Burghouts - Pattern Recognition Letters, 2013 - Elsevier</div><div class="gs_rs">Abstract Multimodal fusion is a complex topic. For surveillance applications audio-visual <br>fusion is very promising given the complementary nature of the two streams. However, <br>drawing the correct conclusion from multi-sensor data is not straightforward. In previous <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'nT9dHidwH1wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB23" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW23"><a href="http://arxiv.org/pdf/1207.1019" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://arxiv.org/abs/1207.1019" class=yC26>PAC-Bayesian Majority Vote for Late Classifier Fusion</a></h3><div class="gs_a"><a href="/citations?user=4dzYdBsAAAAJ&amp;hl=en&amp;oi=sra">E Morvant</a>, <a href="/citations?user=oPemAuMAAAAJ&amp;hl=en&amp;oi=sra">A Habrard</a>, S Ayache - arXiv preprint arXiv:1207.1019, 2012 - arxiv.org</div><div class="gs_rs">Abstract: A lot of attention has been devoted to multimedia indexing over the past few years. <br>In the literature, we often consider two kinds of fusion schemes: The early fusion and the late <br>fusion. In this paper we focus on late classifier fusion, where one combines the scores of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lJhmHx8B9Q4J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1077768919007533204&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'lJhmHx8B9Q4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB24" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW24"><a href="http://www.commit-nl.nl/sites/default/files/675955_survey_mm-search-optimization.pdf" class=yC29><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from commit-nl.nl</span><span class="gs_ggsS">commit-nl.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.commit-nl.nl/sites/default/files/675955_survey_mm-search-optimization.pdf" class=yC28>A Survey on Multimedia Search Optimization based on Multimodal Information Resources</a></h3><div class="gs_a"><a href="/citations?user=Fhmp7lQAAAAJ&amp;hl=en&amp;oi=sra">C Kofler</a>, M Larson, <a href="/citations?user=EoYbukgAAAAJ&amp;hl=en&amp;oi=sra">A Hanjalic</a> - commit-nl.nl</div><div class="gs_rs">Abstract. This survey constitutes a literature study that overviews the state-of-the-art in <br>multimedia search. Techniques that are covered include multimodal re-ranking, pseudo-<br>relevance feedback, query classification and query suggestion. Discussion of the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1DJ7H6YdmWsJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'1DJ7H6YdmWsJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md24', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md24" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:1DJ7H6YdmWsJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://www.teicrete.gr/mta/user_pages/staff/bios/CV_Potamitis_EN.pdf" class=yC2B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from teicrete.gr</span><span class="gs_ggsS">teicrete.gr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.teicrete.gr/mta/user_pages/staff/bios/CV_Potamitis_EN.pdf" class=yC2A>CURRENT OCCUPATION</a></h3><div class="gs_a">POF BIRTH - teicrete.gr</div><div class="gs_rs">11/1998â3/2002, Wire Communications Laboratory (WCL), Electrical and Computer <br>Engineering, Electrical and Computer Engineering Department, University of Patras, Greece <br>under the direction of Prof. Kokkinakis George. Grade 10/10. The title of my thesis is â<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lb9MMY71BxMJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'lb9MMY71BxMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md25', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md25" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:lb9MMY71BxMJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiter/schuessel/2012-mprss-schuessel-belief-fusion.pdf" class=yC2D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-ulm.de</span><span class="gs_ggsS">uni-ulm.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.100/institut/mitarbeiter/schuessel/2012-mprss-schuessel-belief-fusion.pdf" class=yC2C>Using the Transferable Belief Model for Multimodal Input Fusion in Companion Systems</a></h3><div class="gs_a">F SchÃ¼ssel, F Honold, <a href="/citations?user=LQU1oWgAAAAJ&amp;hl=en&amp;oi=sra">M Weber</a> - uni-ulm.de</div><div class="gs_rs">Abstract. We demonstrate how evidential reasoning can be applied in the domain of <br>graphical user interfaces to provide the reliability and robustness of input interpretation <br>expected by users of companion systems. For this purpose an existing approach using the <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'st4fMs7PJTUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md26', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md26" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:st4fMs7PJTUJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6396120" class=yC2E>Human identification system based on feature level fusion using face and gait biometrics</a></h3><div class="gs_a">MS Almohammad, GI Salama&hellip; - &hellip;  and Technology (ICET) &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract During the past years, face and gait recognition in video have received significant <br>attention. Consequently, their recognition problems have challenged due to largely varying <br>appearances and highly complex pattern distributions. However, the complementary <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'m8P0MpCb5_IJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:372"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB28" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW28"><a href="http://www.dfki.uni-kl.de/~ulges/pubs/ICMR12.pdf" class=yC30><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-kl.de</span><span class="gs_ggsS">uni-kl.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2324827" class=yC2F>Linking visual concept detection with viewer demographics</a></h3><div class="gs_a">A Ulges, M Koch, <a href="/citations?user=J-8Z038AAAAJ&amp;hl=en&amp;oi=sra">D Borth</a> - Proceedings of the 2nd ACM International  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract The estimation of demographic target groups for web videos--with applications in <br>ad targeting--poses a challenging problem, as the textual description and view statistics <br>available for many clips is extremely sparse. Therefore, the goal of this paper is to link a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16214655094476290905&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a href="/scholar?q=related:Wbu8ReYGBuEJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16214655094476290905&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'Wbu8ReYGBuEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:371"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB29" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW29"><a href="http://sites.google.com/site/publicationsabhijeetsangwan/ESPA_PostalSorter_2011.pdf" class=yC32><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from google.com</span><span class="gs_ggsS">google.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6152444" class=yC31>Improved parcel sorting by combining automatic speech and character recognition</a></h3><div class="gs_a">A Singh, A Sangwan&hellip; - &hellip;  Applications (ESPA), 2012  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic postal sorting systems have traditionally relied on optical character <br>recognition (OCR) technology. While OCR systems perform well for flat mail items such as <br>envelopes, the performance deteriorates for parcels. In this study, we propose a new <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:U3ZbSQwuSYoJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9964546280839149139&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'U3ZbSQwuSYoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:370"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6392948" class=yC33>Circular Reranking for Visual Search</a></h3><div class="gs_a">T Yao, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">C Ngo</a>, T Mei - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Search reranking is regarded as a common way for boosting retrieval precision. <br>The problem nevertheless is not trivial especially when there are multiple features or <br>modalities to be considered for search, which often happens in image and video retrieval. <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'W1TjVphMREgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:369"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB31" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW31"><a href="http://ict.usc.edu/pubs/Latent%20Mixture%20of%20Discriminative%20Experts.pdf" class=yC35><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from usc.edu</span><span class="gs_ggsS">usc.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6359954" class=yC34>Latent Mixture of Discriminative Experts</a></h3><div class="gs_a">D Ozkan, <a href="/citations?user=APgaFK0AAAAJ&amp;hl=en&amp;oi=sra">L Morency</a> - 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we introduce a new model called Latent Mixture of Discriminative <br>Experts which can automatically learn the temporal relationship between different <br>modalities. Since, we train separate experts for each modality, LMDE is capable of <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'ntkpYWpJuNcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:368"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6229590" class=yC36>A framework for computing quality of information in multi-sensor systems</a></h3><div class="gs_a"><a href="/citations?user=Qq4AAT4AAAAJ&amp;hl=en&amp;oi=sra">MA Hossain</a>, <a href="/citations?user=s2ux4pIAAAAJ&amp;hl=en&amp;oi=sra">DT Ahmed</a>, J Parra - &hellip;  Conference (I2MTC), 2012  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Multi-sensor systems are increasingly used for various monitoring tasks. Information <br>obtained from such systems are imprecise in nature but is used for important decision <br>making tasks. This precipitates the need to dynamically compute the quality of information <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1zP3ai3mW5MJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'1zP3ai3mW5MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:367"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6331545" class=yC37>Multimedia Fusion with Mean-Covariance Analysis</a></h3><div class="gs_a">X Wang, M Kankanhalli - 2013 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The number of multimedia applications has been increasing over the past two <br>decades. Multimedia information fusion has therefore attracted significant attention with <br>many techniques having been proposed. However, the uncertainty and correlation among <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'2Nv-a1bEUNoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:366"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB34" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW34"><a href="http://herve.niderb.fr/download/pdfs/Bredin2012a.pdf" class=yC39><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from niderb.fr</span><span class="gs_ggsS">niderb.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6288381" class=yC38>Community-driven hierarchical fusion of numerous classifiers: Application to video semantic indexing</a></h3><div class="gs_a"><a href="/citations?user=obEwWjZOI0cC&amp;hl=en&amp;oi=sra">H Bredin</a> - Acoustics, Speech and Signal Processing (ICASSP), &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We deal with the issue of combining dozens of classifiers into a better one. Our first <br>contribution is the introduction of the notion of communities of classifiers. We build a <br>complete graph with one node per classifier and edges weighted by a measure of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ZX8RcQLToocJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9773606148787765093&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'ZX8RcQLToocJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:365"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB35" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW35"><a href="http://scholar.lib.vt.edu/theses/available/etd-01312012-204254/unrestricted/Mundle_AR_T_2012.pdf" class=yC3B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from vt.edu</span><span class="gs_ggsS">vt.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholar.lib.vt.edu/theses/available/etd-01312012-204254/" class=yC3A>Blood-Oxygen-Level-Dependent Parameter Identification using Multimodal Neuroimaging and Particle Filters</a></h3><div class="gs_a">AR Mundle - 2012 - scholar.lib.vt.edu</div><div class="gs_rs">Abstract The Blood Oxygen Level Dependent (BOLD) signal provides indirect estimates of <br>neural activity. The parameters of this BOLD signal can give information about the <br>pathophysiological state of the brain. Most of the models for the BOLD signal are <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lW40eznVWxkJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'lW40eznVWxkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:364"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB36" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW36"><a href="http://www.tkl.iis.u-tokyo.ac.jp/top/modules/newdb/extract/1208/data/2012-ICMR-A%20RELIEF-Based%20Modality%20Weighting%20Approach%20for%20Multimodal%20Information%20Retrieval.pdf" class=yC3D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2324858" class=yC3C>A RELIEF-based modality weighting approach for multimodal information retrieval</a></h3><div class="gs_a"><a href="/citations?user=u3GoN7wAAAAJ&amp;hl=en&amp;oi=sra">T Yilmaz</a>, E Gulen, A Yazici&hellip; - Proceedings of the 2nd  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Despite the extensive number of studies for multimodal information fusion, the issue <br>of determining the optimal modalities has not been adequately addressed yet. In this study, <br>a RELIEF-based multimodal feature selection approach (RELIEF-RDR) is proposed. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:-BrsfonukmwJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7823577777003305720&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'-BrsfonukmwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:363"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6328054" class=yC3E>Multimodal and Multi-task Audio-Visual Vehicle Detection and Classification</a></h3><div class="gs_a">T Wang, <a href="/citations?user=2m-u_KoAAAAJ&amp;hl=en&amp;oi=sra">Z Zhu</a> - &hellip;  Video and Signal-Based Surveillance (AVSS), &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Moving vehicle detection and classification usingmultimodal data is a challenging <br>task in data collection, audio-visual alignment, and feature selection, andeffective vehicle <br>classification in uncontrolledenvironments. In this work, we first present a systematicway to <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'twFYg0c8baEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:362"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6400215" class=yC3F>Audiovisual Voice Activity Detection Based on Microphone Arrays and Color Information</a></h3><div class="gs_a">V Peruffo Minotto, C Lopes, J Scharcanski, <a href="/citations?user=3hU1tEkAAAAJ&amp;hl=en&amp;oi=sra">C Jung</a>&hellip; - 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Audiovisual voice activity detection is a necessary stage in several problems, such <br>as advanced teleconferencing, speech recognition, and human-computer interaction. Lip <br>motion and audio analysis provide a large amount of information that can be integrated to <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'9RxviDH04t0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:361"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB39" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW39"><a href="http://www.madm.eu/_media/theses/bachelorthesis-daila_2011.pdf" class=yC41><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from madm.eu</span><span class="gs_ggsS">madm.eu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.madm.eu/_media/theses/bachelorthesis-daila_2011.pdf" class=yC40>Audio Features for Automatic Video Tagging</a></h3><div class="gs_a">D El Badawi, D Borth - madm.eu</div><div class="gs_rs">Abstract While the majority of techniques for automatic video tagging mainly consider the <br>visual aspect of the video, this thesis addresses the analysis of the acoustical counterpartâ<br>the video soundtrackâfor learning concepts. Consequently, the goal of this thesis is to <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:dtfEk7xU-M0J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14841705740909205366&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'dtfEk7xU-M0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md39', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md39" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:dtfEk7xU-M0J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:360"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/581m1452u8882273.pdf" class=yC42>Semi-supervised context adaptation: case study of audience excitement recognition</a></h3><div class="gs_a">E Vildjiounaite, V KyllÃ¶nen, SM MÃ¤kelÃ¤, O Vuorinen&hellip; - Multimedia  &hellip;, 2012 - Springer</div><div class="gs_rs">Abstract To recognise just the same human reaction (for example, a strong excitement) in <br>different contexts, customary behaviours in these contexts have to be taken into account; eg <br>a happy sport audience may be cheering for long time, while a happy theatrical audience <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:WE0tlnCkBPMJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17511302054586174808&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'WE0tlnCkBPMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:359"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB41" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW41"><a href="http://www.tkl.iis.u-tokyo.ac.jp/top/modules/newdb/extract/1210/data/2012-MDDE-A%20Multimodal%20Fusion%20Approach%20By%20Exploiting%20Concept%20Interactions%20for%20Ef%3Fcient%20Multimedia%20Analysis.pdf" class=yC44><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.tkl.iis.u-tokyo.ac.jp/top/modules/newdb/extract/1210/data/2012-MDDE-A%20Multimodal%20Fusion%20Approach%20By%20Exploiting%20Concept%20Interactions%20for%20Ef%3Fcient%20Multimedia%20Analysis.pdf" class=yC43>A Multimodal Fusion Approach By Exploiting Concept Interactions for Efficient Multimedia Analysis</a></h3><div class="gs_a">E Gulen, <a href="/citations?user=u3GoN7wAAAAJ&amp;hl=en&amp;oi=sra">T Yilmaz</a>, A Yazici - tkl.iis.u-tokyo.ac.jp</div><div class="gs_rs">ABSTRACT Multimedia data intrinsically contains multimodal information in it. In order to <br>obtain a successful multimedia analysis, all available information should be utilized by <br>following a multimodal approach. In addition, the interaction between concepts is another <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:s_r5mFKCQn0J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9025919894469343923&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'s_r5mFKCQn0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md41', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md41" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:s_r5mFKCQn0J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:358"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB42" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW42"><a href="http://www.informed.unal.edu.co/jccaicedo/papers/paper66.pdf" class=yC46><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unal.edu.co</span><span class="gs_ggsS">unal.edu.co <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/972351K8K320R075.pdf" class=yC45>Online Matrix Factorization for Multimodal Image Retrieval</a></h3><div class="gs_a"><a href="/citations?user=U50zLvkAAAAJ&amp;hl=en&amp;oi=sra">J Caicedo</a>, <a href="/citations?user=IUB__IwAAAAJ&amp;hl=en&amp;oi=sra">F GonzÃ¡lez</a> - Progress in Pattern Recognition, Image Analysis,  &hellip;, 2012 - Springer</div><div class="gs_rs">In this paper, we propose a method to build an index for image search using multimodal <br>information, that is, using visual features and text data simultaneously. The method <br>combines both data sources and generates one multimodal representation using latent <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7ERSosrC_dkJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15707925250900641004&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'7ERSosrC_dkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:357"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/K18Q0271G1VT0340.pdf" class=yC47>Multimodal retrieval with relevance feedback based on genetic programming</a></h3><div class="gs_a">RT Calumby, <a href="/citations?user=IGZ5WmgAAAAJ&amp;hl=en&amp;oi=sra">R da Silva Torres</a>&hellip; - Multimedia Tools and  &hellip;, 2012 - Springer</div><div class="gs_rs">Abstract This paper presents a framework for multimodal retrieval with relevance feedback <br>based on genetic programming. In this supervised learning-to-rank framework, genetic <br>programming is used for the discovery of effective combination functions of (multimodal) <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:mff-p99yqi0J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'mff-p99yqi0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:356"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB44" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW44"><a href="http://www.semantic-web-journal.net/sites/default/files/swj126_0.pdf" class=yC49><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from semantic-web-journal.net</span><span class="gs_ggsS">semantic-web-journal.net <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.semantic-web-journal.net/sites/default/files/swj126_0.pdf" class=yC48>Semantic integration of TV data and services: A survey on challenges, and approaches</a></h3><div class="gs_a">B Maknia, S Dietzea, J Dominguea - semantic-web-journal.net</div><div class="gs_rs">Abstract. In this paper, we are surveying the impact of semantic Web and semantic Web <br>services on enabling novel television features. These novel features include being Internet <br>based, mobile, interactive, personalised, social and semantic. Many research efforts have <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'qjtitNOLrqQJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md44', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md44" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:qjtitNOLrqQJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:355"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB45" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW45"><a href="http://www.tkl.iis.u-tokyo.ac.jp/top/modules/newdb/extract/1211/data/2012-ICPR-Non-LinearWeighted%20Averaging%20for%20Multimodal%20Information%20Fusion%20by%20Employing%20Analytical%20Network%20Process.pdf" class=yC4B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from u-tokyo.ac.jp</span><span class="gs_ggsS">u-tokyo.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.tkl.iis.u-tokyo.ac.jp/top/modules/newdb/extract/1211/data/2012-ICPR-Non-LinearWeighted%20Averaging%20for%20Multimodal%20Information%20Fusion%20by%20Employing%20Analytical%20Network%20Process.pdf" class=yC4A>Non-Linear Weighted Averaging for Multimodal Information Fusion by Employing Analytical Network Process</a></h3><div class="gs_a"><a href="/citations?user=u3GoN7wAAAAJ&amp;hl=en&amp;oi=sra">T Yilmaz</a>, A Yazici, M Kitsuregawa - tkl.iis.u-tokyo.ac.jp</div><div class="gs_rs">Abstract Linear combination is a popular approach in information fusion due to its simplicity. <br>However, it suffers from the performance upper-bound of linearity and dependency on the <br>selection of weights. In this study, we introduce a &#39;simple&#39;alternative for linear combination, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:PGHptmTmPgwJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=882395897251062076&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'PGHptmTmPgwJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md45', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md45" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:PGHptmTmPgwJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:354"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB46" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW46"><a href="http://smartech.gatech.edu/bitstream/handle/1853/42736/dai_rui_201112_phd.pdf?sequence=1" class=yC4D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from gatech.edu</span><span class="gs_ggsS">gatech.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://smartech.gatech.edu/handle/1853/42736" class=yC4C>Correlation-based communication in wireless multimedia sensor networks</a></h3><div class="gs_a">R Dai - 2011 - smartech.gatech.edu</div><div class="gs_rs">Wireless multimedia sensor networks (WMSNs) are networks of interconnected devices that <br>allow retrieving video and audio streams, still images, and scalar data from the environment. <br>In a densely deployed WMSN, there exists correlation among the observations of camera <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:JGGauLyqN1kJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6428794720622371108&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'JGGauLyqN1kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:353"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2169002" class=yC4E>Sprite generation using sprite fusion</a></h3><div class="gs_a">Y Chen, AA Deshpande, RS AygÃ¼un - ACM Transactions on Multimedia  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract There has been related research for sprite or mosaic generation for over 15 years. <br>In this article, we try to understand the methodologies for sprite generation and identify what <br>has not actually been covered for sprite generation. We first identify issues and focus on <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lnxgve196m4J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7992338948760304790&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'lnxgve196m4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:352"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB48" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW48"><a href="http://www.informed.unal.edu.co/jccaicedo/papers/icmr12.pdf" class=yC50><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unal.edu.co</span><span class="gs_ggsS">unal.edu.co <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2324860" class=yC4F>Multimodal fusion for image retrieval using matrix factorization</a></h3><div class="gs_a"><a href="/citations?user=U50zLvkAAAAJ&amp;hl=en&amp;oi=sra">JC Caicedo</a>, <a href="/citations?user=IUB__IwAAAAJ&amp;hl=en&amp;oi=sra">FA GonzÃ¡lez</a> - Proceedings of the 2nd ACM International  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Image collections on the internet and other sources of information can naturally <br>include attached text descriptions. This work considers the problem of fusing two data <br>modalities: visual content and text keywords, to allow a flexible image indexing scheme. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:0MUSv0XjJrEJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12765140082526176720&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'0MUSv0XjJrEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:351"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/TJ7634160T0Q0478.pdf" class=yC51>Bayesian multimodal fusion in forensic applications</a></h3><div class="gs_a"><a href="/citations?user=HL9Yt8AAAAAJ&amp;hl=en&amp;oi=sra">V Fernandez Arguedas</a>, <a href="/citations?user=XR6C9BoAAAAJ&amp;hl=en&amp;oi=sra">Q Zhang</a>&hellip; - Computer VisionâECCV  &hellip;, 2012 - Springer</div><div class="gs_rs">The public location of CCTV cameras and their connexion with public safety demand high <br>robustness and reliability from surveillance systems. This paper focuses on the development <br>of a multimodal fusion technique which exploits the benefits of a Bayesian inference <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'RWILzLd68rUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:350"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB50" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW50"><a href="http://dare.uva.nl/document/355661" class=yC53><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dare.uva.nl/en/record/410580" class=yC52>Content-based visual search learned from social media</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a> - 2012 - dare.uva.nl</div><div class="gs_rs">Abstract In een wereld waarin de hoeveelheid digitale afbeeldingen alsmaar groeit is het <br>belangrijk te kunnen zoeken op basis van beeldinhoud. Xirong Li liet zich inspireren door <br>sociale media en onderzocht de waarde van beelden met social tags voor visueel zoeken. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:xTQX0HIOAtIJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15132673584198530245&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'xTQX0HIOAtIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:349"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB51" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW51"><a href="http://www.ceng.metu.edu.tr/~e120329/dilek_kucuk_phd_thesis.pdf" class=yC55><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from metu.edu.tr</span><span class="gs_ggsS">metu.edu.tr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ceng.metu.edu.tr/~e120329/dilek_kucuk_phd_thesis.pdf" class=yC54>EXPLOITING INFORMATION EXTRACTION TECHNIQUES FOR AUTOMATIC SEMANTIC ANNOTATION AND RETRIEVAL OF NEWS VIDEOS IN TURKISH</a></h3><div class="gs_a">C Ozgen - 2011 - ceng.metu.edu.tr</div><div class="gs_rs">Information extraction (IE) is known to be an effective technique for automatic semantic <br>indexing of news texts. In this study, we propose a text-based fully automated system for the <br>semantic annotation and retrieval of news videos in Turkish which exploits several IE <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:sJ641CcjnMoJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14599582746007871152&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'sJ641CcjnMoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md51', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md51" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:sJ641CcjnMoJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:348"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB52" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW52"><a href="http://etd.lib.metu.edu.tr/upload/12613755/index.pdf" class=yC57><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from metu.edu.tr</span><span class="gs_ggsS">metu.edu.tr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://etd.lib.metu.edu.tr/upload/12613755/index.pdf" class=yC56>EVENT BOUNDARY DETECTION USING WEB-CASTING TEXTS AND AUDIO-VISUAL FEATURES</a></h3><div class="gs_a">M BAYAR - 2011 - etd.lib.metu.edu.tr</div><div class="gs_rs">We propose a method to detect events and event boundaries in soccer videos by using web-<br>casting texts and audio-visual features. The events and their inaccurate time information <br>given in web-casting texts need to be aligned with the visual content of the video. Most <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ogrL2Y3XJ0AJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'ogrL2Y3XJ0AJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md52', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md52" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:ogrL2Y3XJ0AJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:347"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB53" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW53"><a href="http://isif.org/fusion/proceedings/Fusion_2011/data/papers/099.pdf" class=yC59><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from isif.org</span><span class="gs_ggsS">isif.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5977720" class=yC58>Fusion for the detection of dependent signals using multivariate copulas</a></h3><div class="gs_a">A Subramanian, A Sundaresan&hellip; - &hellip;  Fusion (FUSION), 2011 &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The use of multimodal or heterogeneous sensors for surveillance greatly increases <br>the diversity of information available from a given region of interest. Since the underlying <br>scene is the same for all the sensors, the data across the sensors are inherently <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14079888069747821403&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 2</a> <a href="/scholar?q=related:W0uU2pnPZcMJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14079888069747821403&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'W0uU2pnPZcMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:346"><div class="gs_ri"><h3 class="gs_rt"><a href="http://inderscience.metapress.com/index/R83W4K416610706N.pdf" class=yC5A>Multimedia analysis techniques for eâlearning</a></h3><div class="gs_a">DN Kanellopoulos - International Journal of Learning Technology, 2012 - Inderscience</div><div class="gs_rs">Multimedia analysis techniques can enable eâlearning systems and applications to <br>understand multimedia content automatically. Therefore, such techniques can provide <br>various novel services to both eâlearning video providers and learners. This paper aims at <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ekuh4pfFHa4J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12546401393057024890&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ekuh4pfFHa4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:345"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB55" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW55"><a href="http://publik.tuwien.ac.at/files/PubDat_198589.pdf" class=yC5C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tuwien.ac.at</span><span class="gs_ggsS">tuwien.ac.at <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6005782" class=yC5B>Cross-Modal Analysis of Audio-Visual Film Montage</a></h3><div class="gs_a">M Zeppelzauer, D Mitrovic&hellip; - &hellip;  and Networks (ICCCN),  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract A stylistic device frequently employed by filmmakers is the synchronous montage <br>(composition) of audio and visual elements. Synchronous montage helps to increase <br>tension and tempo in a scene and highlights important events in the story. Sequences with <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7bw-49bZkjgJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4076560129682488557&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'7bw-49bZkjgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:344"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB56" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW56"><a href="http://research.microsoft.com/en-us/um/people/yongrui/ps/mm370-wang.pdf" class=yC5E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from microsoft.com</span><span class="gs_ggsS">microsoft.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2071945" class=yC5D>Up-fusion: an evolving multimedia decision fusion method</a></h3><div class="gs_a">X Wang, <a href="/citations?user=uOJH_AEAAAAJ&amp;hl=en&amp;oi=sra">Y Rui</a>, MS Kankanhalli - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract The amount of multimedia data available on the Internet has increased <br>exponentially in the past few decades and is likely to keep on increasing. Given that a <br>multimedia system has multiple information sources, fusion methods are critical for its <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:TwRF49uFDCgJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2885828640694928463&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'TwRF49uFDCgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:343"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/5256023W07851655.pdf" class=yC5F>Multi-modal solution for unconstrained news story retrieval</a></h3><div class="gs_a">E Younessian, D Rajan - Advances in Multimedia Modeling, 2012 - Springer</div><div class="gs_rs">We propose a multi-modal approach to retrieve associated news stories sharing the same <br>main topic. In the textual domain, we utilize Automatic Speech Recognition (ASR) and <br>refined Optical Character Recognition (OCR) transcripts while in the visual domain we <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Bq205XeADH0J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9010718206904806662&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Bq205XeADH0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:342"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/V6186W67W14273V5.pdf" class=yC60>Aggression Detection in Speech Using Sensor and Semantic Information</a></h3><div class="gs_a"><a href="/citations?user=gLYiYy4AAAAJ&amp;hl=en&amp;oi=sra">I Lefter</a>, <a href="/citations?user=YPXhqgUAAAAJ&amp;hl=en&amp;oi=sra">L Rothkrantz</a>, <a href="/citations?user=zN6afwwAAAAJ&amp;hl=en&amp;oi=sra">G Burghouts</a> - Text, Speech and Dialogue, 2012 - Springer</div><div class="gs_rs">By analyzing a multimodal (audio-visual) database with aggressive incidents in trains, we <br>have observed that there are no trivial fusion algorithms to successfully predict multimodal <br>aggression based on unimodal sensor inputs. We proposed a fusion framework that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7724547347738639258&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=65">Cited by 1</a> <a href="/scholar?q=related:mrul6OIaM2sJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'mrul6OIaM2sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:341"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB59" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW59"><a href="http://scholarbank.nus.sg/bitstream/handle/10635/33358/WangXY.pdf?sequence=1" class=yC62><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.sg</span><span class="gs_ggsS">nus.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.sg/handle/10635/33358" class=yC61>Multimedia Decision Fusion</a></h3><div class="gs_a">X WANG - 2012 - scholarbank.nus.sg</div><div class="gs_rs">The amount of multimedia data available on the Internet has increased exponentially in the <br>past few decades and is likely to keep on increasing. Given multimedia&#39;s nature of having <br>multiple information sources, fusion methods are critical for its data analysis and <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:IcHG9VDtYpsJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11196772555573084449&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'IcHG9VDtYpsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:340"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB60" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW60"><a href="http://downloads.hindawi.com/journals/am/aip/175064.pdf" class=yC64><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://downloads.hindawi.com/journals/am/aip/175064.pdf" class=yC63>Multiple feature fusion based on Co-Training approach and time regularization for place classification in wearable video</a></h3><div class="gs_a">V Dovgalecs, R MÃ©gret, Y Berthoumieu - downloads.hindawi.com</div><div class="gs_rs">Abstract The analysis of video acquired with a wearable camera is a challenge that <br>Multimedia community is facing with the proliferation of such sensors in various applications. <br>In this article we focus on the problem of automatic visual place recognition in a weakly <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'P2-h-ldgGTEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md60', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md60" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:P2-h-ldgGTEJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:339"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6161652" class=yC65>Kernel Cross-Modal Factor Analysis for Information Fusion With Application to Bimodal Emotion Recognition</a></h3><div class="gs_a"><a href="/citations?user=gIZWoKYAAAAJ&amp;hl=en&amp;oi=sra">Y Wang</a>, L Guan&hellip; - &hellip; , IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we investigate kernel based methods for multimodal information <br>analysis and fusion. We introduce a novel approach, kernel cross-modal factor analysis, <br>which identifies the optimal transformations that are capable of representing the coupled <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:dbxO_bH0AJgJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10953023339061230709&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'dbxO_bH0AJgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:338"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB62" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW62"><a href="http://www.lbd.dcc.ufmg.br/colecoes/wtdbd/2012/0013.pdf" class=yC67><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ufmg.br</span><span class="gs_ggsS">ufmg.br <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.lbd.dcc.ufmg.br/colecoes/wtdbd/2012/0013.pdf" class=yC66>IndexaÃ§ ao e RecuperaÃ§ ao Multimodal de VÄ±deo</a></h3><div class="gs_a">BHM GeraisâBrasil - lbd.dcc.ufmg.br</div><div class="gs_rs">Resumo. O objetivo deste trabalho Ã© investigar a indexaÃ§ao e recuperaÃ§ao de vÄ±deos por <br>conteÃºdo utilizando informaÃ§ ao multimodal. Para este fim os vÄ±deos serao descritos atravÃ©s <br>da extraÃ§ ao de caracterÄ±sticas de seu conteÃºdo visual e acÃºstico. A indexaÃ§ao e <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'pDo8DhdXVMYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md62', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md62" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pDo8DhdXVMYJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:337"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB63" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW63"><a href="http://rua.ua.es/dspace/bitstream/10045/22033/1/PLN_48_09.pdf" class=yC69><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ua.es</span><span class="gs_ggsS">ua.es <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://rua.ua.es/dspace/handle/10045/22033" class=yC68>ExperimentaciÃ³n en la BÃºsqueda de ImÃ¡genes a partir de CaracterÃ­sticas Visuales y Textuales: FusiÃ³n TardÃ­a y ExpansiÃ³n de la Consulta</a></h3><div class="gs_a"><a href="/citations?user=YIReZq0AAAAJ&amp;hl=en&amp;oi=sra">R Granados MuÃ±oz</a>, <a href="/citations?user=Y7G5f8MAAAAJ&amp;hl=en&amp;oi=sra">AM GarcÃ­a Serrano</a>&hellip; - 2012 - rua.ua.es</div><div class="gs_rs">La recuperaciÃ³n de informaciÃ³n multimedia es uno de los retos que actualmente se afrontan <br>en el entorno de la web o en grandes colecciones de objetos multimedia (audio, video, <br>imÃ¡genes y textos). En este artÃ­culo se presenta la experimentaciÃ³n realizada para <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:ywoj-62-efIJ:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17472205883812547275&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'ywoj-62-efIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:336"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB64" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW64"><a href="http://dspace.cc.tut.fi/dpub/bitstream/handle/123456789/6807/roininen.pdf?sequence=3" class=yC6B><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tut.fi</span><span class="gs_ggsS">tut.fi <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dspace.cc.tut.fi/dpub/handle/123456789/6807" class=yC6A>Audiovisual Sensing and Context Recognition for Mobile Devices</a></h3><div class="gs_a">MJ Roininen - 2010 - dspace.cc.tut.fi</div><div class="gs_rs">Effective automatic analysis methods are required for the thorough utilization of extensive <br>video collections. Content-based video analysis provides means for automated content <br>interpretation. Popular approach for content-based video analysis is to consider it as a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:1PWi_CAzoo8J:scholar.google.com/&amp;hl=en&amp;num=65&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'1PWi_CAzoo8J')" href="#" class="gs_nph">Cite</a></div></div></div>
