Total results = 15
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a446147.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dtic.mil</span><span class="gs_ggsS">dtic.mil <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/authorize?1704" class=yC0>Discovering models of software processes from event-based data</a></h3><div class="gs_a">JE Cook, AL Wolf - ACM Transactions on Software Engineering and  &hellip;, 1998 - dl.acm.org</div><div class="gs_rs">Abstract Many software process methods and tools presuppose the existence of a formal <br>model of a process. Unfortunately, developing a formal model for an on-going, complex <br>process can be difficult, costly, and error prone. This presents a practical barrier to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10964290205106471302&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 613</a> <a href="/scholar?q=related:hoGqqdn7KJgJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/0C/60/RN051874444.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=10964290205106471302&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 31 versions</a> <a onclick="return gs_ocit(event,'hoGqqdn7KJgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md0', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md0" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:hoGqqdn7KJgJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=7183754668122516174&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.7157&amp;rep=rep1&amp;type=pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0890540198927845" class=yC3>Incremental concept learning for bounded data mining</a></h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a>, <a href="/citations?user=9oNh1Z0AAAAJ&amp;hl=en&amp;oi=sra">S Jain</a>, S Lange, T Zeugmann - Information and Computation, 1999 - Elsevier</div><div class="gs_rs">Important refinements of concept learning in the limit from positive data considerably <br>restricting the accessibility of input data are studied. Let c be any concept; every infinite <br>sequence of elements exhausting c is called positive presentation of c. In all learning <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17242085717973845032&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 88</a> <a href="/scholar?q=related:KMSqppsxSO8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5F/31/RN064972212.html?source=googlescholar" class="gs_nph" class=yC5>BL Direct</a> <a href="/scholar?cluster=17242085717973845032&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'KMSqppsxSO8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.9010&amp;rep=rep1&amp;type=pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=225320" class=yC6>Language learning from texts (extended abstract): mind changes, limited memory and monotonicity</a></h3><div class="gs_a">E Kinber, F Stephan - Proceedings of the eighth annual conference on  &hellip;, 1995 - dl.acm.org</div><div class="gs_rs">Abstract The paper explores language learning in the limit under various constraints on the <br>number of mind-Chmges, memory, and monotonicity. We define language learning with <br>limited (long term) memory and prove that learning with limited memory is exactly the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11603817791429251677&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 71</a> <a href="/scholar?q=related:XZJ_OM0KCaEJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/39/3E/RN000935724.html?source=googlescholar" class="gs_nph" class=yC8>BL Direct</a> <a href="/scholar?cluster=11603817791429251677&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 33 versions</a> <a onclick="return gs_ocit(event,'XZJ_OM0KCaEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www.cs.nmsu.edu/~cliu/publications/compind.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nmsu.edu</span><span class="gs_ggsS">nmsu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0166361503001982" class=yC9>Discovering models of behavior for concurrent workflows</a></h3><div class="gs_a">JE Cook, Z Du, C Liu, AL Wolf - Computers in Industry, 2004 - Elsevier</div><div class="gs_rs">Understanding the dynamic behavior of a workflow is crucial for being able to modify, <br>maintain, and improve it. A particularly difficult aspect of some behavior is concurrency. <br>Automated techniques which seek to mine workflow data logs to discover information <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15813914639330964680&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 64</a> <a href="/scholar?q=related:yPSrKZRPdtsJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15813914639330964680&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 15 versions</a> <a onclick="return gs_ocit(event,'yPSrKZRPdtsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://research.nii.ac.jp/~kanazawa/publications/LP-96-05.text.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nii.ac.jp</span><span class="gs_ggsS">nii.ac.jp <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=238095" class=yCB>Angluin&#39;s theorem for indexed families of re sets and applications</a></h3><div class="gs_a">D De Jongh, <a href="/citations?user=56dbnAUAAAAJ&amp;hl=en&amp;oi=sra">M Kanazawa</a> - Proceedings of the ninth annual conference  &hellip;, 1996 - dl.acm.org</div><div class="gs_rs">Abstract We extend Angluin&#39;s(1980) theorem to characterize identifiability of indexed <br>families of re languages, as opposed to indexed families of recursive languages. We also <br>prove some variants characterizing conservativity and two other similar restrictions, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1320976101546014011&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 34</a> <a href="/scholar?q=related:OzVu-cMMVRIJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1320976101546014011&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 25 versions</a> <a onclick="return gs_ocit(event,'OzVu-cMMVRIJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:OzVu-cMMVRIJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=14918051307358839763&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0304397597000182" class=yCD>Noisy inference and oracles</a></h3><div class="gs_a">F Stephan - Theoretical Computer Science, 1997 - Elsevier</div><div class="gs_rs">The present paper deals with several variants of inductive inference from noisy data. The <br>notion of noise is based on the idea that the learner recieves a sequence of data elements <br>such that each correct element appears infinitely often and each incorrect element <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12324907220817005336&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 22</a> <a href="/scholar?q=related:GMtxMtHdCqsJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12324907220817005336&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'GMtxMtHdCqsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0164121204002080" class=yCE>Discovering thread interactions in a concurrent system</a></h3><div class="gs_a">JE Cook, Z Du - Journal of Systems and Software, 2005 - Elsevier</div><div class="gs_rs">Understanding the behavior of a system is a central reverse engineering task, and is crucial <br>for being able to modify, maintain, and improve the system. An often difficult aspect of some <br>system behaviors is concurrency, in particular identifying those areas that exhibit mutual <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5293681222115578629&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 17</a> <a href="/scholar?q=related:BdNaPgbvdkkJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5293681222115578629&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 12 versions</a> <a onclick="return gs_ocit(event,'BdNaPgbvdkkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/74u9j8083154t170.pdf" class=yCF>Monotonicity versus efficiency for learning languages from texts</a></h3><div class="gs_a">E Kinber - Algorithmic Learning Theory, 1994 - Springer</div><div class="gs_rs">One of the central, problems of learning languages from texts is: how various restrictions on <br>the behaviour of a learner limit the learning abilities. We consider restrictions of two types. <br>Restrictions of the first type concern monotonicity of learning. Monotonicity means actually <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18346427730941859692&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 14</a> <a href="/scholar?q=related:bFcqWNGam_4J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/30/24/EN021431795.html?source=googlescholar" class="gs_nph" class=yC10>BL Direct</a> <a href="/scholar?cluster=18346427730941859692&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'bFcqWNGam_4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/F7357P1580T1575G.pdf" class=yC11>Noisy inference and oracles</a></h3><div class="gs_a">F Stephan - Algorithmic Learning Theory, 1995 - Springer</div><div class="gs_rs">A learner noisily infers a function or set, if every correct item is presented infinitely often <br>while in addition some incorrect data (noise) is presented a finite number of times. It is <br>shown that learning from a noisy informant is equal to finite learning with K-oracle from a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7409414712334555572&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 13</a> <a href="/scholar?q=related:tBlBkmyH02YJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/15/01/EN031725641.html?source=googlescholar" class="gs_nph" class=yC12>BL Direct</a> <a href="/scholar?cluster=7409414712334555572&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'tBlBkmyH02YJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0304397597000170" class=yC13>Probabilistic language learning under monotonicity constraints</a></h3><div class="gs_a">L Meyer - Theoretical Computer Science, 1997 - Elsevier</div><div class="gs_rs">The present paper deals with probabilistic identification of indexed families of uniformly <br>recursive languages from positive data under monotonicity constraints. Thereby, we <br>consider conservative, strong-monotonic and monotonic probabilistic learning of indexed <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18062386564712688104&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 7</a> <a href="/scholar?q=related:6Ol8yeF8qvoJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18062386564712688104&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'6Ol8yeF8qvoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://cs5824.userapi.com/u11728334/docs/9e67ff61eb70/Michael_M_Richter_Algorithmic_Learning_Theory_9.pdf#page=316" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from userapi.com</span><span class="gs_ggsS">userapi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/5n5knxgfmm120bf6.pdf" class=yC14>Comparing the power of probabilistic learning and oracle identification under monotonicity constraints</a></h3><div class="gs_a">L Meyer - Algorithmic Learning Theory, 1998 - Springer</div><div class="gs_rs">In the setting of learning indexed families, probabilistic learning under monotonicity <br>constraints is more powerful than deterministic learning under monotonicity constraints even <br>if the probability is close to 1 provided the learning machines are restricted to proper or <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15227747900700555088&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=15">Cited by 1</a> <a href="/scholar?q=related:UPd0HgnUU9MJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/09/08/RN051857823.html?source=googlescholar" class="gs_nph" class=yC16>BL Direct</a> <a href="/scholar?cluster=15227747900700555088&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'UPd0HgnUU9MJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://dl.comp.nus.edu.sg/dspace/bitstream/1900.100/1392/1/report.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.tandfonline.com/doi/abs/10.1080/095281397147275" class=yC17>Strong monotonic and set-driven inductive inference</a></h3><div class="gs_a"><a href="/citations?user=9oNh1Z0AAAAJ&amp;hl=en&amp;oi=sra">S Jain</a> - Journal of Experimental &amp; Theoretical Artificial  &hellip;, 1997 - Taylor &amp; Francis</div><div class="gs_rs">Abstract. In an earlier paper, Kinber and Stephan posed an open problem about whether <br>every class of languages, which can be identified strong monotonically, can also be <br>identified by a set-driven machine. This question is solved in this paper. The answer to the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:oUu_DHYm4S8J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/26/23/RN020626835.html?source=googlescholar" class="gs_nph" class=yC19>BL Direct</a> <a href="/scholar?cluster=3450081078004370337&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'oUu_DHYm4S8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:oUu_DHYm4S8J:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=10691927380410731698&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0304397507002277" class=yC1A>On the data consumption benefits of accepting increased uncertainty</a></h3><div class="gs_a">E Martin, A Sharma, F Stephan - Theoretical Computer Science, 2007 - Elsevier</div><div class="gs_rs">In the context of learning paradigms of identification in the limit, we address the question: <br>why is uncertainty sometimes desirable? We use mind change bounds on the output <br>hypotheses as a measure of uncertainty and interpret &#39;desirable&#39;as reduction in data <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:YN_1eqwpL5wJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11254259814596206432&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'YN_1eqwpL5wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/6LYLFEKMFX7KTYQ4.pdf" class=yC1B>On the data consumption benefits of accepting increased uncertainty</a></h3><div class="gs_a">E Martin, A Sharma, F Stephan - Algorithmic Learning Theory, 2004 - Springer</div><div class="gs_rs">In the context of learning paradigms of identification in the limit, we address the question: <br>why is uncertainty sometimes desirable? We use mind change bounds on the output <br>hypotheses as a measure of uncertainty, and interpret &#39;desirable&#39;as reduction in data <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:vahgiAbccK4J:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/22/4F/RN157150765.html?source=googlescholar" class="gs_nph" class=yC1C>BL Direct</a> <a href="/scholar?cluster=12569788480607004861&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'vahgiAbccK4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="https://www1.comp.nus.edu.sg/~fstephan/alt2004.ps" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PS]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PS]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PS]</span><span class="gs_ct2">[PS]</span></span> <a href="https://www1.comp.nus.edu.sg/~fstephan/alt2004.ps" class=yC1D>On the Data Consumption Beneï¬ts of Accepting Increased Uncertainty</a></h3><div class="gs_a">E Martin, A Sharma, F Stephan - comp.nus.edu.sg</div><div class="gs_rs">Abstract In the context of learning paradigms of identification in the limit, We address the <br>question: Why is uncertainty sometimes desirable? VVe use mind change bounds on the <br>output hypotheses a measure of uncertainty, and interpret &#39;desirable&#39;as reduction i11 data <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Er3nw-LMkmMJ:scholar.google.com/&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7175022430676040978&amp;hl=en&amp;num=15&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'Er3nw-LMkmMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
