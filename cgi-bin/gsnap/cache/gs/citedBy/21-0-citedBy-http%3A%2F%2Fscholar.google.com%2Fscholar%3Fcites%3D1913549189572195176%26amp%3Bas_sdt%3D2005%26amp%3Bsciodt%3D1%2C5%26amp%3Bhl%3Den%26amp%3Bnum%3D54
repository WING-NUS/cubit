Total results = 21
<div class="gs_r" style="z-index:400"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/AM7558R84445558T.pdf" class=yC0>U-shaped, iterative, and iterative-with-counter learning</a></h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a>, SE Moelius - Machine Learning, 2008 - Springer</div><div class="gs_rs">Abstract This paper solves an important problem left open in the literature by showing that U-<br>shapes are unnecessary in iterative learning from positive data. A U-shape occurs when a <br>learner first learns, then unlearns, and, finally, relearns, some target concept. Iterative <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3868849313996447409&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 13</a> <a href="/scholar?q=related:sZ5F-vPpsDUJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/14/04/RN231197058.html?source=googlescholar" class="gs_nph" class=yC1>BL Direct</a> <a href="/scholar?cluster=3868849313996447409&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'sZ5F-vPpsDUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0304397510002045" class=yC2>Iterative learning of simple external contextual languages</a></h3><div class="gs_a">L Becerra-Bonache, <a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a>, <a href="/citations?user=9oNh1Z0AAAAJ&amp;hl=en&amp;oi=sra">S Jain</a>&hellip; - Theoretical Computer  &hellip;, 2010 - Elsevier</div><div class="gs_rs">It is investigated for which choice of a parameter q, denoting the number of contexts, the <br>class of simple external contextual languages is iteratively learnable. On the one hand, the <br>class admits, for all values of q, polynomial time learnability provided an adequate choice <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1441054131738363263&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 8</a> <a href="/scholar?q=related:f8VfCBmn_xMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1441054131738363263&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'f8VfCBmn_xMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://cs5235.userapi.com/u133638729/docs/e02780dda47c/Yoav_Freund_Algorithmic_Learning_Theory_19_conf.pdf#page=371" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from userapi.com</span><span class="gs_ggsS">userapi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/e654657xu548t251.pdf" class=yC3>Iterative learning of simple external contextual languages</a></h3><div class="gs_a">L Becerra-Bonache, <a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a>, <a href="/citations?user=9oNh1Z0AAAAJ&amp;hl=en&amp;oi=sra">S Jain</a>&hellip; - Algorithmic Learning  &hellip;, 2008 - Springer</div><div class="gs_rs">It is investigated for which choice of a parameter q, denoting the number of contexts, the <br>class of simple external contextual languages is iteratively learnable. On one hand, the class <br>admits, for all values of q, polynomial time learnability provided an adequate choice of the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16435534324706102193&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 6</a> <a href="/scholar?q=related:scvYnWO_FuQJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16435534324706102193&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 25 versions</a> <a onclick="return gs_ocit(event,'scvYnWO_FuQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://cs5235.userapi.com/u133638729/docs/e02780dda47c/Yoav_Freund_Algorithmic_Learning_Theory_19_conf.pdf#page=431" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from userapi.com</span><span class="gs_ggsS">userapi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/1N4582H1226U0066.pdf" class=yC5>Optimal language learning</a></h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a>, S Moelius - Algorithmic Learning Theory, 2008 - Springer</div><div class="gs_rs">Gold&#39;s original paper on inductive inference introduced a notion of an optimal learner. <br>Intuitively, a learner identifies a class of objects optimally iff there is no other learner that: <br>requires as little of each presentation of each object in the class in order to identify that <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4359389727217022240&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 7</a> <a href="/scholar?q=related:IPVBINypfzwJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4359389727217022240&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'IPVBINypfzwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/WW86604W658G5101.pdf" class=yC7>Solutions to open questions for non-U-shaped learning with memory limitations</a></h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a>, T KÃ¶tzing - Algorithmic Learning Theory, 2010 - Springer</div><div class="gs_rs">A U-shape occurs when a learner first learns, then unlearns, and, finally, relearns, some <br>target concept. Within the framework of Inductive Inference, previous results have shown, for <br>example, that U-shapes are unnecessary for explanatory learning, but are necessary for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12332842702605565065&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 5</a> <a href="/scholar?q=related:ibyShxgPJ6sJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12332842702605565065&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'ibyShxgPJ6sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.comp.nus.edu.sg/~sanjay/paps/tem.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0304397510002057" class=yC8>Incremental learning with temporary memory</a></h3><div class="gs_a"><a href="/citations?user=9oNh1Z0AAAAJ&amp;hl=en&amp;oi=sra">S Jain</a>, S Lange, SE Moelius, <a href="/citations?user=gkUY7QMAAAAJ&amp;hl=en&amp;oi=sra">S Zilles</a> - Theoretical Computer Science, 2010 - Elsevier</div><div class="gs_rs">In the inductive inference framework of learning in the limit, a variation of the bounded <br>example memory (Bem) language learning model is considered. Intuitively, the new model <br>constrains the learner&#39;s memory not only in how much data may be stored, but also in how <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14531567877095585502&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 5</a> <a href="/scholar?q=related:3nbCH_5_qskJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14531567877095585502&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'3nbCH_5_qskJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0304397509000668" class=yCA>Parallelism increases iterative learning power</a></h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a>, SE Moelius III - Theoretical Computer Science, 2009 - Elsevier</div><div class="gs_rs">Iterative learning (It-learning) is a Gold-style learning model in which each of a learner&#39;s <br>output conjectures may depend only upon the learner&#39;s current conjecture and the current <br>input element. Two extensions of the It-learning model are considered, each of which <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8888623813914682370&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 3</a> <a href="/scholar?q=related:AmR2ykO8WnsJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8888623813914682370&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'AmR2ykO8WnsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://cs5235.userapi.com/u133638729/docs/e02780dda47c/Yoav_Freund_Algorithmic_Learning_Theory_19_conf.pdf#page=461" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from userapi.com</span><span class="gs_ggsS">userapi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/H18M222XV34JT501.pdf" class=yCB>Learning with temporary memory</a></h3><div class="gs_a">S Lange, S Moelius, <a href="/citations?user=gkUY7QMAAAAJ&amp;hl=en&amp;oi=sra">S Zilles</a> - Algorithmic Learning Theory, 2008 - Springer</div><div class="gs_rs">In the inductive inference framework of learning in the limit, a variation of the bounded <br>example memory (Bem) language learning model is considered. Intuitively, the new model <br>constrains the learner&#39;s memory not only in how much data may be retained, but also in <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13833836604818541277&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 3</a> <a href="/scholar?q=related:3UoB5xip-78J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13833836604818541277&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'3UoB5xip-78J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/74M53LP013269545.pdf" class=yCD>Learning without coding</a></h3><div class="gs_a">S Moelius, <a href="/citations?user=gkUY7QMAAAAJ&amp;hl=en&amp;oi=sra">S Zilles</a> - Algorithmic Learning Theory, 2010 - Springer</div><div class="gs_rs">Iterative learning is a model of language learning from positive data, due to Wiehagen. <br>When compared to a learner in Gold&#39;s original model of language learning from positive <br>data, an iterative learner can be thought of as memory-limited. However, an iterative <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12836458955819415499&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 3</a> <a href="/scholar?q=related:y3d_62hDJLIJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12836458955819415499&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'y3d_62hDJLIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/B827045M7662102M.pdf" class=yCE>Iterative learning from positive data and counters</a></h3><div class="gs_a">T KÃ¶tzing - Algorithmic Learning Theory, 2011 - Springer</div><div class="gs_rs">We analyze iterative learning in the limit from positive data with the additional information <br>provided by a counter. The simplest type of counter provides the current iteration number <br>(counting up from 0 to infinity), which is known to improve learning power over plain <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3496968209057278535&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 2</a> <a href="/scholar?q=related:R2qx8w66hzAJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3496968209057278535&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'R2qx8w66hzAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://www.socsci.uci.edu/~lpearl/colareadinggroup/readings/Heinz2010Manu_CompLearningDevPsych.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uci.edu</span><span class="gs_ggsS">uci.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.socsci.uci.edu/~lpearl/colareadinggroup/readings/Heinz2010Manu_CompLearningDevPsych.pdf" class=yCF>Computational theories of learning and developmental psycholinguistics</a></h3><div class="gs_a">J Heinz - Under review for the The Cambridge Handbook of  &hellip;, 2010 - socsci.uci.edu</div><div class="gs_rs">A computer is something that computes, and since humans make computations when pro- cessing <br>information, humans are computers. What kinds of computations do humans make when they <br>learn languages? Answering this question requires the collaborative efforts of <b> ...</b> </div><div class="gs_fl"><a href="/scholar?cites=13107310615108748753&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:0YVDFo-F5rUJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13107310615108748753&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'0YVDFo-F5rUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:0YVDFo-F5rUJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.8803&amp;rep=rep1&amp;type=pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.8803&amp;rep=rep1&amp;type=pdf" class=yC11>Optimal language learning (expanded version)</a></h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a>, SE Moelius - 2008 - Citeseer</div><div class="gs_rs">Abstract. Gold&#39;s original paper on inductive inference introduced a notion of an optimal <br>learner. Intuitively, a learner identifies a class of objects optimally iff there is no other learner <br>that: requires as little of each presentation of each object in the class in order to identify <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2574599541247357815&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:d7PuVqTQuiMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2574599541247357815&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'d7PuVqTQuiMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:d7PuVqTQuiMJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.2248&amp;rep=rep1&amp;type=pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.2248&amp;rep=rep1&amp;type=pdf" class=yC13>Learning with temporary memory (expanded version)</a></h3><div class="gs_a">S Lange, SE Moelius, <a href="/citations?user=gkUY7QMAAAAJ&amp;hl=en&amp;oi=sra">S Zilles</a> - 2008 - Citeseer</div><div class="gs_rs">Abstract. In the inductive inference framework of learning in the limit, a variation of the <br>bounded example memory (Bem) language learning model is considered. Intuitively, the <br>new model constrains the learner&#39;s memory not only in how much data may be retained, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11027518364955726574&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=21">Cited by 1</a> <a href="/scholar?q=related:7lJI0YadCZkJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11027518364955726574&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'7lJI0YadCZkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md12', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md12" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:7lJI0YadCZkJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="http://www.eecis.udel.edu/~moelius/publications/lwoc_tr.pdf" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from udel.edu</span><span class="gs_ggsS">udel.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=nEBfBghRSgsC&amp;oi=fnd&amp;pg=PA300&amp;ots=42d4s6DyH4&amp;sig=NH-TRDuTfbN5aoWOnb4zb4RnAEU" class=yC15>Learning without Coding</a></h3><div class="gs_a">SEM Iii, <a href="/citations?user=gkUY7QMAAAAJ&amp;hl=en&amp;oi=sra">S Zilles</a> - &hellip; , ALT 2010, Canberra, Australia, October 6-8,  &hellip;, 2010 - books.google.com</div><div class="gs_rs">Abstract. Iterative learning is a model of language learning from positive data, due to <br>Wiehagen. When compared to a learner in Gold&#39;s original model of language learning from <br>positive data, an iterative learner can be thought of as memory-limited. However, an <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:L2vGQFeWA-0J:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17078659513412119343&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'L2vGQFeWA-0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0890540111001088" class=yC17>Optimal language learning from positive data</a></h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a>, SE Moelius - Information and Computation, 2011 - Elsevier</div><div class="gs_rs">GoldÊ¼s original paper on inductive inference introduced a notion of an optimal learner. <br>Intuitively, a learner identifies a class of objects optimally iff there is no other learner that: <br>requires as little of each presentation of each object in the class in order to identify that <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Lka8Q86r5ysJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3163686165639087662&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Lka8Q86r5ysJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www2.cs.uregina.ca/~zilles/langeMZ08TR.pdf" class=yC19><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uregina.ca</span><span class="gs_ggsS">uregina.ca <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www2.cs.uregina.ca/~zilles/langeMZ08TR.pdf" class=yC18>Incremental Learning with Temporary Memory</a></h3><div class="gs_a">S Langea, S Zillesc - cs.uregina.ca</div><div class="gs_rs">Abstract In the inductive inference framework of learning in the limit, a variation of the <br>bounded example memory (Bem) language learning model is considered. Intuitively, the <br>new model constrains the learner&#39;s memory not only in how much data may be retained, <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:MRIpRUDfVdEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15084207994078564913&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'MRIpRUDfVdEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md15', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md15" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:MRIpRUDfVdEJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1111/tops.12004/full" class=yC1A>Editors&#39; Introduction: Why Formal Learning Theory Matters for Cognitive Science</a></h3><div class="gs_a"><a href="/citations?user=iS_6IGAAAAAJ&amp;hl=en&amp;oi=sra">S Fulop</a>, <a href="/citations?user=I19dSOUAAAAJ&amp;hl=en&amp;oi=sra">N Chater</a> - Topics in Cognitive Science, 2013 - Wiley Online Library</div><div class="gs_rs">Abstract This article reviews a number of different areas in the foundations of formal learning <br>theory. After outlining the general framework for formal models of learning, the Bayesian <br>approach to learning is summarized. This leads to a discussion of Solomonoff&#39;s Universal <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'LP9gUpB3SagJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/812361171q2264pu.pdf" class=yC1B>Resource Restricted Computability Theoretic Learning: Illustrative Topics and Problems</a></h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a> - Theory of Computing Systems, 2009 - Springer</div><div class="gs_rs">Abstract Computability theoretic learning theory (machine inductive inference) typically <br>involves learning programs for languages or functions from a stream of complete data about <br>them and, importantly, allows mind changes as to conjectured programs. This theory takes <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:cNGIg7j_YKkJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12205036158119891312&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'cNGIg7j_YKkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://www.eecis.udel.edu/~case/papers/pr2.pdf" class=yC1D><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from udel.edu</span><span class="gs_ggsS">udel.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.eecis.udel.edu/~case/papers/pr2.pdf" class=yC1C>On the Necessity of U-Shaped Learning</a></h3><div class="gs_a">L Carlucci, <a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a> - eecis.udel.edu</div><div class="gs_rs">Abstract. A U-shaped curve in a cognitive-developmental trajectory refers to a three-step <br>process: good performance followed by bad performance followed by good performance <br>once again. U-shaped curves have been observed in a wide variety of cognitive-<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:32wcjMtUlvAJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17336137048815070431&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'32wcjMtUlvAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md18', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md18" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:32wcjMtUlvAJ:scholar.google.com/&amp;hl=en&amp;num=21&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0304397512009310" class=yC1E>Memory-limited non-U-shaped learning with solved open problems</a></h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a>, T KÃ¶tzing - Theoretical Computer Science, 2012 - Elsevier</div><div class="gs_rs">Herein we also distinguish between semantic and syntactic U-shapes. We answer a number <br>of open questions in the prior literature as well as provide new results regarding syntactic <br>U-shapes. Importantly for cognitive science, we see more of a previously noticed pattern <b> ...</b> </div><div class="gs_fl"><a onclick="return gs_ocit(event,'zUS4nbYRNscJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0304397512009322" class=yC1F>Learning without coding</a></h3><div class="gs_a"><a href="/citations?user=9oNh1Z0AAAAJ&amp;hl=en&amp;oi=sra">S Jain</a>, SE Moelius III, <a href="/citations?user=gkUY7QMAAAAJ&amp;hl=en&amp;oi=sra">S Zilles</a> - Theoretical Computer Science, 2012 - Elsevier</div><div class="gs_rs">Abstract Iterative learning is a model of language learning from positive data, due to <br>Wiehagen. When compared to a learner in Gold&#39;s original model of language learning from <br>positive data, an iterative learner can be thought of as memory-limited. However, an <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'HJCk4SjSXFAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
