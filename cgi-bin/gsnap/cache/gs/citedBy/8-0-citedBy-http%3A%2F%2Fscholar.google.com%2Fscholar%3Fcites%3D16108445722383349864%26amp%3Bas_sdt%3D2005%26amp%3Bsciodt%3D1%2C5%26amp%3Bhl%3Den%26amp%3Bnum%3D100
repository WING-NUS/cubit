Total results = 8
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.cecs.uci.edu/~papers/icme06/pdfs/0001485.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uci.edu</span><span class="gs_ggsS">uci.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4036892" class=yC0>Enhanced semi-supervised learning for automatic video annotation</a></h3><div class="gs_a"><a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, LR Dai&hellip; - Multimedia and Expo, 2006 &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract For automatic semantic annotation of large-scale video database, the insufficiency <br>of labeled training samples is a major obstacle. General semi-supervised learning <br>algorithms can help solve the problem but the improvement is limited. In this paper, two <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12375036464000623879&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8">Cited by 6</a> <a href="/scholar?q=related:B2VDJxn2vKsJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12375036464000623879&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'B2VDJxn2vKsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4634202" class=yC2>Learning from testing data: A new view of incremental semi-supervised learning</a></h3><div class="gs_a">Y Cao, H He - Neural Networks, 2008. IJCNN 2008.(IEEE  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, we propose a novel method for incremental semi-supervised learning. <br>Unlike the traditional way of incremental learning or semi-supervised learning, we try to <br>answer a more challenging question: given inadequate labeled training data, can one use <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3431123412980599866&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8">Cited by 3</a> <a href="/scholar?q=related:Ogw8s5DMnS8J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Ogw8s5DMnS8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4667971" class=yC3>On Co-Training Style Algorithms</a></h3><div class="gs_a">C Dong, Y Yin, X Guo, <a href="/citations?user=tJ-izEYAAAAJ&amp;hl=en&amp;oi=sra">G Yang</a>&hellip; - &hellip; Computation, 2008. ICNC &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract During the past few years, semi-supervised learning has become a hot topic in <br>machine learning and data mining, since manually labeling training examples is a tedious, <br>error prone and time-consuming task in many practical applications. As one of the most <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10426231376859709137&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8">Cited by 2</a> <a href="/scholar?q=related:0T7BWzJqsZAJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10426231376859709137&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'0T7BWzJqsZAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231210004923" class=yC4>LIFT: A new framework of learning from testing data for face recognition</a></h3><div class="gs_a">Y Cao, H He, H Huang - Neurocomputing, 2011 - Elsevier</div><div class="gs_rs">In this paper, a novel learning methodology for face recognition, LearnIng From Testing data <br>(LIFT) framework, is proposed. Considering many face recognition problems featured by the <br>inadequate training examples and availability of the vast testing examples, we aim to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2076222983512026941&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=8">Cited by 2</a> <a href="/scholar?q=related:PW-8ns050BwJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2076222983512026941&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'PW-8ns050BwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://hal.archives-ouvertes.fr/docs/00/66/98/74/PDF/vlad_thesis_-_FINAL.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from archives-ouvertes.fr</span><span class="gs_ggsS">archives-ouvertes.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://hal.archives-ouvertes.fr/tel-00669874/" class=yC5>Localisation Ã  partir de camÃ©ra vidÃ©o portÃ©e</a></h3><div class="gs_a">V Dovgalecs - 2011 - hal.archives-ouvertes.fr</div><div class="gs_rs">Abstract Visual lifelog indexing by content has emerged as a high reward application. <br>Enabled by the recent availability of miniaturized recording devices, the demand for <br>automatic extraction of relevant information from wearable sensors generated content has <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:zKuyEsReX00J:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5575279059928263628&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'zKuyEsReX00J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.dcc.ru.nl/~idak/teaching/batheses/BSc_beusekom_final.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ru.nl</span><span class="gs_ggsS">ru.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.dcc.ru.nl/~idak/teaching/batheses/BSc_beusekom_final.pdf" class=yC7>Empirically Evaluating Co-Training</a></h3><div class="gs_a">IG Sprinkhuizen-Kuyper, LG Vuurpijl - 2009 - dcc.ru.nl</div><div class="gs_rs">Abstract Co-training is a classification scheme needing only a small set of training instances <br>for correct classification. The main question assessed in this thesis was how co-training <br>performance varies with varying representativeness of the training data. 1280 co-training <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jU1XaDJB88UJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14263816129567214989&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'jU1XaDJB88UJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md5', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md5" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jU1XaDJB88UJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.see.ed.ac.uk/~s0571365/Files/Articles/ISCAS06/docs/papers/2753.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ed.ac.uk</span><span class="gs_ggsS">ed.ac.uk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1693881" class=yC9>Automatic video annotation based on co-adaptation and label correction</a></h3><div class="gs_a"><a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, Y Song, LR Dal&hellip; - Circuits and Systems,  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract As there is a large gap between high-level semantics and low-level features, it is <br>difficult to obtain high-accuracy video semantic annotation through automatic methods. In <br>this paper, we propose a novel automatic video annotation method, which greatly <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:lNETg3wSN7EJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12769695594416689556&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'lNETg3wSN7EJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://downloads.hindawi.com/journals/am/aip/175064.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hindawi.com</span><span class="gs_ggsS">hindawi.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://downloads.hindawi.com/journals/am/aip/175064.pdf" class=yCB>Multiple feature fusion based on Co-Training approach and time regularization for place classification in wearable video</a></h3><div class="gs_a">V Dovgalecs, R MÃ©gret, Y Berthoumieu - downloads.hindawi.com</div><div class="gs_rs">Abstract The analysis of video acquired with a wearable camera is a challenge that <br>Multimedia community is facing with the proliferation of such sensors in various applications. <br>In this article we focus on the problem of automatic visual place recognition in a weakly <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'P2-h-ldgGTEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:P2-h-ldgGTEJ:scholar.google.com/&amp;hl=en&amp;num=8&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
