Total results = 14
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://dblab.mgt.ncu.edu.tw/%E6%95%99%E6%9D%90/1001-seminar/05.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ncu.edu.tw</span><span class="gs_ggsS">ncu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/54N7570191982620.pdf" class=yC0>Semantic user profiling techniques for personalised multimedia recommendation</a></h3><div class="gs_a"><a href="/citations?user=tiASHnwAAAAJ&amp;hl=en&amp;oi=sra">F Hopfgartner</a>, <a href="/citations?user=ERvFJGkAAAAJ&amp;hl=en&amp;oi=sra">JM Jose</a> - Multimedia systems, 2010 - Springer</div><div class="gs_rs">Abstract Due to the explosion of news materials available through broadcast and other <br>channels, there is an increasing need for personalised news video retrieval. In this work, we <br>introduce a semantic-based user modelling technique to capture users&#39; evolving <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8386826343576215952&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 8</a> <a href="/scholar?q=related:kKG_yR_-Y3QJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8386826343576215952&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'kKG_yR_-Y3QJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://press.liacs.nl/students.mir/Exploiting%20External%20Knowledge%20to%20Improve%20Video%20Retrieval.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from liacs.nl</span><span class="gs_ggsS">liacs.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1743406" class=yC2>Exploiting external knowledge to improve video retrieval</a></h3><div class="gs_a"><a href="/citations?user=HI5HwzoAAAAJ&amp;hl=en&amp;oi=sra">D Vallet</a>, <a href="/citations?user=iCMJ360AAAAJ&amp;hl=en&amp;oi=sra">I Cantador</a>, <a href="/citations?user=ERvFJGkAAAAJ&amp;hl=en&amp;oi=sra">JM Jose</a> - Proceedings of the international  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Most video retrieval systems are multimodal, commonly relying on textual <br>information, low-and high-level semantic features extracted from query visual examples. In <br>this work, we study the impact of exploiting different knowledge sources in order to <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11207411684964154684&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 2</a> <a href="/scholar?q=related:PL3ZTYy5iJsJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11207411684964154684&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'PL3ZTYy5iJsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5329386" class=yC4>A Video Retrieval Algorithm Based on Affective Features</a></h3><div class="gs_a">L Zhaoming, W Xiangming, L Xinqi&hellip; - &hellip;  Technology, 2009. CIT&#39; &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract One important but often overlooked aspect of human interpretation of multimedia <br>data is the affective information. Affective labels of video content can be extracted <br>automatically from multimedia data streams. These can then be used for content-based <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2907383366919190343&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 2</a> <a href="/scholar?q=related:R_cYq8QZWSgJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2907383366919190343&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'R_cYq8QZWSgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/meet.2009.1450460210/full" class=yC5>A twoâstep model for video keyâframe determination</a></h3><div class="gs_a">H Kim, Y Kim - Proceedings of the American Society for  &hellip;, 2009 - Wiley Online Library</div><div class="gs_rs">Abstract What types of information in key-frames of a storyboard are critical when users <br>extract the meaning of a video? For this research question, we reviewed the literature and <br>then conducted the preliminary study. Next, based on the literature review and our <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2363443377100570389&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 2</a> <a href="/scholar?q=related:FT_UujqjzCAJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2363443377100570389&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'FT_UujqjzCAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21317/full" class=yC6>Toward a conceptual framework of keyâframe extraction and storyboard display for video summarization</a></h3><div class="gs_a">HH Kim, YH Kim - Journal of the American Society for  &hellip;, 2010 - Wiley Online Library</div><div class="gs_rs">Abstract Two key problems in developing a storyboard are (a) the extraction of video key <br>frames and (b) the display of the storyboard. On the basis of our findings from a preliminary <br>study as well as the results of previous studies on the computerized extraction of key <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5957296591097130097&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 2</a> <a href="/scholar?q=related:cWz9va-RrFIJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5957296591097130097&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'cWz9va-RrFIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://www.phil-fak.uni-duesseldorf.de/fileadmin/Redaktion/Institute/Informationswissenschaft/stock/1276778759iwp2010-4_.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-duesseldorf.de</span><span class="gs_ggsS">uni-duesseldorf.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.phil-fak.uni-duesseldorf.de/fileadmin/Redaktion/Institute/Informationswissenschaft/stock/1276778759iwp2010-4_.pdf" class=yC7>Indexieren von Emotionen bei Videos</a></h3><div class="gs_a"><a href="/citations?user=t8BsaHQAAAAJ&amp;hl=en&amp;oi=sra">K Knautz</a>, <a href="/citations?user=jviJm1gAAAAJ&amp;hl=en&amp;oi=sra">E DrÃ¶ge</a>, S Finkelmeyer&hellip; - Informationâ &hellip;, 2010 - phil-fak.uni-duesseldorf.de</div><div class="gs_rs">Weltraum dem Astronauten Dave Bowman. HAL ist ein mit einem kÃ¼nstlichen Wesen <br>versehener Supercomputer der Serie 9000, der das Raumschiff âDiscovery âautark lenkt. <br>HAL ist zu einer Gefahr fÃ¼r die Besatzung geworden, und Dave Bowman hat sich Zugang <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15211128803078725183&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 3</a> <a href="/scholar?q=related:P1447A3JGNMJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15211128803078725183&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'P1447A3JGNMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md5', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md5" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:P1447A3JGNMJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://www.ischool.utexas.edu/~geisler/publications/geisler-NRHM-crowdsourcing.pdf" class=yCA><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utexas.edu</span><span class="gs_ggsS">utexas.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.tandfonline.com/doi/abs/10.1080/13614568.2011.552645" class=yC9>A crowdsourcing framework for the production and use of film and television data</a></h3><div class="gs_a">G Geisler, G Willard, C Ovalle - New Review of Hypermedia and  &hellip;, 2011 - Taylor &amp; Francis</div><div class="gs_rs">This paper outlines a framework that would enable the detailed indexing of film and <br>television media through crowdsourcing. By making it easier to generate detailed data about <br>these media on a large scale, fans and scholars can more efficiently produce a wide <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13727815050855210383&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 2</a> <a href="/scholar?q=related:j118QA7_gr4J:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13727815050855210383&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'j118QA7_gr4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.phil-fak.uni-duesseldorf.de/fileadmin/Redaktion/Institute/Informationswissenschaft/stock/Collective_indexing_01.pdf" class=yCC><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uni-duesseldorf.de</span><span class="gs_ggsS">uni-duesseldorf.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.emeraldinsight.com/journals.htm?articleid=1955950&amp;show=abstract" class=yCB>Collective indexing of emotions in videos</a></h3><div class="gs_a"><a href="/citations?user=t8BsaHQAAAAJ&amp;hl=en&amp;oi=sra">K Knautz</a>, <a href="/citations?user=iGItXeEAAAAJ&amp;hl=en&amp;oi=sra">WG Stock</a> - Journal of Documentation, 2011 - emeraldinsight.com</div><div class="gs_rs">PurposeâThe object of this empirical research study is emotion, as depicted and aroused in <br>videos. This paper seeks to answer the questions: Are users able to index such emotions <br>consistently? Are the users&#39; votes usable for emotional video retrieval? Design/<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10992821852823162446&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 1</a> <a href="/scholar?q=related:Tg6tkDtZjpgJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10992821852823162446&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'Tg6tkDtZjpgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="https://aaltodoc.aalto.fi/bitstream/handle/123456789/4906/isbn9789526035468.pdf?sequence=1" class=yCE><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aalto.fi</span><span class="gs_ggsS">aalto.fi <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> <a href="https://aaltodoc.aalto.fi/handle/123456789/4906" class=yCD>Concept-based video search with the PicSOM multimedia retrieval system</a></h3><div class="gs_a">V Viitaniemi, M SjÃ¶berg, <a href="/citations?user=J7HDk50AAAAJ&amp;hl=en&amp;oi=sra">M Koskela</a>, <a href="/citations?user=suHzeyIAAAAJ&amp;hl=en&amp;oi=sra">J Laaksonen</a> - 2010 - aaltodoc.aalto.fi</div><div class="gs_rs">ABSTRACT: In this report we describe the structure of the PicSOM multimedia retrieval <br>system and elaborate on its automatic concept detection and video search subsystems. We <br>evaluate several alternative techniques for implementing these two components of the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3721199931345490325&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 1</a> <a href="/scholar?q=related:lVEOLp9bpDMJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'lVEOLp9bpDMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.cqvip.com/qk/97390x/201105/37684962.html" class=yCF>å©ç¨å¾åä¸è¿ç»­ç¹æ§çæº¶è§£åéå¤´æ£æµç®æ³</a></h3><div class="gs_a">å¼ å¯ï¼ å®æ°¸çº¢ï¼ æ¨è¾ - è®¡ç®æºè¾å©è®¾è®¡ä¸å¾å½¢å­¦å­¦æ¥, 2011 - cqvip.com</div><div class="gs_rs">éå¤´è¾¹çæ£æµæ¯è§é¢åæä¸­çå³é®ææ¯. ä¼ ç»çéå¤´è¾¹çæ£æµç®æ³åªå¨ååææ·¡å¥/æ·¡åºç­è¾ä¸º<br>ç®åçéå¤´è½¬åç±»åä¸æè¾å¥½çæ£æµæ§è½, èéå¯¹æº¶è§£åéå¤´å¸§å°æªææççç®æ³. ä¸ºæ­¤, <br>æåºä¸ç§åºäºå¾åä¸è¿ç»­æ§ç¹æ§çæº¶è§£åéå¤´å¸§æ£æµç®æ³. é¦åæåºä¸ç§åºäºå¾åä¸è¿ç»­ç<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11222069180509186129&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=14">Cited by 1</a> <a href="/scholar?q=related:UegbbXbMvJsJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11222069180509186129&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'UegbbXbMvJsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://fromtimetoti.me/downloads/human_computation_in_online_video_storytelling.pdf" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fromtimetoti.me</span><span class="gs_ggsS">fromtimetoti.me <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://fromtimetoti.me/downloads/human_computation_in_online_video_storytelling.pdf" class=yC10>Human Computation in Online Video Storytelling</a></h3><div class="gs_a">PDI van Kemenade - 2012 - fromtimetoti.me</div><div class="gs_rs">Abstract Tasks like retrieval, filtering and reconfiguration of digital video are difficult to solve <br>using current computational techniques. An important cause of this difficulty is the semantic <br>gap between visual representations and the meaning we address to them. A solution <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'_y6cmmanx_gJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_y6cmmanx_gJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/H31462466K6W3QX1.pdf" class=yC12>Exploiting semantics on external resources to gather visual examples for video retrieval</a></h3><div class="gs_a"><a href="/citations?user=HI5HwzoAAAAJ&amp;hl=en&amp;oi=sra">D Vallet</a>, <a href="/citations?user=iCMJ360AAAAJ&amp;hl=en&amp;oi=sra">I Cantador</a>, <a href="/citations?user=ERvFJGkAAAAJ&amp;hl=en&amp;oi=sra">JM Jose</a> - International Journal of Multimedia  &hellip;, 2012 - Springer</div><div class="gs_rs">Abstract With the huge and ever rising amount of video content available on the Web, there <br>is a need to facilitate video retrieval functionalities on very large collections. Most of the <br>current Web video retrieval systems rely on manual textual annotations to provide <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:kU44sTsGeEkJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5293988215420112529&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'kU44sTsGeEkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://lsas2008.dke-research.de/conftool/uploads/160/1-cigarran_UNED.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dke-research.de</span><span class="gs_ggsS">dke-research.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://lsas2008.dke-research.de/conftool/uploads/160/1-cigarran_UNED.pdf" class=yC13>A two-stage approach for Automatic Video Tagging using Speech Transcriptions1</a></h3><div class="gs_a">J CigarrÃ¡n, <a href="/citations?user=X_95ZxEAAAAJ&amp;hl=en&amp;oi=sra">V Fresno</a>, A Rodrigo, <a href="/citations?user=Y7G5f8MAAAAJ&amp;hl=en&amp;oi=sra">A GarcÃ­a-Serrano</a> - lsas2008.dke-research.de</div><div class="gs_rs">Abstract. When there is a lack of metadata information in real-world use cases and only <br>speech transcriptions are available, classical techniques for multi-label classification are <br>insufficient to perform video tagging task. In this paper we present an approach based on <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'AiqexKUxHZoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md12', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md12" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:AiqexKUxHZoJ:scholar.google.com/&amp;hl=en&amp;num=14&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6398155" class=yC15>Review of significant researches on multimedia information retrieval</a></h3><div class="gs_a">SM Jadhav, VS Patil - Communication, Information &amp;  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Traditionally, the database is of text and numerical data only, which is having less <br>attention nowadays because of the massive amount of multimedia content. In the multimedia <br>and storage technology, the preceding two decades have resulted in a substantial <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'XFqw-h50_cUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
