Total results = 23
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.5031&amp;rep=rep1&amp;type=pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1576260" class=yC0>Concept-based video retrieval</a></h3><div class="gs_a"><a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a> - Foundations and Trends in Information  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we review 300 references on video retrieval, indicating when text-only <br>solutions are unsatisfactory and showing the promising alternatives which are in majority <br>concept-based. Therefore, central to our discussion is the notion of a semantic concept: an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1240556430566916602&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 140</a> <a href="/scholar?q=related:-oHEN4BXNxEJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1240556430566916602&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 19 versions</a> <a onclick="return gs_ocit(event,'-oHEN4BXNxEJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md0', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md0" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:-oHEN4BXNxEJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=868717410844952179&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5422679" class=yC2>Survey on contemporary remote surveillance systems for public safety</a></h3><div class="gs_a">TD Raty - Systems, Man, and Cybernetics, Part C: Applications  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Surveillance systems provide the capability of collecting authentic and purposeful <br>information and forming appropriate decisions to enhance safety. This paper reviews <br>concisely the historical development and current state of the three different generations of <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17093176863420173682&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 34</a> <a href="/scholar?q=related:cs0VM8spN-0J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17093176863420173682&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'cs0VM8spN-0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="https://wwwx.cs.unc.edu/Courses/comp790-090-s11/Presentations/p1169-gupta.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from unc.edu</span><span class="gs_ggsS">unc.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1835951" class=yC3>Nonnegative shared subspace learning and its application to social media retrieval</a></h3><div class="gs_a"><a href="/citations?user=bXeL2t8AAAAJ&amp;hl=en&amp;oi=sra">SK Gupta</a>, <a href="/citations?user=OtA9SwIAAAAJ&amp;hl=en&amp;oi=sra">D Phung</a>, <a href="/citations?user=kbcVlyAAAAAJ&amp;hl=en&amp;oi=sra">B Adams</a>, <a href="/citations?user=zvspVLwAAAAJ&amp;hl=en&amp;oi=sra">T Tran</a>&hellip; - Proceedings of the 16th  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Although tagging has become increasingly popular in online image and video <br>sharing systems, tags are known to be noisy, ambiguous, incomplete and subjective. These <br>factors can seriously affect the precision of a social tag-based web retrieval system. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1690403091048116772&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 17</a> <a href="/scholar?q=related:JDIluaeEdRcJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1690403091048116772&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'JDIluaeEdRcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://infoscience.epfl.ch/record/142394/files/2010_mir_ramzan_et_al.pdf?version=1" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from epfl.ch</span><span class="gs_ggsS">epfl.ch <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1743470" class=yC5>The participation payoff: Challenges and opportunities for multimedia access in networked communities</a></h3><div class="gs_a">N Ramzan, <a href="/citations?user=eIiM958AAAAJ&amp;hl=en&amp;oi=sra">M Larson</a>, <a href="/citations?user=ziqjbTIAAAAJ&amp;hl=en&amp;oi=sra">F Dufaux</a>, K ClÃ¼ver - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Increasingly, multimedia collections are associated with networked communities <br>consisting of interconnected groups of users who create, annotate, browse, search, share, <br>view, critique and remix collection content. Information arises within networked <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10139794574956355261&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 8</a> <a href="/scholar?q=related:vd60Z3HJt4wJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10139794574956355261&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'vd60Z3HJt4wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/ft_gateway.cfm?id=1460135&amp;type=pdf" class=yC7>Multimedia information retrieval: watershed events</a></h3><div class="gs_a">R Jain - Proceeding of the 1st ACM international conference on  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Since its inception, Multimedia Information Retrieval (MIR) has made good steady <br>progress, but has not been able to keep up with the speed at which needs for this <br>technology evolved. We are at a very important juncture in the development of technology. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3615582948195656845&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 7</a> <a href="/scholar?q=related:jWQpA4khLTIJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3615582948195656845&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'jWQpA4khLTIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://staff.science.uva.nl/~cgmsnoek/pub/snoek-crowdsourcing-acm2010.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1873951.1874278" class=yC8>Crowdsourcing rock n&#39;roll multimedia retrieval</a></h3><div class="gs_a"><a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, B Freiburg, J Oomen&hellip; - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract In this technical demonstration, we showcase a multimedia search engine that <br>facilitates semantic access to archival rock n&#39;roll concert video. The key novelty is the <br>crowdsourcing mechanism, which relies on online users to improve, extend, and share, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14455780521541580845&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 8</a> <a href="/scholar?q=related:LTh_Wcs_ncgJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14455780521541580845&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'LTh_Wcs_ncgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://ivylab.kaist.ac.kr/htm/publication/paper/63.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0031320310004061" class=yCA>A comparative study of preprocessing mismatch effects in color image based face recognition</a></h3><div class="gs_a">JY Choi, YM Ro, <a href="/citations?user=W-4N_2gAAAAJ&amp;hl=en&amp;oi=sra">KN Plataniotis</a> - Pattern Recognition, 2011 - Elsevier</div><div class="gs_rs">Face color information can play an important role in face recognition (FR) and it can be used <br>to considerably improve FR performance obtained using only grayscale images. The color-<br>based FR methods involve a preprocessing step where a color image is converted into <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13252011156247873974&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 6</a> <a href="/scholar?q=related:tgmrN--Z6LcJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13252011156247873974&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'tgmrN--Z6LcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://ivylab.kaist.ac.kr/htm/publication/1.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5523909" class=yCC>Automatic face annotation in personal photo collections using context-based unsupervised clustering and face information fusion</a></h3><div class="gs_a">JY Choi, <a href="/citations?user=VpjWb7wAAAAJ&amp;hl=en&amp;oi=sra">W De Neve</a>, YM Ro&hellip; - Circuits and Systems  &hellip;, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, a novel face annotation framework is proposed that systematically <br>leverages context information such as situation awareness information with current face <br>recognition (FR) solutions. In particular, unsupervised situation and subject clustering <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=14547637655562451106&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 4</a> <a href="/scholar?q=related:onjl_F6X48kJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=14547637655562451106&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'onjl_F6X48kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://kusu.comp.nus.edu/proceedings/mm09/imce/p27.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1631040.1631046" class=yCE>Key frame vector and its application to shot retrieval</a></h3><div class="gs_a">Y Gao, J Tang, X Xie - Proceedings of the 1st international workshop on  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract This paper proposes a video representation method named key frame vector (KFV) <br>to support video shot retrieval. By considering temporal correlation between frames reflects <br>the relations between visual contents of frames, and video content weighting has <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3138230237522718604&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 3</a> <a href="/scholar?q=related:jG_uIsY7jSsJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3138230237522718604&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'jG_uIsY7jSsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5443168" class=yC10>Biologically-inspired abstraction model to analyze sound signal</a></h3><div class="gs_a">HI bin Hamzah, A bin Abdullah&hellip; - &hellip;  (SCOReD), 2009 IEEE  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This research studies the human ear and human brain as a new idea to analyze <br>sound. The human ear to be exact; the eardrum detects the sound signal and the cochlea <br>filters the frequency signal. Subsequently, the brain is capable to recognize and learn the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4348708749720220168&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 3</a> <a href="/scholar?q=related:CPK0QpG3WTwJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'CPK0QpG3WTwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://koasas.kaist.ac.kr/bitstream/10203/22678/1/Face%20Annotation%20For%20Personal%20Photos%20Using%20Collaborative%20Face%20Recognition%20in%20Online%20Social%20Networks.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5201095" class=yC11>Face annotation for personal photos using collaborative face recognition in online social networks</a></h3><div class="gs_a">JY Choi, <a href="/citations?user=VpjWb7wAAAAJ&amp;hl=en&amp;oi=sra">W De Neve</a>, YM Ro&hellip; - &hellip;  Processing, 2009 16th  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic face annotation (or tagging) facilitates improved retrieval and <br>organization of personal photos in online social networks. In this paper, we present a new <br>collaborative face recognition (FR) method that aims to improve face annotation accuracy. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7246717783701849528&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 2</a> <a href="/scholar?q=related:uAUO9W2DkWQJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7246717783701849528&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'uAUO9W2DkWQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://www.science.uva.nl/research/publications/2011/SnoekIM2011/snoek_grandchallenge_mm.pdf" class=yC14><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5720672" class=yC13>Academia Meets Industry at the Multimedia Grand Challenge</a></h3><div class="gs_a"><a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=lD5J91cAAAAJ&amp;hl=en&amp;oi=sra">M Slaney</a> - Multimedia, IEEE, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract This column is about last year&#39;s ACM Multimedia Grand Challenge in Florence, <br>Italy, an event that endeavors to connect (academic) researchers more effectively with the <br>realities of the business world. The authors describe the 10 challenges and present the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17408011908177923053&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 1</a> <a href="/scholar?q=related:7Qu3UJmulfEJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17408011908177923053&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 14 versions</a> <a onclick="return gs_ocit(event,'7Qu3UJmulfEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://ro.uow.edu.au/cgi/viewcontent.cgi?article=4222&amp;context=theses" class=yC16><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uow.edu.au</span><span class="gs_ggsS">uow.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ro.uow.edu.au/theses/3222/" class=yC15>Robust content-based image retrieval of multi-example queries</a></h3><div class="gs_a"><a href="/citations?user=QmsjV8QAAAAJ&amp;hl=en&amp;oi=sra">J Zhang</a> - 2011 - ro.uow.edu.au</div><div class="gs_rs">Abstract This thesis investigates three major issues in the active field of content-based <br>image retrieval (CBIR), which are feature aggregation for similarity measure, robust <br>contentbased image retrieval and retrieval model by incorporating background knowledge.</div><div class="gs_fl"><a href="/scholar?cites=17615136502393770643&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 1</a> <a href="/scholar?q=related:k5Iy51GJdfQJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'k5Iy51GJdfQJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB13" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW13"><a href="https://viscenter.uncc.edu/sites/viscenter.uncc.edu/files/CVC-UNCC-12-02.pdf" class=yC18><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uncc.edu</span><span class="gs_ggsS">uncc.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2324840" class=yC17>Efficient graffiti image retrieval</a></h3><div class="gs_a">C Yang, <a href="/citations?user=l9ysrkUAAAAJ&amp;hl=en&amp;oi=sra">PC Wong</a>, W Ribarsky, J Fan - Proceedings of the 2nd ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Research of graffiti character recognition and retrieval, as a branch of traditional <br>optical character recognition (OCR), has started to gain attention in recent years. We have <br>investigated the special challenge of the graffiti image retrieval problem and propose a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:kLfsB57smJ4J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11428144217934575504&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'kLfsB57smJ4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB14" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW14"><a href="http://uhdspace.uhasselt.be/dspace/bitstream/1942/12151/1/000003604%20Haesen_et_al_final_MTAP2011.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uhasselt.be</span><span class="gs_ggsS">uhasselt.be <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/N7V5517Q736V9484.pdf" class=yC19>Finding a needle in a haystack: an interactive video archive explorer for professional video searchers</a></h3><div class="gs_a"><a href="/citations?user=2V8SgO0AAAAJ&amp;hl=en&amp;oi=sra">M Haesen</a>, J Meskens, <a href="/citations?user=k3i997gAAAAJ&amp;hl=en&amp;oi=sra">K Luyten</a>, K Coninx&hellip; - Multimedia Tools and  &hellip;, 2011 - Springer</div><div class="gs_rs">Abstract Professional video searchers typically have to search for particular video fragments <br>in a vast video archive that contains many hours of video data. Without having the right video <br>archive exploration tools, this is a difficult and time consuming task that induces hours of <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:AFjmERKnL1sJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6570654077408401408&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'AFjmERKnL1sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB15" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW15"><a href="http://www.ijcsi.org/papers/IJCSI-9-5-3-191-202.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ijcsi.org</span><span class="gs_ggsS">ijcsi.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ijcsi.org/papers/IJCSI-9-5-3-191-202.pdf" class=yC1B>Contextual Query Perfection by Affective Features Based Implicit Contextual Semantic Relevance Feedback in Multimedia Information Retrieval</a></h3><div class="gs_a">KV Singh, <a href="/citations?user=0P7sr_kAAAAJ&amp;hl=en&amp;oi=sra">AK Tripathi</a> - 2012 - ijcsi.org</div><div class="gs_rs">Abstract Multimedia Information may have multiple semantics depending on context, a <br>temporal interest and user preferences. Hence we are exploiting the plausibility of context <br>associated with semantic concept in retrieving relevance information. We are proposing <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'jn-MLnnXcUMJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md15', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md15" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jn-MLnnXcUMJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://www.ijser.org/researchpaper%5CIntelligent-Information-Retrieval-in-Data-Mining.pdf" class=yC1E><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ijser.org</span><span class="gs_ggsS">ijser.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.ijser.org/researchpaper%5CIntelligent-Information-Retrieval-in-Data-Mining.pdf" class=yC1D>Intelligent Information Retrieval in Data Mining</a></h3><div class="gs_a">RP Singh, P Yadav - ijser.org</div><div class="gs_rs">Abstract: In this paper we present the methodologies and challenges of information retrieval. <br>We will focus on data mining, data warehousing, information retrieval, data mining ontology, <br>intelligent information retrieval. We will also focus on fundamental concepts behind all <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:z4UPg7xMnBkJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'z4UPg7xMnBkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md16', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md16" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:z4UPg7xMnBkJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Fundamentals of Multimedia Computing</h3><div class="gs_a"><a href="/citations?user=iBl-QgEAAAAJ&amp;hl=en&amp;oi=sra">G Friedland</a>, R Jain</div><div class="gs_fl"><a href="/scholar?q=related:yxL2j6GPXI4J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10258231975310922443&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'yxL2j6GPXI4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://fromtimetoti.me/downloads/human_computation_in_online_video_storytelling.pdf" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fromtimetoti.me</span><span class="gs_ggsS">fromtimetoti.me <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://fromtimetoti.me/downloads/human_computation_in_online_video_storytelling.pdf" class=yC1F>Human Computation in Online Video Storytelling</a></h3><div class="gs_a">PDI van Kemenade - 2012 - fromtimetoti.me</div><div class="gs_rs">Abstract Tasks like retrieval, filtering and reconfiguration of digital video are difficult to solve <br>using current computational techniques. An important cause of this difficulty is the semantic <br>gap between visual representations and the meaning we address to them. A solution <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'_y6cmmanx_gJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md18', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md18" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:_y6cmmanx_gJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5712695" class=yC21>Query by humming system for embedded platforms</a></h3><div class="gs_a"><a href="/citations?user=eLW_23AAAAAJ&amp;hl=en&amp;oi=sra">P Krishnamoorthy</a>, R Bhatt, A Srinivas&hellip; - &hellip; ), 2010 Annual IEEE, 2010 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Query by humming (QBH) is a method of searching for a songs in a multimedia <br>database system that contains the melody descriptions of songs. The database of songs can <br>be searched by hummed queries. The user hums a melody into a microphone that is <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:FJgmtIqe3XoJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'FJgmtIqe3XoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://dspace.kaist.ac.kr/bitstream/10203/22676/1/Image%20Compression%20Mismatch%20Effect%20on%20Color%20Image%20Based%20Face%20Recognition%20System.pdf" class=yC23><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from kaist.ac.kr</span><span class="gs_ggsS">kaist.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5413445" class=yC22>Image compression mismatch effect on color image based face recognition system</a></h3><div class="gs_a">JY Choi, YM Ro, <a href="/citations?user=W-4N_2gAAAAJ&amp;hl=en&amp;oi=sra">KN Plataniotis</a> - Image Processing (ICIP),  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Face recognition (FR) for emerging applications such as face tagging for social <br>networking, consumer products, and gamming utilize color images stored in distributed <br>repositories. Such images are often in compressed format and of different dimensions. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16391061932347356551&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 1</a> <a href="/scholar?q=related:h_X94Pu_eOMJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16391061932347356551&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'h_X94Pu_eOMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://arxiv.org/pdf/1205.1641" class=yC25><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://arxiv.org/abs/1205.1641" class=yC24>Content based video retrieval systems</a></h3><div class="gs_a">BV Patel, BB Meshram - arXiv preprint arXiv:1205.1641, 2012 - arxiv.org</div><div class="gs_rs">Abstract: With the development of multimedia data types and available bandwidth there is <br>huge demand of video retrieval systems, as users shift from text based retrieval systems to <br>content based retrieval systems. Selection of extracted features play an important role in <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:5dsk8MR6gS4J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3351094584001485797&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'5dsk8MR6gS4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://eprints.utp.edu.my/869/1/7._SCOReD2009_-__paper_-_hammuzamer.pdf" class=yC27><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from utp.edu.my</span><span class="gs_ggsS">utp.edu.my <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://eprints.utp.edu.my/869/" class=yC26>Biologically-Inspired Abstraction Model to Analyze Sound Signal</a></h3><div class="gs_a">H Hammuzamer Irwan, A Azween - 2008 - eprints.utp.edu.my</div><div class="gs_rs">This research studies the human ear and human brain as a new idea to analyze sound. The <br>human ear to be exact; the eardrum detects the sound signal and the cochlea filters the <br>frequency signal. Subsequently, the brain is capable to recognize and learn the sound <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:Utlh8_W7iR4J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Utlh8_W7iR4J')" href="#" class="gs_nph">Cite</a></div></div></div>
