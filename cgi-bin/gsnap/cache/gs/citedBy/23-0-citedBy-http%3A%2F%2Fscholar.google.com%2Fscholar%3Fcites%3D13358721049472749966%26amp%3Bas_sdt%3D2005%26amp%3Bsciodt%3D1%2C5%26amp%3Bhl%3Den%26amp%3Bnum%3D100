Total results = 23
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://pdf.aminer.org/000/893/979/video_modeling_using_strata_based_annotation.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from aminer.org</span><span class="gs_ggsS">aminer.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=839313" class=yC0>Video modeling using strata-based annotation</a></h3><div class="gs_a">MS Kankanhalli, TS Chua - Multimedia, IEEE, 2000 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Our project on digital video modeling aims to achieve efficient browsing and <br>retrieval. Video is not merely a huge collection of still images, it is a complex temporal <br>medium capturing high-level semantic ideas. For almost a decade researchers have <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16019540471568062465&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 32</a> <a href="/scholar?q=related:AdjOCC_XUN4J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/03/19/RN077309202.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=16019540471568062465&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 16 versions</a> <a onclick="return gs_ocit(event,'AdjOCC_XUN4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://www.csie.mcu.edu.tw/~yklee/CVGIP03/CD/Paper/VP/VP-05.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from mcu.edu.tw</span><span class="gs_ggsS">mcu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.csie.mcu.edu.tw/~yklee/CVGIP03/CD/Paper/VP/VP-05.pdf" class=yC3>Semantic scenes detection and classification in sports videos</a></h3><div class="gs_a">SC Pei, F Chen - Proceedings of IPPR Conference on Computer  &hellip;, 2003 - csie.mcu.edu.tw</div><div class="gs_rs">Abstract This paper presents the method to index and retrieval the tennis video and the <br>baseball video. First we will introduce semantic scenes and how to detect and classify them. <br>We use some low-level features and combine domain-knowledge to detect semantic <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7736978006503346290&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 26</a> <a href="/scholar?q=related:clCkMIFEX2sJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7736978006503346290&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'clCkMIFEX2sJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md1', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md1" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:clCkMIFEX2sJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.idi.ntnu.no/~heggland/ontolog/OntoLog.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntnu.no</span><span class="gs_ggsS">ntnu.no <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/uk2k391682jcn58e.pdf" class=yC5>Ontolog: Temporal annotation using ad hoc ontologies and application profiles</a></h3><div class="gs_a">J Heggland - Research and Advanced Technology for Digital  &hellip;, 2002 - Springer</div><div class="gs_rs">This paper describes OntoLog, a prototype annotation system for temporal media. It is a <br>Java application built to explore the issues and benefits of using ontologies, application <br>profiles and RDF for temporal annotation. It uses an annotation scheme based on <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16621731764190102021&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 10</a> <a href="/scholar?q=related:BcJgTPlArOYJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/38/4A/RN119940461.html?source=googlescholar" class="gs_nph" class=yC7>BL Direct</a> <a href="/scholar?cluster=16621731764190102021&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'BcJgTPlArOYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.9383&amp;rep=rep1&amp;type=pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.9383&amp;rep=rep1&amp;type=pdf" class=yC8>Adaptive Video Summarization</a></h3><div class="gs_a">P Mulhem, J Gensel, <a href="/citations?user=QwWiE6AAAAAJ&amp;hl=en&amp;oi=sra">H Martin</a> - The Handbook of Video Databases  &hellip;, 2003 - Citeseer</div><div class="gs_rs">One of the specific characteristics of the video medium is to be a temporal medium: it has an <br>inherent duration and the time spent to find information present in a video depends <br>somehow on its duration. Without any knowledge about the video, it is necessary to use <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11973387459082960438&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 7</a> <a href="/scholar?q=related:Nr7B0HQEKqYJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11973387459082960438&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'Nr7B0HQEKqYJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:Nr7B0HQEKqYJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www.irit.fr/page-perso/Benjamin.Bigot/pdf/TowardsTheDetectionAndTheCharacterizationOfConversationalSpeechZonesInAudiovisualDocuments.pdf" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from irit.fr</span><span class="gs_ggsS">irit.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4564942" class=yCA>Towards the detection and the characterization of conversational speech zones in audiovisual documents</a></h3><div class="gs_a">B Bigot, I FerranÃ©, Z Ibrahim - Content-Based Multimedia  &hellip;, 2008 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Giving access to the semantically rich content of large amounts of digital <br>audiovisual data using an automatic and generic method is still an important challenge. The <br>aim of our work is to address this issue while focusing on temporal aspects. Our approach <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5763373531809782831&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 5</a> <a href="/scholar?q=related:LxxpB7Gd-08J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5763373531809782831&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'LxxpB7Gd-08J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/TK9467438X839741.pdf" class=yCC>Savantaâsearch, analysis, visualisation and navigation of temporal annotations</a></h3><div class="gs_a">JO Hauglid, J Heggland - Multimedia Tools and Applications, 2008 - Springer</div><div class="gs_rs">Abstract In this article, we present Savanta, an information gathering interface for temporal, <br>semantic video annotations. In Savanta, we integrate various methods and paradigms for <br>information gathering, including visualisation, filtering, data mining, navigation and search<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9121870399646584618&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 5</a> <a href="/scholar?q=related:Ks8Vesxkl34J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9121870399646584618&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'Ks8Vesxkl34J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/spe.722/abstract" class=yCD>A modular and adaptive framework for large scale video indexing and contentâbased retrieval: the SIRSALE system</a></h3><div class="gs_a">A Mostefaoui - Software: Practice and Experience, 2006 - Wiley Online Library</div><div class="gs_rs">Abstract In this paper, we present the design and the implementation of SIRSALE: a <br>distributed video data management system. SIRSALE allows users to manipulate video <br>streams stored in large distributed repositories, ie it provides remote users with <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5563475475211414029&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 5</a> <a href="/scholar?q=related:DeqS2HdvNU0J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/33/60/RN188610156.html?source=googlescholar" class="gs_nph" class=yCE>BL Direct</a> <a href="/scholar?cluster=5563475475211414029&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'DeqS2HdvNU0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/50H7M6LVNFNHAX3A.pdf" class=yCF>High-speed dialog detection for automatic segmentation of recorded TV program</a></h3><div class="gs_a">H Aoki - Image and Video Retrieval, 2005 - Springer</div><div class="gs_rs">Abstract. To provide easy access to scenes of interest in recorded video, structure-sensitive <br>segmentation is necessary. In TV programs, similar shots appear repeatedly, and such <br>appearance can be a clue to estimate a contextual group of shots. The author introduces a <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4168805971963246525&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 4</a> <a href="/scholar?q=related:vXdluPGS2jkJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/48/2B/RN171992073.html?source=googlescholar" class="gs_nph" class=yC10>BL Direct</a> <a href="/scholar?cluster=4168805971963246525&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'vXdluPGS2jkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1743437" class=yC11>Stratification-based keyframe cliques for removal of near-duplicates in video search results</a></h3><div class="gs_a">X Cheng, <a href="/citations?user=Eeolw80AAAAJ&amp;hl=en&amp;oi=sra">LT Chia</a> - Proceedings of the international conference on  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract The current volume of videos available for distribution or viewing on the internet is <br>increasing exponentially, there is an urgent need for designing effective and efficient video <br>management systems. However, due to the tremendous amounts of video data, it is highly <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10980255124347820449&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 3</a> <a href="/scholar?q=related:oRnqFdyzYZgJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'oRnqFdyzYZgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.8364&amp;rep=rep1&amp;type=pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.8364&amp;rep=rep1&amp;type=pdf" class=yC12>Detection of objects in video in contrast feature domain</a></h3><div class="gs_a">TS Chua, Y Zhao, Y Zhang - Proc. of IEEE Pacific-Rim Conf. on Multimedia &hellip;, 2000 - Citeseer</div><div class="gs_rs">ABSTRACT Recent advances in computing, networking and multimedia technologies have <br>brought about a surge in digital multimedia applications. In particular, there are great <br>interests to develop automated tools to manage the huge amount of digital video. To <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13224834987627220307&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 3</a> <a href="/scholar?q=related:U42tYloNiLcJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13224834987627220307&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'U42tYloNiLcJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md9', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md9" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:U42tYloNiLcJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://ntnu.diva-portal.org/smash/get/diva2:125892/FULLTEXT01" class=yC15><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from diva-portal.org</span><span class="gs_ggsS">diva-portal.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ntnu.diva-portal.org/smash/record.jsf?pid=diva2:125892" class=yC14>Ontolog: Flexible management of semantic video content annotations</a></h3><div class="gs_a">J Heggland - 2005 - ntnu.diva-portal.org</div><div class="gs_rs">Abstract To encode, query and present the semantic content of digital video precisely and <br>flexibly is very useful for many kinds of knowledge work: system analysis and evaluation, <br>documentation and education, to name a few. However, that kind of video management is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=2288667807283695422&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 3</a> <a href="/scholar?q=related:Pk8Erj77wh8J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=2288667807283695422&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'Pk8Erj77wh8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB11" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW11"><a href="http://mrim.imag.fr/publications/2003/PM001/mulhem03bISI.pdf" class=yC17><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from imag.fr</span><span class="gs_ggsS">imag.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://mrim.imag.fr/publications/2003/PM001/mulhem03bISI.pdf" class=yC16>ModÃ¨les pour rÃ©sumÃ©s adaptatifs de vidÃ©os</a></h3><div class="gs_a">P Mulhem, J Gensel, <a href="/citations?user=QwWiE6AAAAAJ&amp;hl=en&amp;oi=sra">H Martin</a> - INGENIERIE DES SYSTEMS D  &hellip;, 2002 - mrim.imag.fr</div><div class="gs_rs">RÃSUMÃ. La vidÃ©o est un mÃ©dia qui pose des problÃ¨mes complexes en raison du volume <br>important de donnÃ©es Ã  traiter et de la difficultÃ© de reprÃ©senter et d&#39;extraire des informations <br>de son contenu. Nous proposons d&#39;annoter le contenu d&#39;une vidÃ©o Ã  l&#39;aide de Graphes <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1550349735689949545&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 5</a> <a href="/scholar?q=related:aZFr2d7ygxUJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/22/0B/RN130434150.html?source=googlescholar" class="gs_nph" class=yC18>BL Direct</a> <a href="/scholar?cluster=1550349735689949545&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'aZFr2d7ygxUJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md11', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md11" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:aZFr2d7ygxUJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB12" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW12"><a href="http://www.irit.fr/GDR-I3/fichiers/assises2002/papers/11-RechercheDInformationMultimedia.pdf" class=yC1A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from irit.fr</span><span class="gs_ggsS">irit.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.irit.fr/GDR-I3/fichiers/assises2002/papers/11-RechercheDInformationMultimedia.pdf" class=yC19>Recherche d&#39;information multimÃ©dia</a></h3><div class="gs_a">F SÃ¨des, <a href="/citations?user=QwWiE6AAAAAJ&amp;hl=en&amp;oi=sra">H Martin</a> - 2002 - irit.fr</div><div class="gs_rs">Le multimÃ©dia permet de combiner des donnÃ©es de diffÃ©rents types (texte, image, audio, <br>vidÃ©o) Ã  l&#39;intÃ©rieur d&#39;un mÃªme document numÃ©rique. De nombreux logiciels permettent de <br>rÃ©aliser de tels documents ou hyperdocuments de faÃ§on ad hoc. NÃ©anmoins, pour <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3087534796947009578&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=23">Cited by 1</a> <a href="/scholar?q=related:KjCvE4og2SoJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3087534796947009578&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'KjCvE4og2SoJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md12', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md12" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:KjCvE4og2SoJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/5bwmgg1pd7hxaqmk.pdf" class=yC1B>Practical applications of multimedia search</a></h3><div class="gs_a">R Jain - Image and Video Retrieval, 2005 - Springer</div><div class="gs_rs">Abstract. Just one decade ago image and video retrieval was a technology looking for <br>applications. Now people are dying to get image and video retrieval technology, but there <br>are no good practical solutions. Advances in devices, processing, and storage have <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jZ6_V74ZqncJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/04/40/RN171992057.html?source=googlescholar" class="gs_nph" class=yC1C>BL Direct</a> <a href="/scholar?cluster=8622732741860892301&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'jZ6_V74ZqncJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6009224" class=yC1D>Stratification-Based Keyframe Cliques for Effective and Efficient Video Representation</a></h3><div class="gs_a">X Cheng, <a href="/citations?user=Eeolw80AAAAJ&amp;hl=en&amp;oi=sra">LT Chia</a> - Multimedia, IEEE Transactions on, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract As there is an exponential increase of web videos, it is time-consuming to get a <br>query result from the tremendous data. An effective and efficient video management system <br>is in urgent need. To increase the efficiency of video retrieval and storage, the most widely <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:0m-mmaj8TbIJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12848203113020420050&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'0m-mmaj8TbIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://books.google.com/books?hl=en&amp;lr=&amp;id=Nqt0VKDmLyMC&amp;oi=fnd&amp;pg=PA181&amp;ots=yxLtFNy_iq&amp;sig=TnKUfp82UlXv7_yX9QZMbgA178I" class=yC1E>An MPEG-7 Based Video Database Management System</a></h3><div class="gs_a">A Yazicii, O Yavuzi, R George - Spatio-Temporal Databases:  &hellip;, 2004 - books.google.com</div><div class="gs_rs">The availability of higher storage capacities and faster processing speeds have made it <br>possible to store and operate on multimedia data resources such as images, graphics, 3D <br>models, audio, speech, and video. The quantity of multimedia data available in digital <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:y2qZrEpUL_cJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'y2qZrEpUL_cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB16" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW16"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/13633/Zhao_Yunlong_PhD_thesis.pdf?sequence=1" class=yC20><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/13633" class=yC1F>Automatic extraction and tracking of face sequences in MPEG video</a></h3><div class="gs_a">Z Yunlong - 2004 - scholarbank.nus.edu</div><div class="gs_rs">This PhD work focuses on the problem of extracting multiple face sequences from MPEG <br>video based on face detection and tracking. It aims to facilitate the strata-based digital video <br>modelling to achieve efficient video retrieval and browsing. The research includes the <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:LWH47rNLYawJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12421292483445023021&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'LWH47rNLYawJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB17" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW17"><a href="https://www.seco.tkk.fi/publications/2006/salminen-sisallonkuvailu-2006.pdf" class=yC22><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from tkk.fi</span><span class="gs_ggsS">tkk.fi <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="https://www.seco.tkk.fi/publications/2006/salminen-sisallonkuvailu-2006.pdf" class=yC21>HELSINGIN YLIOPISTO TietojenkÃ¤sittelytieteen laitos</a></h3><div class="gs_a">M Salminen - seco.tkk.fi</div><div class="gs_rs">TÃ¤ssÃ¤ tutkielmassa muodostetaan kuvien sisÃ¤llÃ¶nkuvailuun sopiva rakenteellinen <br>annotointiskeema. Aineistona kÃ¤ytetÃ¤Ã¤n taideteoksia, valokuvia ja videoita, joiden avulla <br>annetaan monipuolisia esimerkkejÃ¤ muodostettavan annotointiskeeman kÃ¤ytÃ¶stÃ¤. TyÃ¶ssÃ¤ <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:rnctA-RrqIgJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9847239212295616430&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 13 versions</a> <a onclick="return gs_ocit(event,'rnctA-RrqIgJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md17', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md17" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:rnctA-RrqIgJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://ntur.lib.ntu.edu.tw/bitstream/246246/8007/1/922219E002013.pdf" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntu.edu.tw</span><span class="gs_ggsS">ntu.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> <a href="http://ntur.lib.ntu.edu.tw/handle/246246/8007" class=yC23>æºæ§åé³è¦è¨åå³è¼¸æè¡åå¤åªé«æç¨ (II)âç¸½è¨ç«</a></h3><div class="gs_a">è²èç«  - 2004 - ntur.lib.ntu.edu.tw</div><div class="gs_rs">éåè¦è¨å¨å¤§é¨åçé»è¦é »éæ¯ç¸ç¶ä¸»è¦çç¯ç®, èä¸æè¨±å¤çè§ç¾ç¾¤. åå ä¸éåè¦è¨æå¾<br>åºå®çå§å®¹æ¶æ§ä¸æå¶è¦å. å æ­¤, æåå¨æ­¤è«æä¸­æåºä¸ç³»åçæ¹æ³å»åæ, ç´¢å¼, <br>æ¨¡çµåç¶²çéåè¦è¨. æ­¤å¤, æåéææåºä¸è¦è¨ç©é«æ·åçæ¹æ³. éæ¹æ³å¯ä»¥å¹«å©æåè¿½è¹¤æ<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:gJ5pKjr4c7sJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13507412636117606016&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'gJ5pKjr4c7sJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> UNIVERSITE JOSEPH FOURIERâGRENOBLE</h3><div class="gs_a">J GENSEL</div><div class="gs_fl"><a href="/scholar?q=related:ds06M-isvBkJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=1854547259861683574&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'ds06M-isvBkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB20" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW20"><a href="http://brutal.googlecode.com/svn-history/r48/trunk/CarlosPimentelDissertacao.pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from googlecode.com</span><span class="gs_ggsS">googlecode.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://brutal.googlecode.com/svn-history/r48/trunk/CarlosPimentelDissertacao.pdf" class=yC25>UNIVERSIDADE SALVADORâUNIFACS</a></h3><div class="gs_a">UMAPIE RECUPERAÃÃO, DEVBEM DE CONTEÃDO - brutal.googlecode.com</div><div class="gs_rs">ABSTRACT This work is centered in the area of video information retrieval based on its <br>visual characteristics. It proposes an environment for automatic summarization and indexing <br>of digital videos in order to support the operations of query based on visual content in a <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:7bTUQKXu-f4J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18372978548359345389&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'7bTUQKXu-f4J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md20', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md20" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:7bTUQKXu-f4J:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB21" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW21"><a href="http://stephane.ayache.perso.esil.univmed.fr/these.pdf" class=yC28><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from univmed.fr</span><span class="gs_ggsS">univmed.fr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://stephane.ayache.perso.esil.univmed.fr/these.pdf" class=yC27>StÃ©phane Ayache</a></h3><div class="gs_a">A Lux, <a href="/citations?user=rFaxB20AAAAJ&amp;hl=en&amp;oi=sra">P Gallinari</a>, P Joly, G QuÃ©not - stephane.ayache.perso.esil.univmed &hellip;</div><div class="gs_rs">This work deals with information retrieval and aims to reach semantic indexing of multimedia <br>documents. The state of the art approach tackle this problem by bridging of the semantic gap <br>between low-level features, from each modality, and high-level features (concepts), which <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:W_wKljMSU3IJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8237948156160703579&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'W_wKljMSU3IJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md21', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md21" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:W_wKljMSU3IJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB22" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW22"><a href="http://ir.lib.ntust.edu.tw/bitstream/987654321/4939/1/NSC92-2219-E002-013.pdf" class=yC2A><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from ntust.edu.tw</span><span class="gs_ggsS">ntust.edu.tw <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> <a href="http://ir.lib.ntust.edu.tw/handle/987654321/4939" class=yC29>æºæ§åé³è¦è¨åå³è¼¸æè¡åå¤åªé«æç¨---ç¸½è¨ç« (II)</a></h3><div class="gs_a">è²èç« ï¼ ææå®ï¼ é¦®ä¸éï¼ é³è¯åº - 2003 - ir.lib.ntust.edu.tw</div><div class="gs_rs">æè¦éåè¦è¨å¨å¤§é¨åçé»è¦é »éæ¯ç¸ç¶ä¸»è¦çç¯ç®, èä¸æè¨±å¤çè§ç¾ç¾¤. <br>åå ä¸éåè¦è¨æå¾åºå®çå§å®¹æ¶æ§ä¸æå¶è¦å. å æ­¤, æåå¨æ­¤è«æä¸­æåºä¸ç³»åçæ¹æ³å»<br>åæ, ç´¢å¼, æ¨¡çµåç¶²çéåè¦è¨. æ­¤å¤, æåéææåºä¸è¦è¨ç©é«æ·åçæ¹æ³. éæ¹æ³å¯ä»¥å¹«å©<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:-kZHpEwWs4wJ:scholar.google.com/&amp;hl=en&amp;num=23&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'-kZHpEwWs4wJ')" href="#" class="gs_nph">Cite</a></div></div></div>
