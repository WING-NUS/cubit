Total results = 11
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://mmas.comp.nus.edu.sg/63140030.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/j80g848m551677u7.pdf" class=yC0>An eye fixation database for saliency detection in images</a></h3><div class="gs_a"><a href="/citations?user=mUvcmRsAAAAJ&amp;hl=en&amp;oi=sra">S Ramanathan</a>, <a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, <a href="/citations?user=fOmROdkAAAAJ&amp;hl=en&amp;oi=sra">N Sebe</a>, M Kankanhalli&hellip; - Computer VisionâECCV  &hellip;, 2010 - Springer</div><div class="gs_rs">To learn the preferential visual attention given by humans to specific image content, we <br>present NUSEF-an eye fixation database compiled from a pool of 758 images and 75 <br>subjects. Eye fixations are an excellent modality to learn semantics-driven human <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5254088566672117251&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 22</a> <a href="/scholar?q=related:A87uyrZF6kgJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5254088566672117251&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'A87uyrZF6kgJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://infolab.stanford.edu/~wangz/project/imsearch/Aesthetics/SPM11/joshi.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from stanford.edu</span><span class="gs_ggsS">stanford.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5999579" class=yC2>Aesthetics and emotions in images</a></h3><div class="gs_a"><a href="/citations?user=TYmV4V8AAAAJ&amp;hl=en&amp;oi=sra">D Joshi</a>, <a href="/citations?user=GmJRVxgAAAAJ&amp;hl=en&amp;oi=sra">R Datta</a>, E Fedorovskaya&hellip; - Signal Processing  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this tutorial, we define and discuss key aspects of the problem of computational <br>inference of aesthetics and emotion from images. We begin with a background discussion <br>on philosophy, photography, paintings, visual arts, and psychology. This is followed by <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7798653402718243434&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 9</a> <a href="/scholar?q=related:aoppY_NhOmwJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7798653402718243434&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'aoppY_NhOmwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://www.comp.nus.edu.sg/~harishk/homepage_material/ACM_MM_2010_harish.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1874047" class=yC4>Making computers look the way we look: exploiting visual attention for image understanding</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, <a href="/citations?user=mUvcmRsAAAAJ&amp;hl=en&amp;oi=sra">R Subramanian</a>, M Kankanhalli&hellip; - Proceedings of the  &hellip;, 2010 - dl.acm.org</div><div class="gs_rs">Abstract Human Visual attention (HVA) is an important strategy to focus on specific <br>information while observing and understanding visual stimuli. HVA involves making a series <br>of fixations on select locations while performing tasks such as object recognition, scene <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10386692180968100685&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 4</a> <a href="/scholar?q=related:TVPu7YHxJJAJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10386692180968100685&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'TVPu7YHxJJAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://kola.opus.hbz-nrw.de/volltexte/2011/640/pdf/2011_08_Arbeitsberichte.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from hbz-nrw.de</span><span class="gs_ggsS">hbz-nrw.de <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://kola.opus.hbz-nrw.de/volltexte/2011/640/" class=yC6>Towards improving the understanding of image semantics by gaze-based tag-to-region assignments</a></h3><div class="gs_a">T Walber, <a href="/citations?user=LFTRuMEAAAAJ&amp;hl=en&amp;oi=sra">A Scherp</a>, <a href="/citations?user=QvpcUn8AAAAJ&amp;hl=en&amp;oi=sra">S Staab</a> - 2007 - kola.opus.hbz-nrw.de</div><div class="gs_rs">Abstract Eye-trackers have been used in the past to identify visual foci in images, find task-<br>related image regions, or localize affective regions in images. However, they have not been <br>used for identifying specific objects in images. In this paper, we investigate whether it is <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15735015944495490409&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 1</a> <a href="/scholar?q=related:aeXCQqIBXtoJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15735015944495490409&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'aeXCQqIBXtoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/harish-ISM2011.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6123364" class=yC8>Affective video summarization and story board generation using pupillary dilation and eye gaze</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, K Yadati, M Kankanhalli&hellip; - Multimedia (ISM), 2011  &hellip;, 2011 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract We propose a semi-automated, eye-gaze based method for affective analysis of <br>videos. Pupillary Dilation (PD) is introduced as a valuable behavioural signal for <br>assessment of subject arousal and engagement. We use PD information for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7986207911324922454&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 1</a> <a href="/scholar?q=related:VrLvWMi11G4J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7986207911324922454&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'VrLvWMi11G4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/B4115452361JX378.pdf" class=yCA>Identifying Objects in Images from Analyzing the Users&#39; Gaze Movements for Provided Tags</a></h3><div class="gs_a">T Walber, <a href="/citations?user=LFTRuMEAAAAJ&amp;hl=en&amp;oi=sra">A Scherp</a>, <a href="/citations?user=QvpcUn8AAAAJ&amp;hl=en&amp;oi=sra">S Staab</a> - Advances in Multimedia Modeling, 2012 - Springer</div><div class="gs_rs">Assuming that eye tracking will be a common input device in the near future in notebooks <br>and mobile devices like iPads, it is possible to implicitly gain information about images and <br>image regions from these users&#39; gaze movements. In this paper, we investigate the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15712919633136089155&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 2</a> <a href="/scholar?q=related:Q3hW3SeBD9oJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15712919633136089155&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'Q3hW3SeBD9oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072305" class=yCB>Can computers learn from humans to see better?: inferring scene semantics from viewers&#39; eye movements</a></h3><div class="gs_a"><a href="/citations?user=mUvcmRsAAAAJ&amp;hl=en&amp;oi=sra">R Subramanian</a>, <a href="/citations?user=vOHjMGcAAAAJ&amp;hl=en&amp;oi=sra">V Yanulevskaya</a>, <a href="/citations?user=fOmROdkAAAAJ&amp;hl=en&amp;oi=sra">N Sebe</a> - Proceedings of the 19th  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract This paper describes an attempt to bridge the semantic gap between computer <br>vision and scene understanding employing eye movements. Even as computer vision <br>algorithms can efficiently detect scene objects, discovering semantic relationships <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=1058432919547193662&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 1</a> <a href="/scholar?q=related:Pqkw7CFPsA4J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'Pqkw7CFPsA4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://www.comp.nus.edu.sg/~harishk/homepage_material/research_description.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www.comp.nus.edu.sg/~harishk/homepage_material/research_description.pdf" class=yCC>Brief summary of work done during PhD</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a> - comp.nus.edu.sg</div><div class="gs_rs">The focus of my PhD thesis has been to get a better understanding of visual perception and <br>attention as people interact with digital images and video. My first problem was on finding <br>how low level global and local information in images influence category discrimination <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:wlNrPDqYTssJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'wlNrPDqYTssJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md7', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md7" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:wlNrPDqYTssJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/harish-mm11.pdf" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072406" class=yCE>Eye-tracking methodology and applications to images and video</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a>, M Kankanhalli - Proceedings of the 19th ACM international  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Our tutorial introduces eye-tracking as an exciting, non-intrusive method of <br>capturing user attention during human interaction with digital images and videos. We <br>believe eye-gaze can play a valuable role in understanding and processing (a) huge <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:9cC-iDePvZoJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11150225721119391989&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'9cC-iDePvZoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396504" class=yC10>Making use of eye tracking information in image collection creation and region annotation</a></h3><div class="gs_a">T Walber - Proceedings of the 20th ACM international conference  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract The goal of this work is to implicitly gain information about images from human eye <br>movements and to use this information to improve the handling of images. Users&#39; points of <br>gaze are measured with an eye tracker while they are viewing or tagging images. By <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'ogigkp3z0ecJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://137.132.14.55/handle/10635/33304" class=yC11>Human Visual Perception, study and applications to understanding Images and Videos</a></h3><div class="gs_a"><a href="/citations?user=Cja9MMgAAAAJ&amp;hl=en&amp;oi=sra">H Katti</a> - 2011 - 137.132.14.55</div><div class="gs_rs">Assessing whether a photograph is interesting, or spotting people in conversation or <br>important objects in an images and videos, are visual tasks that we humans do effortlessly <br>and in a robust manner. In this thesis I first explore and quantify how humans distinguish <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:jna1s1LyMH0J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9020976490639357582&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'jna1s1LyMH0J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:jna1s1LyMH0J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">Cached</a></div>  </div>  </div></div></div>
