Total results = 28
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://vireo.cs.cityu.edu.hk/papers/beyond%20search%20event%20driven%20summarization%20for%20web%20videos_acmtmm10.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2043613" class=yC0>Beyond search: Event-driven summarization for web videos</a></h3><div class="gs_a"><a href="/citations?user=-ReoUxUAAAAJ&amp;hl=en&amp;oi=sra">R Hong</a>, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, HK Tan, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - ACM Transactions on  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract The explosive growth of Web videos brings out the challenge of how to efficiently <br>browse hundreds or even thousands of videos at a glance. Given an event-driven query, <br>social media Web sites usually return a large number of videos that are diverse and noisy <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4403112343352155411&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 13</a> <a href="/scholar?q=related:E52k-1j_Gj0J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=4403112343352155411&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'E52k-1j_Gj0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.5646&amp;rep=rep1&amp;type=pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072330" class=yC2>Personalizing automated image annotation using cross-entropy</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a>, E Gavves, <a href="/citations?user=0uKdbscAAAAJ&amp;hl=en&amp;oi=sra">CGM Snoek</a>, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>&hellip; - Proceedings of the 19th &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract Annotating the increasing amounts of user-contributed images in a personalized <br>manner is in great demand. However, this demand is largely ignored by the mainstream of <br>automated image annotation research. In this paper we aim for personalizing automated <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15388403704339045842&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 3</a> <a href="/scholar?q=related:0h1vSaOXjtUJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15388403704339045842&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 8 versions</a> <a onclick="return gs_ocit(event,'0h1vSaOXjtUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://research.microsoft.com/en-us/people/xjwang/ieeep_arista.pdf" class=yC5><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from microsoft.com</span><span class="gs_ggsS">microsoft.com <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6210348" class=yC4>Duplicate-Search-Based Image Annotation Using Web-Scale Data</a></h3><div class="gs_a">XJ Wang, <a href="/citations?user=fIlGZToAAAAJ&amp;hl=en&amp;oi=sra">L Zhang</a>, WY Ma - Proceedings of the IEEE, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Easy photo-taking and photo-sharing today make image an increasingly important <br>type of media in people&#39;s everyday life, which arouses a growing demand for a practical <br>image understanding technique. Traditional computer vision or machine learning methods <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3720039134173134828&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 2</a> <a href="/scholar?q=related:7P_wBuI7oDMJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=3720039134173134828&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'7P_wBuI7oDMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://people.cs.clemson.edu/~jzwang/1301863/mm2012/p499-lu.pdf" class=yC7><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2393418" class=yC6>Image annotation by semantic sparse recoding of visual content</a></h3><div class="gs_a"><a href="/citations?user=OUXS8doAAAAJ&amp;hl=en&amp;oi=sra">Z Lu</a>, Y Peng - Proceedings of the 20th ACM international conference  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract This paper presents a new semantic sparse recoding method to generate more <br>descriptive and robust representation of visual content for image annotation. Although the <br>visual bag-of-words (BOW) representation has been reported to achieve promising results <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3967404799588172502&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 1</a> <a onclick="return gs_ocit(event,'1gKiH6UNDzcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://www.cs.clemson.edu/~jzwang/1201863/mm2011/p353-wang.pdf" class=yC9><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2072345" class=yC8>Retrieval-based face annotation by weak label regularized local coordinate coding</a></h3><div class="gs_a">D Wang, <a href="/citations?user=JoLjflYAAAAJ&amp;hl=en&amp;oi=sra">SCH Hoi</a>, Y He, <a href="/citations?user=SC-WmzwAAAAJ&amp;hl=en&amp;oi=sra">J Zhu</a> - Proceedings of the 19th ACM  &hellip;, 2011 - dl.acm.org</div><div class="gs_rs">Abstract This paper investigates a retrieval-based annotation paradigm of mining web facial <br>images for automated face annotation. In general, there are two key challenges for such an <br>annotation paradigm. The first challenge is how to efficiently retrieve a short list of most <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=13198758977507442116&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 3</a> <a href="/scholar?q=related:xD1UKlxpK7cJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13198758977507442116&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'xD1UKlxpK7cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB5" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW5"><a href="http://arxiv.org/pdf/1107.2859" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from arxiv.org</span><span class="gs_ggsS">arxiv.org <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S016516841200148X" class=yCA>Label-specific training set construction from web resource for image annotation</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>, C Zhao, TS Chua, R Jain - Signal Processing, 2012 - Elsevier</div><div class="gs_rs">Abstract Recently many research efforts have been devoted to image annotation by <br>leveraging on the associated tags/keywords of web images as training labels. A key issue to <br>resolve is the relatively low accuracy of the tags. In this paper, we propose a novel semi-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=15877522534659742940&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 1</a> <a href="/scholar?q=related:3FynX59KWNwJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15877522534659742940&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'3FynX59KWNwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://vireo.cs.cityu.edu.hk/papers/tmm12-zhushiai.pdf" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from cityu.edu.hk</span><span class="gs_ggsS">cityu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6166364" class=yCC>Sampling and Ontologically Pooling Web Images for Visual Concept Learning</a></h3><div class="gs_a">S Zhu, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a>, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a> - Multimedia, IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Sufficient training examples are essential for effective learning of semantic visual <br>concepts. In practice, however, acquiring noise-free training examples has always been <br>expensive. Recently the rapid popularization of social media websites, such as Flickr, has <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12515950008825682404&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 2</a> <a href="/scholar?q=related:5BmafjiWsa0J:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12515950008825682404&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'5BmafjiWsa0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231211006849" class=yCE>Scale invariant image matching using triplewise constraint and weighted voting</a></h3><div class="gs_a">Y Pang, M Shang, Y Yuan, J Pan - Neurocomputing, 2011 - Elsevier</div><div class="gs_rs">Due to limited computational resource, image matching on mobile phone places great <br>demand on efficiency and scale invariant. Though spectral matching (SM) with pairwisely <br>geometric constraints is widely used in matching, it is not efficient and scale invariant for <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=40556142128859267&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 1</a> <a href="/scholar?q=related:gwgNrZgVkAAJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=40556142128859267&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'gwgNrZgVkAAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.cis.pku.edu.cn/faculty/vision/zlin/Publications/2012-CVPR-NNLRS.pdf" class=yC10><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from pku.edu.cn</span><span class="gs_ggsS">pku.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6247944" class=yCF>Non-negative low rank and sparse graph for semi-supervised learning</a></h3><div class="gs_a">L Zhuang, H Gao, <a href="/citations?user=TanjFwoAAAAJ&amp;hl=en&amp;oi=sra">Z Lin</a>, <a href="/citations?user=XqLiBQMAAAAJ&amp;hl=en&amp;oi=sra">Y Ma</a>&hellip; - Computer Vision and  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Constructing a good graph to represent data structures is critical for many important <br>machine learning tasks such as clustering and classification. This paper proposes a novel <br>non-negative low-rank and sparse (NNLRS) graph for semi-supervised learning. The <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17036247048808512354&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 1</a> <a href="/scholar?q=related:YjsH0m3obOwJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=17036247048808512354&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'YjsH0m3obOwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="http://137.132.145.151/lms/sites/default/files/publication-attachments/ZhaZJ_J03_Semantic-Gap-Oriented%20Active%20Learning%20for%20Multilable%20Image%20Annotation.pdf" class=yC12><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from 137.132.145.151</span><span class="gs_ggsS">137.132.145.151 <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6111295" class=yC11>Semantic-Gap-Oriented Active Learning for Multilabel Image Annotation</a></h3><div class="gs_a"><a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, ZJ Zha, D Tao, TS Chua - Image Processing, IEEE  &hellip;, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract User interaction is an effective way to handle the semantic gap problem in image <br>annotation. To minimize user effort in the interactions, many active learning methods were <br>proposed. These methods treat the semantic concepts individually or correlatively. <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12951969610158265254&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 5</a> <a href="/scholar?q=related:plMj67yjvrMJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12951969610158265254&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'plMj67yjvrMJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212009022" class=yC13>Kai Jiang, Huagang Yin, Peng Wang, Nenghai Yu</a></h3><div class="gs_a"><a href="/citations?user=nz-Li8kAAAAJ&amp;hl=en&amp;oi=sra">K Jiang</a>, H Yin, P Wang, N Yu - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract This paper proposed a method that fully exploits contextual information of geo-<br>tagged web photos to recommend tourism attractions to a user according to his personal <br>interest and current time and location. The proposed method first detects tourism <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'47hWBsoEwdcJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:389"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2382380" class=yC14>Tag ranking by propagating relevance over tag and image graphs</a></h3><div class="gs_a">M Li, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a>, H Li, C Zhao - &hellip;  of the 4th International Conference on  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In this paper, we explore the problem of tag ranking by propagating relevance over <br>community-contributed images and their associated tags. To rank the tags more accurately, <br>we propose a novel tag ranking scheme through a two-stage graph-based relevance <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'edO4DW-1rSsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:388"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6335467" class=yC15>Sparsity Induced Similarity Measure and Its Applications</a></h3><div class="gs_a">H Cheng, Z Liu, L Hou, J Yang - ieeexplore.ieee.org</div><div class="gs_rs">Abstract The structures of feature vectors based semi-supervised/supervised learning has <br>gained considerable interests in the past several years thanks to its effectiveness for better <br>object modeling and classication. In many machine learning and computer vision tasks, a <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'uCpLO_te7KYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:387"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212001609" class=yC16>Collaborative visual modeling for automatic image annotation via sparse model coding</a></h3><div class="gs_a">M Wang, F Li, <a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a> - Neurocomputing, 2012 - Elsevier</div><div class="gs_rs">Building visual models provides an important way to detect visual concepts from images. <br>However, due to the problems of visual diversity and uncertainty, the estimation based on <br>these models is not satisfactory. The visual relatedness among visual models is ignored <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:64sTPjJAVpEJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=10472628568030677995&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'64sTPjJAVpEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:386"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2382688" class=yC17>Automatic annotation of tagged content using predefined semantic concepts</a></h3><div class="gs_a">MG Manzato, <a href="/citations?user=6lc68RgAAAAJ&amp;hl=en&amp;oi=sra">R Goularte</a> - Proceedings of the 18th Brazilian symposium  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract User tags are an important source of information that can be used to gather <br>semantic data about the content, reducing the semantic gap and the restrictive domain of <br>automatic indexing approaches. In this paper, we propose an automatic technique for <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'3KgeQqL2a44J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:385"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S092523121200906X" class=yC18>Hybrid Image Summarization by Hypergraph Partition</a></h3><div class="gs_a">M Li, C Zhao, <a href="/citations?user=ByBLlEwAAAAJ&amp;hl=en&amp;oi=sra">J Tang</a> - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract The objectiveof hybrid image summarization is selecting a few visual exemplars <br>and semantic exemplars of a large-scale image collection and organizing them to represent <br>the collection. In this paper, we present a framework for hybrid image summarization in <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'ldtgTpHIVcYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:384"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6175141" class=yC19>A Generic Framework for Video Annotation via Semi-Supervised Learning</a></h3><div class="gs_a"><a href="/citations?user=9sCGe-gAAAAJ&amp;hl=en&amp;oi=sra">T Zhang</a>, C Xu, G Zhu, S Liu&hellip; - &hellip; , IEEE Transactions on, 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Learning-based video annotation is essential for video analysis and understanding, <br>and many various approaches have been proposed to avoid the intensive labor costs of <br>purely manual annotation. However, there lacks a generic framework due to several <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=5397141813445007384&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 1</a> <a href="/scholar?q=related:GBjMUOJ_5koJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=5397141813445007384&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'GBjMUOJ_5koJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:383"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6362232" class=yC1A>Beyond Text QA: Multimedia Answer Generation by Harvesting Web Information</a></h3><div class="gs_a">L Nie, M Wang, Y Gao, ZJ Zha, TS Chua - 2012 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Community question answering (cQA) services have gained popularity over the <br>past years. It not only allows community members to post and answer questions but also <br>enables general users to seek information from a comprehensive set of well-answered <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'RkwabUTK41cJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:382"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB18" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW18"><a href="http://people.cs.clemson.edu/~jzwang/1301863/mm2012/p479-qi.pdf" class=yC1C><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from clemson.edu</span><span class="gs_ggsS">clemson.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2393347.2393416" class=yC1B>Multi-view learning from imperfect tagging</a></h3><div class="gs_a">Z Qi, M Yang, ZM Zhang, Z Zhang - Proceedings of the 20th ACM  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract In many real-world applications, tagging is imperfect: incomplete, inconsistent, and <br>error-prone. Solutions to this problem will generate societal and technical impacts. In this <br>paper, we investigate this arguably new problem: learning from imperfect tagging. We <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'RwEYdGGu9KUJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:381"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2333120" class=yC1D>Assistive tagging: A survey of multimedia tagging with human-computer joint exploration</a></h3><div class="gs_a"><a href="/citations?user=rHagaaIAAAAJ&amp;hl=en&amp;oi=sra">M Wang</a>, <a href="/citations?user=V9W87PYAAAAJ&amp;hl=en&amp;oi=sra">B Ni</a>, <a href="/citations?user=6G-l4o0AAAAJ&amp;hl=en&amp;oi=sra">XS Hua</a>, TS Chua - ACM Computing Surveys (CSUR), 2012 - dl.acm.org</div><div class="gs_rs">Abstract Along with the explosive growth of multimedia data, automatic multimedia tagging <br>has attracted great interest of various research communities, such as computer vision, <br>multimedia, and information retrieval. However, despite the great progress achieved in the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9840991510067465628&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=28">Cited by 4</a> <a href="/scholar?q=related:nM00e6M5kogJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'nM00e6M5kogJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:380"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S1047320312001253" class=yC1E>Fast Human Action Classification and VOI Localization with Enhanced Sparse Coding</a></h3><div class="gs_a">S Lu, J Zhang, Z Wang, DD Feng - Journal of Visual Communication and  &hellip;, 2012 - Elsevier</div><div class="gs_rs">Abstract Sparse coding which encodes the natural visual signal into a sparse space for <br>visual codebook generation and feature quantization, has been successfully utilized for <br>many image classification applications. However, it has been seldom explored for many <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:_vDsnR6Ui5oJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'_vDsnR6Ui5oJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:379"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396761.2398444" class=yC1F>A unified learning framework for auto face annotation by mining web facial images</a></h3><div class="gs_a">D Wang, <a href="/citations?user=JoLjflYAAAAJ&amp;hl=en&amp;oi=sra">SCH Hoi</a>, Y He - Proceedings of the 21st ACM international  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Auto face annotation plays an important role in many real-world multimedia <br>information and knowledge management systems. Recently there is a surge of research <br>interests in mining weakly-labeled facial images on the internet to tackle this long-<b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'49q5qf5t0L8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:378"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2393422" class=yC20>Correlated attribute transfer with multi-task graph-guided fusion</a></h3><div class="gs_a"><a href="/citations?user=t4283loAAAAJ&amp;hl=en&amp;oi=sra">Y Han</a>, F Wu, X Lu, Q Tian, Y Zhuang&hellip; - Proceedings of the 20th  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Due to the describable or human-nameable nature of visual attributes, the attribute-<br>based methods have been receiving much attentions in recent years in many applications. <br>The advantages of the utilization of visual attributes are that they can be composed to <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'NanJsbWmwQ0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:377"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/ft_gateway.cfm?id=2398545&amp;ftid=1313392&amp;dwn=1" class=yC21>Mining Noisy Tagging from Multi-label Space</a></h3><div class="gs_a">Z Qi, M Yang, ZM Zhang, Z Zhang - 2012 - dl.acm.org</div><div class="gs_rs">ABSTRACT In this paper we study the problem of mining noisy tagging. Most of the existing <br>discriminative classification methods to this problem only consider one tag at a time as the <br>classification target, and completely ignore the rest of the given tags at the same time. In <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'9yyCylZMQz0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:376"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=2396346" class=yC22>Towards relevance and saliency ranking of image tags</a></h3><div class="gs_a">S Feng, C Lang, B Li - Proceedings of the 20th ACM international  &hellip;, 2012 - dl.acm.org</div><div class="gs_rs">Abstract Social image tag ranking has emerged as an important research topic recently due <br>to its potential application on web image search. This paper presents an adaptive all-season <br>tag ranking algorithm which can handle the images with and without distinct object (s) <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'kikuzFQC9noJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:375"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB25" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW25"><a href="http://dare.uva.nl/document/355661" class=yC24><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uva.nl</span><span class="gs_ggsS">uva.nl <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dare.uva.nl/en/record/410580" class=yC23>Content-based visual search learned from social media</a></h3><div class="gs_a"><a href="/citations?user=6m-ZQ1EAAAAJ&amp;hl=en&amp;oi=sra">X Li</a> - 2012 - dare.uva.nl</div><div class="gs_rs">Abstract In een wereld waarin de hoeveelheid digitale afbeeldingen alsmaar groeit is het <br>belangrijk te kunnen zoeken op basis van beeldinhoud. Xirong Li liet zich inspireren door <br>sociale media en onderzocht de waarde van beelden met social tags voor visueel zoeken. <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:xTQX0HIOAtIJ:scholar.google.com/&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15132673584198530245&amp;hl=en&amp;num=28&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'xTQX0HIOAtIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:374"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB26" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW26"><a href="http://homepage.fudan.edu.cn/xdzhou/files/2012/11/eccv2012-zhou.pdf" class=yC26><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from fudan.edu.cn</span><span class="gs_ggsS">fudan.edu.cn <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/F17328344142J823.pdf" class=yC25>Labeling images by integrating sparse multiple distance learning and semantic context modeling</a></h3><div class="gs_a">C Ji, <a href="/citations?user=QUrLihYAAAAJ&amp;hl=en&amp;oi=sra">X Zhou</a>, L Lin, W Yang - Computer VisionâECCV 2012, 2012 - Springer</div><div class="gs_rs">Recent progress on Automatic Image Annotation (AIA) is achieved by either exploiting low <br>level visual features or high level semantic context. Integrating these two paradigms to <br>further leverage the performance of AIA is promising. However, very few previous works <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'Qt3o2BUoBqoJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:373"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0925231212009034" class=yC27>A novel compound regularization and fast algorithm for compressive sensing deconvolution</a></h3><div class="gs_a"><a href="/citations?user=YGsIVRIAAAAJ&amp;hl=en&amp;oi=sra">L Xiao</a>, J Shao, L Huang, Z Wei - Neurocomputing, 2013 - Elsevier</div><div class="gs_rs">Abstract Compressive Sensing Deconvolution (CS-Deconvolution) is a new challenge <br>problem encountered in a wide variety of image processing fields. Since CS is more efficient <br>for sparse signals, in our scheme, the input image is firstly sparse represented by curvelet <b> ...</b></div><div class="gs_fl"><a onclick="return gs_ocit(event,'30GR5dyfJCkJ')" href="#" class="gs_nph">Cite</a></div></div></div>
