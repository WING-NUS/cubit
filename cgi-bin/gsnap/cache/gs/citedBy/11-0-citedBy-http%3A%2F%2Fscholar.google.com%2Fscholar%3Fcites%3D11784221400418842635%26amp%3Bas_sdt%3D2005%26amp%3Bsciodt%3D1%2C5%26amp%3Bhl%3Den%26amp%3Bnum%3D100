Total results = 11
<div class="gs_r" style="z-index:400"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0031320306000021" class=yC0>Attention-driven image interpretation with application to image retrieval</a></h3><div class="gs_a">H Fu, Z Chi, D Feng - Pattern Recognition, 2006 - Elsevier</div><div class="gs_rs">Visual attention, a selective procedure of human&#39;s early vision, plays a very important role for <br>humans to understand a scene by intuitively emphasizing some focused regions/objects. <br>Being aware of this, we propose an attention-driven image interpretation method that pops <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=943395202800791704&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 48</a> <a href="/scholar?q=related:mLC_SfCcFw0J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=943395202800791704&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'mLC_SfCcFw0J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://embio.yonsei.ac.kr/files/paper/Image%20Retrieval%20Model%20Based%20on%20Weighted%20Visual%20Features%20Determined%20by%20Relevance%20Feedback.pdf" class=yC2><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from yonsei.ac.kr</span><span class="gs_ggsS">yonsei.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.sciencedirect.com/science/article/pii/S0020025508002478" class=yC1>Image retrieval model based on weighted visual features determined by relevance feedback</a></h3><div class="gs_a">WC Kim, JY Song, SW Kim, <a href="/citations?user=wVcle04AAAAJ&amp;hl=en&amp;oi=sra">S Park</a> - Information Sciences, 2008 - Elsevier</div><div class="gs_rs">An accurate and rapid method is required to retrieve the overwhelming majority of digital <br>images. To date, image retrieval methods include content-based retrieval and keyword-<br>based retrieval, the former utilizing visual features such as color and brightness, and the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=16590892648551161726&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 13</a> <a href="/scholar?q=related:fh-IJ_awPuYJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=16590892648551161726&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'fh-IJ_awPuYJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://lms.comp.nus.edu.sg/papers/media/2006/acmmm06-art-liza.pdf" class=yC4><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1180752" class=yC3>Semi-supervised annotation of brushwork in paintings domain using serial combinations of multiple experts</a></h3><div class="gs_a">M Yelizaveta, C Tat-Seng, J Ramesh - Proceedings of the 14th annual  &hellip;, 2006 - dl.acm.org</div><div class="gs_rs">Abstract Many recent studies perform annotation of paintings based on brushwork. They <br>model the brushwork indirectly as part of annotation of high-level artistic concepts such as <br>artist name using low-level texture features and supervised inference methods. In this <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=7971909497790602166&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 10</a> <a href="/scholar?q=related:thtfhXPpoW4J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=7971909497790602166&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 11 versions</a> <a onclick="return gs_ocit(event,'thtfhXPpoW4J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://pixel.otago.ac.nz/ipapers/79.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from otago.ac.nz</span><span class="gs_ggsS">otago.ac.nz <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://pixel.otago.ac.nz/ipapers/79.pdf" class=yC5>Video Scene Retrieval Based on The Layerization of Images and The Matching of Layer-Trees</a></h3><div class="gs_a">N Nakamura, S Takano, K Niijima - Proceedings of IVCNZ, 2005 - pixel.otago.ac.nz</div><div class="gs_rs">Abstract A video consists of a large number of scenes, which include many interesting shots. <br>So, a retrieval system of searching many scenes for a desired shot is required. This paper <br>presents a video scene retrieval system using the layerization of images and the <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=9285417086541943415&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 3</a> <a href="/scholar?q=related:d9Ks2qRt3IAJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9285417086541943415&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'d9Ks2qRt3IAJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:d9Ks2qRt3IAJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://repository.lib.polyu.edu.hk/jspui/bitstream/10397/2661/2/b21167746_ir.pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from polyu.edu.hk</span><span class="gs_ggsS">polyu.edu.hk <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://repository.lib.polyu.edu.hk/jspui/handle/10397/2661" class=yC7>Attention-driven image interpretation, annotation and retrieval</a></h3><div class="gs_a">H Fu - 2007 - repository.lib.polyu.edu.hk</div><div class="gs_rs">This thesis presents novel attention-driven techniques for image interpretation, annotation <br>and retrieval. Four main contributions are reported in the thesis. They include:(1) an <br>attention-driven image interpretation method with application to image retrieval;(2) an <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=18005716306759656845&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 2</a> <a href="/scholar?q=related:jSXeJpUn4fkJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=18005716306759656845&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'jSXeJpUn4fkJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none">  <a href="/scholar?q=info:jSXeJpUn4fkJ:scholar.google.com/&amp;output=instlink&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5&amp;scillfp=15173639394264538055&amp;oi=llo" class="gs_md_li">Library Search</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1551975" class=yC9>ConVeS: a context verification framework for object recognition system</a></h3><div class="gs_a"><a href="/citations?user=CVo6274AAAAJ&amp;hl=en&amp;oi=sra">MHA Hasanat</a>, <a href="/citations?user=0nVlbNgAAAAJ&amp;hl=en&amp;oi=sra">D Ramachandram</a>&hellip; - Proceedings of the 2009  &hellip;, 2009 - dl.acm.org</div><div class="gs_rs">Abstract Context is a vital element in both biological as well as synthetic vision systems. It is <br>essential for deriving meaningful explanation of an image. Unfortunately, there is a lack of <br>consensus in the computer vision community on what context is and how it should be <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11503034396803014839&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 2</a> <a href="/scholar?q=related:tyCLlNb8op8J:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'tyCLlNb8op8J')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:394"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB6" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW6"><a href="http://scholarbank.nus.edu/bitstream/handle/10635/13418/Thesis_Marchenko_Yelizaveta_PhDDegree.pdf?sequence=1" class=yCB><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu</span><span class="gs_ggsS">nus.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://scholarbank.nus.edu/handle/10635/13418" class=yCA>Ontology-based annotation of paintings with artistic concepts</a></h3><div class="gs_a">M YELIZAVETA - 2007 - scholarbank.nus.edu</div><div class="gs_rs">In this thesis, we focus on the automatic annotation of paintings with various artistic <br>concepts. These concepts originate from the several domain ontologies. In our work we <br>combine such domain knowledge with trasductive inference and demonstrate that the use <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=77623979425578235&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 1</a> <a href="/scholar?q=related:-7y2F5nGEwEJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=77623979425578235&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'-7y2F5nGEwEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:393"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB7" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW7"><a href="http://ro.uow.edu.au/cgi/viewcontent.cgi?article=1535&amp;context=infopapers" class=yCD><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from uow.edu.au</span><span class="gs_ggsS">uow.edu.au <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4061152" class=yCC>Image content annotation based on visual features</a></h3><div class="gs_a">P Ogunbona, L Ye, J Wang - Multimedia, 2006. ISM&#39;06. Eighth  &hellip;, 2006 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract Automatic image content annotation techniques attempt to explore structural visual <br>features of images that describe image content and associate them with image semantics. In <br>this paper, two types of concept spaces, atomic concept and collective concept spaces, <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=6585903637928523612&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=11">Cited by 1</a> <a href="/scholar?q=related:XLeuD3fUZVsJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=6585903637928523612&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'XLeuD3fUZVsJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:392"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB8" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW8"><a href="http://www.scholarbank.nus.edu.sg/bitstream/handle/10635/14783/thesis_body%20(huamin).pdf?sequence=2" class=yCF><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.scholarbank.nus.edu.sg/handle/10635/14783" class=yCE>Auto-annotation of multimedia contents: Theory and application</a></h3><div class="gs_a">F HUAMIN - 2005 - scholarbank.nus.edu.sg</div><div class="gs_rs">In this thesis, we propose a learning-based framework for auto-annotation of multimedia <br>contents. The framework is open and is designed to incorporate different base learners, <br>including the single-view machine learning (traditional) and the bootstrapping approaches<b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:3raxSNrVELEJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=12758932877839808222&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'3raxSNrVELEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:391"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB9" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW9"><a href="https://scholarbank.nus.edu.sg/bitstream/handle/10635/15994/SHIRUI_PHDThesis_BayesianLearningofConceptOntologyforAutomaticImageAnnotation.pdf?sequence=1" class=yC11><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nus.edu.sg</span><span class="gs_ggsS">nus.edu.sg <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="https://scholarbank.nus.edu.sg/handle/10635/15994" class=yC10>Bayesian learning of concept ontology for automatic image annotation</a></h3><div class="gs_a">RUI SHI - 2007 - scholarbank.nus.edu.sg</div><div class="gs_rs">Automatic image annotation (AIA) has been a hot research topic in recent years since it can <br>be used to support concept-based image retrieval. In the field of AIA, characterizing image <br>concepts by mixture models is one of the most effective techniques. However, mixture <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:hevhtMNxNoEJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=9310754365002345349&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'hevhtMNxNoEJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:390"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB10" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW10"><a href="http://embio.yonsei.ac.kr/files/paper/dbpia0843896.pdf" class=yC13><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from yonsei.ac.kr</span><span class="gs_ggsS">yonsei.ac.kr <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://embio.yonsei.ac.kr/files/paper/dbpia0843896.pdf" class=yC12>ì í©ì± í¼ëë°±ì íµí´ ê²°ì ë ê°ì¤ì¹ë¥¼ ê°ëìê°ì  í¹ì±ì ê¸°ë°ì ë ì´ë¯¸ì§ ê²ì ëª¨ë¸</a></h3><div class="gs_a">ì¡ì§ìï¼ ê¹ì°ì² ï¼ ê¹ì¹ì°ï¼ ë°ìí - ì ë³´ê³¼ííë¼ë¬¸ì§: ë°ì´í &hellip;, 2007 - embio.yonsei.ac.kr</div><div class="gs_rs">ì ì½ ëì§í¸ ì´ë¯¸ì§ì ìì´ ì¦ê°í¨ì ë°ë¼ ìíë ì´ë¯¸ì§ë¥¼ ì ííê³  ë¹ ë¥´ê² ì°¾ì ì ìë <br>ë°©ë²ìíìì±ì´ ì¦ê°íê³  ìë¤. ì´ë¯¸ì§ ê²ì ë°©ë²ì¼ë¡ë ì´ë¯¸ì§ì ììì´ë ëªìê³¼ ê°ì <br>ìê°ì  í¹ì±ì ê²ì ì¡°ê±´ì¼ë¡ ì´ì©íë ë´ì© ê¸°ë° ê²ìê³¼ ì´ë¯¸ì§ë¥¼ ì¤ëªíë í¤ìëë¥¼ ê²ì <b> ...</b></div><div class="gs_fl"><a href="/scholar?q=related:4e5wtRbV29MJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=15266029655412502241&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5">All 2 versions</a> <a onclick="return gs_ocit(event,'4e5wtRbV29MJ')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md10', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md10" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:4e5wtRbV29MJ:scholar.google.com/&amp;hl=en&amp;num=11&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
