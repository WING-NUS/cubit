Total results = 6
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.47.5822&amp;rep=rep1&amp;type=pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.tandfonline.com/doi/abs/10.1080/09528139408953778" class=yC0>Infinitary self-reference in learning theory</a></h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a> - Journal of Experimental &amp; Theoretical Artificial  &hellip;, 1994 - Taylor &amp; Francis</div><div class="gs_rs">Abstract Kleene&#39;s second recursion theorem provides a means for transforming any program <br>p into a program e (p) which first creates a quiescent self-copy and then runs p on that self-<br>copy together with any externally given input. e (p), in effect, has complete (low level), self-<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=17298606135970394459&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 47</a> <a href="/scholar?q=related:W3Ho86D-EPAJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5A/3B/EN014014548.html?source=googlescholar" class="gs_nph" class=yC2>BL Direct</a> <a href="/scholar?cluster=17298606135970394459&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 6 versions</a> <a onclick="return gs_ocit(event,'W3Ho86D-EPAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/f074701617482876.pdf" class=yC3>Language learning from good examples</a></h3><div class="gs_a">S Lange, J Nessel, R Wiehagen - Algorithmic learning theory, 1994 - Springer</div><div class="gs_rs">We study learning of indexable families of recursive languages from good examples. We <br>show that this approach is considerably more powerful than learning from all examples and <br>point out reasons for this additional power. We present several characterizations of types <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=10151459107844676270&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 14</a> <a href="/scholar?q=related:rlr8GkY64YwJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/42/14/EN021431813.html?source=googlescholar" class="gs_nph" class=yC4>BL Direct</a> <a href="/scholar?cluster=10151459107844676270&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'rlr8GkY64YwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB2" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW2"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.2851&amp;rep=rep1&amp;type=pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/PXQ20Q54786440M4.pdf" class=yC5>Learning recursive languages from good examples</a></h3><div class="gs_a">S Lange, J Nessel, R Wiehagen - Annals of Mathematics and Artificial  &hellip;, 1998 - Springer</div><div class="gs_rs">Abstract We study learning of indexable families of recursive languages from good <br>examples. We show that this approach can be considerably more powerful than learning <br>from all examples and point out reasons for this additional power. We present several <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=3075477474035979069&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 13</a> <a href="/scholar?q=related:PVOh0ndKrioJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/3E/15/RN052079410.html?source=googlescholar" class="gs_nph" class=yC7>BL Direct</a> <a href="/scholar?cluster=3075477474035979069&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'PVOh0ndKrioJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/78867137810P8066.pdf" class=yC8>Learning with higher order additional information</a></h3><div class="gs_a">G Baliga, <a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a> - Algorithmic Learning Theory, 1994 - Springer</div><div class="gs_rs">[Go167] studied, among other things, algorithmic learning (in the limit) of deci- sion procedures <br>for languages given informants, ie, given enumerations of the characteristic functions of the <br>languages. [FW79] shows that learning power is increased if, in addition to the informants, <b> ...</b> </div><div class="gs_fl"><a href="/scholar?cites=9252420788343492984&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 12</a> <a href="/scholar?q=related:eFGnga8zZ4AJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/5B/0C/EN021431540.html?source=googlescholar" class="gs_nph" class=yC9>BL Direct</a> <a href="/scholar?cluster=9252420788343492984&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 5 versions</a> <a onclick="return gs_ocit(event,'eFGnga8zZ4AJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ri"><h3 class="gs_rt"><a href="http://www.springerlink.com/index/P3T027KM6X24LN74.pdf" class=yCA>Learning from good examples</a></h3><div class="gs_a"><a href="/citations?user=zDbJmlEAAAAJ&amp;hl=en&amp;oi=sra">R Freivalds</a>, E Kinber, R Wiehagen - Algorithmic Learning for Knowledge- &hellip;, 1995 - Springer</div><div class="gs_rs">The usual information in inductive inference for the purposes of learning an unknown <br>recursive function f is the set of all input/output examples (n, f (n)), nâ â. In contrast to this <br>approach we show that it is considerably more powerful to work with finite sets of âgoodâ <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=4656239447123935138&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 6</a> <a href="/scholar?q=related:or_7ehtJnkAJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="http://direct.bl.uk/research/2A/1D/EN030582627.html?source=googlescholar" class="gs_nph" class=yCB>BL Direct</a> <a href="/scholar?cluster=4656239447123935138&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'or_7ehtJnkAJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctu"><span class="gs_ct1">[CITATION]</span><span class="gs_ct2">[C]</span></span> Department of Computer and Information Sciences University of Delaware Newark, DE 19716, USA</h3><div class="gs_a"><a href="/citations?user=lMV_QwEAAAAJ&amp;hl=en&amp;oi=sra">J Case</a></div><div class="gs_fl"><a href="/scholar?q=related:RLkDr-qUHr8J:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13771608446225070404&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'RLkDr-qUHr8J')" href="#" class="gs_nph">Cite</a></div></div></div>
