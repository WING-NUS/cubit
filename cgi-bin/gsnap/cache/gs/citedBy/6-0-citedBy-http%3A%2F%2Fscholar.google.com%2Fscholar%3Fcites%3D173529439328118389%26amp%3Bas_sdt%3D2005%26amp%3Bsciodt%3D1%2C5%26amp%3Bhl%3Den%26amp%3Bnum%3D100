Total results = 6
<div class="gs_r" style="z-index:400"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB0" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW0"><a href="http://www.yugangjiang.info/publication/mm08_yjiang.pdf" class=yC1><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from yugangjiang.info</span><span class="gs_ggsS">yugangjiang.info <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://dl.acm.org/citation.cfm?id=1459392" class=yC0>Video event detection using motion relativity and visual relatedness</a></h3><div class="gs_a">F Wang, <a href="/citations?user=f3_FP8AAAAAJ&amp;hl=en&amp;oi=sra">YG Jiang</a>, <a href="/citations?user=jk5DWVMAAAAJ&amp;hl=en&amp;oi=sra">CW Ngo</a> - Proceedings of the 16th ACM international  &hellip;, 2008 - dl.acm.org</div><div class="gs_rs">Abstract Event detection plays an essential role in video content analysis. However, the <br>existing features are still weak in event detection because: i) most features just capture what <br>is involved in an event or how the event evolves separately, and thus cannot completely <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=8988837647516082508&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 52</a> <a href="/scholar?q=related:TKHaKzfEvnwJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=8988837647516082508&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 9 versions</a> <a onclick="return gs_ocit(event,'TKHaKzfEvnwJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:399"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB1" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW1"><a href="http://doras.dcu.ie/620/3/submitted_version.pdf" class=yC3><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from dcu.ie</span><span class="gs_ggsS">dcu.ie <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><a href="http://onlinelibrary.wiley.com/doi/10.1002/ima.20150/abstract" class=yC2>Contentâbased video retrieval: Three example systems from TRECVid</a></h3><div class="gs_a"><a href="/citations?user=o7xnW2MAAAAJ&amp;hl=en&amp;oi=sra">AF Smeaton</a>, P Wilkins, <a href="/citations?user=pdu8f3sAAAAJ&amp;hl=en&amp;oi=sra">M Worring</a>&hellip; - &hellip;  Journal of Imaging  &hellip;, 2008 - Wiley Online Library</div><div class="gs_rs">Abstract The growth in available online video material over the Internet is generally <br>combined with user-assigned tags or content description, which is the mechanism by which <br>we then access such video. However, user-assigned tags have limitations for retrieval and <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11920786069704697960&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 14</a> <a href="/scholar?q=related:aGgseMUjb6UJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11920786069704697960&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 10 versions</a> <a onclick="return gs_ocit(event,'aGgseMUjb6UJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:398"><div class="gs_ri"><h3 class="gs_rt"><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5357647" class=yC4>A novel framework for semantic-based video retrieval</a></h3><div class="gs_a">X Nan, Z Zhao, A Cai, X Xie - Intelligent Computing and  &hellip;, 2009 - ieeexplore.ieee.org</div><div class="gs_rs">Abstract In this paper, a novel framework for semantic-based video retrieval is proposed. 15 <br>low-level visual features on different levels are extracted and a supervised SVM classifier is <br>trained for each feature. We have explored early fusion schemes between SIFT and SURF<b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=154928328354264270&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 6</a> <a href="/scholar?q=related:znQzaX5qJgIJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=154928328354264270&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 3 versions</a> <a onclick="return gs_ocit(event,'znQzaX5qJgIJ')" href="#" class="gs_nph">Cite</a></div></div></div>
<div class="gs_r" style="z-index:397"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB3" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW3"><a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC6><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from nist.gov</span><span class="gs_ggsS">nist.gov <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://www-nlpir.nist.gov/projects/tvpubs/tv10.papers/nus-lms.pdf" class=yC5>TRECVID 2010 Known-item Search by NUS</a></h3><div class="gs_a">XY Chen, J Yuan, L Nie, ZJ Zha, <a href="/citations?user=DNuiPHwAAAAJ&amp;hl=en&amp;oi=sra">S Yan</a>&hellip; - TRECVID  &hellip;, 2010 - www-nlpir.nist.gov</div><div class="gs_rs">Abstract. This paper describes our system for auto search and interactive search in the <br>known-item search (KIS) task in TRECVID 2010. KIS task aims to find an unique video <br>answer for each text query. The shift from traditional video search has prompted a series <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=12664714192218118309&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 3</a> <a href="/scholar?q=related:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a onclick="return gs_ocit(event,'pVgCEXUawq8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md3', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md3" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:pVgCEXUawq8J:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:396"><div class="gs_ggs gs_fl"><button type="button" id="gs_ggsB4" class="gs_btnFI gs_in_ib gs_btn_half"><span class="gs_wr"><span class="gs_bg"></span><span class="gs_lbl"></span><span class="gs_ico"></span></span></button><div class="gs_md_wp" id="gs_ggsW4"><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.189.4167&amp;rep=rep1&amp;type=pdf" class=yC8><span class="gs_ggsL"><span class=gs_ctg2>[PDF]</span> from psu.edu</span><span class="gs_ggsS">psu.edu <span class=gs_ctg2>[PDF]</span></span></a></div></div><div class="gs_ri"><h3 class="gs_rt"><span class="gs_ctc"><span class="gs_ct1">[PDF]</span><span class="gs_ct2">[PDF]</span></span> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.189.4167&amp;rep=rep1&amp;type=pdf" class=yC7>BUPT-MCPRL at TRECVID 2009</a></h3><div class="gs_a">Z Zhao, Y Zhao, Z Gao, X Nan, M Mei, H Zhang&hellip; - TREC Video Retrieval  &hellip;, 2009 - Citeseer</div><div class="gs_rs">Page 1. BUPT-MCPRL at TRECVID 2009 * Zhicheng Zhao, Yanyun Zhao, Zan Gao,<br>Xiaoming Nan, Mei Mei, Hui Zhang, Heng Chen, Xu Peng, Yuanbo Chen, Junfang Guo,<br>Anni Cai Multimedia Communication and Pattern Recognition <b>...</b> </div><div class="gs_fl"><a href="/scholar?cites=13808817665184838258&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 5</a> <a href="/scholar?q=related:cjJ7FYHGor8J:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=13808817665184838258&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 4 versions</a> <a onclick="return gs_ocit(event,'cjJ7FYHGor8J')" href="#" class="gs_nph">Cite</a> <div class="gs_rm_dd gs_nph"><a href="#" class="gs_rm_l" onclick="return gs_md_opn('gs_rm_md4', event)">More<span class="gs_ico"></span></a><div id="gs_rm_md4" class="gs_md_wn" style="display:none"><a href="http://scholar.googleusercontent.com/scholar?q=cache:cjJ7FYHGor8J:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5" class="gs_md_li">View as HTML</a></div>  </div>  </div></div></div>
<div class="gs_r" style="z-index:395"><div class="gs_ri"><h3 class="gs_rt"><a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=752197" class=yC9>Detection of illegal transfer of videos over the Internet</a></h3><div class="gs_a">L Chaisorn, J Sainui&hellip; - Visual  &hellip;, 2010 - proceedings.spiedigitallibrary.org</div><div class="gs_rs">abstract In this paper, a method for detecting infringements or modifications of a video in real-<br>time is proposed. The method first segments a video stream into shots, after which it extracts <br>some reference frames as keyframes. This process is performed employing a Singular <b> ...</b></div><div class="gs_fl"><a href="/scholar?cites=11038300084856331798&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en&amp;num=6">Cited by 1</a> <a href="/scholar?q=related:Fp4GmnHrL5kJ:scholar.google.com/&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">Related articles</a> <a href="/scholar?cluster=11038300084856331798&amp;hl=en&amp;num=6&amp;as_sdt=0,5&amp;sciodt=0,5">All 7 versions</a> <a onclick="return gs_ocit(event,'Fp4GmnHrL5kJ')" href="#" class="gs_nph">Cite</a></div></div></div>
